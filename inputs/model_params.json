{
  "model_name": "Llama3_70B_Instruct",
  "vocab_size": 128256,
  "hidden_size": 8192,
  "intermediate_size": 28672,
  "num_hidden_layers": 80,
  "num_attention_heads": 64,
  "num_key_value_heads": 8,
  "max_position_embeddings": 8192,
  "rms_norm_eps": 1e-05,
  "rope_theta": 500000.0,
  "attention_bias": false,
  "hidden_act": "silu",
  "tie_word_embeddings": false,
  "model_type": "llama",
  "architectures": ["LlamaForCausalLM"],
  "quantization": {
    "weights": "fp16",
    "activations": "fp16",
    "kv_cache": "fp16"
  },
  "moe": {
    "num_experts": 1,
    "top_k": 1,
    "shared_expert": false
  },
  "memory_estimates": {
    "fp16_model_weights_gb": 140,
    "fp16_kv_cache_per_token_kb": 1.0,
    "fp16_activation_per_token_kb": 0.5
  }
}