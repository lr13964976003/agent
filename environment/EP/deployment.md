```
## **Environment Condition**

* **Model**: 4-layer Mixture-of-Experts (MoE), Each layer has 64 experts
* **Precision**: FP8
* **Batch size**: Each batch consists of 128 sequences.
* **Sequence Length**: 128 tokens per sequence.
* **Token Dimension**: The dimension of each token is 1024.
* **Dimension of MHA**: The number of heads is 16 and the dimension of each heads is 64
* **Hidden size of MOE**: The hidden is of MOE is 2048



---


## **Model Configuration**

* **Model**: 16-layer Mixture-of-Experts (MoE), Each layer has 64 experts
* **Precision**: FP8
* **Batch size**: Each batch consists of 128 sequences.
* **Sequence Length**: 128 tokens per sequence.
* **Token Dimension**: The dimension of each token is 1024.
* **Dimension of MHA**: The number of heads is 16 and the dimension of each heads is 64
* **Hidden size of MOE**: The hidden is of MOE is 2048


```
