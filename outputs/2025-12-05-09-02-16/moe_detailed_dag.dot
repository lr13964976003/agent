// 30B MoE Model Deployment DAG - EP16-TP8-PP4-DP4
digraph {
	bgcolor=white rankdir=TB splines=ortho
	node [fillcolor=lightblue shape=rectangle style=filled]
	node [fillcolor=lightgreen shape=ellipse style=filled]
	node [fillcolor=lightyellow shape=parallelogram style=filled]
	subgraph cluster_input {
		fillcolor=lightgray label="Input Layer" style=rounded
		input [label="Input Embedding
GPU: [0-511]
Input: [batch_size=128, seq_len=1024, hidden=1024]
Output: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
	}
	dp_split [label="DP Split
GPU: [0-511]
Input: [batch_size=128, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
	input -> dp_split
	subgraph cluster_pp0 {
		fillcolor=lightcoral label="Pipeline Stage 0: Layers 0-3 (GPUs 0-127)" style=rounded
		pp0_layer0_attn [label="Layer 0: Attention
GPU: [0-31]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp0_layer0_tp_comm [label="TP All-Reduce
GPU: [0-7], [8-15], [16-23], [24-31]
Input: [batch_size=32, seq_len=1024, hidden=128]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp0_layer0_moe [label="Layer 0: MoE Routing
GPU: [0-31]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
		pp0_layer0_gate [label="Expert Gate Selection
GPU: [0-31]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: routing decisions" fillcolor=lightyellow shape=parallelogram style=dashed]
		pp0_layer0_expert0 [label="Expert 0
GPU: [0-7]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp0_layer0_expert0_tp [label="TP All-Reduce
GPU: [0-7]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp0_layer0_expert1 [label="Expert 1
GPU: [8-15]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp0_layer0_expert1_tp [label="TP All-Reduce
GPU: [8-15]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp0_layer0_expert2 [label="Expert 2
GPU: [16-23]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp0_layer0_expert2_tp [label="TP All-Reduce
GPU: [16-23]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp0_layer0_expert3 [label="Expert 3
GPU: [24-31]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp0_layer0_expert3_tp [label="TP All-Reduce
GPU: [24-31]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp0_layer0_agg [label="Expert Aggregation
GPU: [0-31]
Input: [batch_size=~2, seq_len=1024, hidden=1024] × 4
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
		pp0_layer0_ep_comm [label="EP All-to-All
GPU: [0-31]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
	}
	dp_split -> pp0_layer0_attn
	pp0_layer0_attn -> pp0_layer0_tp_comm
	pp0_layer0_tp_comm -> pp0_layer0_moe
	pp0_layer0_moe -> pp0_layer0_gate
	pp0_layer0_gate -> pp0_layer0_expert0 [style=dashed]
	pp0_layer0_expert0 -> pp0_layer0_expert0_tp
	pp0_layer0_expert0_tp -> pp0_layer0_agg
	pp0_layer0_gate -> pp0_layer0_expert1 [style=dashed]
	pp0_layer0_expert1 -> pp0_layer0_expert1_tp
	pp0_layer0_expert1_tp -> pp0_layer0_agg
	pp0_layer0_gate -> pp0_layer0_expert2 [style=dashed]
	pp0_layer0_expert2 -> pp0_layer0_expert2_tp
	pp0_layer0_expert2_tp -> pp0_layer0_agg
	pp0_layer0_gate -> pp0_layer0_expert3 [style=dashed]
	pp0_layer0_expert3 -> pp0_layer0_expert3_tp
	pp0_layer0_expert3_tp -> pp0_layer0_agg
	pp0_layer0_agg -> pp0_layer0_ep_comm
	subgraph cluster_pp1 {
		fillcolor=lightsteelblue label="Pipeline Stage 1: Layers 4-7 (GPUs 128-255)" style=rounded
		pp1_layer4_attn [label="Layer 4: Attention
GPU: [128-159]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp1_layer4_tp_comm [label="TP All-Reduce
GPU: [128-135], [136-143], [144-151], [152-159]
Input: [batch_size=32, seq_len=1024, hidden=128]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp1_layer4_moe [label="Layer 4: MoE Routing
GPU: [128-159]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
		pp1_layer4_gate [label="Expert Gate Selection
GPU: [128-159]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: routing decisions" fillcolor=lightyellow shape=parallelogram style=dashed]
		pp1_layer4_expert0 [label="Expert 0
GPU: [128+0-128+7]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp1_layer4_expert0_tp [label="TP All-Reduce
GPU: [128+0-128+7]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp1_layer4_expert1 [label="Expert 1
GPU: [128+8-128+15]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp1_layer4_expert1_tp [label="TP All-Reduce
GPU: [128+8-128+15]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp1_layer4_expert2 [label="Expert 2
GPU: [128+16-128+23]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp1_layer4_expert2_tp [label="TP All-Reduce
GPU: [128+16-128+23]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp1_layer4_expert3 [label="Expert 3
GPU: [128+24-128+31]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp1_layer4_expert3_tp [label="TP All-Reduce
GPU: [128+24-128+31]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp1_layer4_agg [label="Expert Aggregation
GPU: [128-159]
Input: [batch_size=~2, seq_len=1024, hidden=1024] × 4
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
		pp1_layer4_ep_comm [label="EP All-to-All
GPU: [128-159]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
	}
	subgraph cluster_pp2 {
		fillcolor=lightseagreen label="Pipeline Stage 2: Layers 8-11 (GPUs 256-383)" style=rounded
		pp2_layer8_attn [label="Layer 8: Attention
GPU: [256-287]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp2_layer8_tp_comm [label="TP All-Reduce
GPU: [256-263], [264-271], [272-279], [280-287]
Input: [batch_size=32, seq_len=1024, hidden=128]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp2_layer8_moe [label="Layer 8: MoE Routing
GPU: [256-287]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
		pp2_layer8_gate [label="Expert Gate Selection
GPU: [256-287]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: routing decisions" fillcolor=lightyellow shape=parallelogram style=dashed]
		pp2_layer8_expert0 [label="Expert 0
GPU: [256+0-256+7]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp2_layer8_expert0_tp [label="TP All-Reduce
GPU: [256+0-256+7]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp2_layer8_expert1 [label="Expert 1
GPU: [256+8-256+15]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp2_layer8_expert1_tp [label="TP All-Reduce
GPU: [256+8-256+15]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp2_layer8_expert2 [label="Expert 2
GPU: [256+16-256+23]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp2_layer8_expert2_tp [label="TP All-Reduce
GPU: [256+16-256+23]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp2_layer8_expert3 [label="Expert 3
GPU: [256+24-256+31]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp2_layer8_expert3_tp [label="TP All-Reduce
GPU: [256+24-256+31]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp2_layer8_agg [label="Expert Aggregation
GPU: [256-287]
Input: [batch_size=~2, seq_len=1024, hidden=1024] × 4
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
		pp2_layer8_ep_comm [label="EP All-to-All
GPU: [256-287]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
	}
	subgraph cluster_pp3 {
		fillcolor=lightsalmon label="Pipeline Stage 3: Layers 12-15 (GPUs 384-511)" style=rounded
		pp3_layer12_attn [label="Layer 12: Attention
GPU: [384-415]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp3_layer12_tp_comm [label="TP All-Reduce
GPU: [384-391], [392-399], [400-407], [408-415]
Input: [batch_size=32, seq_len=1024, hidden=128]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp3_layer12_moe [label="Layer 12: MoE Routing
GPU: [384-415]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
		pp3_layer12_gate [label="Expert Gate Selection
GPU: [384-415]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: routing decisions" fillcolor=lightyellow shape=parallelogram style=dashed]
		pp3_layer12_expert0 [label="Expert 0
GPU: [384+0-384+7]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp3_layer12_expert0_tp [label="TP All-Reduce
GPU: [384+0-384+7]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp3_layer12_expert1 [label="Expert 1
GPU: [384+8-384+15]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp3_layer12_expert1_tp [label="TP All-Reduce
GPU: [384+8-384+15]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp3_layer12_expert2 [label="Expert 2
GPU: [384+16-384+23]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp3_layer12_expert2_tp [label="TP All-Reduce
GPU: [384+16-384+23]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp3_layer12_expert3 [label="Expert 3
GPU: [384+24-384+31]
Input: [batch_size=~2, seq_len=1024, hidden=1024]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=rectangle]
		pp3_layer12_expert3_tp [label="TP All-Reduce
GPU: [384+24-384+31]
Input: [batch_size=~2, seq_len=1024, hidden=256]
Output: [batch_size=~2, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
		pp3_layer12_agg [label="Expert Aggregation
GPU: [384-415]
Input: [batch_size=~2, seq_len=1024, hidden=1024] × 4
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
		pp3_layer12_ep_comm [label="EP All-to-All
GPU: [384-415]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, hidden=1024]" fillcolor=lightgreen shape=ellipse]
	}
	subgraph cluster_output {
		fillcolor=lightgray label="Output Layer" style=rounded
		output [label="Output Layer
GPU: [384-415]
Input: [batch_size=32, seq_len=1024, hidden=1024]
Output: [batch_size=32, seq_len=1024, vocab_size=32000]" fillcolor=lightblue shape=rectangle]
		dp_agg [label="DP Aggregation
GPU: [0-511]
Input: [batch_size=32, seq_len=1024, vocab_size=32000]
Output: [batch_size=128, seq_len=1024, vocab_size=32000]" fillcolor=lightyellow shape=parallelogram]
	}
	pp0_layer0_ep_comm -> pp1_layer4_attn
	pp1_layer4_ep_comm -> pp2_layer8_attn
	pp2_layer8_ep_comm -> pp3_layer12_attn
	pp3_layer12_ep_comm -> output
	output -> dp_agg
	pp0_layer0_ep_comm -> pp1_layer4_moe [constraint=false style=dotted]
	pp0_layer0_ep_comm -> pp2_layer8_moe [constraint=false style=dotted]
	pp0_layer0_ep_comm -> pp3_layer12_moe [constraint=false style=dotted]
	pp1_layer4_ep_comm -> pp0_layer0_moe [constraint=false style=dotted]
	pp1_layer4_ep_comm -> pp2_layer8_moe [constraint=false style=dotted]
	pp1_layer4_ep_comm -> pp3_layer12_moe [constraint=false style=dotted]
	pp2_layer8_ep_comm -> pp0_layer0_moe [constraint=false style=dotted]
	pp2_layer8_ep_comm -> pp1_layer4_moe [constraint=false style=dotted]
	pp2_layer8_ep_comm -> pp3_layer12_moe [constraint=false style=dotted]
	pp3_layer12_ep_comm -> pp0_layer0_moe [constraint=false style=dotted]
	pp3_layer12_ep_comm -> pp1_layer4_moe [constraint=false style=dotted]
	pp3_layer12_ep_comm -> pp2_layer8_moe [constraint=false style=dotted]
}
