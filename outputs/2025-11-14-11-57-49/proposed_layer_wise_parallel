// 16-layer Transformer with Layer-wise Parallelism (1 layer per GPU)
digraph proposed_layer_wise_parallel {
	rankdir=TB size="30,30"
	node [shape=rectangle style=filled]
	input [label="Model Input
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU 0" fillcolor=lightgreen shape=ellipse]
	subgraph cluster_layer1 {
		label="Layer 1
GPU 0 (L2 Cache Optimized)"
		layer1_norm1 [label="Layer 1 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 0" fillcolor=lightyellow]
		layer1_qkv_proj [label="Layer 1 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 0" fillcolor=lightcoral]
		layer1_q_split [label="Layer 1 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 0" fillcolor=lightpink]
		layer1_k_split [label="Layer 1 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 0" fillcolor=lightpink]
		layer1_v_split [label="Layer 1 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 0" fillcolor=lightpink]
		layer1_attention [label="Layer 1 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 0" fillcolor=lightcoral]
		layer1_attention_concat [label="Layer 1 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 0" fillcolor=lightcoral]
		layer1_attention_out [label="Layer 1 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 0" fillcolor=lightcoral]
		layer1_residual1 [label="Layer 1 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 0" fillcolor=lightgreen]
		layer1_norm2 [label="Layer 1 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 0" fillcolor=lightyellow]
		layer1_mlp_gate [label="Layer 1 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 0" fillcolor=lightcyan]
		layer1_mlp_up [label="Layer 1 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 0" fillcolor=lightcyan]
		layer1_mlp_activation [label="Layer 1 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 0" fillcolor=lightcyan]
		layer1_mlp_down [label="Layer 1 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 0" fillcolor=lightcyan]
		layer1_residual2 [label="Layer 1 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 0" fillcolor=lightgreen]
	}
	subgraph cluster_layer2 {
		label="Layer 2
GPU 1 (L2 Cache Optimized)"
		layer2_norm1 [label="Layer 2 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 1" fillcolor=lightyellow]
		layer2_qkv_proj [label="Layer 2 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 1" fillcolor=lightcoral]
		layer2_q_split [label="Layer 2 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 1" fillcolor=lightpink]
		layer2_k_split [label="Layer 2 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 1" fillcolor=lightpink]
		layer2_v_split [label="Layer 2 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 1" fillcolor=lightpink]
		layer2_attention [label="Layer 2 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 1" fillcolor=lightcoral]
		layer2_attention_concat [label="Layer 2 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 1" fillcolor=lightcoral]
		layer2_attention_out [label="Layer 2 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 1" fillcolor=lightcoral]
		layer2_residual1 [label="Layer 2 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 1" fillcolor=lightgreen]
		layer2_norm2 [label="Layer 2 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 1" fillcolor=lightyellow]
		layer2_mlp_gate [label="Layer 2 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 1" fillcolor=lightcyan]
		layer2_mlp_up [label="Layer 2 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 1" fillcolor=lightcyan]
		layer2_mlp_activation [label="Layer 2 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 1" fillcolor=lightcyan]
		layer2_mlp_down [label="Layer 2 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 1" fillcolor=lightcyan]
		layer2_residual2 [label="Layer 2 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 1" fillcolor=lightgreen]
	}
	subgraph cluster_layer3 {
		label="Layer 3
GPU 2 (L2 Cache Optimized)"
		layer3_norm1 [label="Layer 3 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 2" fillcolor=lightyellow]
		layer3_qkv_proj [label="Layer 3 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 2" fillcolor=lightcoral]
		layer3_q_split [label="Layer 3 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 2" fillcolor=lightpink]
		layer3_k_split [label="Layer 3 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 2" fillcolor=lightpink]
		layer3_v_split [label="Layer 3 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 2" fillcolor=lightpink]
		layer3_attention [label="Layer 3 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 2" fillcolor=lightcoral]
		layer3_attention_concat [label="Layer 3 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 2" fillcolor=lightcoral]
		layer3_attention_out [label="Layer 3 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 2" fillcolor=lightcoral]
		layer3_residual1 [label="Layer 3 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 2" fillcolor=lightgreen]
		layer3_norm2 [label="Layer 3 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 2" fillcolor=lightyellow]
		layer3_mlp_gate [label="Layer 3 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 2" fillcolor=lightcyan]
		layer3_mlp_up [label="Layer 3 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 2" fillcolor=lightcyan]
		layer3_mlp_activation [label="Layer 3 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 2" fillcolor=lightcyan]
		layer3_mlp_down [label="Layer 3 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 2" fillcolor=lightcyan]
		layer3_residual2 [label="Layer 3 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 2" fillcolor=lightgreen]
	}
	subgraph cluster_layer4 {
		label="Layer 4
GPU 3 (L2 Cache Optimized)"
		layer4_norm1 [label="Layer 4 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 3" fillcolor=lightyellow]
		layer4_qkv_proj [label="Layer 4 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 3" fillcolor=lightcoral]
		layer4_q_split [label="Layer 4 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 3" fillcolor=lightpink]
		layer4_k_split [label="Layer 4 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 3" fillcolor=lightpink]
		layer4_v_split [label="Layer 4 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 3" fillcolor=lightpink]
		layer4_attention [label="Layer 4 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 3" fillcolor=lightcoral]
		layer4_attention_concat [label="Layer 4 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 3" fillcolor=lightcoral]
		layer4_attention_out [label="Layer 4 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 3" fillcolor=lightcoral]
		layer4_residual1 [label="Layer 4 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 3" fillcolor=lightgreen]
		layer4_norm2 [label="Layer 4 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 3" fillcolor=lightyellow]
		layer4_mlp_gate [label="Layer 4 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 3" fillcolor=lightcyan]
		layer4_mlp_up [label="Layer 4 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 3" fillcolor=lightcyan]
		layer4_mlp_activation [label="Layer 4 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 3" fillcolor=lightcyan]
		layer4_mlp_down [label="Layer 4 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 3" fillcolor=lightcyan]
		layer4_residual2 [label="Layer 4 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 3" fillcolor=lightgreen]
	}
	subgraph cluster_layer5 {
		label="Layer 5
GPU 4 (L2 Cache Optimized)"
		layer5_norm1 [label="Layer 5 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 4" fillcolor=lightyellow]
		layer5_qkv_proj [label="Layer 5 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 4" fillcolor=lightcoral]
		layer5_q_split [label="Layer 5 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 4" fillcolor=lightpink]
		layer5_k_split [label="Layer 5 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 4" fillcolor=lightpink]
		layer5_v_split [label="Layer 5 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 4" fillcolor=lightpink]
		layer5_attention [label="Layer 5 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 4" fillcolor=lightcoral]
		layer5_attention_concat [label="Layer 5 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 4" fillcolor=lightcoral]
		layer5_attention_out [label="Layer 5 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 4" fillcolor=lightcoral]
		layer5_residual1 [label="Layer 5 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 4" fillcolor=lightgreen]
		layer5_norm2 [label="Layer 5 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 4" fillcolor=lightyellow]
		layer5_mlp_gate [label="Layer 5 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 4" fillcolor=lightcyan]
		layer5_mlp_up [label="Layer 5 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 4" fillcolor=lightcyan]
		layer5_mlp_activation [label="Layer 5 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 4" fillcolor=lightcyan]
		layer5_mlp_down [label="Layer 5 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 4" fillcolor=lightcyan]
		layer5_residual2 [label="Layer 5 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 4" fillcolor=lightgreen]
	}
	subgraph cluster_layer6 {
		label="Layer 6
GPU 5 (L2 Cache Optimized)"
		layer6_norm1 [label="Layer 6 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 5" fillcolor=lightyellow]
		layer6_qkv_proj [label="Layer 6 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 5" fillcolor=lightcoral]
		layer6_q_split [label="Layer 6 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 5" fillcolor=lightpink]
		layer6_k_split [label="Layer 6 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 5" fillcolor=lightpink]
		layer6_v_split [label="Layer 6 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 5" fillcolor=lightpink]
		layer6_attention [label="Layer 6 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 5" fillcolor=lightcoral]
		layer6_attention_concat [label="Layer 6 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 5" fillcolor=lightcoral]
		layer6_attention_out [label="Layer 6 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 5" fillcolor=lightcoral]
		layer6_residual1 [label="Layer 6 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 5" fillcolor=lightgreen]
		layer6_norm2 [label="Layer 6 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 5" fillcolor=lightyellow]
		layer6_mlp_gate [label="Layer 6 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 5" fillcolor=lightcyan]
		layer6_mlp_up [label="Layer 6 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 5" fillcolor=lightcyan]
		layer6_mlp_activation [label="Layer 6 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 5" fillcolor=lightcyan]
		layer6_mlp_down [label="Layer 6 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 5" fillcolor=lightcyan]
		layer6_residual2 [label="Layer 6 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 5" fillcolor=lightgreen]
	}
	subgraph cluster_layer7 {
		label="Layer 7
GPU 6 (L2 Cache Optimized)"
		layer7_norm1 [label="Layer 7 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 6" fillcolor=lightyellow]
		layer7_qkv_proj [label="Layer 7 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 6" fillcolor=lightcoral]
		layer7_q_split [label="Layer 7 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 6" fillcolor=lightpink]
		layer7_k_split [label="Layer 7 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 6" fillcolor=lightpink]
		layer7_v_split [label="Layer 7 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 6" fillcolor=lightpink]
		layer7_attention [label="Layer 7 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 6" fillcolor=lightcoral]
		layer7_attention_concat [label="Layer 7 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 6" fillcolor=lightcoral]
		layer7_attention_out [label="Layer 7 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 6" fillcolor=lightcoral]
		layer7_residual1 [label="Layer 7 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 6" fillcolor=lightgreen]
		layer7_norm2 [label="Layer 7 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 6" fillcolor=lightyellow]
		layer7_mlp_gate [label="Layer 7 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 6" fillcolor=lightcyan]
		layer7_mlp_up [label="Layer 7 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 6" fillcolor=lightcyan]
		layer7_mlp_activation [label="Layer 7 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 6" fillcolor=lightcyan]
		layer7_mlp_down [label="Layer 7 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 6" fillcolor=lightcyan]
		layer7_residual2 [label="Layer 7 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 6" fillcolor=lightgreen]
	}
	subgraph cluster_layer8 {
		label="Layer 8
GPU 7 (L2 Cache Optimized)"
		layer8_norm1 [label="Layer 8 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 7" fillcolor=lightyellow]
		layer8_qkv_proj [label="Layer 8 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 7" fillcolor=lightcoral]
		layer8_q_split [label="Layer 8 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 7" fillcolor=lightpink]
		layer8_k_split [label="Layer 8 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 7" fillcolor=lightpink]
		layer8_v_split [label="Layer 8 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 7" fillcolor=lightpink]
		layer8_attention [label="Layer 8 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 7" fillcolor=lightcoral]
		layer8_attention_concat [label="Layer 8 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 7" fillcolor=lightcoral]
		layer8_attention_out [label="Layer 8 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 7" fillcolor=lightcoral]
		layer8_residual1 [label="Layer 8 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 7" fillcolor=lightgreen]
		layer8_norm2 [label="Layer 8 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 7" fillcolor=lightyellow]
		layer8_mlp_gate [label="Layer 8 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 7" fillcolor=lightcyan]
		layer8_mlp_up [label="Layer 8 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 7" fillcolor=lightcyan]
		layer8_mlp_activation [label="Layer 8 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 7" fillcolor=lightcyan]
		layer8_mlp_down [label="Layer 8 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 7" fillcolor=lightcyan]
		layer8_residual2 [label="Layer 8 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 7" fillcolor=lightgreen]
	}
	subgraph cluster_layer9 {
		label="Layer 9
GPU 8 (L2 Cache Optimized)"
		layer9_norm1 [label="Layer 9 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 8" fillcolor=lightyellow]
		layer9_qkv_proj [label="Layer 9 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 8" fillcolor=lightcoral]
		layer9_q_split [label="Layer 9 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 8" fillcolor=lightpink]
		layer9_k_split [label="Layer 9 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 8" fillcolor=lightpink]
		layer9_v_split [label="Layer 9 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 8" fillcolor=lightpink]
		layer9_attention [label="Layer 9 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 8" fillcolor=lightcoral]
		layer9_attention_concat [label="Layer 9 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 8" fillcolor=lightcoral]
		layer9_attention_out [label="Layer 9 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 8" fillcolor=lightcoral]
		layer9_residual1 [label="Layer 9 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 8" fillcolor=lightgreen]
		layer9_norm2 [label="Layer 9 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 8" fillcolor=lightyellow]
		layer9_mlp_gate [label="Layer 9 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 8" fillcolor=lightcyan]
		layer9_mlp_up [label="Layer 9 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 8" fillcolor=lightcyan]
		layer9_mlp_activation [label="Layer 9 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 8" fillcolor=lightcyan]
		layer9_mlp_down [label="Layer 9 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 8" fillcolor=lightcyan]
		layer9_residual2 [label="Layer 9 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 8" fillcolor=lightgreen]
	}
	subgraph cluster_layer10 {
		label="Layer 10
GPU 9 (L2 Cache Optimized)"
		layer10_norm1 [label="Layer 10 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 9" fillcolor=lightyellow]
		layer10_qkv_proj [label="Layer 10 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 9" fillcolor=lightcoral]
		layer10_q_split [label="Layer 10 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 9" fillcolor=lightpink]
		layer10_k_split [label="Layer 10 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 9" fillcolor=lightpink]
		layer10_v_split [label="Layer 10 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 9" fillcolor=lightpink]
		layer10_attention [label="Layer 10 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 9" fillcolor=lightcoral]
		layer10_attention_concat [label="Layer 10 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 9" fillcolor=lightcoral]
		layer10_attention_out [label="Layer 10 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 9" fillcolor=lightcoral]
		layer10_residual1 [label="Layer 10 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 9" fillcolor=lightgreen]
		layer10_norm2 [label="Layer 10 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 9" fillcolor=lightyellow]
		layer10_mlp_gate [label="Layer 10 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 9" fillcolor=lightcyan]
		layer10_mlp_up [label="Layer 10 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 9" fillcolor=lightcyan]
		layer10_mlp_activation [label="Layer 10 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 9" fillcolor=lightcyan]
		layer10_mlp_down [label="Layer 10 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 9" fillcolor=lightcyan]
		layer10_residual2 [label="Layer 10 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 9" fillcolor=lightgreen]
	}
	subgraph cluster_layer11 {
		label="Layer 11
GPU 10 (L2 Cache Optimized)"
		layer11_norm1 [label="Layer 11 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 10" fillcolor=lightyellow]
		layer11_qkv_proj [label="Layer 11 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 10" fillcolor=lightcoral]
		layer11_q_split [label="Layer 11 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 10" fillcolor=lightpink]
		layer11_k_split [label="Layer 11 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 10" fillcolor=lightpink]
		layer11_v_split [label="Layer 11 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 10" fillcolor=lightpink]
		layer11_attention [label="Layer 11 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 10" fillcolor=lightcoral]
		layer11_attention_concat [label="Layer 11 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 10" fillcolor=lightcoral]
		layer11_attention_out [label="Layer 11 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 10" fillcolor=lightcoral]
		layer11_residual1 [label="Layer 11 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 10" fillcolor=lightgreen]
		layer11_norm2 [label="Layer 11 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 10" fillcolor=lightyellow]
		layer11_mlp_gate [label="Layer 11 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 10" fillcolor=lightcyan]
		layer11_mlp_up [label="Layer 11 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 10" fillcolor=lightcyan]
		layer11_mlp_activation [label="Layer 11 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 10" fillcolor=lightcyan]
		layer11_mlp_down [label="Layer 11 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 10" fillcolor=lightcyan]
		layer11_residual2 [label="Layer 11 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 10" fillcolor=lightgreen]
	}
	subgraph cluster_layer12 {
		label="Layer 12
GPU 11 (L2 Cache Optimized)"
		layer12_norm1 [label="Layer 12 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 11" fillcolor=lightyellow]
		layer12_qkv_proj [label="Layer 12 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 11" fillcolor=lightcoral]
		layer12_q_split [label="Layer 12 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 11" fillcolor=lightpink]
		layer12_k_split [label="Layer 12 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 11" fillcolor=lightpink]
		layer12_v_split [label="Layer 12 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 11" fillcolor=lightpink]
		layer12_attention [label="Layer 12 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 11" fillcolor=lightcoral]
		layer12_attention_concat [label="Layer 12 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 11" fillcolor=lightcoral]
		layer12_attention_out [label="Layer 12 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 11" fillcolor=lightcoral]
		layer12_residual1 [label="Layer 12 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 11" fillcolor=lightgreen]
		layer12_norm2 [label="Layer 12 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 11" fillcolor=lightyellow]
		layer12_mlp_gate [label="Layer 12 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 11" fillcolor=lightcyan]
		layer12_mlp_up [label="Layer 12 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 11" fillcolor=lightcyan]
		layer12_mlp_activation [label="Layer 12 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 11" fillcolor=lightcyan]
		layer12_mlp_down [label="Layer 12 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 11" fillcolor=lightcyan]
		layer12_residual2 [label="Layer 12 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 11" fillcolor=lightgreen]
	}
	subgraph cluster_layer13 {
		label="Layer 13
GPU 12 (L2 Cache Optimized)"
		layer13_norm1 [label="Layer 13 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 12" fillcolor=lightyellow]
		layer13_qkv_proj [label="Layer 13 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 12" fillcolor=lightcoral]
		layer13_q_split [label="Layer 13 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 12" fillcolor=lightpink]
		layer13_k_split [label="Layer 13 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 12" fillcolor=lightpink]
		layer13_v_split [label="Layer 13 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 12" fillcolor=lightpink]
		layer13_attention [label="Layer 13 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 12" fillcolor=lightcoral]
		layer13_attention_concat [label="Layer 13 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 12" fillcolor=lightcoral]
		layer13_attention_out [label="Layer 13 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 12" fillcolor=lightcoral]
		layer13_residual1 [label="Layer 13 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 12" fillcolor=lightgreen]
		layer13_norm2 [label="Layer 13 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 12" fillcolor=lightyellow]
		layer13_mlp_gate [label="Layer 13 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 12" fillcolor=lightcyan]
		layer13_mlp_up [label="Layer 13 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 12" fillcolor=lightcyan]
		layer13_mlp_activation [label="Layer 13 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 12" fillcolor=lightcyan]
		layer13_mlp_down [label="Layer 13 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 12" fillcolor=lightcyan]
		layer13_residual2 [label="Layer 13 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 12" fillcolor=lightgreen]
	}
	subgraph cluster_layer14 {
		label="Layer 14
GPU 13 (L2 Cache Optimized)"
		layer14_norm1 [label="Layer 14 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 13" fillcolor=lightyellow]
		layer14_qkv_proj [label="Layer 14 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 13" fillcolor=lightcoral]
		layer14_q_split [label="Layer 14 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 13" fillcolor=lightpink]
		layer14_k_split [label="Layer 14 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 13" fillcolor=lightpink]
		layer14_v_split [label="Layer 14 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 13" fillcolor=lightpink]
		layer14_attention [label="Layer 14 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 13" fillcolor=lightcoral]
		layer14_attention_concat [label="Layer 14 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 13" fillcolor=lightcoral]
		layer14_attention_out [label="Layer 14 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 13" fillcolor=lightcoral]
		layer14_residual1 [label="Layer 14 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 13" fillcolor=lightgreen]
		layer14_norm2 [label="Layer 14 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 13" fillcolor=lightyellow]
		layer14_mlp_gate [label="Layer 14 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 13" fillcolor=lightcyan]
		layer14_mlp_up [label="Layer 14 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 13" fillcolor=lightcyan]
		layer14_mlp_activation [label="Layer 14 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 13" fillcolor=lightcyan]
		layer14_mlp_down [label="Layer 14 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 13" fillcolor=lightcyan]
		layer14_residual2 [label="Layer 14 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 13" fillcolor=lightgreen]
	}
	subgraph cluster_layer15 {
		label="Layer 15
GPU 14 (L2 Cache Optimized)"
		layer15_norm1 [label="Layer 15 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 14" fillcolor=lightyellow]
		layer15_qkv_proj [label="Layer 15 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 14" fillcolor=lightcoral]
		layer15_q_split [label="Layer 15 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 14" fillcolor=lightpink]
		layer15_k_split [label="Layer 15 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 14" fillcolor=lightpink]
		layer15_v_split [label="Layer 15 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 14" fillcolor=lightpink]
		layer15_attention [label="Layer 15 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 14" fillcolor=lightcoral]
		layer15_attention_concat [label="Layer 15 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 14" fillcolor=lightcoral]
		layer15_attention_out [label="Layer 15 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 14" fillcolor=lightcoral]
		layer15_residual1 [label="Layer 15 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 14" fillcolor=lightgreen]
		layer15_norm2 [label="Layer 15 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 14" fillcolor=lightyellow]
		layer15_mlp_gate [label="Layer 15 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 14" fillcolor=lightcyan]
		layer15_mlp_up [label="Layer 15 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 14" fillcolor=lightcyan]
		layer15_mlp_activation [label="Layer 15 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 14" fillcolor=lightcyan]
		layer15_mlp_down [label="Layer 15 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 14" fillcolor=lightcyan]
		layer15_residual2 [label="Layer 15 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 14" fillcolor=lightgreen]
	}
	subgraph cluster_layer16 {
		label="Layer 16
GPU 15 (L2 Cache Optimized)"
		layer16_norm1 [label="Layer 16 Input Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 15" fillcolor=lightyellow]
		layer16_qkv_proj [label="Layer 16 QKV Projection
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 12288]
GPU 15" fillcolor=lightcoral]
		layer16_q_split [label="Layer 16 Q Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 15" fillcolor=lightpink]
		layer16_k_split [label="Layer 16 K Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 15" fillcolor=lightpink]
		layer16_v_split [label="Layer 16 V Split
Input: [128, 10000, 4096]
Output: [128, 10000, 32, 128]
GPU 15" fillcolor=lightpink]
		layer16_attention [label="Layer 16 Multi-Head Attention
Input: [128, 10000, 32, 128]
Output: [128, 10000, 32, 128]
GPU 15" fillcolor=lightcoral]
		layer16_attention_concat [label="Layer 16 Attention Concat
Input: [128, 10000, 32, 128]
Output: [128, 10000, 4096]
GPU 15" fillcolor=lightcoral]
		layer16_attention_out [label="Layer 16 Attention Output
Compressed: 33.6 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 15" fillcolor=lightcoral]
		layer16_residual1 [label="Layer 16 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 15" fillcolor=lightgreen]
		layer16_norm2 [label="Layer 16 MLP Norm
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 15" fillcolor=lightyellow]
		layer16_mlp_gate [label="Layer 16 MLP Gate
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 15" fillcolor=lightcyan]
		layer16_mlp_up [label="Layer 16 MLP Up
Compressed: 50.3 MB
Input: [128, 10000, 4096]
Output: [128, 10000, 16384]
GPU 15" fillcolor=lightcyan]
		layer16_mlp_activation [label="Layer 16 MLP Activation (GELU)
Input: [128, 10000, 16384]
Output: [128, 10000, 16384]
GPU 15" fillcolor=lightcyan]
		layer16_mlp_down [label="Layer 16 MLP Down
Compressed: 50.3 MB
Input: [128, 10000, 16384]
Output: [128, 10000, 4096]
GPU 15" fillcolor=lightcyan]
		layer16_residual2 [label="Layer 16 Residual Add
Input1: [128, 10000, 4096]
Input2: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 15" fillcolor=lightgreen]
	}
	output [label="Model Output
Input: [128, 10000, 4096]
Output: [128, 10000, 4096]
GPU 15" fillcolor=lightgreen shape=ellipse]
	input -> layer1_norm1
	layer1_norm1 -> layer1_qkv_proj
	layer1_qkv_proj -> layer1_q_split
	layer1_qkv_proj -> layer1_k_split
	layer1_qkv_proj -> layer1_v_split
	layer1_q_split -> layer1_attention
	layer1_k_split -> layer1_attention
	layer1_v_split -> layer1_attention
	layer1_attention -> layer1_attention_concat
	layer1_attention_concat -> layer1_attention_out
	layer1_attention_out -> layer1_residual1
	input -> layer1_residual1
	layer1_residual1 -> layer1_norm2
	layer1_norm2 -> layer1_mlp_gate
	layer1_norm2 -> layer1_mlp_up
	layer1_mlp_gate -> layer1_mlp_activation
	layer1_mlp_up -> layer1_mlp_activation
	layer1_mlp_activation -> layer1_mlp_down
	layer1_mlp_down -> layer1_residual2
	layer1_residual1 -> layer1_residual2
	comm_1_2 [label="GPU 0 → GPU 1
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer1_residual2 -> comm_1_2
	comm_1_2 -> layer2_norm1
	layer2_norm1 -> layer2_qkv_proj
	layer2_qkv_proj -> layer2_q_split
	layer2_qkv_proj -> layer2_k_split
	layer2_qkv_proj -> layer2_v_split
	layer2_q_split -> layer2_attention
	layer2_k_split -> layer2_attention
	layer2_v_split -> layer2_attention
	layer2_attention -> layer2_attention_concat
	layer2_attention_concat -> layer2_attention_out
	layer2_attention_out -> layer2_residual1
	comm_1_2 -> layer2_residual1
	layer2_residual1 -> layer2_norm2
	layer2_norm2 -> layer2_mlp_gate
	layer2_norm2 -> layer2_mlp_up
	layer2_mlp_gate -> layer2_mlp_activation
	layer2_mlp_up -> layer2_mlp_activation
	layer2_mlp_activation -> layer2_mlp_down
	layer2_mlp_down -> layer2_residual2
	layer2_residual1 -> layer2_residual2
	comm_2_3 [label="GPU 1 → GPU 2
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer2_residual2 -> comm_2_3
	comm_2_3 -> layer3_norm1
	layer3_norm1 -> layer3_qkv_proj
	layer3_qkv_proj -> layer3_q_split
	layer3_qkv_proj -> layer3_k_split
	layer3_qkv_proj -> layer3_v_split
	layer3_q_split -> layer3_attention
	layer3_k_split -> layer3_attention
	layer3_v_split -> layer3_attention
	layer3_attention -> layer3_attention_concat
	layer3_attention_concat -> layer3_attention_out
	layer3_attention_out -> layer3_residual1
	comm_2_3 -> layer3_residual1
	layer3_residual1 -> layer3_norm2
	layer3_norm2 -> layer3_mlp_gate
	layer3_norm2 -> layer3_mlp_up
	layer3_mlp_gate -> layer3_mlp_activation
	layer3_mlp_up -> layer3_mlp_activation
	layer3_mlp_activation -> layer3_mlp_down
	layer3_mlp_down -> layer3_residual2
	layer3_residual1 -> layer3_residual2
	comm_3_4 [label="GPU 2 → GPU 3
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer3_residual2 -> comm_3_4
	comm_3_4 -> layer4_norm1
	layer4_norm1 -> layer4_qkv_proj
	layer4_qkv_proj -> layer4_q_split
	layer4_qkv_proj -> layer4_k_split
	layer4_qkv_proj -> layer4_v_split
	layer4_q_split -> layer4_attention
	layer4_k_split -> layer4_attention
	layer4_v_split -> layer4_attention
	layer4_attention -> layer4_attention_concat
	layer4_attention_concat -> layer4_attention_out
	layer4_attention_out -> layer4_residual1
	comm_3_4 -> layer4_residual1
	layer4_residual1 -> layer4_norm2
	layer4_norm2 -> layer4_mlp_gate
	layer4_norm2 -> layer4_mlp_up
	layer4_mlp_gate -> layer4_mlp_activation
	layer4_mlp_up -> layer4_mlp_activation
	layer4_mlp_activation -> layer4_mlp_down
	layer4_mlp_down -> layer4_residual2
	layer4_residual1 -> layer4_residual2
	comm_4_5 [label="GPU 3 → GPU 4
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer4_residual2 -> comm_4_5
	comm_4_5 -> layer5_norm1
	layer5_norm1 -> layer5_qkv_proj
	layer5_qkv_proj -> layer5_q_split
	layer5_qkv_proj -> layer5_k_split
	layer5_qkv_proj -> layer5_v_split
	layer5_q_split -> layer5_attention
	layer5_k_split -> layer5_attention
	layer5_v_split -> layer5_attention
	layer5_attention -> layer5_attention_concat
	layer5_attention_concat -> layer5_attention_out
	layer5_attention_out -> layer5_residual1
	comm_4_5 -> layer5_residual1
	layer5_residual1 -> layer5_norm2
	layer5_norm2 -> layer5_mlp_gate
	layer5_norm2 -> layer5_mlp_up
	layer5_mlp_gate -> layer5_mlp_activation
	layer5_mlp_up -> layer5_mlp_activation
	layer5_mlp_activation -> layer5_mlp_down
	layer5_mlp_down -> layer5_residual2
	layer5_residual1 -> layer5_residual2
	comm_5_6 [label="GPU 4 → GPU 5
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer5_residual2 -> comm_5_6
	comm_5_6 -> layer6_norm1
	layer6_norm1 -> layer6_qkv_proj
	layer6_qkv_proj -> layer6_q_split
	layer6_qkv_proj -> layer6_k_split
	layer6_qkv_proj -> layer6_v_split
	layer6_q_split -> layer6_attention
	layer6_k_split -> layer6_attention
	layer6_v_split -> layer6_attention
	layer6_attention -> layer6_attention_concat
	layer6_attention_concat -> layer6_attention_out
	layer6_attention_out -> layer6_residual1
	comm_5_6 -> layer6_residual1
	layer6_residual1 -> layer6_norm2
	layer6_norm2 -> layer6_mlp_gate
	layer6_norm2 -> layer6_mlp_up
	layer6_mlp_gate -> layer6_mlp_activation
	layer6_mlp_up -> layer6_mlp_activation
	layer6_mlp_activation -> layer6_mlp_down
	layer6_mlp_down -> layer6_residual2
	layer6_residual1 -> layer6_residual2
	comm_6_7 [label="GPU 5 → GPU 6
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer6_residual2 -> comm_6_7
	comm_6_7 -> layer7_norm1
	layer7_norm1 -> layer7_qkv_proj
	layer7_qkv_proj -> layer7_q_split
	layer7_qkv_proj -> layer7_k_split
	layer7_qkv_proj -> layer7_v_split
	layer7_q_split -> layer7_attention
	layer7_k_split -> layer7_attention
	layer7_v_split -> layer7_attention
	layer7_attention -> layer7_attention_concat
	layer7_attention_concat -> layer7_attention_out
	layer7_attention_out -> layer7_residual1
	comm_6_7 -> layer7_residual1
	layer7_residual1 -> layer7_norm2
	layer7_norm2 -> layer7_mlp_gate
	layer7_norm2 -> layer7_mlp_up
	layer7_mlp_gate -> layer7_mlp_activation
	layer7_mlp_up -> layer7_mlp_activation
	layer7_mlp_activation -> layer7_mlp_down
	layer7_mlp_down -> layer7_residual2
	layer7_residual1 -> layer7_residual2
	comm_7_8 [label="GPU 6 → GPU 7
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer7_residual2 -> comm_7_8
	comm_7_8 -> layer8_norm1
	layer8_norm1 -> layer8_qkv_proj
	layer8_qkv_proj -> layer8_q_split
	layer8_qkv_proj -> layer8_k_split
	layer8_qkv_proj -> layer8_v_split
	layer8_q_split -> layer8_attention
	layer8_k_split -> layer8_attention
	layer8_v_split -> layer8_attention
	layer8_attention -> layer8_attention_concat
	layer8_attention_concat -> layer8_attention_out
	layer8_attention_out -> layer8_residual1
	comm_7_8 -> layer8_residual1
	layer8_residual1 -> layer8_norm2
	layer8_norm2 -> layer8_mlp_gate
	layer8_norm2 -> layer8_mlp_up
	layer8_mlp_gate -> layer8_mlp_activation
	layer8_mlp_up -> layer8_mlp_activation
	layer8_mlp_activation -> layer8_mlp_down
	layer8_mlp_down -> layer8_residual2
	layer8_residual1 -> layer8_residual2
	comm_8_9 [label="GPU 7 → GPU 8
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer8_residual2 -> comm_8_9
	comm_8_9 -> layer9_norm1
	layer9_norm1 -> layer9_qkv_proj
	layer9_qkv_proj -> layer9_q_split
	layer9_qkv_proj -> layer9_k_split
	layer9_qkv_proj -> layer9_v_split
	layer9_q_split -> layer9_attention
	layer9_k_split -> layer9_attention
	layer9_v_split -> layer9_attention
	layer9_attention -> layer9_attention_concat
	layer9_attention_concat -> layer9_attention_out
	layer9_attention_out -> layer9_residual1
	comm_8_9 -> layer9_residual1
	layer9_residual1 -> layer9_norm2
	layer9_norm2 -> layer9_mlp_gate
	layer9_norm2 -> layer9_mlp_up
	layer9_mlp_gate -> layer9_mlp_activation
	layer9_mlp_up -> layer9_mlp_activation
	layer9_mlp_activation -> layer9_mlp_down
	layer9_mlp_down -> layer9_residual2
	layer9_residual1 -> layer9_residual2
	comm_9_10 [label="GPU 8 → GPU 9
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer9_residual2 -> comm_9_10
	comm_9_10 -> layer10_norm1
	layer10_norm1 -> layer10_qkv_proj
	layer10_qkv_proj -> layer10_q_split
	layer10_qkv_proj -> layer10_k_split
	layer10_qkv_proj -> layer10_v_split
	layer10_q_split -> layer10_attention
	layer10_k_split -> layer10_attention
	layer10_v_split -> layer10_attention
	layer10_attention -> layer10_attention_concat
	layer10_attention_concat -> layer10_attention_out
	layer10_attention_out -> layer10_residual1
	comm_9_10 -> layer10_residual1
	layer10_residual1 -> layer10_norm2
	layer10_norm2 -> layer10_mlp_gate
	layer10_norm2 -> layer10_mlp_up
	layer10_mlp_gate -> layer10_mlp_activation
	layer10_mlp_up -> layer10_mlp_activation
	layer10_mlp_activation -> layer10_mlp_down
	layer10_mlp_down -> layer10_residual2
	layer10_residual1 -> layer10_residual2
	comm_10_11 [label="GPU 9 → GPU 10
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer10_residual2 -> comm_10_11
	comm_10_11 -> layer11_norm1
	layer11_norm1 -> layer11_qkv_proj
	layer11_qkv_proj -> layer11_q_split
	layer11_qkv_proj -> layer11_k_split
	layer11_qkv_proj -> layer11_v_split
	layer11_q_split -> layer11_attention
	layer11_k_split -> layer11_attention
	layer11_v_split -> layer11_attention
	layer11_attention -> layer11_attention_concat
	layer11_attention_concat -> layer11_attention_out
	layer11_attention_out -> layer11_residual1
	comm_10_11 -> layer11_residual1
	layer11_residual1 -> layer11_norm2
	layer11_norm2 -> layer11_mlp_gate
	layer11_norm2 -> layer11_mlp_up
	layer11_mlp_gate -> layer11_mlp_activation
	layer11_mlp_up -> layer11_mlp_activation
	layer11_mlp_activation -> layer11_mlp_down
	layer11_mlp_down -> layer11_residual2
	layer11_residual1 -> layer11_residual2
	comm_11_12 [label="GPU 10 → GPU 11
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer11_residual2 -> comm_11_12
	comm_11_12 -> layer12_norm1
	layer12_norm1 -> layer12_qkv_proj
	layer12_qkv_proj -> layer12_q_split
	layer12_qkv_proj -> layer12_k_split
	layer12_qkv_proj -> layer12_v_split
	layer12_q_split -> layer12_attention
	layer12_k_split -> layer12_attention
	layer12_v_split -> layer12_attention
	layer12_attention -> layer12_attention_concat
	layer12_attention_concat -> layer12_attention_out
	layer12_attention_out -> layer12_residual1
	comm_11_12 -> layer12_residual1
	layer12_residual1 -> layer12_norm2
	layer12_norm2 -> layer12_mlp_gate
	layer12_norm2 -> layer12_mlp_up
	layer12_mlp_gate -> layer12_mlp_activation
	layer12_mlp_up -> layer12_mlp_activation
	layer12_mlp_activation -> layer12_mlp_down
	layer12_mlp_down -> layer12_residual2
	layer12_residual1 -> layer12_residual2
	comm_12_13 [label="GPU 11 → GPU 12
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer12_residual2 -> comm_12_13
	comm_12_13 -> layer13_norm1
	layer13_norm1 -> layer13_qkv_proj
	layer13_qkv_proj -> layer13_q_split
	layer13_qkv_proj -> layer13_k_split
	layer13_qkv_proj -> layer13_v_split
	layer13_q_split -> layer13_attention
	layer13_k_split -> layer13_attention
	layer13_v_split -> layer13_attention
	layer13_attention -> layer13_attention_concat
	layer13_attention_concat -> layer13_attention_out
	layer13_attention_out -> layer13_residual1
	comm_12_13 -> layer13_residual1
	layer13_residual1 -> layer13_norm2
	layer13_norm2 -> layer13_mlp_gate
	layer13_norm2 -> layer13_mlp_up
	layer13_mlp_gate -> layer13_mlp_activation
	layer13_mlp_up -> layer13_mlp_activation
	layer13_mlp_activation -> layer13_mlp_down
	layer13_mlp_down -> layer13_residual2
	layer13_residual1 -> layer13_residual2
	comm_13_14 [label="GPU 12 → GPU 13
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer13_residual2 -> comm_13_14
	comm_13_14 -> layer14_norm1
	layer14_norm1 -> layer14_qkv_proj
	layer14_qkv_proj -> layer14_q_split
	layer14_qkv_proj -> layer14_k_split
	layer14_qkv_proj -> layer14_v_split
	layer14_q_split -> layer14_attention
	layer14_k_split -> layer14_attention
	layer14_v_split -> layer14_attention
	layer14_attention -> layer14_attention_concat
	layer14_attention_concat -> layer14_attention_out
	layer14_attention_out -> layer14_residual1
	comm_13_14 -> layer14_residual1
	layer14_residual1 -> layer14_norm2
	layer14_norm2 -> layer14_mlp_gate
	layer14_norm2 -> layer14_mlp_up
	layer14_mlp_gate -> layer14_mlp_activation
	layer14_mlp_up -> layer14_mlp_activation
	layer14_mlp_activation -> layer14_mlp_down
	layer14_mlp_down -> layer14_residual2
	layer14_residual1 -> layer14_residual2
	comm_14_15 [label="GPU 13 → GPU 14
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer14_residual2 -> comm_14_15
	comm_14_15 -> layer15_norm1
	layer15_norm1 -> layer15_qkv_proj
	layer15_qkv_proj -> layer15_q_split
	layer15_qkv_proj -> layer15_k_split
	layer15_qkv_proj -> layer15_v_split
	layer15_q_split -> layer15_attention
	layer15_k_split -> layer15_attention
	layer15_v_split -> layer15_attention
	layer15_attention -> layer15_attention_concat
	layer15_attention_concat -> layer15_attention_out
	layer15_attention_out -> layer15_residual1
	comm_14_15 -> layer15_residual1
	layer15_residual1 -> layer15_norm2
	layer15_norm2 -> layer15_mlp_gate
	layer15_norm2 -> layer15_mlp_up
	layer15_mlp_gate -> layer15_mlp_activation
	layer15_mlp_up -> layer15_mlp_activation
	layer15_mlp_activation -> layer15_mlp_down
	layer15_mlp_down -> layer15_residual2
	layer15_residual1 -> layer15_residual2
	comm_15_16 [label="GPU 14 → GPU 15
Activation Transfer
[128, 10000, 4096]
900 Gbps" fillcolor=lightpink shape=parallelogram]
	layer15_residual2 -> comm_15_16
	comm_15_16 -> layer16_norm1
	layer16_norm1 -> layer16_qkv_proj
	layer16_qkv_proj -> layer16_q_split
	layer16_qkv_proj -> layer16_k_split
	layer16_qkv_proj -> layer16_v_split
	layer16_q_split -> layer16_attention
	layer16_k_split -> layer16_attention
	layer16_v_split -> layer16_attention
	layer16_attention -> layer16_attention_concat
	layer16_attention_concat -> layer16_attention_out
	layer16_attention_out -> layer16_residual1
	comm_15_16 -> layer16_residual1
	layer16_residual1 -> layer16_norm2
	layer16_norm2 -> layer16_mlp_gate
	layer16_norm2 -> layer16_mlp_up
	layer16_mlp_gate -> layer16_mlp_activation
	layer16_mlp_up -> layer16_mlp_activation
	layer16_mlp_activation -> layer16_mlp_down
	layer16_mlp_down -> layer16_residual2
	layer16_residual1 -> layer16_residual2
	layer16_residual2 -> output
}
