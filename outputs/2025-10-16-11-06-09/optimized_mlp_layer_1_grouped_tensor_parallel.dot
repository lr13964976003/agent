digraph optimized_mlp_layer_1_grouped_tensor_parallel {
    rankdir=TB size="35,45"
    node [fillcolor=lightblue shape=ellipse style=filled]
    
    // Input and output
    mlp_input [label="MLP Layer 1 Input\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 8-15" fillcolor=lightgreen shape=parallelogram]
    mlp_output [label="MLP Layer 1 Output\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 8-15" fillcolor=lightgreen shape=parallelogram]
    
    // LayerNorm - properly connected
    ln [label="LayerNorm\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 8-15" fillcolor=lightyellow shape=rectangle]
    
    // Input split - properly connected
    input_split [label="Split Input\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: 2×[batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 8-15" fillcolor=lightsteelblue shape=parallelogram]
    
    // Grouped tensor parallelism - 2 groups of 4 GPUs each
    
    // Tensor Group 2: GPUs 8-11
    fc1_tg2 [label="FC1 Linear (Grouped Column Parallel)\nTensor Group 2\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=8192]\nGPU: 8-11" fillcolor=lightcoral shape=rectangle]
    gelu_tg2 [label="GELU Activation\nTensor Group 2\nInput: [batch_size=1024, seq_len=10000, hidden_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=8192]\nGPU: 8-11" fillcolor=lightyellow shape=rectangle]
    fc2_split_tg2 [label="FC2 Split (Grouped Row Parallel)\nTensor Group 2\nInput: [batch_size=1024, seq_len=10000, hidden_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 8-11" fillcolor=lightsteelblue shape=parallelogram]
    all_reduce_tg2 [label="All-Reduce\nTensor Group 2\nInput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 8-11" fillcolor=lightsteelblue shape=parallelogram]
    
    // Tensor Group 3: GPUs 12-15
    fc1_tg3 [label="FC1 Linear (Grouped Column Parallel)\nTensor Group 3\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=8192]\nGPU: 12-15" fillcolor=lightcoral shape=rectangle]
    gelu_tg3 [label="GELU Activation\nTensor Group 3\nInput: [batch_size=1024, seq_len=10000, hidden_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=8192]\nGPU: 12-15" fillcolor=lightyellow shape=rectangle]
    fc2_split_tg3 [label="FC2 Split (Grouped Row Parallel)\nTensor Group 3\nInput: [batch_size=1024, seq_len=10000, hidden_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 12-15" fillcolor=lightsteelblue shape=parallelogram]
    all_reduce_tg3 [label="All-Reduce\nTensor Group 3\nInput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 12-15" fillcolor=lightsteelblue shape=parallelogram]
    
    // Final aggregation - properly connected
    final_concat [label="Final Concatenate\nInput: 2×[batch_size=1024, seq_len=10000, embed_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 8-15" fillcolor=lightsteelblue shape=parallelogram]
    residual [label="Residual Add\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]×2\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 8-15" fillcolor=lightgray shape=rectangle]
    
    // Complete flow - all nodes properly connected
    mlp_input -> ln
    ln -> input_split
    input_split -> fc1_tg2
    input_split -> fc1_tg3
    
    fc1_tg2 -> gelu_tg2
    gelu_tg2 -> fc2_split_tg2
    fc2_split_tg2 -> all_reduce_tg2
    all_reduce_tg2 -> final_concat
    
    fc1_tg3 -> gelu_tg3
    gelu_tg3 -> fc2_split_tg3
    fc2_split_tg3 -> all_reduce_tg3
    all_reduce_tg3 -> final_concat
    
    final_concat -> residual
    mlp_input -> residual
    residual -> mlp_output
}