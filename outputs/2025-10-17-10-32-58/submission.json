{
  "generated_dags": [
    {
      "model": "MA Separation",
      "type": "Novel parallel strategy",
      "file_path": "../outputs/2025-10-17-10-32-58/ma_separation_complete.dot",
      "description": "Complete DAG showing MA Separation with 12 GPUs for attention and 4 GPUs for MoE",
      "gpus": {
        "attention": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
        "moe": [12, 13, 14, 15]
      },
      "features": [
        "Attention replication across 12 GPUs",
        "Expert parallelism across 4 GPUs",
        "Detailed dimension tracking",
        "Complete communication paths",
        "No cycles detected",
        "All tensor dimensions specified"
      ]
    },
    {
      "model": "Baseline Hybrid",
      "type": "Tensor Parallelism + Pipeline Parallelism",
      "file_path": "../outputs/2025-10-17-10-32-58/baseline_hybrid.dot",
      "description": "Complete DAG showing baseline with TP=8, PP=2 across 16 GPUs",
      "gpus": {
        "stage_0": [0, 1, 2, 3, 4, 5, 6, 7],
        "stage_1": [8, 9, 10, 11, 12, 13, 14, 15]
      },
      "features": [
        "Tensor parallelism within each stage",
        "Pipeline parallelism between stages",
        "Detailed dimension slicing",
        "Complete communication paths",
        "No cycles detected",
        "All tensor dimensions specified"
      ]
    }
  ],
  "verification": {
    "ma_separation": {
      "total_gpus": 16,
      "attention_gpus": 12,
      "moe_gpus": 4,
      "head_distribution": "32 heads across 12 GPUs",
      "expert_distribution": "16 experts across 4 GPUs",
      "cycles": false
    },
    "baseline": {
      "total_gpus": 16,
      "tp_degree": 8,
      "pp_degree": 2,
      "head_distribution": "32 heads across 8 GPUs per stage",
      "expert_distribution": "16 experts across 8 GPUs per stage",
      "cycles": false
    }
  }
}