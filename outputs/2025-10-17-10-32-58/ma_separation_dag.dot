digraph MA_Separation_Model {
    rankdir=TB;
    node [shape=rectangle];
    
    // Global parameters
    batch_size = 1024;
    seq_len = 2048;
    hidden_dim = 4096;
    num_heads = 32;
    head_dim = 128;
    num_experts = 16;
    expert_hidden_dim = 16384;
    top_k = 2;
    
    // Input node
    Input [shape=ellipse, label="Input\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs"];
    
    // Layer 0
    subgraph cluster_layer0 {
        label="Layer 0";
        style=dashed;
        
        // LayerNorm - replicated across all GPUs
        LayerNorm0 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs"];
        
        // Attention Module - distributed across 12 GPUs
        subgraph cluster_attention0 {
            label="Attention Module (12 GPUs)";
            style=solid;
            
            // QKV Projections - distributed
            Q_proj_0 [label="Q Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 0"];
            K_proj_0 [label="K Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 0"];
            V_proj_0 [label="V Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 0"];
            
            Q_proj_1 [label="Q Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 1"];
            K_proj_1 [label="K Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 1"];
            V_proj_1 [label="V Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 1"];
            
            // Continue pattern for all 12 GPUs
            Q_proj_2 [label="Q Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 2"];
            K_proj_2 [label="K Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 2"];
            V_proj_2 [label="V Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 2"];
            
            Q_proj_3 [label="Q Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 3"];
            K_proj_3 [label="K Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 3"];
            V_proj_3 [label="V Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 3"];
            
            Q_proj_4 [label="Q Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 4"];
            K_proj_4 [label="K Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 4"];
            V_proj_4 [label="V Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 4"];
            
            Q_proj_5 [label="Q Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 5"];
            K_proj_5 [label="K Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 5"];
            V_proj_5 [label="V Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 5"];
            
            Q_proj_6 [label="Q Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 6"];
            K_proj_6 [label="K Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 6"];
            V_proj_6 [label="V Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 6"];
            
            Q_proj_7 [label="Q Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 7"];
            K_proj_7 [label="K Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 7"];
            V_proj_7 [label="V Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 7"];
            
            Q_proj_8 [label="Q Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 8"];
            K_proj_8 [label="K Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 8"];
            V_proj_8 [label="V Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 8"];
            
            Q_proj_9 [label="Q Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 9"];
            K_proj_9 [label="K Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 9"];
            V_proj_9 [label="V Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 9"];
            
            Q_proj_10 [label="Q Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 10"];
            K_proj_10 [label="K Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 10"];
            V_proj_10 [label="V Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 10"];
            
            Q_proj_11 [label="Q Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 11"];
            K_proj_11 [label="K Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 11"];
            V_proj_11 [label="V Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 11"];
            
            // All-gather for K/V across all attention GPUs
            AllGather_KV [shape=parallelogram, label="All-Gather K/V\nInput: [batch_size=1024, seq_len=2048, local_heads, head_dim]\nOutput: [batch_size=1024, seq_len=2048, total_heads=32, head_dim=128]\nGPU: 0-11"];
            
            // Attention computation on each GPU
            Attn_0 [label="Multi-Head Attention\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 0"];
            Attn_1 [label="Multi-Head Attention\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 1"];
            Attn_2 [label="Multi-Head Attention\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 2"];
            Attn_3 [label="Multi-Head Attention\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 3"];
            Attn_4 [label="Multi-Head Attention\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 4"];
            Attn_5 [label="Multi-Head Attention\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 5"];
            Attn_6 [label="Multi-Head Attention\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 6"];
            Attn_7 [label="Multi-Head Attention\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 7"];
            Attn_8 [label="Multi-Head Attention\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 8"];
            Attn_9 [label="Multi-Head Attention\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 9"];
            Attn_10 [label="Multi-Head Attention\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 10"];
            Attn_11 [label="Multi-Head Attention\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 11"];
            
            // All-reduce for attention outputs
            AllReduce_Attn [shape=parallelogram, label="All-Reduce Attention\nInput: [batch_size=1024, seq_len=2048, local_heads, head_dim]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        }
        
        // Output projection for attention
        Attn_Output_Proj [label="Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        // Residual connection
        Residual_Add_Attn [label="Residual Add\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        // LayerNorm for FFN
        LayerNorm_FFN_0 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12-15"];
        
        // MoE Module - distributed across 4 GPUs
        subgraph cluster_moe0 {
            label="MoE Module (4 GPUs)";
            style=solid;
            
            // Gate computation
            Gate_0 [label="Gate\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 12-15"];
            
            // Top-2 selection
            Top2_Select_0 [shape=parallelogram, style=dashed, label="Top-2 Selection\nInput: [batch_size=1024, seq_len=2048, num_experts=16]\nOutput: [batch_size=1024, seq_len=2048, top_k=2]\nGPU: 12-15"];
            
            // Expert routing
            Route_0 [shape=parallelogram, label="All-to-All Routing\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 12-15"];
            
            // Expert computations
            Expert0_GPU12 [label="Expert 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 12"];
            Expert1_GPU12 [label="Expert 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 12"];
            Expert2_GPU12 [label="Expert 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 12"];
            Expert3_GPU12 [label="Expert 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 12"];
            
            Expert4_GPU13 [label="Expert 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 13"];
            Expert5_GPU13 [label="Expert 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 13"];
            Expert6_GPU13 [label="Expert 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 13"];
            Expert7_GPU13 [label="Expert 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 13"];
            
            Expert8_GPU14 [label="Expert 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 14"];
            Expert9_GPU14 [label="Expert 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 14"];
            Expert10_GPU14 [label="Expert 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 14"];
            Expert11_GPU14 [label="Expert 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 14"];
            
            Expert12_GPU15 [label="Expert 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 15"];
            Expert13_GPU15 [label="Expert 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 15"];
            Expert14_GPU15 [label="Expert 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 15"];
            Expert15_GPU15 [label="Expert 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nGPU: 15"];
            
            // Expert output aggregation
            Aggregate_Experts_0 [shape=parallelogram, label="All-to-All Aggregation\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12-15"];
            
            // Expert output projection
            Expert_Output_Proj_0 [label="Expert Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12-15"];
        }
        
        // Final residual connection
        Residual_Add_FFN_0 [label="Residual Add\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12-15"];
        
        // Synchronization point
        Sync_0 [shape=ellipse, label="Layer 0 Synchronization\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs"];
    }
    
    // Layer 1 (similar structure)
    subgraph cluster_layer1 {
        label="Layer 1";
        style=dashed;
        
        LayerNorm1 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs"];
        
        // Attention components (replicated from layer 0)
        // ... [similar to layer 0]
        
        // MoE components (replicated from layer 0)
        // ... [similar to layer 0]
        
        Sync_1 [shape=ellipse, label="Layer 1 Synchronization\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs"];
    }
    
    // Layer 2
    subgraph cluster_layer2 {
        label="Layer 2";
        style=dashed;
        
        LayerNorm2 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs"];
        
        // Similar structure as layer 0
        Sync_2 [shape=ellipse, label="Layer 2 Synchronization\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs"];
    }
    
    // Layer 3
    subgraph cluster_layer3 {
        label="Layer 3";
        style=dashed;
        
        LayerNorm3 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs"];
        
        // Similar structure as layer 0
        Sync_3 [shape=ellipse, label="Layer 3 Synchronization\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs"];
    }
    
    // Output
    Output [shape=ellipse, label="Output\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs"];
    
    // Connections
    Input -> LayerNorm0;
    LayerNorm0 -> {Q_proj_0 K_proj_0 V_proj_0 Q_proj_1 K_proj_1 V_proj_1};
    // ... complete the connections for all projections
    
    // For brevity, showing key connections
    AllReduce_Attn -> Attn_Output_Proj;
    Attn_Output_Proj -> Residual_Add_Attn;
    Residual_Add_Attn -> LayerNorm_FFN_0;
    LayerNorm_FFN_0 -> Gate_0;
    Gate_0 -> Top2_Select_0 [style=dashed];
    Top2_Select_0 -> Route_0;
    Route_0 -> {Expert0_GPU12 Expert1_GPU12 ... Expert15_GPU15};
    Expert0_GPU12 -> Aggregate_Experts_0;
    Aggregate_Experts_0 -> Expert_Output_Proj_0;
    Expert_Output_Proj_0 -> Residual_Add_FFN_0;
    Residual_Add_FFN_0 -> Sync_0;
    Sync_0 -> LayerNorm1;
    // ... continue for all layers
    Sync_3 -> Output;
}