// MoE Baseline with Tensor Parallel=8, Pipeline Parallel=2
digraph MoE_Baseline_TP8_PP2 {
	rankdir=TB size="20,30"
	node [fillcolor=lightblue shape=rectangle style=filled]
	input [label="Model Input\nbatch_size=128, seq_len=10000, token_dim=4096" fillcolor=lightgreen shape=ellipse]
	node [fillcolor=lightblue shape=rectangle]
	subgraph cluster_stage0 {
		color=red label="Pipeline Stage 0\nGPUs 0-7" style=dashed
		layer_0_mha [label="Layer 0\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 0-7"]
		layer_0_gate [label="Layer 0\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_0_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
		layer_0_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
		layer_0_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
		layer_0_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
		layer_0_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
		layer_0_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
		layer_0_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
		layer_0_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
		layer_0_expert_agg [label="Layer 0\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_0_residual [label="Layer 0\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_1_mha [label="Layer 1\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 0-7"]
		layer_1_gate [label="Layer 1\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_1_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
		layer_1_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
		layer_1_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
		layer_1_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
		layer_1_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
		layer_1_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
		layer_1_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
		layer_1_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
		layer_1_expert_agg [label="Layer 1\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_1_residual [label="Layer 1\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_2_mha [label="Layer 2\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 0-7"]
		layer_2_gate [label="Layer 2\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_2_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
		layer_2_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
		layer_2_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
		layer_2_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
		layer_2_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
		layer_2_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
		layer_2_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
		layer_2_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
		layer_2_expert_agg [label="Layer 2\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_2_residual [label="Layer 2\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_3_mha [label="Layer 3\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 0-7"]
		layer_3_gate [label="Layer 3\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_3_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
		layer_3_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
		layer_3_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
		layer_3_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
		layer_3_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
		layer_3_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
		layer_3_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
		layer_3_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
		layer_3_expert_agg [label="Layer 3\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_3_residual [label="Layer 3\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_4_mha [label="Layer 4\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 0-7"]
		layer_4_gate [label="Layer 4\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_4_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
		layer_4_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
		layer_4_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
		layer_4_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
		layer_4_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
		layer_4_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
		layer_4_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
		layer_4_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
		layer_4_expert_agg [label="Layer 4\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_4_residual [label="Layer 4\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_5_mha [label="Layer 5\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 0-7"]
		layer_5_gate [label="Layer 5\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_5_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
		layer_5_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
		layer_5_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
		layer_5_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
		layer_5_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
		layer_5_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
		layer_5_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
		layer_5_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
		layer_5_expert_agg [label="Layer 5\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_5_residual [label="Layer 5\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_6_mha [label="Layer 6\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 0-7"]
		layer_6_gate [label="Layer 6\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_6_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
		layer_6_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
		layer_6_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
		layer_6_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
		layer_6_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
		layer_6_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
		layer_6_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
		layer_6_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
		layer_6_expert_agg [label="Layer 6\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_6_residual [label="Layer 6\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_7_mha [label="Layer 7\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 0-7"]
		layer_7_gate [label="Layer 7\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_7_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
		layer_7_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
		layer_7_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
		layer_7_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
		layer_7_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
		layer_7_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
		layer_7_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
		layer_7_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
		layer_7_expert_agg [label="Layer 7\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_7_residual [label="Layer 7\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
	}
	subgraph cluster_stage1 {
		color=blue label="Pipeline Stage 1\nGPUs 8-15" style=dashed
		layer_8_mha [label="Layer 8\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 8-15"]
		layer_8_gate [label="Layer 8\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_8_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
		layer_8_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
		layer_8_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
		layer_8_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
		layer_8_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
		layer_8_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
		layer_8_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
		layer_8_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
		layer_8_expert_agg [label="Layer 8\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_8_residual [label="Layer 8\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_9_mha [label="Layer 9\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 8-15"]
		layer_9_gate [label="Layer 9\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_9_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
		layer_9_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
		layer_9_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
		layer_9_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
		layer_9_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
		layer_9_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
		layer_9_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
		layer_9_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
		layer_9_expert_agg [label="Layer 9\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_9_residual [label="Layer 9\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_10_mha [label="Layer 10\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 8-15"]
		layer_10_gate [label="Layer 10\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_10_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
		layer_10_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
		layer_10_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
		layer_10_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
		layer_10_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
		layer_10_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
		layer_10_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
		layer_10_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
		layer_10_expert_agg [label="Layer 10\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_10_residual [label="Layer 10\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_11_mha [label="Layer 11\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 8-15"]
		layer_11_gate [label="Layer 11\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_11_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
		layer_11_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
		layer_11_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
		layer_11_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
		layer_11_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
		layer_11_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
		layer_11_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
		layer_11_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
		layer_11_expert_agg [label="Layer 11\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_11_residual [label="Layer 11\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_12_mha [label="Layer 12\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 8-15"]
		layer_12_gate [label="Layer 12\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_12_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
		layer_12_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
		layer_12_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
		layer_12_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
		layer_12_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
		layer_12_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
		layer_12_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
		layer_12_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
		layer_12_expert_agg [label="Layer 12\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_12_residual [label="Layer 12\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_13_mha [label="Layer 13\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 8-15"]
		layer_13_gate [label="Layer 13\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_13_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
		layer_13_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
		layer_13_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
		layer_13_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
		layer_13_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
		layer_13_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
		layer_13_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
		layer_13_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
		layer_13_expert_agg [label="Layer 13\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_13_residual [label="Layer 13\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_14_mha [label="Layer 14\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 8-15"]
		layer_14_gate [label="Layer 14\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_14_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
		layer_14_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
		layer_14_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
		layer_14_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
		layer_14_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
		layer_14_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
		layer_14_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
		layer_14_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
		layer_14_expert_agg [label="Layer 14\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_14_residual [label="Layer 14\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_15_mha [label="Layer 15\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nTP=8 across GPUs 8-15"]
		layer_15_gate [label="Layer 15\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nGPU: all 8 GPUs"]
		layer_15_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
		layer_15_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
		layer_15_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
		layer_15_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
		layer_15_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
		layer_15_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
		layer_15_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
		layer_15_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
		layer_15_expert_agg [label="Layer 15\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
		layer_15_residual [label="Layer 15\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPU: all 8 GPUs"]
	}
	output [label="Model Output\nbatch_size=128, seq_len=10000, token_dim=4096" fillcolor=lightgreen shape=ellipse]
	input -> layer_0_mha
	layer_0_mha -> layer_0_gate
	layer_0_gate -> layer_0_expert_0 [label="routed tokens" style=dashed]
	layer_0_gate -> layer_0_expert_1 [label="routed tokens" style=dashed]
	layer_0_gate -> layer_0_expert_2 [label="routed tokens" style=dashed]
	layer_0_gate -> layer_0_expert_3 [label="routed tokens" style=dashed]
	layer_0_gate -> layer_0_expert_4 [label="routed tokens" style=dashed]
	layer_0_gate -> layer_0_expert_5 [label="routed tokens" style=dashed]
	layer_0_gate -> layer_0_expert_6 [label="routed tokens" style=dashed]
	layer_0_gate -> layer_0_expert_7 [label="routed tokens" style=dashed]
	layer_0_expert_0 -> layer_0_expert_agg
	layer_0_expert_1 -> layer_0_expert_agg
	layer_0_expert_2 -> layer_0_expert_agg
	layer_0_expert_3 -> layer_0_expert_agg
	layer_0_expert_4 -> layer_0_expert_agg
	layer_0_expert_5 -> layer_0_expert_agg
	layer_0_expert_6 -> layer_0_expert_agg
	layer_0_expert_7 -> layer_0_expert_agg
	layer_0_mha -> layer_0_residual [label=identity]
	layer_0_expert_agg -> layer_0_residual
	layer_0_residual -> layer_1_mha
	layer_1_mha -> layer_1_gate
	layer_1_gate -> layer_1_expert_0 [label="routed tokens" style=dashed]
	layer_1_gate -> layer_1_expert_1 [label="routed tokens" style=dashed]
	layer_1_gate -> layer_1_expert_2 [label="routed tokens" style=dashed]
	layer_1_gate -> layer_1_expert_3 [label="routed tokens" style=dashed]
	layer_1_gate -> layer_1_expert_4 [label="routed tokens" style=dashed]
	layer_1_gate -> layer_1_expert_5 [label="routed tokens" style=dashed]
	layer_1_gate -> layer_1_expert_6 [label="routed tokens" style=dashed]
	layer_1_gate -> layer_1_expert_7 [label="routed tokens" style=dashed]
	layer_1_expert_0 -> layer_1_expert_agg
	layer_1_expert_1 -> layer_1_expert_agg
	layer_1_expert_2 -> layer_1_expert_agg
	layer_1_expert_3 -> layer_1_expert_agg
	layer_1_expert_4 -> layer_1_expert_agg
	layer_1_expert_5 -> layer_1_expert_agg
	layer_1_expert_6 -> layer_1_expert_agg
	layer_1_expert_7 -> layer_1_expert_agg
	layer_1_mha -> layer_1_residual [label=identity]
	layer_1_expert_agg -> layer_1_residual
	layer_1_residual -> layer_2_mha
	layer_2_mha -> layer_2_gate
	layer_2_gate -> layer_2_expert_0 [label="routed tokens" style=dashed]
	layer_2_gate -> layer_2_expert_1 [label="routed tokens" style=dashed]
	layer_2_gate -> layer_2_expert_2 [label="routed tokens" style=dashed]
	layer_2_gate -> layer_2_expert_3 [label="routed tokens" style=dashed]
	layer_2_gate -> layer_2_expert_4 [label="routed tokens" style=dashed]
	layer_2_gate -> layer_2_expert_5 [label="routed tokens" style=dashed]
	layer_2_gate -> layer_2_expert_6 [label="routed tokens" style=dashed]
	layer_2_gate -> layer_2_expert_7 [label="routed tokens" style=dashed]
	layer_2_expert_0 -> layer_2_expert_agg
	layer_2_expert_1 -> layer_2_expert_agg
	layer_2_expert_2 -> layer_2_expert_agg
	layer_2_expert_3 -> layer_2_expert_agg
	layer_2_expert_4 -> layer_2_expert_agg
	layer_2_expert_5 -> layer_2_expert_agg
	layer_2_expert_6 -> layer_2_expert_agg
	layer_2_expert_7 -> layer_2_expert_agg
	layer_2_mha -> layer_2_residual [label=identity]
	layer_2_expert_agg -> layer_2_residual
	layer_2_residual -> layer_3_mha
	layer_3_mha -> layer_3_gate
	layer_3_gate -> layer_3_expert_0 [label="routed tokens" style=dashed]
	layer_3_gate -> layer_3_expert_1 [label="routed tokens" style=dashed]
	layer_3_gate -> layer_3_expert_2 [label="routed tokens" style=dashed]
	layer_3_gate -> layer_3_expert_3 [label="routed tokens" style=dashed]
	layer_3_gate -> layer_3_expert_4 [label="routed tokens" style=dashed]
	layer_3_gate -> layer_3_expert_5 [label="routed tokens" style=dashed]
	layer_3_gate -> layer_3_expert_6 [label="routed tokens" style=dashed]
	layer_3_gate -> layer_3_expert_7 [label="routed tokens" style=dashed]
	layer_3_expert_0 -> layer_3_expert_agg
	layer_3_expert_1 -> layer_3_expert_agg
	layer_3_expert_2 -> layer_3_expert_agg
	layer_3_expert_3 -> layer_3_expert_agg
	layer_3_expert_4 -> layer_3_expert_agg
	layer_3_expert_5 -> layer_3_expert_agg
	layer_3_expert_6 -> layer_3_expert_agg
	layer_3_expert_7 -> layer_3_expert_agg
	layer_3_mha -> layer_3_residual [label=identity]
	layer_3_expert_agg -> layer_3_residual
	layer_3_residual -> layer_4_mha
	layer_4_mha -> layer_4_gate
	layer_4_gate -> layer_4_expert_0 [label="routed tokens" style=dashed]
	layer_4_gate -> layer_4_expert_1 [label="routed tokens" style=dashed]
	layer_4_gate -> layer_4_expert_2 [label="routed tokens" style=dashed]
	layer_4_gate -> layer_4_expert_3 [label="routed tokens" style=dashed]
	layer_4_gate -> layer_4_expert_4 [label="routed tokens" style=dashed]
	layer_4_gate -> layer_4_expert_5 [label="routed tokens" style=dashed]
	layer_4_gate -> layer_4_expert_6 [label="routed tokens" style=dashed]
	layer_4_gate -> layer_4_expert_7 [label="routed tokens" style=dashed]
	layer_4_expert_0 -> layer_4_expert_agg
	layer_4_expert_1 -> layer_4_expert_agg
	layer_4_expert_2 -> layer_4_expert_agg
	layer_4_expert_3 -> layer_4_expert_agg
	layer_4_expert_4 -> layer_4_expert_agg
	layer_4_expert_5 -> layer_4_expert_agg
	layer_4_expert_6 -> layer_4_expert_agg
	layer_4_expert_7 -> layer_4_expert_agg
	layer_4_mha -> layer_4_residual [label=identity]
	layer_4_expert_agg -> layer_4_residual
	layer_4_residual -> layer_5_mha
	layer_5_mha -> layer_5_gate
	layer_5_gate -> layer_5_expert_0 [label="routed tokens" style=dashed]
	layer_5_gate -> layer_5_expert_1 [label="routed tokens" style=dashed]
	layer_5_gate -> layer_5_expert_2 [label="routed tokens" style=dashed]
	layer_5_gate -> layer_5_expert_3 [label="routed tokens" style=dashed]
	layer_5_gate -> layer_5_expert_4 [label="routed tokens" style=dashed]
	layer_5_gate -> layer_5_expert_5 [label="routed tokens" style=dashed]
	layer_5_gate -> layer_5_expert_6 [label="routed tokens" style=dashed]
	layer_5_gate -> layer_5_expert_7 [label="routed tokens" style=dashed]
	layer_5_expert_0 -> layer_5_expert_agg
	layer_5_expert_1 -> layer_5_expert_agg
	layer_5_expert_2 -> layer_5_expert_agg
	layer_5_expert_3 -> layer_5_expert_agg
	layer_5_expert_4 -> layer_5_expert_agg
	layer_5_expert_5 -> layer_5_expert_agg
	layer_5_expert_6 -> layer_5_expert_agg
	layer_5_expert_7 -> layer_5_expert_agg
	layer_5_mha -> layer_5_residual [label=identity]
	layer_5_expert_agg -> layer_5_residual
	layer_5_residual -> layer_6_mha
	layer_6_mha -> layer_6_gate
	layer_6_gate -> layer_6_expert_0 [label="routed tokens" style=dashed]
	layer_6_gate -> layer_6_expert_1 [label="routed tokens" style=dashed]
	layer_6_gate -> layer_6_expert_2 [label="routed tokens" style=dashed]
	layer_6_gate -> layer_6_expert_3 [label="routed tokens" style=dashed]
	layer_6_gate -> layer_6_expert_4 [label="routed tokens" style=dashed]
	layer_6_gate -> layer_6_expert_5 [label="routed tokens" style=dashed]
	layer_6_gate -> layer_6_expert_6 [label="routed tokens" style=dashed]
	layer_6_gate -> layer_6_expert_7 [label="routed tokens" style=dashed]
	layer_6_expert_0 -> layer_6_expert_agg
	layer_6_expert_1 -> layer_6_expert_agg
	layer_6_expert_2 -> layer_6_expert_agg
	layer_6_expert_3 -> layer_6_expert_agg
	layer_6_expert_4 -> layer_6_expert_agg
	layer_6_expert_5 -> layer_6_expert_agg
	layer_6_expert_6 -> layer_6_expert_agg
	layer_6_expert_7 -> layer_6_expert_agg
	layer_6_mha -> layer_6_residual [label=identity]
	layer_6_expert_agg -> layer_6_residual
	layer_6_residual -> layer_7_mha
	layer_7_mha -> layer_7_gate
	layer_7_gate -> layer_7_expert_0 [label="routed tokens" style=dashed]
	layer_7_gate -> layer_7_expert_1 [label="routed tokens" style=dashed]
	layer_7_gate -> layer_7_expert_2 [label="routed tokens" style=dashed]
	layer_7_gate -> layer_7_expert_3 [label="routed tokens" style=dashed]
	layer_7_gate -> layer_7_expert_4 [label="routed tokens" style=dashed]
	layer_7_gate -> layer_7_expert_5 [label="routed tokens" style=dashed]
	layer_7_gate -> layer_7_expert_6 [label="routed tokens" style=dashed]
	layer_7_gate -> layer_7_expert_7 [label="routed tokens" style=dashed]
	layer_7_expert_0 -> layer_7_expert_agg
	layer_7_expert_1 -> layer_7_expert_agg
	layer_7_expert_2 -> layer_7_expert_agg
	layer_7_expert_3 -> layer_7_expert_agg
	layer_7_expert_4 -> layer_7_expert_agg
	layer_7_expert_5 -> layer_7_expert_agg
	layer_7_expert_6 -> layer_7_expert_agg
	layer_7_expert_7 -> layer_7_expert_agg
	layer_7_mha -> layer_7_residual [label=identity]
	layer_7_expert_agg -> layer_7_residual
	layer_7_residual -> layer_8_mha
	layer_8_mha -> layer_8_gate
	layer_8_gate -> layer_8_expert_0 [label="routed tokens" style=dashed]
	layer_8_gate -> layer_8_expert_1 [label="routed tokens" style=dashed]
	layer_8_gate -> layer_8_expert_2 [label="routed tokens" style=dashed]
	layer_8_gate -> layer_8_expert_3 [label="routed tokens" style=dashed]
	layer_8_gate -> layer_8_expert_4 [label="routed tokens" style=dashed]
	layer_8_gate -> layer_8_expert_5 [label="routed tokens" style=dashed]
	layer_8_gate -> layer_8_expert_6 [label="routed tokens" style=dashed]
	layer_8_gate -> layer_8_expert_7 [label="routed tokens" style=dashed]
	layer_8_expert_0 -> layer_8_expert_agg
	layer_8_expert_1 -> layer_8_expert_agg
	layer_8_expert_2 -> layer_8_expert_agg
	layer_8_expert_3 -> layer_8_expert_agg
	layer_8_expert_4 -> layer_8_expert_agg
	layer_8_expert_5 -> layer_8_expert_agg
	layer_8_expert_6 -> layer_8_expert_agg
	layer_8_expert_7 -> layer_8_expert_agg
	layer_8_mha -> layer_8_residual [label=identity]
	layer_8_expert_agg -> layer_8_residual
	layer_8_residual -> layer_9_mha
	layer_9_mha -> layer_9_gate
	layer_9_gate -> layer_9_expert_0 [label="routed tokens" style=dashed]
	layer_9_gate -> layer_9_expert_1 [label="routed tokens" style=dashed]
	layer_9_gate -> layer_9_expert_2 [label="routed tokens" style=dashed]
	layer_9_gate -> layer_9_expert_3 [label="routed tokens" style=dashed]
	layer_9_gate -> layer_9_expert_4 [label="routed tokens" style=dashed]
	layer_9_gate -> layer_9_expert_5 [label="routed tokens" style=dashed]
	layer_9_gate -> layer_9_expert_6 [label="routed tokens" style=dashed]
	layer_9_gate -> layer_9_expert_7 [label="routed tokens" style=dashed]
	layer_9_expert_0 -> layer_9_expert_agg
	layer_9_expert_1 -> layer_9_expert_agg
	layer_9_expert_2 -> layer_9_expert_agg
	layer_9_expert_3 -> layer_9_expert_agg
	layer_9_expert_4 -> layer_9_expert_agg
	layer_9_expert_5 -> layer_9_expert_agg
	layer_9_expert_6 -> layer_9_expert_agg
	layer_9_expert_7 -> layer_9_expert_agg
	layer_9_mha -> layer_9_residual [label=identity]
	layer_9_expert_agg -> layer_9_residual
	layer_9_residual -> layer_10_mha
	layer_10_mha -> layer_10_gate
	layer_10_gate -> layer_10_expert_0 [label="routed tokens" style=dashed]
	layer_10_gate -> layer_10_expert_1 [label="routed tokens" style=dashed]
	layer_10_gate -> layer_10_expert_2 [label="routed tokens" style=dashed]
	layer_10_gate -> layer_10_expert_3 [label="routed tokens" style=dashed]
	layer_10_gate -> layer_10_expert_4 [label="routed tokens" style=dashed]
	layer_10_gate -> layer_10_expert_5 [label="routed tokens" style=dashed]
	layer_10_gate -> layer_10_expert_6 [label="routed tokens" style=dashed]
	layer_10_gate -> layer_10_expert_7 [label="routed tokens" style=dashed]
	layer_10_expert_0 -> layer_10_expert_agg
	layer_10_expert_1 -> layer_10_expert_agg
	layer_10_expert_2 -> layer_10_expert_agg
	layer_10_expert_3 -> layer_10_expert_agg
	layer_10_expert_4 -> layer_10_expert_agg
	layer_10_expert_5 -> layer_10_expert_agg
	layer_10_expert_6 -> layer_10_expert_agg
	layer_10_expert_7 -> layer_10_expert_agg
	layer_10_mha -> layer_10_residual [label=identity]
	layer_10_expert_agg -> layer_10_residual
	layer_10_residual -> layer_11_mha
	layer_11_mha -> layer_11_gate
	layer_11_gate -> layer_11_expert_0 [label="routed tokens" style=dashed]
	layer_11_gate -> layer_11_expert_1 [label="routed tokens" style=dashed]
	layer_11_gate -> layer_11_expert_2 [label="routed tokens" style=dashed]
	layer_11_gate -> layer_11_expert_3 [label="routed tokens" style=dashed]
	layer_11_gate -> layer_11_expert_4 [label="routed tokens" style=dashed]
	layer_11_gate -> layer_11_expert_5 [label="routed tokens" style=dashed]
	layer_11_gate -> layer_11_expert_6 [label="routed tokens" style=dashed]
	layer_11_gate -> layer_11_expert_7 [label="routed tokens" style=dashed]
	layer_11_expert_0 -> layer_11_expert_agg
	layer_11_expert_1 -> layer_11_expert_agg
	layer_11_expert_2 -> layer_11_expert_agg
	layer_11_expert_3 -> layer_11_expert_agg
	layer_11_expert_4 -> layer_11_expert_agg
	layer_11_expert_5 -> layer_11_expert_agg
	layer_11_expert_6 -> layer_11_expert_agg
	layer_11_expert_7 -> layer_11_expert_agg
	layer_11_mha -> layer_11_residual [label=identity]
	layer_11_expert_agg -> layer_11_residual
	layer_11_residual -> layer_12_mha
	layer_12_mha -> layer_12_gate
	layer_12_gate -> layer_12_expert_0 [label="routed tokens" style=dashed]
	layer_12_gate -> layer_12_expert_1 [label="routed tokens" style=dashed]
	layer_12_gate -> layer_12_expert_2 [label="routed tokens" style=dashed]
	layer_12_gate -> layer_12_expert_3 [label="routed tokens" style=dashed]
	layer_12_gate -> layer_12_expert_4 [label="routed tokens" style=dashed]
	layer_12_gate -> layer_12_expert_5 [label="routed tokens" style=dashed]
	layer_12_gate -> layer_12_expert_6 [label="routed tokens" style=dashed]
	layer_12_gate -> layer_12_expert_7 [label="routed tokens" style=dashed]
	layer_12_expert_0 -> layer_12_expert_agg
	layer_12_expert_1 -> layer_12_expert_agg
	layer_12_expert_2 -> layer_12_expert_agg
	layer_12_expert_3 -> layer_12_expert_agg
	layer_12_expert_4 -> layer_12_expert_agg
	layer_12_expert_5 -> layer_12_expert_agg
	layer_12_expert_6 -> layer_12_expert_agg
	layer_12_expert_7 -> layer_12_expert_agg
	layer_12_mha -> layer_12_residual [label=identity]
	layer_12_expert_agg -> layer_12_residual
	layer_12_residual -> layer_13_mha
	layer_13_mha -> layer_13_gate
	layer_13_gate -> layer_13_expert_0 [label="routed tokens" style=dashed]
	layer_13_gate -> layer_13_expert_1 [label="routed tokens" style=dashed]
	layer_13_gate -> layer_13_expert_2 [label="routed tokens" style=dashed]
	layer_13_gate -> layer_13_expert_3 [label="routed tokens" style=dashed]
	layer_13_gate -> layer_13_expert_4 [label="routed tokens" style=dashed]
	layer_13_gate -> layer_13_expert_5 [label="routed tokens" style=dashed]
	layer_13_gate -> layer_13_expert_6 [label="routed tokens" style=dashed]
	layer_13_gate -> layer_13_expert_7 [label="routed tokens" style=dashed]
	layer_13_expert_0 -> layer_13_expert_agg
	layer_13_expert_1 -> layer_13_expert_agg
	layer_13_expert_2 -> layer_13_expert_agg
	layer_13_expert_3 -> layer_13_expert_agg
	layer_13_expert_4 -> layer_13_expert_agg
	layer_13_expert_5 -> layer_13_expert_agg
	layer_13_expert_6 -> layer_13_expert_agg
	layer_13_expert_7 -> layer_13_expert_agg
	layer_13_mha -> layer_13_residual [label=identity]
	layer_13_expert_agg -> layer_13_residual
	layer_13_residual -> layer_14_mha
	layer_14_mha -> layer_14_gate
	layer_14_gate -> layer_14_expert_0 [label="routed tokens" style=dashed]
	layer_14_gate -> layer_14_expert_1 [label="routed tokens" style=dashed]
	layer_14_gate -> layer_14_expert_2 [label="routed tokens" style=dashed]
	layer_14_gate -> layer_14_expert_3 [label="routed tokens" style=dashed]
	layer_14_gate -> layer_14_expert_4 [label="routed tokens" style=dashed]
	layer_14_gate -> layer_14_expert_5 [label="routed tokens" style=dashed]
	layer_14_gate -> layer_14_expert_6 [label="routed tokens" style=dashed]
	layer_14_gate -> layer_14_expert_7 [label="routed tokens" style=dashed]
	layer_14_expert_0 -> layer_14_expert_agg
	layer_14_expert_1 -> layer_14_expert_agg
	layer_14_expert_2 -> layer_14_expert_agg
	layer_14_expert_3 -> layer_14_expert_agg
	layer_14_expert_4 -> layer_14_expert_agg
	layer_14_expert_5 -> layer_14_expert_agg
	layer_14_expert_6 -> layer_14_expert_agg
	layer_14_expert_7 -> layer_14_expert_agg
	layer_14_mha -> layer_14_residual [label=identity]
	layer_14_expert_agg -> layer_14_residual
	layer_14_residual -> layer_15_mha
	layer_15_mha -> layer_15_gate
	layer_15_gate -> layer_15_expert_0 [label="routed tokens" style=dashed]
	layer_15_gate -> layer_15_expert_1 [label="routed tokens" style=dashed]
	layer_15_gate -> layer_15_expert_2 [label="routed tokens" style=dashed]
	layer_15_gate -> layer_15_expert_3 [label="routed tokens" style=dashed]
	layer_15_gate -> layer_15_expert_4 [label="routed tokens" style=dashed]
	layer_15_gate -> layer_15_expert_5 [label="routed tokens" style=dashed]
	layer_15_gate -> layer_15_expert_6 [label="routed tokens" style=dashed]
	layer_15_gate -> layer_15_expert_7 [label="routed tokens" style=dashed]
	layer_15_expert_0 -> layer_15_expert_agg
	layer_15_expert_1 -> layer_15_expert_agg
	layer_15_expert_2 -> layer_15_expert_agg
	layer_15_expert_3 -> layer_15_expert_agg
	layer_15_expert_4 -> layer_15_expert_agg
	layer_15_expert_5 -> layer_15_expert_agg
	layer_15_expert_6 -> layer_15_expert_agg
	layer_15_expert_7 -> layer_15_expert_agg
	layer_15_mha -> layer_15_residual [label=identity]
	layer_15_expert_agg -> layer_15_residual
	layer_15_residual -> output
	layer_7_residual -> layer_8_mha [label="Pipeline stage boundary\nGPU 7 â†’ GPU 8"]
}
