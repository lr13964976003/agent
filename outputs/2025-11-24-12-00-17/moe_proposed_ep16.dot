// MoE with Expert Parallel=16 (1 expert per GPU)
digraph MoE_Large_EP16_CrossNode {
	rankdir=TB size="25,30"
	node [fillcolor=lightblue shape=rectangle style=filled]
	input [label="Model Input\nbatch_size=128, seq_len=10000, token_dim=4096" fillcolor=lightgreen shape=ellipse]
	layer_0_mha [label="Layer 0\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_0_gate [label="Layer 0\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_0_all2all [label="Layer 0\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_0_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_0_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_0_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_0_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_0_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_0_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_0_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_0_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_0_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_0_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_0_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_0_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_0_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_0_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_0_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_0_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_0_all2all_result [label="Layer 0\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_0_expert_agg [label="Layer 0\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_0_residual [label="Layer 0\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_1_mha [label="Layer 1\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_1_gate [label="Layer 1\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_1_all2all [label="Layer 1\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_1_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_1_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_1_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_1_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_1_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_1_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_1_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_1_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_1_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_1_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_1_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_1_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_1_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_1_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_1_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_1_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_1_all2all_result [label="Layer 1\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_1_expert_agg [label="Layer 1\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_1_residual [label="Layer 1\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_2_mha [label="Layer 2\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_2_gate [label="Layer 2\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_2_all2all [label="Layer 2\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_2_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_2_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_2_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_2_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_2_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_2_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_2_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_2_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_2_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_2_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_2_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_2_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_2_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_2_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_2_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_2_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_2_all2all_result [label="Layer 2\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_2_expert_agg [label="Layer 2\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_2_residual [label="Layer 2\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_3_mha [label="Layer 3\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_3_gate [label="Layer 3\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_3_all2all [label="Layer 3\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_3_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_3_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_3_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_3_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_3_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_3_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_3_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_3_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_3_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_3_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_3_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_3_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_3_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_3_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_3_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_3_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_3_all2all_result [label="Layer 3\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_3_expert_agg [label="Layer 3\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_3_residual [label="Layer 3\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_4_mha [label="Layer 4\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_4_gate [label="Layer 4\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_4_all2all [label="Layer 4\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_4_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_4_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_4_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_4_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_4_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_4_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_4_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_4_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_4_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_4_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_4_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_4_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_4_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_4_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_4_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_4_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_4_all2all_result [label="Layer 4\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_4_expert_agg [label="Layer 4\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_4_residual [label="Layer 4\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_5_mha [label="Layer 5\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_5_gate [label="Layer 5\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_5_all2all [label="Layer 5\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_5_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_5_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_5_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_5_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_5_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_5_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_5_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_5_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_5_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_5_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_5_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_5_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_5_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_5_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_5_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_5_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_5_all2all_result [label="Layer 5\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_5_expert_agg [label="Layer 5\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_5_residual [label="Layer 5\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_6_mha [label="Layer 6\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_6_gate [label="Layer 6\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_6_all2all [label="Layer 6\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_6_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_6_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_6_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_6_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_6_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_6_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_6_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_6_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_6_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_6_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_6_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_6_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_6_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_6_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_6_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_6_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_6_all2all_result [label="Layer 6\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_6_expert_agg [label="Layer 6\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_6_residual [label="Layer 6\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_7_mha [label="Layer 7\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_7_gate [label="Layer 7\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_7_all2all [label="Layer 7\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_7_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_7_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_7_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_7_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_7_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_7_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_7_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_7_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_7_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_7_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_7_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_7_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_7_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_7_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_7_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_7_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_7_all2all_result [label="Layer 7\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_7_expert_agg [label="Layer 7\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_7_residual [label="Layer 7\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_8_mha [label="Layer 8\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_8_gate [label="Layer 8\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_8_all2all [label="Layer 8\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_8_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_8_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_8_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_8_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_8_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_8_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_8_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_8_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_8_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_8_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_8_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_8_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_8_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_8_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_8_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_8_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_8_all2all_result [label="Layer 8\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_8_expert_agg [label="Layer 8\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_8_residual [label="Layer 8\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_9_mha [label="Layer 9\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_9_gate [label="Layer 9\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_9_all2all [label="Layer 9\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_9_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_9_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_9_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_9_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_9_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_9_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_9_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_9_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_9_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_9_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_9_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_9_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_9_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_9_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_9_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_9_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_9_all2all_result [label="Layer 9\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_9_expert_agg [label="Layer 9\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_9_residual [label="Layer 9\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_10_mha [label="Layer 10\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_10_gate [label="Layer 10\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_10_all2all [label="Layer 10\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_10_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_10_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_10_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_10_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_10_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_10_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_10_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_10_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_10_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_10_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_10_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_10_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_10_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_10_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_10_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_10_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_10_all2all_result [label="Layer 10\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_10_expert_agg [label="Layer 10\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_10_residual [label="Layer 10\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_11_mha [label="Layer 11\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_11_gate [label="Layer 11\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_11_all2all [label="Layer 11\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_11_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_11_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_11_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_11_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_11_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_11_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_11_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_11_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_11_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_11_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_11_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_11_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_11_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_11_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_11_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_11_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_11_all2all_result [label="Layer 11\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_11_expert_agg [label="Layer 11\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_11_residual [label="Layer 11\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_12_mha [label="Layer 12\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_12_gate [label="Layer 12\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_12_all2all [label="Layer 12\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_12_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_12_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_12_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_12_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_12_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_12_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_12_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_12_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_12_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_12_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_12_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_12_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_12_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_12_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_12_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_12_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_12_all2all_result [label="Layer 12\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_12_expert_agg [label="Layer 12\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_12_residual [label="Layer 12\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_13_mha [label="Layer 13\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_13_gate [label="Layer 13\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_13_all2all [label="Layer 13\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_13_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_13_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_13_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_13_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_13_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_13_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_13_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_13_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_13_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_13_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_13_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_13_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_13_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_13_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_13_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_13_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_13_all2all_result [label="Layer 13\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_13_expert_agg [label="Layer 13\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_13_residual [label="Layer 13\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_14_mha [label="Layer 14\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_14_gate [label="Layer 14\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_14_all2all [label="Layer 14\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_14_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_14_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_14_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_14_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_14_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_14_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_14_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_14_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_14_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_14_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_14_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_14_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_14_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_14_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_14_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_14_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_14_all2all_result [label="Layer 14\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_14_expert_agg [label="Layer 14\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_14_residual [label="Layer 14\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_15_mha [label="Layer 15\nMulti-Head Attention\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_15_gate [label="Layer 15\nExpert Gate\nInput: [128,10000,4096]\nOutput: routing decisions\nAll 16 GPUs"]
	layer_15_all2all [label="Layer 15\nAll-to-All Communication\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_15_expert_0 [label="Expert 0\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 0"]
	layer_15_expert_1 [label="Expert 1\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 1"]
	layer_15_expert_2 [label="Expert 2\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 2"]
	layer_15_expert_3 [label="Expert 3\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 3"]
	layer_15_expert_4 [label="Expert 4\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 4"]
	layer_15_expert_5 [label="Expert 5\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 5"]
	layer_15_expert_6 [label="Expert 6\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 6"]
	layer_15_expert_7 [label="Expert 7\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 7"]
	layer_15_expert_8 [label="Expert 8\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 8"]
	layer_15_expert_9 [label="Expert 9\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 9"]
	layer_15_expert_10 [label="Expert 10\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 10"]
	layer_15_expert_11 [label="Expert 11\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 11"]
	layer_15_expert_12 [label="Expert 12\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 12"]
	layer_15_expert_13 [label="Expert 13\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 13"]
	layer_15_expert_14 [label="Expert 14\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 14"]
	layer_15_expert_15 [label="Expert 15\nMLP\nInput: [128,subset_tokens,4096]\nOutput: [128,subset_tokens,4096]\nGPU: 15"]
	layer_15_all2all_result [label="Layer 15\nAll-to-All Result Merge\nGPU: all 16 GPUs\nShape: [128,10000,4096]\nvia NVLink/InfiniBand" fillcolor=yellow shape=ellipse]
	layer_15_expert_agg [label="Layer 15\nExpert Aggregation\nInput: [128,subset_tokens,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	layer_15_residual [label="Layer 15\nResidual Add\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nAll 16 GPUs"]
	output [label="Model Output\nbatch_size=128, seq_len=10000, token_dim=4096" fillcolor=lightgreen shape=ellipse]
	input -> layer_0_mha
	layer_0_mha -> layer_0_gate
	layer_0_gate -> layer_0_all2all
	layer_0_all2all -> layer_0_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_0_all2all -> layer_0_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_0_all2all -> layer_0_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_0_all2all -> layer_0_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_0_all2all -> layer_0_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_0_all2all -> layer_0_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_0_all2all -> layer_0_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_0_all2all -> layer_0_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_0_all2all -> layer_0_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_0_all2all -> layer_0_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_0_all2all -> layer_0_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_0_all2all -> layer_0_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_0_all2all -> layer_0_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_0_all2all -> layer_0_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_0_all2all -> layer_0_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_0_all2all -> layer_0_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_0_expert_0 -> layer_0_all2all_result
	layer_0_expert_1 -> layer_0_all2all_result
	layer_0_expert_2 -> layer_0_all2all_result
	layer_0_expert_3 -> layer_0_all2all_result
	layer_0_expert_4 -> layer_0_all2all_result
	layer_0_expert_5 -> layer_0_all2all_result
	layer_0_expert_6 -> layer_0_all2all_result
	layer_0_expert_7 -> layer_0_all2all_result
	layer_0_expert_8 -> layer_0_all2all_result
	layer_0_expert_9 -> layer_0_all2all_result
	layer_0_expert_10 -> layer_0_all2all_result
	layer_0_expert_11 -> layer_0_all2all_result
	layer_0_expert_12 -> layer_0_all2all_result
	layer_0_expert_13 -> layer_0_all2all_result
	layer_0_expert_14 -> layer_0_all2all_result
	layer_0_expert_15 -> layer_0_all2all_result
	layer_0_all2all_result -> layer_0_expert_agg
	layer_0_mha -> layer_0_residual [label=identity]
	layer_0_expert_agg -> layer_0_residual
	layer_0_residual -> layer_1_mha
	layer_1_mha -> layer_1_gate
	layer_1_gate -> layer_1_all2all
	layer_1_all2all -> layer_1_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_1_all2all -> layer_1_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_1_all2all -> layer_1_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_1_all2all -> layer_1_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_1_all2all -> layer_1_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_1_all2all -> layer_1_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_1_all2all -> layer_1_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_1_all2all -> layer_1_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_1_all2all -> layer_1_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_1_all2all -> layer_1_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_1_all2all -> layer_1_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_1_all2all -> layer_1_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_1_all2all -> layer_1_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_1_all2all -> layer_1_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_1_all2all -> layer_1_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_1_all2all -> layer_1_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_1_expert_0 -> layer_1_all2all_result
	layer_1_expert_1 -> layer_1_all2all_result
	layer_1_expert_2 -> layer_1_all2all_result
	layer_1_expert_3 -> layer_1_all2all_result
	layer_1_expert_4 -> layer_1_all2all_result
	layer_1_expert_5 -> layer_1_all2all_result
	layer_1_expert_6 -> layer_1_all2all_result
	layer_1_expert_7 -> layer_1_all2all_result
	layer_1_expert_8 -> layer_1_all2all_result
	layer_1_expert_9 -> layer_1_all2all_result
	layer_1_expert_10 -> layer_1_all2all_result
	layer_1_expert_11 -> layer_1_all2all_result
	layer_1_expert_12 -> layer_1_all2all_result
	layer_1_expert_13 -> layer_1_all2all_result
	layer_1_expert_14 -> layer_1_all2all_result
	layer_1_expert_15 -> layer_1_all2all_result
	layer_1_all2all_result -> layer_1_expert_agg
	layer_1_mha -> layer_1_residual [label=identity]
	layer_1_expert_agg -> layer_1_residual
	layer_1_residual -> layer_2_mha
	layer_2_mha -> layer_2_gate
	layer_2_gate -> layer_2_all2all
	layer_2_all2all -> layer_2_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_2_all2all -> layer_2_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_2_all2all -> layer_2_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_2_all2all -> layer_2_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_2_all2all -> layer_2_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_2_all2all -> layer_2_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_2_all2all -> layer_2_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_2_all2all -> layer_2_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_2_all2all -> layer_2_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_2_all2all -> layer_2_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_2_all2all -> layer_2_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_2_all2all -> layer_2_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_2_all2all -> layer_2_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_2_all2all -> layer_2_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_2_all2all -> layer_2_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_2_all2all -> layer_2_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_2_expert_0 -> layer_2_all2all_result
	layer_2_expert_1 -> layer_2_all2all_result
	layer_2_expert_2 -> layer_2_all2all_result
	layer_2_expert_3 -> layer_2_all2all_result
	layer_2_expert_4 -> layer_2_all2all_result
	layer_2_expert_5 -> layer_2_all2all_result
	layer_2_expert_6 -> layer_2_all2all_result
	layer_2_expert_7 -> layer_2_all2all_result
	layer_2_expert_8 -> layer_2_all2all_result
	layer_2_expert_9 -> layer_2_all2all_result
	layer_2_expert_10 -> layer_2_all2all_result
	layer_2_expert_11 -> layer_2_all2all_result
	layer_2_expert_12 -> layer_2_all2all_result
	layer_2_expert_13 -> layer_2_all2all_result
	layer_2_expert_14 -> layer_2_all2all_result
	layer_2_expert_15 -> layer_2_all2all_result
	layer_2_all2all_result -> layer_2_expert_agg
	layer_2_mha -> layer_2_residual [label=identity]
	layer_2_expert_agg -> layer_2_residual
	layer_2_residual -> layer_3_mha
	layer_3_mha -> layer_3_gate
	layer_3_gate -> layer_3_all2all
	layer_3_all2all -> layer_3_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_3_all2all -> layer_3_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_3_all2all -> layer_3_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_3_all2all -> layer_3_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_3_all2all -> layer_3_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_3_all2all -> layer_3_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_3_all2all -> layer_3_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_3_all2all -> layer_3_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_3_all2all -> layer_3_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_3_all2all -> layer_3_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_3_all2all -> layer_3_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_3_all2all -> layer_3_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_3_all2all -> layer_3_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_3_all2all -> layer_3_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_3_all2all -> layer_3_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_3_all2all -> layer_3_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_3_expert_0 -> layer_3_all2all_result
	layer_3_expert_1 -> layer_3_all2all_result
	layer_3_expert_2 -> layer_3_all2all_result
	layer_3_expert_3 -> layer_3_all2all_result
	layer_3_expert_4 -> layer_3_all2all_result
	layer_3_expert_5 -> layer_3_all2all_result
	layer_3_expert_6 -> layer_3_all2all_result
	layer_3_expert_7 -> layer_3_all2all_result
	layer_3_expert_8 -> layer_3_all2all_result
	layer_3_expert_9 -> layer_3_all2all_result
	layer_3_expert_10 -> layer_3_all2all_result
	layer_3_expert_11 -> layer_3_all2all_result
	layer_3_expert_12 -> layer_3_all2all_result
	layer_3_expert_13 -> layer_3_all2all_result
	layer_3_expert_14 -> layer_3_all2all_result
	layer_3_expert_15 -> layer_3_all2all_result
	layer_3_all2all_result -> layer_3_expert_agg
	layer_3_mha -> layer_3_residual [label=identity]
	layer_3_expert_agg -> layer_3_residual
	layer_3_residual -> layer_4_mha
	layer_4_mha -> layer_4_gate
	layer_4_gate -> layer_4_all2all
	layer_4_all2all -> layer_4_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_4_all2all -> layer_4_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_4_all2all -> layer_4_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_4_all2all -> layer_4_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_4_all2all -> layer_4_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_4_all2all -> layer_4_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_4_all2all -> layer_4_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_4_all2all -> layer_4_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_4_all2all -> layer_4_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_4_all2all -> layer_4_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_4_all2all -> layer_4_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_4_all2all -> layer_4_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_4_all2all -> layer_4_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_4_all2all -> layer_4_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_4_all2all -> layer_4_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_4_all2all -> layer_4_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_4_expert_0 -> layer_4_all2all_result
	layer_4_expert_1 -> layer_4_all2all_result
	layer_4_expert_2 -> layer_4_all2all_result
	layer_4_expert_3 -> layer_4_all2all_result
	layer_4_expert_4 -> layer_4_all2all_result
	layer_4_expert_5 -> layer_4_all2all_result
	layer_4_expert_6 -> layer_4_all2all_result
	layer_4_expert_7 -> layer_4_all2all_result
	layer_4_expert_8 -> layer_4_all2all_result
	layer_4_expert_9 -> layer_4_all2all_result
	layer_4_expert_10 -> layer_4_all2all_result
	layer_4_expert_11 -> layer_4_all2all_result
	layer_4_expert_12 -> layer_4_all2all_result
	layer_4_expert_13 -> layer_4_all2all_result
	layer_4_expert_14 -> layer_4_all2all_result
	layer_4_expert_15 -> layer_4_all2all_result
	layer_4_all2all_result -> layer_4_expert_agg
	layer_4_mha -> layer_4_residual [label=identity]
	layer_4_expert_agg -> layer_4_residual
	layer_4_residual -> layer_5_mha
	layer_5_mha -> layer_5_gate
	layer_5_gate -> layer_5_all2all
	layer_5_all2all -> layer_5_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_5_all2all -> layer_5_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_5_all2all -> layer_5_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_5_all2all -> layer_5_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_5_all2all -> layer_5_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_5_all2all -> layer_5_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_5_all2all -> layer_5_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_5_all2all -> layer_5_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_5_all2all -> layer_5_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_5_all2all -> layer_5_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_5_all2all -> layer_5_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_5_all2all -> layer_5_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_5_all2all -> layer_5_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_5_all2all -> layer_5_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_5_all2all -> layer_5_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_5_all2all -> layer_5_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_5_expert_0 -> layer_5_all2all_result
	layer_5_expert_1 -> layer_5_all2all_result
	layer_5_expert_2 -> layer_5_all2all_result
	layer_5_expert_3 -> layer_5_all2all_result
	layer_5_expert_4 -> layer_5_all2all_result
	layer_5_expert_5 -> layer_5_all2all_result
	layer_5_expert_6 -> layer_5_all2all_result
	layer_5_expert_7 -> layer_5_all2all_result
	layer_5_expert_8 -> layer_5_all2all_result
	layer_5_expert_9 -> layer_5_all2all_result
	layer_5_expert_10 -> layer_5_all2all_result
	layer_5_expert_11 -> layer_5_all2all_result
	layer_5_expert_12 -> layer_5_all2all_result
	layer_5_expert_13 -> layer_5_all2all_result
	layer_5_expert_14 -> layer_5_all2all_result
	layer_5_expert_15 -> layer_5_all2all_result
	layer_5_all2all_result -> layer_5_expert_agg
	layer_5_mha -> layer_5_residual [label=identity]
	layer_5_expert_agg -> layer_5_residual
	layer_5_residual -> layer_6_mha
	layer_6_mha -> layer_6_gate
	layer_6_gate -> layer_6_all2all
	layer_6_all2all -> layer_6_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_6_all2all -> layer_6_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_6_all2all -> layer_6_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_6_all2all -> layer_6_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_6_all2all -> layer_6_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_6_all2all -> layer_6_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_6_all2all -> layer_6_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_6_all2all -> layer_6_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_6_all2all -> layer_6_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_6_all2all -> layer_6_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_6_all2all -> layer_6_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_6_all2all -> layer_6_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_6_all2all -> layer_6_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_6_all2all -> layer_6_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_6_all2all -> layer_6_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_6_all2all -> layer_6_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_6_expert_0 -> layer_6_all2all_result
	layer_6_expert_1 -> layer_6_all2all_result
	layer_6_expert_2 -> layer_6_all2all_result
	layer_6_expert_3 -> layer_6_all2all_result
	layer_6_expert_4 -> layer_6_all2all_result
	layer_6_expert_5 -> layer_6_all2all_result
	layer_6_expert_6 -> layer_6_all2all_result
	layer_6_expert_7 -> layer_6_all2all_result
	layer_6_expert_8 -> layer_6_all2all_result
	layer_6_expert_9 -> layer_6_all2all_result
	layer_6_expert_10 -> layer_6_all2all_result
	layer_6_expert_11 -> layer_6_all2all_result
	layer_6_expert_12 -> layer_6_all2all_result
	layer_6_expert_13 -> layer_6_all2all_result
	layer_6_expert_14 -> layer_6_all2all_result
	layer_6_expert_15 -> layer_6_all2all_result
	layer_6_all2all_result -> layer_6_expert_agg
	layer_6_mha -> layer_6_residual [label=identity]
	layer_6_expert_agg -> layer_6_residual
	layer_6_residual -> layer_7_mha
	layer_7_mha -> layer_7_gate
	layer_7_gate -> layer_7_all2all
	layer_7_all2all -> layer_7_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_7_all2all -> layer_7_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_7_all2all -> layer_7_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_7_all2all -> layer_7_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_7_all2all -> layer_7_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_7_all2all -> layer_7_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_7_all2all -> layer_7_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_7_all2all -> layer_7_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_7_all2all -> layer_7_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_7_all2all -> layer_7_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_7_all2all -> layer_7_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_7_all2all -> layer_7_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_7_all2all -> layer_7_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_7_all2all -> layer_7_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_7_all2all -> layer_7_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_7_all2all -> layer_7_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_7_expert_0 -> layer_7_all2all_result
	layer_7_expert_1 -> layer_7_all2all_result
	layer_7_expert_2 -> layer_7_all2all_result
	layer_7_expert_3 -> layer_7_all2all_result
	layer_7_expert_4 -> layer_7_all2all_result
	layer_7_expert_5 -> layer_7_all2all_result
	layer_7_expert_6 -> layer_7_all2all_result
	layer_7_expert_7 -> layer_7_all2all_result
	layer_7_expert_8 -> layer_7_all2all_result
	layer_7_expert_9 -> layer_7_all2all_result
	layer_7_expert_10 -> layer_7_all2all_result
	layer_7_expert_11 -> layer_7_all2all_result
	layer_7_expert_12 -> layer_7_all2all_result
	layer_7_expert_13 -> layer_7_all2all_result
	layer_7_expert_14 -> layer_7_all2all_result
	layer_7_expert_15 -> layer_7_all2all_result
	layer_7_all2all_result -> layer_7_expert_agg
	layer_7_mha -> layer_7_residual [label=identity]
	layer_7_expert_agg -> layer_7_residual
	layer_7_residual -> layer_8_mha
	layer_8_mha -> layer_8_gate
	layer_8_gate -> layer_8_all2all
	layer_8_all2all -> layer_8_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_8_all2all -> layer_8_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_8_all2all -> layer_8_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_8_all2all -> layer_8_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_8_all2all -> layer_8_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_8_all2all -> layer_8_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_8_all2all -> layer_8_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_8_all2all -> layer_8_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_8_all2all -> layer_8_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_8_all2all -> layer_8_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_8_all2all -> layer_8_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_8_all2all -> layer_8_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_8_all2all -> layer_8_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_8_all2all -> layer_8_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_8_all2all -> layer_8_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_8_all2all -> layer_8_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_8_expert_0 -> layer_8_all2all_result
	layer_8_expert_1 -> layer_8_all2all_result
	layer_8_expert_2 -> layer_8_all2all_result
	layer_8_expert_3 -> layer_8_all2all_result
	layer_8_expert_4 -> layer_8_all2all_result
	layer_8_expert_5 -> layer_8_all2all_result
	layer_8_expert_6 -> layer_8_all2all_result
	layer_8_expert_7 -> layer_8_all2all_result
	layer_8_expert_8 -> layer_8_all2all_result
	layer_8_expert_9 -> layer_8_all2all_result
	layer_8_expert_10 -> layer_8_all2all_result
	layer_8_expert_11 -> layer_8_all2all_result
	layer_8_expert_12 -> layer_8_all2all_result
	layer_8_expert_13 -> layer_8_all2all_result
	layer_8_expert_14 -> layer_8_all2all_result
	layer_8_expert_15 -> layer_8_all2all_result
	layer_8_all2all_result -> layer_8_expert_agg
	layer_8_mha -> layer_8_residual [label=identity]
	layer_8_expert_agg -> layer_8_residual
	layer_8_residual -> layer_9_mha
	layer_9_mha -> layer_9_gate
	layer_9_gate -> layer_9_all2all
	layer_9_all2all -> layer_9_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_9_all2all -> layer_9_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_9_all2all -> layer_9_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_9_all2all -> layer_9_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_9_all2all -> layer_9_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_9_all2all -> layer_9_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_9_all2all -> layer_9_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_9_all2all -> layer_9_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_9_all2all -> layer_9_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_9_all2all -> layer_9_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_9_all2all -> layer_9_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_9_all2all -> layer_9_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_9_all2all -> layer_9_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_9_all2all -> layer_9_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_9_all2all -> layer_9_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_9_all2all -> layer_9_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_9_expert_0 -> layer_9_all2all_result
	layer_9_expert_1 -> layer_9_all2all_result
	layer_9_expert_2 -> layer_9_all2all_result
	layer_9_expert_3 -> layer_9_all2all_result
	layer_9_expert_4 -> layer_9_all2all_result
	layer_9_expert_5 -> layer_9_all2all_result
	layer_9_expert_6 -> layer_9_all2all_result
	layer_9_expert_7 -> layer_9_all2all_result
	layer_9_expert_8 -> layer_9_all2all_result
	layer_9_expert_9 -> layer_9_all2all_result
	layer_9_expert_10 -> layer_9_all2all_result
	layer_9_expert_11 -> layer_9_all2all_result
	layer_9_expert_12 -> layer_9_all2all_result
	layer_9_expert_13 -> layer_9_all2all_result
	layer_9_expert_14 -> layer_9_all2all_result
	layer_9_expert_15 -> layer_9_all2all_result
	layer_9_all2all_result -> layer_9_expert_agg
	layer_9_mha -> layer_9_residual [label=identity]
	layer_9_expert_agg -> layer_9_residual
	layer_9_residual -> layer_10_mha
	layer_10_mha -> layer_10_gate
	layer_10_gate -> layer_10_all2all
	layer_10_all2all -> layer_10_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_10_all2all -> layer_10_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_10_all2all -> layer_10_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_10_all2all -> layer_10_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_10_all2all -> layer_10_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_10_all2all -> layer_10_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_10_all2all -> layer_10_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_10_all2all -> layer_10_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_10_all2all -> layer_10_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_10_all2all -> layer_10_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_10_all2all -> layer_10_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_10_all2all -> layer_10_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_10_all2all -> layer_10_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_10_all2all -> layer_10_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_10_all2all -> layer_10_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_10_all2all -> layer_10_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_10_expert_0 -> layer_10_all2all_result
	layer_10_expert_1 -> layer_10_all2all_result
	layer_10_expert_2 -> layer_10_all2all_result
	layer_10_expert_3 -> layer_10_all2all_result
	layer_10_expert_4 -> layer_10_all2all_result
	layer_10_expert_5 -> layer_10_all2all_result
	layer_10_expert_6 -> layer_10_all2all_result
	layer_10_expert_7 -> layer_10_all2all_result
	layer_10_expert_8 -> layer_10_all2all_result
	layer_10_expert_9 -> layer_10_all2all_result
	layer_10_expert_10 -> layer_10_all2all_result
	layer_10_expert_11 -> layer_10_all2all_result
	layer_10_expert_12 -> layer_10_all2all_result
	layer_10_expert_13 -> layer_10_all2all_result
	layer_10_expert_14 -> layer_10_all2all_result
	layer_10_expert_15 -> layer_10_all2all_result
	layer_10_all2all_result -> layer_10_expert_agg
	layer_10_mha -> layer_10_residual [label=identity]
	layer_10_expert_agg -> layer_10_residual
	layer_10_residual -> layer_11_mha
	layer_11_mha -> layer_11_gate
	layer_11_gate -> layer_11_all2all
	layer_11_all2all -> layer_11_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_11_all2all -> layer_11_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_11_all2all -> layer_11_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_11_all2all -> layer_11_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_11_all2all -> layer_11_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_11_all2all -> layer_11_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_11_all2all -> layer_11_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_11_all2all -> layer_11_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_11_all2all -> layer_11_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_11_all2all -> layer_11_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_11_all2all -> layer_11_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_11_all2all -> layer_11_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_11_all2all -> layer_11_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_11_all2all -> layer_11_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_11_all2all -> layer_11_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_11_all2all -> layer_11_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_11_expert_0 -> layer_11_all2all_result
	layer_11_expert_1 -> layer_11_all2all_result
	layer_11_expert_2 -> layer_11_all2all_result
	layer_11_expert_3 -> layer_11_all2all_result
	layer_11_expert_4 -> layer_11_all2all_result
	layer_11_expert_5 -> layer_11_all2all_result
	layer_11_expert_6 -> layer_11_all2all_result
	layer_11_expert_7 -> layer_11_all2all_result
	layer_11_expert_8 -> layer_11_all2all_result
	layer_11_expert_9 -> layer_11_all2all_result
	layer_11_expert_10 -> layer_11_all2all_result
	layer_11_expert_11 -> layer_11_all2all_result
	layer_11_expert_12 -> layer_11_all2all_result
	layer_11_expert_13 -> layer_11_all2all_result
	layer_11_expert_14 -> layer_11_all2all_result
	layer_11_expert_15 -> layer_11_all2all_result
	layer_11_all2all_result -> layer_11_expert_agg
	layer_11_mha -> layer_11_residual [label=identity]
	layer_11_expert_agg -> layer_11_residual
	layer_11_residual -> layer_12_mha
	layer_12_mha -> layer_12_gate
	layer_12_gate -> layer_12_all2all
	layer_12_all2all -> layer_12_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_12_all2all -> layer_12_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_12_all2all -> layer_12_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_12_all2all -> layer_12_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_12_all2all -> layer_12_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_12_all2all -> layer_12_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_12_all2all -> layer_12_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_12_all2all -> layer_12_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_12_all2all -> layer_12_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_12_all2all -> layer_12_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_12_all2all -> layer_12_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_12_all2all -> layer_12_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_12_all2all -> layer_12_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_12_all2all -> layer_12_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_12_all2all -> layer_12_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_12_all2all -> layer_12_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_12_expert_0 -> layer_12_all2all_result
	layer_12_expert_1 -> layer_12_all2all_result
	layer_12_expert_2 -> layer_12_all2all_result
	layer_12_expert_3 -> layer_12_all2all_result
	layer_12_expert_4 -> layer_12_all2all_result
	layer_12_expert_5 -> layer_12_all2all_result
	layer_12_expert_6 -> layer_12_all2all_result
	layer_12_expert_7 -> layer_12_all2all_result
	layer_12_expert_8 -> layer_12_all2all_result
	layer_12_expert_9 -> layer_12_all2all_result
	layer_12_expert_10 -> layer_12_all2all_result
	layer_12_expert_11 -> layer_12_all2all_result
	layer_12_expert_12 -> layer_12_all2all_result
	layer_12_expert_13 -> layer_12_all2all_result
	layer_12_expert_14 -> layer_12_all2all_result
	layer_12_expert_15 -> layer_12_all2all_result
	layer_12_all2all_result -> layer_12_expert_agg
	layer_12_mha -> layer_12_residual [label=identity]
	layer_12_expert_agg -> layer_12_residual
	layer_12_residual -> layer_13_mha
	layer_13_mha -> layer_13_gate
	layer_13_gate -> layer_13_all2all
	layer_13_all2all -> layer_13_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_13_all2all -> layer_13_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_13_all2all -> layer_13_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_13_all2all -> layer_13_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_13_all2all -> layer_13_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_13_all2all -> layer_13_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_13_all2all -> layer_13_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_13_all2all -> layer_13_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_13_all2all -> layer_13_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_13_all2all -> layer_13_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_13_all2all -> layer_13_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_13_all2all -> layer_13_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_13_all2all -> layer_13_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_13_all2all -> layer_13_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_13_all2all -> layer_13_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_13_all2all -> layer_13_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_13_expert_0 -> layer_13_all2all_result
	layer_13_expert_1 -> layer_13_all2all_result
	layer_13_expert_2 -> layer_13_all2all_result
	layer_13_expert_3 -> layer_13_all2all_result
	layer_13_expert_4 -> layer_13_all2all_result
	layer_13_expert_5 -> layer_13_all2all_result
	layer_13_expert_6 -> layer_13_all2all_result
	layer_13_expert_7 -> layer_13_all2all_result
	layer_13_expert_8 -> layer_13_all2all_result
	layer_13_expert_9 -> layer_13_all2all_result
	layer_13_expert_10 -> layer_13_all2all_result
	layer_13_expert_11 -> layer_13_all2all_result
	layer_13_expert_12 -> layer_13_all2all_result
	layer_13_expert_13 -> layer_13_all2all_result
	layer_13_expert_14 -> layer_13_all2all_result
	layer_13_expert_15 -> layer_13_all2all_result
	layer_13_all2all_result -> layer_13_expert_agg
	layer_13_mha -> layer_13_residual [label=identity]
	layer_13_expert_agg -> layer_13_residual
	layer_13_residual -> layer_14_mha
	layer_14_mha -> layer_14_gate
	layer_14_gate -> layer_14_all2all
	layer_14_all2all -> layer_14_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_14_all2all -> layer_14_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_14_all2all -> layer_14_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_14_all2all -> layer_14_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_14_all2all -> layer_14_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_14_all2all -> layer_14_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_14_all2all -> layer_14_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_14_all2all -> layer_14_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_14_all2all -> layer_14_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_14_all2all -> layer_14_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_14_all2all -> layer_14_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_14_all2all -> layer_14_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_14_all2all -> layer_14_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_14_all2all -> layer_14_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_14_all2all -> layer_14_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_14_all2all -> layer_14_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_14_expert_0 -> layer_14_all2all_result
	layer_14_expert_1 -> layer_14_all2all_result
	layer_14_expert_2 -> layer_14_all2all_result
	layer_14_expert_3 -> layer_14_all2all_result
	layer_14_expert_4 -> layer_14_all2all_result
	layer_14_expert_5 -> layer_14_all2all_result
	layer_14_expert_6 -> layer_14_all2all_result
	layer_14_expert_7 -> layer_14_all2all_result
	layer_14_expert_8 -> layer_14_all2all_result
	layer_14_expert_9 -> layer_14_all2all_result
	layer_14_expert_10 -> layer_14_all2all_result
	layer_14_expert_11 -> layer_14_all2all_result
	layer_14_expert_12 -> layer_14_all2all_result
	layer_14_expert_13 -> layer_14_all2all_result
	layer_14_expert_14 -> layer_14_all2all_result
	layer_14_expert_15 -> layer_14_all2all_result
	layer_14_all2all_result -> layer_14_expert_agg
	layer_14_mha -> layer_14_residual [label=identity]
	layer_14_expert_agg -> layer_14_residual
	layer_14_residual -> layer_15_mha
	layer_15_mha -> layer_15_gate
	layer_15_gate -> layer_15_all2all
	layer_15_all2all -> layer_15_expert_0 [label="tokens routed to expert 0" style=dashed]
	layer_15_all2all -> layer_15_expert_1 [label="tokens routed to expert 1" style=dashed]
	layer_15_all2all -> layer_15_expert_2 [label="tokens routed to expert 2" style=dashed]
	layer_15_all2all -> layer_15_expert_3 [label="tokens routed to expert 3" style=dashed]
	layer_15_all2all -> layer_15_expert_4 [label="tokens routed to expert 4" style=dashed]
	layer_15_all2all -> layer_15_expert_5 [label="tokens routed to expert 5" style=dashed]
	layer_15_all2all -> layer_15_expert_6 [label="tokens routed to expert 6" style=dashed]
	layer_15_all2all -> layer_15_expert_7 [label="tokens routed to expert 7" style=dashed]
	layer_15_all2all -> layer_15_expert_8 [label="tokens routed to expert 8" style=dashed]
	layer_15_all2all -> layer_15_expert_9 [label="tokens routed to expert 9" style=dashed]
	layer_15_all2all -> layer_15_expert_10 [label="tokens routed to expert 10" style=dashed]
	layer_15_all2all -> layer_15_expert_11 [label="tokens routed to expert 11" style=dashed]
	layer_15_all2all -> layer_15_expert_12 [label="tokens routed to expert 12" style=dashed]
	layer_15_all2all -> layer_15_expert_13 [label="tokens routed to expert 13" style=dashed]
	layer_15_all2all -> layer_15_expert_14 [label="tokens routed to expert 14" style=dashed]
	layer_15_all2all -> layer_15_expert_15 [label="tokens routed to expert 15" style=dashed]
	layer_15_expert_0 -> layer_15_all2all_result
	layer_15_expert_1 -> layer_15_all2all_result
	layer_15_expert_2 -> layer_15_all2all_result
	layer_15_expert_3 -> layer_15_all2all_result
	layer_15_expert_4 -> layer_15_all2all_result
	layer_15_expert_5 -> layer_15_all2all_result
	layer_15_expert_6 -> layer_15_all2all_result
	layer_15_expert_7 -> layer_15_all2all_result
	layer_15_expert_8 -> layer_15_all2all_result
	layer_15_expert_9 -> layer_15_all2all_result
	layer_15_expert_10 -> layer_15_all2all_result
	layer_15_expert_11 -> layer_15_all2all_result
	layer_15_expert_12 -> layer_15_all2all_result
	layer_15_expert_13 -> layer_15_all2all_result
	layer_15_expert_14 -> layer_15_all2all_result
	layer_15_expert_15 -> layer_15_all2all_result
	layer_15_all2all_result -> layer_15_expert_agg
	layer_15_mha -> layer_15_residual [label=identity]
	layer_15_expert_agg -> layer_15_residual
	layer_15_residual -> output
}
