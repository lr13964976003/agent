{
  "models": {
    "baseline": {
      "name": "4-layer Dense Baseline",
      "parameters": "13B",
      "layers": 4,
      "dimensions": {
        "hidden_dimension": 4096,
        "attention_heads": 32,
        "head_dimension": 128,
        "feed_forward_dimension": 16384,
        "batch_size": 1024
      },
      "parallel_strategy": {
        "type": "static_tensor_pipeline",
        "tensor_parallelism": 8,
        "pipeline_parallelism": 2,
        "total_gpus": 16,
        "configuration": "TP=8,PP=2"
      },
      "modules": {
        "embeddings": {
          "device_map": [0, 1, 2, 3, 4, 5, 6, 7],
          "type": "replicated"
        },
        "layers": [
          {
            "layer_id": 0,
            "pipeline_stage": 0,
            "gpus": [0, 1, 2, 3, 4, 5, 6, 7],
            "attention": {
              "type": "tensor_parallel",
              "qkv_linear": {
                "devices": [0, 1, 2, 3, 4, 5, 6, 7],
                "partition": "column_wise",
                "dimensions": [4096, 4096],
                "shard_size": [4096, 512]
              },
              "output_linear": {
                "devices": [0, 1, 2, 3, 4, 5, 6, 7],
                "partition": "row_wise",
                "dimensions": [4096, 4096],
                "shard_size": [512, 4096]
              }
            },
            "ffn": {
              "type": "tensor_parallel",
              "gate_up_linear": {
                "devices": [0, 1, 2, 3, 4, 5, 6, 7],
                "partition": "column_wise",
                "dimensions": [16384, 4096],
                "shard_size": [2048, 4096]
              },
              "down_linear": {
                "devices": [0, 1, 2, 3, 4, 5, 6, 7],
                "partition": "row_wise",
                "dimensions": [4096, 16384],
                "shard_size": [4096, 2048]
              }
            }
          },
          {
            "layer_id": 1,
            "pipeline_stage": 0,
            "gpus": [0, 1, 2, 3, 4, 5, 6, 7],
            "attention": {
              "type": "tensor_parallel",
              "qkv_linear": {
                "devices": [0, 1, 2, 3, 4, 5, 6, 7],
                "partition": "column_wise",
                "dimensions": [4096, 4096],
                "shard_size": [4096, 512]
              },
              "output_linear": {
                "devices": [0, 1, 2, 3, 4, 5, 6, 7],
                "partition": "row_wise",
                "dimensions": [4096, 4096],
                "shard_size": [512, 4096]
              }
            },
            "ffn": {
              "type": "tensor_parallel",
              "gate_up_linear": {
                "devices": [0, 1, 2, 3, 4, 5, 6, 7],
                "partition": "column_wise",
                "dimensions": [16384, 4096],
                "shard_size": [2048, 4096]
              },
              "down_linear": {
                "devices": [0, 1, 2, 3, 4, 5, 6, 7],
                "partition": "row_wise",
                "dimensions": [4096, 16384],
                "shard_size": [4096, 2048]
              }
            }
          },
          {
            "layer_id": 2,
            "pipeline_stage": 1,
            "gpus": [8, 9, 10, 11, 12, 13, 14, 15],
            "attention": {
              "type": "tensor_parallel",
              "qkv_linear": {
                "devices": [8, 9, 10, 11, 12, 13, 14, 15],
                "partition": "column_wise",
                "dimensions": [4096, 4096],
                "shard_size": [4096, 512]
              },
              "output_linear": {
                "devices": [8, 9, 10, 11, 12, 13, 14, 15],
                "partition": "row_wise",
                "dimensions": [4096, 4096],
                "shard_size": [512, 4096]
              }
            },
            "ffn": {
              "type": "tensor_parallel",
              "gate_up_linear": {
                "devices": [8, 9, 10, 11, 12, 13, 14, 15],
                "partition": "column_wise",
                "dimensions": [16384, 4096],
                "shard_size": [2048, 4096]
              },
              "down_linear": {
                "devices": [8, 9, 10, 11, 12, 13, 14, 15],
                "partition": "row_wise",
                "dimensions": [4096, 16384],
                "shard_size": [4096, 2048]
              }
            }
          },
          {
            "layer_id": 3,
            "pipeline_stage": 1,
            "gpus": [8, 9, 10, 11, 12, 13, 14, 15],
            "attention": {
              "type": "tensor_parallel",
              "qkv_linear": {
                "devices": [8, 9, 10, 11, 12, 13, 14, 15],
                "partition": "column_wise",
                "dimensions": [4096, 4096],
                "shard_size": [4096, 512]
              },
              "output_linear": {
                "devices": [8, 9, 10, 11, 12, 13, 14, 15],
                "partition": "row_wise",
                "dimensions": [4096, 4096],
                "shard_size": [512, 4096]
              }
            },
            "ffn": {
              "type": "tensor_parallel",
              "gate_up_linear": {
                "devices": [8, 9, 10, 11, 12, 13, 14, 15],
                "partition": "column_wise",
                "dimensions": [16384, 4096],
                "shard_size": [2048, 4096]
              },
              "down_linear": {
                "devices": [8, 9, 10, 11, 12, 13, 14, 15],
                "partition": "row_wise",
                "dimensions": [4096, 16384],
                "shard_size": [4096, 2048]
              }
            }
          }
        ],
        "output_projection": {
          "device_map": [8, 9, 10, 11, 12, 13, 14, 15],
          "type": "row_parallel"
        }
      }
    },
    "fa_pool": {
      "name": "4-layer Dense FA Pool",
      "parameters": "13B",
      "layers": 4,
      "dimensions": {
        "hidden_dimension": 4096,
        "attention_heads": 32,
        "head_dimension": 128,
        "feed_forward_dimension": 16384,
        "batch_size": 1024
      },
      "parallel_strategy": {
        "type": "dynamic_attention_pool",
        "base_layer_gpus": 8,
        "max_attention_pool_gpus": 32,
        "sequence_threshold": 4096,
        "gpu_allocation_formula": "min(ceil(sequence_length/1024), 32)",
        "total_gpus_range": [8, 40]
      },
      "modules": {
        "base_layer": {
          "device_map": [0, 1, 2, 3, 4, 5, 6, 7],
          "type": "fixed_base",
          "components": {
            "embeddings": {
              "devices": [0, 1, 2, 3, 4, 5, 6, 7],
              "type": "replicated",
              "memory": "65GB_per_gpu"
            },
            "positional_encoding": {
              "devices": [0, 1, 2, 3, 4, 5, 6, 7],
              "type": "replicated"
            },
            "layer_norms": {
              "devices": [0, 1, 2, 3, 4, 5, 6, 7],
              "type": "replicated"
            },
            "feed_forward_networks": {
              "layer_0": {
                "devices": [0, 1, 2, 3, 4, 5, 6, 7],
                "type": "tensor_parallel",
                "gate_up_linear": {
                  "partition": "column_wise",
                  "dimensions": [16384, 4096],
                  "shard_size": [2048, 4096]
                },
                "down_linear": {
                  "partition": "row_wise", 
                  "dimensions": [4096, 16384],
                  "shard_size": [4096, 2048]
                }
              },
              "layer_1": {
                "devices": [0, 1, 2, 3, 4, 5, 6, 7],
                "type": "tensor_parallel",
                "gate_up_linear": {
                  "partition": "column_wise",
                  "dimensions": [16384, 4096],
                  "shard_size": [2048, 4096]
                },
                "down_linear": {
                  "partition": "row_wise",
                  "dimensions": [4096, 16384], 
                  "shard_size": [4096, 2048]
                }
              },
              "layer_2": {
                "devices": [0, 1, 2, 3, 4, 5, 6, 7],
                "type": "tensor_parallel",
                "gate_up_linear": {
                  "partition": "column_wise",
                  "dimensions": [16384, 4096],
                  "shard_size": [2048, 4096]
                },
                "down_linear": {
                  "partition": "row_wise",
                  "dimensions": [4096, 16384],
                  "shard_size": [4096, 2048]
                }
              },
              "layer_3": {
                "devices": [0, 1, 2, 3, 4, 5, 6, 7],
                "type": "tensor_parallel",
                "gate_up_linear": {
                  "partition": "column_wise",
                  "dimensions": [16384, 4096],
                  "shard_size": [2048, 4096]
                },
                "down_linear": {
                  "partition": "row_wise",
                  "dimensions": [4096, 16384],
                  "shard_size": [4096, 2048]
                }
              }
            },
            "output_projection": {
              "devices": [0, 1, 2, 3, 4, 5, 6, 7],
              "type": "row_parallel"
            }
          }
        },
        "attention_pool": {
          "type": "dynamic_pool",
          "device_range": [8, 39],
          "allocation_strategy": "sequence_length_based",
          "components": {
            "attention_computation": {
              "layer_0": {
                "devices": "dynamic_based_on_sequence_length",
                "attention_heads_per_gpu": "32 / pool_gpus",
                "sequence_partitioning": "block_wise",
                "block_size": "ceil(sequence_length / pool_gpus)",
                "memory_per_gpu": "45GB"
              },
              "layer_1": {
                "devices": "dynamic_based_on_sequence_length", 
                "attention_heads_per_gpu": "32 / pool_gpus",
                "sequence_partitioning": "block_wise",
                "block_size": "ceil(sequence_length / pool_gpus)",
                "memory_per_gpu": "45GB"
              },
              "layer_2": {
                "devices": "dynamic_based_on_sequence_length",
                "attention_heads_per_gpu": "32 / pool_gpus", 
                "sequence_partitioning": "block_wise",
                "block_size": "ceil(sequence_length / pool_gpus)",
                "memory_per_gpu": "45GB"
              },
              "layer_3": {
                "devices": "dynamic_based_on_sequence_length",
                "attention_heads_per_gpu": "32 / pool_gpus",
                "sequence_partitioning": "block_wise", 
                "block_size": "ceil(sequence_length / pool_gpus)",
                "memory_per_gpu": "45GB"
              }
            }
          }
        }
      }
    }
  },
  "deployment_mapping": {
    "baseline": {
      "device_0": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_0_tensor_shard_0",
        "modules": [
          "embeddings",
          "layer_0_attention_qkv_linear_shard_0",
          "layer_0_attention_output_linear_shard_0", 
          "layer_0_ffn_gate_up_linear_shard_0",
          "layer_0_ffn_down_linear_shard_0",
          "layer_1_attention_qkv_linear_shard_0",
          "layer_1_attention_output_linear_shard_0",
          "layer_1_ffn_gate_up_linear_shard_0", 
          "layer_1_ffn_down_linear_shard_0"
        ]
      },
      "device_1": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_0_tensor_shard_1",
        "modules": [
          "embeddings",
          "layer_0_attention_qkv_linear_shard_1",
          "layer_0_attention_output_linear_shard_1",
          "layer_0_ffn_gate_up_linear_shard_1", 
          "layer_0_ffn_down_linear_shard_1",
          "layer_1_attention_qkv_linear_shard_1",
          "layer_1_attention_output_linear_shard_1",
          "layer_1_ffn_gate_up_linear_shard_1",
          "layer_1_ffn_down_linear_shard_1"
        ]
      },
      "device_2": {
        "model": "4-layer Dense Baseline", 
        "role": "pipeline_stage_0_tensor_shard_2",
        "modules": [
          "embeddings",
          "layer_0_attention_qkv_linear_shard_2",
          "layer_0_attention_output_linear_shard_2",
          "layer_0_ffn_gate_up_linear_shard_2",
          "layer_0_ffn_down_linear_shard_2",
          "layer_1_attention_qkv_linear_shard_2",
          "layer_1_attention_output_linear_shard_2",
          "layer_1_ffn_gate_up_linear_shard_2",
          "layer_1_ffn_down_linear_shard_2"
        ]
      },
      "device_3": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_0_tensor_shard_3", 
        "modules": [
          "embeddings",
          "layer_0_attention_qkv_linear_shard_3",
          "layer_0_attention_output_linear_shard_3",
          "layer_0_ffn_gate_up_linear_shard_3",
          "layer_0_ffn_down_linear_shard_3",
          "layer_1_attention_qkv_linear_shard_3",
          "layer_1_attention_output_linear_shard_3",
          "layer_1_ffn_gate_up_linear_shard_3",
          "layer_1_ffn_down_linear_shard_3"
        ]
      },
      "device_4": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_0_tensor_shard_4",
        "modules": [
          "embeddings",
          "layer_0_attention_qkv_linear_shard_4",
          "layer_0_attention_output_linear_shard_4",
          "layer_0_ffn_gate_up_linear_shard_4",
          "layer_0_ffn_down_linear_shard_4",
          "layer_1_attention_qkv_linear_shard_4",
          "layer_1_attention_output_linear_shard_4",
          "layer_1_ffn_gate_up_linear_shard_4",
          "layer_1_ffn_down_linear_shard_4"
        ]
      },
      "device_5": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_0_tensor_shard_5",
        "modules": [
          "embeddings",
          "layer_0_attention_qkv_linear_shard_5",
          "layer_0_attention_output_linear_shard_5",
          "layer_0_ffn_gate_up_linear_shard_5",
          "layer_0_ffn_down_linear_shard_5",
          "layer_1_attention_qkv_linear_shard_5",
          "layer_1_attention_output_linear_shard_5",
          "layer_1_ffn_gate_up_linear_shard_5",
          "layer_1_ffn_down_linear_shard_5"
        ]
      },
      "device_6": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_0_tensor_shard_6",
        "modules": [
          "embeddings",
          "layer_0_attention_qkv_linear_shard_6",
          "layer_0_attention_output_linear_shard_6",
          "layer_0_ffn_gate_up_linear_shard_6",
          "layer_0_ffn_down_linear_shard_6",
          "layer_1_attention_qkv_linear_shard_6",
          "layer_1_attention_output_linear_shard_6",
          "layer_1_ffn_gate_up_linear_shard_6",
          "layer_1_ffn_down_linear_shard_6"
        ]
      },
      "device_7": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_0_tensor_shard_7",
        "modules": [
          "embeddings",
          "layer_0_attention_qkv_linear_shard_7",
          "layer_0_attention_output_linear_shard_7",
          "layer_0_ffn_gate_up_linear_shard_7",
          "layer_0_ffn_down_linear_shard_7",
          "layer_1_attention_qkv_linear_shard_7",
          "layer_1_attention_output_linear_shard_7",
          "layer_1_ffn_gate_up_linear_shard_7",
          "layer_1_ffn_down_linear_shard_7"
        ]
      },
      "device_8": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_1_tensor_shard_0",
        "modules": [
          "layer_2_attention_qkv_linear_shard_0",
          "layer_2_attention_output_linear_shard_0",
          "layer_2_ffn_gate_up_linear_shard_0",
          "layer_2_ffn_down_linear_shard_0",
          "layer_3_attention_qkv_linear_shard_0",
          "layer_3_attention_output_linear_shard_0",
          "layer_3_ffn_gate_up_linear_shard_0",
          "layer_3_ffn_down_linear_shard_0",
          "output_projection_shard_0"
        ]
      },
      "device_9": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_1_tensor_shard_1",
        "modules": [
          "layer_2_attention_qkv_linear_shard_1",
          "layer_2_attention_output_linear_shard_1",
          "layer_2_ffn_gate_up_linear_shard_1",
          "layer_2_ffn_down_linear_shard_1",
          "layer_3_attention_qkv_linear_shard_1",
          "layer_3_attention_output_linear_shard_1",
          "layer_3_ffn_gate_up_linear_shard_1",
          "layer_3_ffn_down_linear_shard_1",
          "output_projection_shard_1"
        ]
      },
      "device_10": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_1_tensor_shard_2",
        "modules": [
          "layer_2_attention_qkv_linear_shard_2",
          "layer_2_attention_output_linear_shard_2",
          "layer_2_ffn_gate_up_linear_shard_2",
          "layer_2_ffn_down_linear_shard_2",
          "layer_3_attention_qkv_linear_shard_2",
          "layer_3_attention_output_linear_shard_2",
          "layer_3_ffn_gate_up_linear_shard_2",
          "layer_3_ffn_down_linear_shard_2",
          "output_projection_shard_2"
        ]
      },
      "device_11": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_1_tensor_shard_3",
        "modules": [
          "layer_2_attention_qkv_linear_shard_3",
          "layer_2_attention_output_linear_shard_3",
          "layer_2_ffn_gate_up_linear_shard_3",
          "layer_2_ffn_down_linear_shard_3",
          "layer_3_attention_qkv_linear_shard_3",
          "layer_3_attention_output_linear_shard_3",
          "layer_3_ffn_gate_up_linear_shard_3",
          "layer_3_ffn_down_linear_shard_3",
          "output_projection_shard_3"
        ]
      },
      "device_12": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_1_tensor_shard_4",
        "modules": [
          "layer_2_attention_qkv_linear_shard_4",
          "layer_2_attention_output_linear_shard_4",
          "layer_2_ffn_gate_up_linear_shard_4",
          "layer_2_ffn_down_linear_shard_4",
          "layer_3_attention_qkv_linear_shard_4",
          "layer_3_attention_output_linear_shard_4",
          "layer_3_ffn_gate_up_linear_shard_4",
          "layer_3_ffn_down_linear_shard_4",
          "output_projection_shard_4"
        ]
      },
      "device_13": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_1_tensor_shard_5",
        "modules": [
          "layer_2_attention_qkv_linear_shard_5",
          "layer_2_attention_output_linear_shard_5",
          "layer_2_ffn_gate_up_linear_shard_5",
          "layer_2_ffn_down_linear_shard_5",
          "layer_3_attention_qkv_linear_shard_5",
          "layer_3_attention_output_linear_shard_5",
          "layer_3_ffn_gate_up_linear_shard_5",
          "layer_3_ffn_down_linear_shard_5",
          "output_projection_shard_5"
        ]
      },
      "device_14": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_1_tensor_shard_6",
        "modules": [
          "layer_2_attention_qkv_linear_shard_6",
          "layer_2_attention_output_linear_shard_6",
          "layer_2_ffn_gate_up_linear_shard_6",
          "layer_2_ffn_down_linear_shard_6",
          "layer_3_attention_qkv_linear_shard_6",
          "layer_3_attention_output_linear_shard_6",
          "layer_3_ffn_gate_up_linear_shard_6",
          "layer_3_ffn_down_linear_shard_6",
          "output_projection_shard_6"
        ]
      },
      "device_15": {
        "model": "4-layer Dense Baseline",
        "role": "pipeline_stage_1_tensor_shard_7",
        "modules": [
          "layer_2_attention_qkv_linear_shard_7",
          "layer_2_attention_output_linear_shard_7",
          "layer_2_ffn_gate_up_linear_shard_7",
          "layer_2_ffn_down_linear_shard_7",
          "layer_3_attention_qkv_linear_shard_7",
          "layer_3_attention_output_linear_shard_7",
          "layer_3_ffn_gate_up_linear_shard_7",
          "layer_3_ffn_down_linear_shard_7",
          "output_projection_shard_7"
        ]
      }
    },
    "fa_pool": {
      "sequence_length_threshold": 4096,
      "dynamic_allocation": {
        "device_0": {
          "model": "4-layer Dense FA Pool",
          "role": "base_layer_tensor_shard_0",
          "modules": [
            "embeddings",
            "layer_norms",
            "layer_0_feed_forward_gate_up_linear_shard_0",
            "layer_0_feed_forward_down_linear_shard_0",
            "layer_1_feed_forward_gate_up_linear_shard_0",
            "layer_1_feed_forward_down_linear_shard_0",
            "layer_2_feed_forward_gate_up_linear_shard_0",
            "layer_2_feed_forward_down_linear_shard_0",
            "layer_3_feed_forward_gate_up_linear_shard_0",
            "layer_3_feed_forward_down_linear_shard_0",
            "output_projection_shard_0"
          ]
        },
        "device_1": {
          "model": "4-layer Dense FA Pool", 
          "role": "base_layer_tensor_shard_1",
          "modules": [
            "embeddings",
            "layer_norms",
            "layer_0_feed_forward_gate_up_linear_shard_1",
            "layer_0_feed_forward_down_linear_shard_1",
            "layer_1_feed_forward_gate_up_linear_shard_1",
            "layer_1_feed_forward_down_linear_shard_1",
            "layer_2_feed_forward_gate_up_linear_shard_1",
            "layer_2_feed_forward_down_linear_shard_1",
            "layer_3_feed_forward_gate_up_linear_shard_1",
            "layer_3_feed_forward_down_linear_shard_1",
            "output_projection_shard_1"
          ]
        },
        "device_2": {
          "model": "4-layer Dense FA Pool",
          "role": "base_layer_tensor_shard_2", 
          "modules": [
            "embeddings",
            "layer_norms",
            "layer_0_feed_forward_gate_up_linear_shard_2",
            "layer_0_feed_forward_down_linear_shard_2",
            "layer_1_feed_forward_gate_up_linear_shard_2",
            "layer_1_feed_forward_down_linear_shard_2",
            "layer_2_feed_forward_gate_up_linear_shard_2",
            "layer_2_feed_forward_down_linear_shard_2",
            "layer_3_feed_forward_gate_up_linear_shard_2",
            "layer_3_feed_forward_down_linear_shard_2",
            "output_projection_shard_2"
          ]
        },
        "device_3": {
          "model": "4-layer Dense FA Pool",
          "role": "base_layer_tensor_shard_3",
          "modules": [
            "embeddings", 
            "layer_norms",
            "layer_0_feed_forward_gate_up_linear_shard_3",
            "layer_0_feed_forward_down_linear_shard_3",
            "layer_1_feed_forward_gate_up_linear_shard_3",
            "layer_1_feed_forward_down_linear_shard_3",
            "layer_2_feed_forward_gate_up_linear_shard_3",
            "layer_2_feed_forward_down_linear_shard_3",
            "layer_3_feed_forward_gate_up_linear_shard_3",
            "layer_3_feed_forward_down_linear_shard_3",
            "output_projection_shard_3"
          ]
        },
        "device_4": {
          "model": "4-layer Dense FA Pool",
          "role": "base_layer_tensor_shard_4",
          "modules": [
            "embeddings",
            "layer_norms",
            "layer_0_feed_forward_gate_up_linear_shard_4", 
            "layer_0_feed_forward_down_linear_shard_4",
            "layer_1_feed_forward_gate_up_linear_shard_4",
            "layer_1_feed_forward_down_linear_shard_4",
            "layer_2_feed_forward_gate_up_linear_shard_4",
            "layer_2_feed_forward_down_linear_shard_4",
            "layer_3_feed_forward_gate_up_linear_shard_4",
            "layer_3_feed_forward_down_linear_shard_4",
            "output_projection_shard_4"
          ]
        },
        "device_5": {
          "model": "4-layer Dense FA Pool",
          "role": "base_layer_tensor_shard_5",
          "modules": [
            "embeddings",
            "layer_norms",
            "layer_0_feed_forward_gate_up_linear_shard_5",
            "layer_0_feed_forward_down_linear_shard_5",
            "layer_1_feed_forward_gate_up_linear_shard_5", 
            "layer_1_feed_forward_down_linear_shard_5",
            "layer_2_feed_forward_gate_up_linear_shard_5",
            "layer_2_feed_forward_down_linear_shard_5",
            "layer_3_feed_forward_gate_up_linear_shard_5",
            "layer_3_feed_forward_down_linear_shard_5",
            "output_projection_shard_5"
          ]
        },
        "device_6": {
          "model": "4-layer Dense FA Pool",
          "role": "base_layer_tensor_shard_6",
          "modules": [
            "embeddings",
            "layer_norms",
            "layer_0_feed_forward_gate_up_linear_shard_6",
            "layer_0_feed_forward_down_linear_shard_6",
            "layer_1_feed_forward_gate_up_linear_shard_6",
            "layer_1_feed_forward_down_linear_shard_6",
            "layer_2_feed_forward_gate_up_linear_shard_6",
            "layer_2_feed_forward_down_linear_shard_6",
            "layer_3_feed_forward_gate_up_linear_shard_6",
            "layer_3_feed_forward_down_linear_shard_6",
            "output_projection_shard_6"
          ]
        },
        "device_7": {
          "model": "4-layer Dense FA Pool",
          "role": "base_layer_tensor_shard_7",
          "modules": [
            "embeddings",
            "layer_norms",
            "layer_0_feed_forward_gate_up_linear_shard_7",
            "layer_0_feed_forward_down_linear_shard_7",
            "layer_1_feed_forward_gate_up_linear_shard_7",
            "layer_1_feed_forward_down_linear_shard_7",
            "layer_2_feed_forward_gate_up_linear_shard_7",
            "layer_2_feed_forward_down_linear_shard_7",
            "layer_3_feed_forward_gate_up_linear_shard_7",
            "layer_3_feed_forward_down_linear_shard_7",
            "output_projection_shard_7"
          ]
        }
      },
      "dynamic_attention_devices": {
        "device_8_to_39": {
          "activation_condition": "sequence_length > 4096",
          "allocation_rules": {
            "512_to_2048_tokens": {
              "pool_gpus": 2,
              "device_map": [8, 9],
              "sequence_blocks": ["0-1024", "1024-2048"]
            },
            "2049_to_4096_tokens": {
              "pool_gpus": 4,
              "device_map": [8, 9, 10, 11],
              "sequence_blocks": ["0-1024", "1024-2048", "2048-3072", "3072-4096"]
            },
            "4097_to_8192_tokens": {
              "pool_gpus": 8,
              "device_map": [8, 9, 10, 11, 12, 13, 14, 15],
              "sequence_blocks": ["0-1024", "1024-2048", "2048-3072", "3072-4096", "4096-5120", "5120-6144", "6144-7168", "7168-8192"]
            },
            "8193_to_12288_tokens": {
              "pool_gpus": 12,
              "device_map": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
              "sequence_blocks": ["0-1024", "1024-2048", "2048-3072", "3072-4096", "4096-5120", "5120-6144", "6144-7168", "7168-8192", "8192-9216", "9216-10240", "10240-11264", "11264-12288"]
            },
            "12289_to_16384_tokens": {
              "pool_gpus": 16,
              "device_map": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],
              "sequence_blocks": ["0-1024", "1024-2048", "2048-3072", "3072-4096", "4096-5120", "5120-6144", "6144-7168", "7168-8192", "8192-9216", "9216-10240", "10240-11264", "11264-12288", "12288-13312", "13312-14336", "14336-15360", "15360-16384"]
            },
            "16385_to_20480_tokens": {
              "pool_gpus": 20,
              "device_map": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27],
              "sequence_blocks": ["0-1024", "1024-2048", "2048-3072", "3072-4096", "4096-5120", "5120-6144", "6144-7168", "7168-8192", "8192-9216", "9216-10240", "10240-11264", "11264-12288", "12288-13312", "13312-14336", "14336-15360", "15360-16384", "16384-17408", "17408-18432", "18432-19456", "19456-20480"]
            },
            "20481_to_24576_tokens": {
              "pool_gpus": 24,
              "device_map": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],
              "sequence_blocks": ["0-1024", "1024-2048", "2048-3072", "3072-4096", "4096-5120", "5120-6144", "6144-7168", "7168-8192", "8192-9216", "9216-10240", "10240-11264", "11264-12288", "12288-13312", "13312-14336", "14336-15360", "15360-16384", "16384-17408", "17408-18432", "18432-19456", "19456-20480", "20480-21504", "21504-22528", "22528-23552", "23552-24576"]
            },
            "24577_to_32768_tokens": {
              "pool_gpus": 32,
              "device_map": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39],
              "sequence_blocks": ["0-1024", "1024-2048", "2048-3072", "3072-4096", "4096-5120", "5120-6144", "6144-7168", "7168-8192", "8192-9216", "9216-10240", "10240-11264", "11264-12288", "12288-13312", "13312-14336", "14336-15360", "15360-16384", "16384-17408", "17408-18432", "18432-19456", "19456-20480", "20480-21504", "21504-22528", "22528-23552", "23552-24576", "24576-25600", "25600-26624", "26624-27648", "27648-28672", "28672-29696", "29696-30720", "30720-31744", "31744-32768"]
            }
          },
          "attention_modules_per_device": {
            "device_mapping": "dynamic",
            "layer_0_attention": {
              "qkv_linear": {
                "partition": "replicated_full_matrix",
                "dimensions": [4096, 4096],
                "memory": "KV_cache_shared"
              },
              "attention_computation": {
                "type": "flash_attention_block_wise",
                "block_size": "ceil(sequence_length / pool_gpus)",
                "local_query_range": "device_specific_block"
              }
            },
            "layer_1_attention": {
              "qkv_linear": {
                "partition": "replicated_full_matrix", 
                "dimensions": [4096, 4096],
                "memory": "KV_cache_shared"
              },
              "attention_computation": {
                "type": "flash_attention_block_wise",
                "block_size": "ceil(sequence_length / pool_gpus)",
                "local_query_range": "device_specific_block"
              }
            },
            "layer_2_attention": {
              "qkv_linear": {
                "partition": "replicated_full_matrix",
                "dimensions": [4096, 4096], 
                "memory": "KV_cache_shared"
              },
              "attention_computation": {
                "type": "flash_attention_block_wise",
                "block_size": "ceil(sequence_length / pool_gpus)",
                "local_query_range": "device_specific_block"
              }
            },
            "layer_3_attention": {
              "qkv_linear": {
                "partition": "replicated_full_matrix",
                "dimensions": [4096, 4096],
                "memory": "KV_cache_shared"
              },
              "attention_computation": {
                "type": "flash_attention_block_wise", 
                "block_size": "ceil(sequence_length / pool_gpus)",
                "local_query_range": "device_specific_block"
              }
            }
          }
        }
      }
    }
  },
  "communication_config": {
    "baseline": {
      "tensor_parallel_communication": {
        "type": "all_reduce",
        "algorithm": "ring",
        "message_size": "4096 * 4096 * 4 bytes",
        "participants": [0, 1, 2, 3, 4, 5, 6, 7]
      },
      "pipeline_parallel_communication": {
        "type": "point_to_point",
        "stages": [
          {"from": [0,1,2,3,4,5,6,7], "to": [8,9,10,11,12,13,14,15], "size": "batch_size * hidden_dim * 4 bytes"},
          {"from": [8,9,10,11,12,13,14,15], "to": [0,1,2,3,4,5,6,7], "size": "batch_size * hidden_dim * 4 bytes"}
        ]
      }
    },
    "fa_pool": {
      "base_layer_communication": {
        "tensor_parallel": {
          "type": "all_reduce",
          "algorithm": "ring",
          "message_size": "4096 * 4096 * 4 bytes",
          "participants": [0, 1, 2, 3, 4, 5, 6, 7]
        }
      },
      "attention_pool_communication": {
        "kv_cache_broadcast": {
          "type": "broadcast",
          "source": [0,1,2,3,4,5,6,7],
          "targets": "dynamic_pool_devices",
          "message_size": "sequence_length * 4096 * 2 * 4 bytes"
        },
        "result_reduction": {
          "type": "hierarchical_all_gather",
          "algorithm": "tree_reduction",
          "message_size": "sequence_length * 4096 * 4 bytes / pool_size"
        }
      }
    }
  }
}