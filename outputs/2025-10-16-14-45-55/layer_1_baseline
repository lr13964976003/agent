// Layer 1 - Baseline
digraph layer_1_baseline {
	nodesep=0.3 rankdir=TB ranksep=0.8 splines=ortho
	node [fillcolor=lightblue shape=rectangle style=filled]
	layer1_input [label="Layer 1 Input
Input: [1024,2048,4096]
GPU: 0-7" fillcolor=lightgreen shape=ellipse]
	l1_mha_qkv [label="QKV Projection
Input: [1024,2048,4096]
Output: [1024,2048,128] per GPU
GPU: 0-7" fillcolor=yellow]
	l1_mha_attn [label="Multi-Head Attention
Input: [1024,2048,128]
Output: [1024,2048,128]
GPU: 0-7" fillcolor=yellow]
	l1_mha_out [label="Output Projection
Input: [1024,2048,128]
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=yellow]
	l1_mha_res [label="Residual Add
Input: [1024,2048,4096], [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=orange]
	l1_moe_gate [label="Gate Network
Input: [1024,2048,4096]
Output: [1024,2048,16]
GPU: 0-7" fillcolor=lightcoral]
	l1_exp0_gpu0 [label="Experts 0,1
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 0" fillcolor=lightpink]
	l1_exp2_gpu1 [label="Experts 2,3
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 1" fillcolor=lightpink]
	l1_exp4_gpu2 [label="Experts 4,5
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 2" fillcolor=lightpink]
	l1_exp6_gpu3 [label="Experts 6,7
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 3" fillcolor=lightpink]
	l1_exp8_gpu4 [label="Experts 8,9
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 4" fillcolor=lightpink]
	l1_exp10_gpu5 [label="Experts 10,11
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 5" fillcolor=lightpink]
	l1_exp12_gpu6 [label="Experts 12,13
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 6" fillcolor=lightpink]
	l1_exp14_gpu7 [label="Experts 14,15
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 7" fillcolor=lightpink]
	l1_moe_agg [label="Expert Aggregation
Input: [1024,2048,4096]Ã—8
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=lightcyan]
	l1_moe_res [label="Residual Add
Input: [1024,2048,4096], [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=orange]
	layer1_input -> l1_mha_qkv
	l1_mha_qkv -> l1_mha_attn
	l1_mha_attn -> l1_mha_out
	l1_mha_out -> l1_mha_res
	layer1_input -> l1_mha_res
	l1_mha_res -> l1_moe_gate
	l1_moe_gate -> l1_exp0_gpu0 [style=dashed]
	l1_mha_res -> l1_exp0_gpu0
	l1_exp0_gpu0 -> l1_moe_agg
	l1_moe_gate -> l1_exp2_gpu1 [style=dashed]
	l1_mha_res -> l1_exp2_gpu1
	l1_exp2_gpu1 -> l1_moe_agg
	l1_moe_gate -> l1_exp4_gpu2 [style=dashed]
	l1_mha_res -> l1_exp4_gpu2
	l1_exp4_gpu2 -> l1_moe_agg
	l1_moe_gate -> l1_exp6_gpu3 [style=dashed]
	l1_mha_res -> l1_exp6_gpu3
	l1_exp6_gpu3 -> l1_moe_agg
	l1_moe_gate -> l1_exp8_gpu4 [style=dashed]
	l1_mha_res -> l1_exp8_gpu4
	l1_exp8_gpu4 -> l1_moe_agg
	l1_moe_gate -> l1_exp10_gpu5 [style=dashed]
	l1_mha_res -> l1_exp10_gpu5
	l1_exp10_gpu5 -> l1_moe_agg
	l1_moe_gate -> l1_exp12_gpu6 [style=dashed]
	l1_mha_res -> l1_exp12_gpu6
	l1_exp12_gpu6 -> l1_moe_agg
	l1_moe_gate -> l1_exp14_gpu7 [style=dashed]
	l1_mha_res -> l1_exp14_gpu7
	l1_exp14_gpu7 -> l1_moe_agg
	l1_moe_agg -> l1_moe_res
	l1_mha_res -> l1_moe_res
}
