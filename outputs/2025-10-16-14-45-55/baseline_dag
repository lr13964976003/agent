// Baseline MoE Model with TP=8, PP=2
digraph baseline_moe_model {
	nodesep=0.5 rankdir=TB ranksep=1.0 splines=ortho
	node [fillcolor=lightblue shape=rectangle style=filled]
	input [label="Input Embedding
Input: [batch_size=1024, seq_len=2048, hidden=4096]
GPU: all GPUs" fillcolor=lightgreen shape=ellipse]
	subgraph cluster_pipeline_0 {
		color=red label="Pipeline Stage 0 (Layers 0-1)
GPUs 0-7" style=dashed
		subgraph cluster_layer_0 {
			label="Layer 0" style=rounded
			l0_mha_qkv [label="QKV Projection
Input: [1024,2048,4096]
Output: [1024,2048,128] per GPU
GPU: 0-7" fillcolor=yellow]
			l0_mha_attn [label="Attention Computation
Input: [1024,2048,128]
Output: [1024,2048,128]
GPU: 0-7" fillcolor=yellow]
			l0_mha_out [label="Output Projection
Input: [1024,2048,128]
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=yellow]
			l0_mha_res [label="Residual Add
Input: [1024,2048,4096], [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=orange]
			l0_moe_gate [label="Gate Network
Input: [1024,2048,4096]
Output: [1024,2048,16]
GPU: 0-7" fillcolor=lightcoral]
			l0_moe_exp0 [label="Expert 0,1
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 0-1" fillcolor=lightpink]
			l0_moe_exp2 [label="Expert 2,3
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 2-3" fillcolor=lightpink]
			l0_moe_exp4 [label="Expert 4,5
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 4-5" fillcolor=lightpink]
			l0_moe_exp6 [label="Expert 6,7
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 6-7" fillcolor=lightpink]
			l0_moe_agg [label="Expert Aggregation
Input: [1024,2048,4096]×2
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=lightcyan]
			l0_moe_res [label="Residual Add
Input: [1024,2048,4096], [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=orange]
		}
		subgraph cluster_layer_1 {
			label="Layer 1" style=rounded
			l1_mha_qkv [label="QKV Projection
Input: [1024,2048,4096]
Output: [1024,2048,128] per GPU
GPU: 0-7" fillcolor=yellow]
			l1_mha_attn [label="Attention Computation
Input: [1024,2048,128]
Output: [1024,2048,128]
GPU: 0-7" fillcolor=yellow]
			l1_mha_out [label="Output Projection
Input: [1024,2048,128]
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=yellow]
			l1_mha_res [label="Residual Add
Input: [1024,2048,4096], [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=orange]
			l1_moe_gate [label="Gate Network
Input: [1024,2048,4096]
Output: [1024,2048,16]
GPU: 0-7" fillcolor=lightcoral]
			l1_moe_exp0 [label="Expert 0,1
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 0-1" fillcolor=lightpink]
			l1_moe_exp2 [label="Expert 2,3
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 2-3" fillcolor=lightpink]
			l1_moe_exp4 [label="Expert 4,5
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 4-5" fillcolor=lightpink]
			l1_moe_exp6 [label="Expert 6,7
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 6-7" fillcolor=lightpink]
			l1_moe_agg [label="Expert Aggregation
Input: [1024,2048,4096]×2
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=lightcyan]
			l1_moe_res [label="Residual Add
Input: [1024,2048,4096], [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=orange]
		}
	}
	stage0_to_stage1 [label="Pipeline Communication
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 0-7 → 8-15" fillcolor=gray shape=parallelogram]
	subgraph cluster_pipeline_1 {
		color=blue label="Pipeline Stage 1 (Layers 2-3)
GPUs 8-15" style=dashed
		subgraph cluster_layer_2 {
			label="Layer 2" style=rounded
			l2_mha_qkv [label="QKV Projection
Input: [1024,2048,4096]
Output: [1024,2048,128] per GPU
GPU: 8-15" fillcolor=yellow]
			l2_mha_attn [label="Attention Computation
Input: [1024,2048,128]
Output: [1024,2048,128]
GPU: 8-15" fillcolor=yellow]
			l2_mha_out [label="Output Projection
Input: [1024,2048,128]
Output: [1024,2048,4096]
GPU: 8-15" fillcolor=yellow]
			l2_mha_res [label="Residual Add
Input: [1024,2048,4096], [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 8-15" fillcolor=orange]
			l2_moe_gate [label="Gate Network
Input: [1024,2048,4096]
Output: [1024,2048,16]
GPU: 8-15" fillcolor=lightcoral]
			l2_moe_exp0 [label="Expert 0,1
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 8-9" fillcolor=lightpink]
			l2_moe_exp2 [label="Expert 2,3
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 10-11" fillcolor=lightpink]
			l2_moe_exp4 [label="Expert 4,5
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 12-13" fillcolor=lightpink]
			l2_moe_exp6 [label="Expert 6,7
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 14-15" fillcolor=lightpink]
			l2_moe_agg [label="Expert Aggregation
Input: [1024,2048,4096]×2
Output: [1024,2048,4096]
GPU: 8-15" fillcolor=lightcyan]
			l2_moe_res [label="Residual Add
Input: [1024,2048,4096], [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 8-15" fillcolor=orange]
		}
		subgraph cluster_layer_3 {
			label="Layer 3" style=rounded
			l3_mha_qkv [label="QKV Projection
Input: [1024,2048,4096]
Output: [1024,2048,128] per GPU
GPU: 8-15" fillcolor=yellow]
			l3_mha_attn [label="Attention Computation
Input: [1024,2048,128]
Output: [1024,2048,128]
GPU: 8-15" fillcolor=yellow]
			l3_mha_out [label="Output Projection
Input: [1024,2048,128]
Output: [1024,2048,4096]
GPU: 8-15" fillcolor=yellow]
			l3_mha_res [label="Residual Add
Input: [1024,2048,4096], [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 8-15" fillcolor=orange]
			l3_moe_gate [label="Gate Network
Input: [1024,2048,4096]
Output: [1024,2048,16]
GPU: 8-15" fillcolor=lightcoral]
			l3_moe_exp0 [label="Expert 0,1
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 8-9" fillcolor=lightpink]
			l3_moe_exp2 [label="Expert 2,3
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 10-11" fillcolor=lightpink]
			l3_moe_exp4 [label="Expert 4,5
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 12-13" fillcolor=lightpink]
			l3_moe_exp6 [label="Expert 6,7
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 14-15" fillcolor=lightpink]
			l3_moe_agg [label="Expert Aggregation
Input: [1024,2048,4096]×2
Output: [1024,2048,4096]
GPU: 8-15" fillcolor=lightcyan]
			l3_moe_res [label="Residual Add
Input: [1024,2048,4096], [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 8-15" fillcolor=orange]
		}
	}
	output [label="Final Output
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 8-15" fillcolor=lightgreen shape=ellipse]
	input -> l0_mha_qkv
	l0_mha_qkv -> l0_mha_attn
	l0_mha_attn -> l0_mha_out
	l0_mha_out -> l0_mha_res
	input -> l0_mha_res
	l0_mha_res -> l0_moe_gate
	l0_moe_gate -> l0_moe_exp0 [style=dashed]
	l0_moe_gate -> l0_moe_exp2 [style=dashed]
	l0_moe_gate -> l0_moe_exp4 [style=dashed]
	l0_moe_gate -> l0_moe_exp6 [style=dashed]
	l0_moe_exp0 -> l0_moe_agg
	l0_moe_exp2 -> l0_moe_agg
	l0_moe_exp4 -> l0_moe_agg
	l0_moe_exp6 -> l0_moe_agg
	l0_moe_agg -> l0_moe_res
	l0_mha_res -> l0_moe_res
	l0_moe_res -> l1_mha_qkv
	l1_mha_qkv -> l1_mha_attn
	l1_mha_attn -> l1_mha_out
	l1_mha_out -> l1_mha_res
	l0_moe_res -> l1_mha_res
	l1_mha_res -> l1_moe_gate
	l1_moe_gate -> l1_moe_exp0 [style=dashed]
	l1_moe_gate -> l1_moe_exp2 [style=dashed]
	l1_moe_gate -> l1_moe_exp4 [style=dashed]
	l1_moe_gate -> l1_moe_exp6 [style=dashed]
	l1_moe_exp0 -> l1_moe_agg
	l1_moe_exp2 -> l1_moe_agg
	l1_moe_exp4 -> l1_moe_agg
	l1_moe_exp6 -> l1_moe_agg
	l1_moe_agg -> l1_moe_res
	l1_mha_res -> l1_moe_res
	l1_moe_res -> stage0_to_stage1
	stage0_to_stage1 -> l2_mha_qkv
	l2_mha_qkv -> l2_mha_attn
	l2_mha_attn -> l2_mha_out
	l2_mha_out -> l2_mha_res
	stage0_to_stage1 -> l2_mha_res
	l2_mha_res -> l2_moe_gate
	l2_moe_gate -> l2_moe_exp0 [style=dashed]
	l2_moe_gate -> l2_moe_exp2 [style=dashed]
	l2_moe_gate -> l2_moe_exp4 [style=dashed]
	l2_moe_gate -> l2_moe_exp6 [style=dashed]
	l2_moe_exp0 -> l2_moe_agg
	l2_moe_exp2 -> l2_moe_agg
	l2_moe_exp4 -> l2_moe_agg
	l2_moe_exp6 -> l2_moe_agg
	l2_moe_agg -> l2_moe_res
	l2_mha_res -> l2_moe_res
	l2_moe_res -> l3_mha_qkv
	l3_mha_qkv -> l3_mha_attn
	l3_mha_attn -> l3_mha_out
	l3_mha_out -> l3_mha_res
	l2_moe_res -> l3_mha_res
	l3_mha_res -> l3_moe_gate
	l3_moe_gate -> l3_moe_exp0 [style=dashed]
	l3_moe_gate -> l3_moe_exp2 [style=dashed]
	l3_moe_gate -> l3_moe_exp4 [style=dashed]
	l3_moe_gate -> l3_moe_exp6 [style=dashed]
	l3_moe_exp0 -> l3_moe_agg
	l3_moe_exp2 -> l3_moe_agg
	l3_moe_exp4 -> l3_moe_agg
	l3_moe_exp6 -> l3_moe_agg
	l3_moe_agg -> l3_moe_res
	l3_mha_res -> l3_moe_res
	l3_moe_res -> output
}
