// Layer 2 - Baseline
digraph layer_2_baseline {
	nodesep=0.3 rankdir=TB ranksep=0.8 splines=ortho
	node [fillcolor=lightblue shape=rectangle style=filled]
	layer2_input [label="Layer 2 Input
Input: [1024,2048,4096]
GPU: 0-7" fillcolor=lightgreen shape=ellipse]
	l2_mha_qkv [label="QKV Projection
Input: [1024,2048,4096]
Output: [1024,2048,128] per GPU
GPU: 0-7" fillcolor=yellow]
	l2_mha_attn [label="Multi-Head Attention
Input: [1024,2048,128]
Output: [1024,2048,128]
GPU: 0-7" fillcolor=yellow]
	l2_mha_out [label="Output Projection
Input: [1024,2048,128]
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=yellow]
	l2_mha_res [label="Residual Add
Input: [1024,2048,4096], [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=orange]
	l2_moe_gate [label="Gate Network
Input: [1024,2048,4096]
Output: [1024,2048,16]
GPU: 0-7" fillcolor=lightcoral]
	l2_exp0_gpu0 [label="Experts 0,1
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 0" fillcolor=lightpink]
	l2_exp2_gpu1 [label="Experts 2,3
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 1" fillcolor=lightpink]
	l2_exp4_gpu2 [label="Experts 4,5
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 2" fillcolor=lightpink]
	l2_exp6_gpu3 [label="Experts 6,7
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 3" fillcolor=lightpink]
	l2_exp8_gpu4 [label="Experts 8,9
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 4" fillcolor=lightpink]
	l2_exp10_gpu5 [label="Experts 10,11
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 5" fillcolor=lightpink]
	l2_exp12_gpu6 [label="Experts 12,13
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 6" fillcolor=lightpink]
	l2_exp14_gpu7 [label="Experts 14,15
Input: [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 7" fillcolor=lightpink]
	l2_moe_agg [label="Expert Aggregation
Input: [1024,2048,4096]Ã—8
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=lightcyan]
	l2_moe_res [label="Residual Add
Input: [1024,2048,4096], [1024,2048,4096]
Output: [1024,2048,4096]
GPU: 0-7" fillcolor=orange]
	layer2_input -> l2_mha_qkv
	l2_mha_qkv -> l2_mha_attn
	l2_mha_attn -> l2_mha_out
	l2_mha_out -> l2_mha_res
	layer2_input -> l2_mha_res
	l2_mha_res -> l2_moe_gate
	l2_moe_gate -> l2_exp0_gpu0 [style=dashed]
	l2_mha_res -> l2_exp0_gpu0
	l2_exp0_gpu0 -> l2_moe_agg
	l2_moe_gate -> l2_exp2_gpu1 [style=dashed]
	l2_mha_res -> l2_exp2_gpu1
	l2_exp2_gpu1 -> l2_moe_agg
	l2_moe_gate -> l2_exp4_gpu2 [style=dashed]
	l2_mha_res -> l2_exp4_gpu2
	l2_exp4_gpu2 -> l2_moe_agg
	l2_moe_gate -> l2_exp6_gpu3 [style=dashed]
	l2_mha_res -> l2_exp6_gpu3
	l2_exp6_gpu3 -> l2_moe_agg
	l2_moe_gate -> l2_exp8_gpu4 [style=dashed]
	l2_mha_res -> l2_exp8_gpu4
	l2_exp8_gpu4 -> l2_moe_agg
	l2_moe_gate -> l2_exp10_gpu5 [style=dashed]
	l2_mha_res -> l2_exp10_gpu5
	l2_exp10_gpu5 -> l2_moe_agg
	l2_moe_gate -> l2_exp12_gpu6 [style=dashed]
	l2_mha_res -> l2_exp12_gpu6
	l2_exp12_gpu6 -> l2_moe_agg
	l2_moe_gate -> l2_exp14_gpu7 [style=dashed]
	l2_mha_res -> l2_exp14_gpu7
	l2_exp14_gpu7 -> l2_moe_agg
	l2_moe_agg -> l2_moe_res
	l2_mha_res -> l2_moe_res
}
