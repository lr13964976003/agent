{
  "generated_dags": {
    "baseline": {
      "description": "Baseline Tensor + Pipeline Parallelism (TP=8, PP=2) on 16 GPUs",
      "model": "16-layer dense network",
      "parallel_strategy": "Hybrid Tensor + Pipeline Parallelism",
      "files": {
        "dot_file": "../outputs/2025-10-19-20-32-46/baseline_tensor_pipeline_parallel.dot",
        "svg_file": "../outputs/2025-10-19-20-32-46/baseline_tensor_pipeline_parallel.svg"
      },
      "key_features": {
        "pipeline_stages": 2,
        "tensor_parallel_groups": 8,
        "gpus_per_stage": 8,
        "total_gpus": 16,
        "layers_per_stage": [8, 8]
      }
    },
    "proposed": {
      "description": "Proposed Layer-wise Cache-Optimized Deployment (1 layer per GPU)",
      "model": "16-layer dense network",
      "parallel_strategy": "Layer-wise Parallelism with Cache Constraints",
      "files": {
        "dot_file": "../outputs/2025-10-19-20-32-46/proposed_layer_wise_cache_optimized.dot",
        "svg_file": "../outputs/2025-10-19-20-32-46/proposed_layer_wise_cache_optimized.svg"
      },
      "key_features": {
        "layers_per_gpu": 1,
        "total_gpus": 16,
        "cache_utilization": "100% per device",
        "communication_pattern": "point-to-point sequential"
      }
    }
  },
  "verification": {
    "both_dags_valid": true,
    "no_cycles_detected": true,
    "complete_model_coverage": true,
    "dimension_consistency": "verified",
    "gpu_load_balancing": "optimized for both strategies"
  },
  "technical_specifications": {
    "batch_size": 1024,
    "sequence_length": 10000,
    "hidden_size": 8192,
    "num_heads": 16,
    "head_dim": 512,
    "mlp_hidden_size": 32768,
    "precision": "FP16"
  }
}