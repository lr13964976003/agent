digraph proposed_layer_wise_cache_optimized {
	rankdir=TB size="200,400"
	node [fontname=Arial fontsize=10]
	node [shape=rectangle style=filled]
	edge [fontname=Arial fontsize=8]
	layer1_transfer [label="Layer 1 → 2 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 0
To: GPU 1" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_0 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 0 - Layer 1
100% SRAM/L2 Cache" style=filled
		layer1_input [label="Layer 1 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" fillcolor=lightyellow shape=parallelogram]
		layer1_mha_query [label="Layer 1 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 0" fillcolor=pink]
		layer1_mha_key [label="Layer 1 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 0" fillcolor=pink]
		layer1_mha_value [label="Layer 1 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 0" fillcolor=pink]
		layer1_mha_qkt [label="Layer 1 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 0" fillcolor=lightcoral]
		layer1_mha_softmax [label="Layer 1 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 0" fillcolor=lightcoral]
		layer1_mha_apply [label="Layer 1 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 0" fillcolor=lightcoral]
		layer1_mha_out [label="Layer 1 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" fillcolor=pink]
		layer1_residual1 [label="Layer 1 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" fillcolor=lightblue shape=diamond]
		layer1_layernorm1 [label="Layer 1 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" fillcolor=lightgreen]
		layer1_mlp_gate [label="Layer 1 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 0" fillcolor=lightyellow]
		layer1_mlp_up [label="Layer 1 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 0" fillcolor=lightyellow]
		layer1_mlp_gelu [label="Layer 1 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 0" fillcolor=orange]
		layer1_mlp_multiply [label="Layer 1 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 0" fillcolor=orange]
		layer1_mlp_down [label="Layer 1 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" fillcolor=lightyellow]
		layer1_residual2 [label="Layer 1 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" fillcolor=lightblue shape=diamond]
		layer1_layernorm2 [label="Layer 1 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" fillcolor=lightgreen]
	}
	layer2_transfer [label="Layer 2 → 3 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 1
To: GPU 2" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_1 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 1 - Layer 2
100% SRAM/L2 Cache" style=filled
		layer2_input [label="Layer 2 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" fillcolor=lightyellow shape=parallelogram]
		layer2_mha_query [label="Layer 2 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 1" fillcolor=pink]
		layer2_mha_key [label="Layer 2 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 1" fillcolor=pink]
		layer2_mha_value [label="Layer 2 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 1" fillcolor=pink]
		layer2_mha_qkt [label="Layer 2 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 1" fillcolor=lightcoral]
		layer2_mha_softmax [label="Layer 2 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 1" fillcolor=lightcoral]
		layer2_mha_apply [label="Layer 2 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 1" fillcolor=lightcoral]
		layer2_mha_out [label="Layer 2 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" fillcolor=pink]
		layer2_residual1 [label="Layer 2 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" fillcolor=lightblue shape=diamond]
		layer2_layernorm1 [label="Layer 2 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" fillcolor=lightgreen]
		layer2_mlp_gate [label="Layer 2 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 1" fillcolor=lightyellow]
		layer2_mlp_up [label="Layer 2 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 1" fillcolor=lightyellow]
		layer2_mlp_gelu [label="Layer 2 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 1" fillcolor=orange]
		layer2_mlp_multiply [label="Layer 2 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 1" fillcolor=orange]
		layer2_mlp_down [label="Layer 2 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" fillcolor=lightyellow]
		layer2_residual2 [label="Layer 2 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" fillcolor=lightblue shape=diamond]
		layer2_layernorm2 [label="Layer 2 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" fillcolor=lightgreen]
	}
	layer3_transfer [label="Layer 3 → 4 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 2
To: GPU 3" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_2 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 2 - Layer 3
100% SRAM/L2 Cache" style=filled
		layer3_input [label="Layer 3 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" fillcolor=lightyellow shape=parallelogram]
		layer3_mha_query [label="Layer 3 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 2" fillcolor=pink]
		layer3_mha_key [label="Layer 3 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 2" fillcolor=pink]
		layer3_mha_value [label="Layer 3 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 2" fillcolor=pink]
		layer3_mha_qkt [label="Layer 3 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 2" fillcolor=lightcoral]
		layer3_mha_softmax [label="Layer 3 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 2" fillcolor=lightcoral]
		layer3_mha_apply [label="Layer 3 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 2" fillcolor=lightcoral]
		layer3_mha_out [label="Layer 3 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" fillcolor=pink]
		layer3_residual1 [label="Layer 3 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" fillcolor=lightblue shape=diamond]
		layer3_layernorm1 [label="Layer 3 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" fillcolor=lightgreen]
		layer3_mlp_gate [label="Layer 3 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 2" fillcolor=lightyellow]
		layer3_mlp_up [label="Layer 3 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 2" fillcolor=lightyellow]
		layer3_mlp_gelu [label="Layer 3 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 2" fillcolor=orange]
		layer3_mlp_multiply [label="Layer 3 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 2" fillcolor=orange]
		layer3_mlp_down [label="Layer 3 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" fillcolor=lightyellow]
		layer3_residual2 [label="Layer 3 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" fillcolor=lightblue shape=diamond]
		layer3_layernorm2 [label="Layer 3 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" fillcolor=lightgreen]
	}
	layer4_transfer [label="Layer 4 → 5 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 3
To: GPU 4" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_3 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 3 - Layer 4
100% SRAM/L2 Cache" style=filled
		layer4_input [label="Layer 4 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" fillcolor=lightyellow shape=parallelogram]
		layer4_mha_query [label="Layer 4 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 3" fillcolor=pink]
		layer4_mha_key [label="Layer 4 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 3" fillcolor=pink]
		layer4_mha_value [label="Layer 4 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 3" fillcolor=pink]
		layer4_mha_qkt [label="Layer 4 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 3" fillcolor=lightcoral]
		layer4_mha_softmax [label="Layer 4 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 3" fillcolor=lightcoral]
		layer4_mha_apply [label="Layer 4 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 3" fillcolor=lightcoral]
		layer4_mha_out [label="Layer 4 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" fillcolor=pink]
		layer4_residual1 [label="Layer 4 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" fillcolor=lightblue shape=diamond]
		layer4_layernorm1 [label="Layer 4 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" fillcolor=lightgreen]
		layer4_mlp_gate [label="Layer 4 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 3" fillcolor=lightyellow]
		layer4_mlp_up [label="Layer 4 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 3" fillcolor=lightyellow]
		layer4_mlp_gelu [label="Layer 4 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 3" fillcolor=orange]
		layer4_mlp_multiply [label="Layer 4 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 3" fillcolor=orange]
		layer4_mlp_down [label="Layer 4 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" fillcolor=lightyellow]
		layer4_residual2 [label="Layer 4 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" fillcolor=lightblue shape=diamond]
		layer4_layernorm2 [label="Layer 4 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" fillcolor=lightgreen]
	}
	layer5_transfer [label="Layer 5 → 6 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 4
To: GPU 5" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_4 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 4 - Layer 5
100% SRAM/L2 Cache" style=filled
		layer5_input [label="Layer 5 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" fillcolor=lightyellow shape=parallelogram]
		layer5_mha_query [label="Layer 5 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 4" fillcolor=pink]
		layer5_mha_key [label="Layer 5 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 4" fillcolor=pink]
		layer5_mha_value [label="Layer 5 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 4" fillcolor=pink]
		layer5_mha_qkt [label="Layer 5 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 4" fillcolor=lightcoral]
		layer5_mha_softmax [label="Layer 5 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 4" fillcolor=lightcoral]
		layer5_mha_apply [label="Layer 5 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 4" fillcolor=lightcoral]
		layer5_mha_out [label="Layer 5 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" fillcolor=pink]
		layer5_residual1 [label="Layer 5 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" fillcolor=lightblue shape=diamond]
		layer5_layernorm1 [label="Layer 5 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" fillcolor=lightgreen]
		layer5_mlp_gate [label="Layer 5 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 4" fillcolor=lightyellow]
		layer5_mlp_up [label="Layer 5 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 4" fillcolor=lightyellow]
		layer5_mlp_gelu [label="Layer 5 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 4" fillcolor=orange]
		layer5_mlp_multiply [label="Layer 5 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 4" fillcolor=orange]
		layer5_mlp_down [label="Layer 5 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" fillcolor=lightyellow]
		layer5_residual2 [label="Layer 5 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" fillcolor=lightblue shape=diamond]
		layer5_layernorm2 [label="Layer 5 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" fillcolor=lightgreen]
	}
	layer6_transfer [label="Layer 6 → 7 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 5
To: GPU 6" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_5 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 5 - Layer 6
100% SRAM/L2 Cache" style=filled
		layer6_input [label="Layer 6 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" fillcolor=lightyellow shape=parallelogram]
		layer6_mha_query [label="Layer 6 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 5" fillcolor=pink]
		layer6_mha_key [label="Layer 6 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 5" fillcolor=pink]
		layer6_mha_value [label="Layer 6 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 5" fillcolor=pink]
		layer6_mha_qkt [label="Layer 6 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 5" fillcolor=lightcoral]
		layer6_mha_softmax [label="Layer 6 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 5" fillcolor=lightcoral]
		layer6_mha_apply [label="Layer 6 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 5" fillcolor=lightcoral]
		layer6_mha_out [label="Layer 6 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" fillcolor=pink]
		layer6_residual1 [label="Layer 6 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" fillcolor=lightblue shape=diamond]
		layer6_layernorm1 [label="Layer 6 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" fillcolor=lightgreen]
		layer6_mlp_gate [label="Layer 6 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 5" fillcolor=lightyellow]
		layer6_mlp_up [label="Layer 6 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 5" fillcolor=lightyellow]
		layer6_mlp_gelu [label="Layer 6 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 5" fillcolor=orange]
		layer6_mlp_multiply [label="Layer 6 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 5" fillcolor=orange]
		layer6_mlp_down [label="Layer 6 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" fillcolor=lightyellow]
		layer6_residual2 [label="Layer 6 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" fillcolor=lightblue shape=diamond]
		layer6_layernorm2 [label="Layer 6 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" fillcolor=lightgreen]
	}
	layer7_transfer [label="Layer 7 → 8 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 6
To: GPU 7" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_6 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 6 - Layer 7
100% SRAM/L2 Cache" style=filled
		layer7_input [label="Layer 7 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" fillcolor=lightyellow shape=parallelogram]
		layer7_mha_query [label="Layer 7 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 6" fillcolor=pink]
		layer7_mha_key [label="Layer 7 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 6" fillcolor=pink]
		layer7_mha_value [label="Layer 7 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 6" fillcolor=pink]
		layer7_mha_qkt [label="Layer 7 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 6" fillcolor=lightcoral]
		layer7_mha_softmax [label="Layer 7 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 6" fillcolor=lightcoral]
		layer7_mha_apply [label="Layer 7 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 6" fillcolor=lightcoral]
		layer7_mha_out [label="Layer 7 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" fillcolor=pink]
		layer7_residual1 [label="Layer 7 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" fillcolor=lightblue shape=diamond]
		layer7_layernorm1 [label="Layer 7 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" fillcolor=lightgreen]
		layer7_mlp_gate [label="Layer 7 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 6" fillcolor=lightyellow]
		layer7_mlp_up [label="Layer 7 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 6" fillcolor=lightyellow]
		layer7_mlp_gelu [label="Layer 7 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 6" fillcolor=orange]
		layer7_mlp_multiply [label="Layer 7 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 6" fillcolor=orange]
		layer7_mlp_down [label="Layer 7 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" fillcolor=lightyellow]
		layer7_residual2 [label="Layer 7 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" fillcolor=lightblue shape=diamond]
		layer7_layernorm2 [label="Layer 7 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" fillcolor=lightgreen]
	}
	layer8_transfer [label="Layer 8 → 9 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 7
To: GPU 8" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_7 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 7 - Layer 8
100% SRAM/L2 Cache" style=filled
		layer8_input [label="Layer 8 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" fillcolor=lightyellow shape=parallelogram]
		layer8_mha_query [label="Layer 8 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 7" fillcolor=pink]
		layer8_mha_key [label="Layer 8 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 7" fillcolor=pink]
		layer8_mha_value [label="Layer 8 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 7" fillcolor=pink]
		layer8_mha_qkt [label="Layer 8 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 7" fillcolor=lightcoral]
		layer8_mha_softmax [label="Layer 8 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 7" fillcolor=lightcoral]
		layer8_mha_apply [label="Layer 8 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 7" fillcolor=lightcoral]
		layer8_mha_out [label="Layer 8 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" fillcolor=pink]
		layer8_residual1 [label="Layer 8 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" fillcolor=lightblue shape=diamond]
		layer8_layernorm1 [label="Layer 8 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" fillcolor=lightgreen]
		layer8_mlp_gate [label="Layer 8 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 7" fillcolor=lightyellow]
		layer8_mlp_up [label="Layer 8 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 7" fillcolor=lightyellow]
		layer8_mlp_gelu [label="Layer 8 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 7" fillcolor=orange]
		layer8_mlp_multiply [label="Layer 8 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 7" fillcolor=orange]
		layer8_mlp_down [label="Layer 8 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" fillcolor=lightyellow]
		layer8_residual2 [label="Layer 8 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" fillcolor=lightblue shape=diamond]
		layer8_layernorm2 [label="Layer 8 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" fillcolor=lightgreen]
	}
	layer9_transfer [label="Layer 9 → 10 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 8
To: GPU 9" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_8 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 8 - Layer 9
100% SRAM/L2 Cache" style=filled
		layer9_input [label="Layer 9 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" fillcolor=lightyellow shape=parallelogram]
		layer9_mha_query [label="Layer 9 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 8" fillcolor=pink]
		layer9_mha_key [label="Layer 9 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 8" fillcolor=pink]
		layer9_mha_value [label="Layer 9 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 8" fillcolor=pink]
		layer9_mha_qkt [label="Layer 9 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 8" fillcolor=lightcoral]
		layer9_mha_softmax [label="Layer 9 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 8" fillcolor=lightcoral]
		layer9_mha_apply [label="Layer 9 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 8" fillcolor=lightcoral]
		layer9_mha_out [label="Layer 9 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" fillcolor=pink]
		layer9_residual1 [label="Layer 9 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" fillcolor=lightblue shape=diamond]
		layer9_layernorm1 [label="Layer 9 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" fillcolor=lightgreen]
		layer9_mlp_gate [label="Layer 9 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 8" fillcolor=lightyellow]
		layer9_mlp_up [label="Layer 9 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 8" fillcolor=lightyellow]
		layer9_mlp_gelu [label="Layer 9 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 8" fillcolor=orange]
		layer9_mlp_multiply [label="Layer 9 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 8" fillcolor=orange]
		layer9_mlp_down [label="Layer 9 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" fillcolor=lightyellow]
		layer9_residual2 [label="Layer 9 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" fillcolor=lightblue shape=diamond]
		layer9_layernorm2 [label="Layer 9 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" fillcolor=lightgreen]
	}
	layer10_transfer [label="Layer 10 → 11 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 9
To: GPU 10" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_9 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 9 - Layer 10
100% SRAM/L2 Cache" style=filled
		layer10_input [label="Layer 10 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" fillcolor=lightyellow shape=parallelogram]
		layer10_mha_query [label="Layer 10 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 9" fillcolor=pink]
		layer10_mha_key [label="Layer 10 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 9" fillcolor=pink]
		layer10_mha_value [label="Layer 10 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 9" fillcolor=pink]
		layer10_mha_qkt [label="Layer 10 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 9" fillcolor=lightcoral]
		layer10_mha_softmax [label="Layer 10 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 9" fillcolor=lightcoral]
		layer10_mha_apply [label="Layer 10 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 9" fillcolor=lightcoral]
		layer10_mha_out [label="Layer 10 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" fillcolor=pink]
		layer10_residual1 [label="Layer 10 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" fillcolor=lightblue shape=diamond]
		layer10_layernorm1 [label="Layer 10 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" fillcolor=lightgreen]
		layer10_mlp_gate [label="Layer 10 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 9" fillcolor=lightyellow]
		layer10_mlp_up [label="Layer 10 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 9" fillcolor=lightyellow]
		layer10_mlp_gelu [label="Layer 10 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 9" fillcolor=orange]
		layer10_mlp_multiply [label="Layer 10 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 9" fillcolor=orange]
		layer10_mlp_down [label="Layer 10 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" fillcolor=lightyellow]
		layer10_residual2 [label="Layer 10 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" fillcolor=lightblue shape=diamond]
		layer10_layernorm2 [label="Layer 10 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" fillcolor=lightgreen]
	}
	layer11_transfer [label="Layer 11 → 12 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 10
To: GPU 11" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_10 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 10 - Layer 11
100% SRAM/L2 Cache" style=filled
		layer11_input [label="Layer 11 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" fillcolor=lightyellow shape=parallelogram]
		layer11_mha_query [label="Layer 11 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 10" fillcolor=pink]
		layer11_mha_key [label="Layer 11 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 10" fillcolor=pink]
		layer11_mha_value [label="Layer 11 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 10" fillcolor=pink]
		layer11_mha_qkt [label="Layer 11 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 10" fillcolor=lightcoral]
		layer11_mha_softmax [label="Layer 11 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 10" fillcolor=lightcoral]
		layer11_mha_apply [label="Layer 11 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 10" fillcolor=lightcoral]
		layer11_mha_out [label="Layer 11 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" fillcolor=pink]
		layer11_residual1 [label="Layer 11 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" fillcolor=lightblue shape=diamond]
		layer11_layernorm1 [label="Layer 11 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" fillcolor=lightgreen]
		layer11_mlp_gate [label="Layer 11 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 10" fillcolor=lightyellow]
		layer11_mlp_up [label="Layer 11 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 10" fillcolor=lightyellow]
		layer11_mlp_gelu [label="Layer 11 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 10" fillcolor=orange]
		layer11_mlp_multiply [label="Layer 11 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 10" fillcolor=orange]
		layer11_mlp_down [label="Layer 11 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" fillcolor=lightyellow]
		layer11_residual2 [label="Layer 11 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" fillcolor=lightblue shape=diamond]
		layer11_layernorm2 [label="Layer 11 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" fillcolor=lightgreen]
	}
	layer12_transfer [label="Layer 12 → 13 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 11
To: GPU 12" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_11 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 11 - Layer 12
100% SRAM/L2 Cache" style=filled
		layer12_input [label="Layer 12 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" fillcolor=lightyellow shape=parallelogram]
		layer12_mha_query [label="Layer 12 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 11" fillcolor=pink]
		layer12_mha_key [label="Layer 12 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 11" fillcolor=pink]
		layer12_mha_value [label="Layer 12 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 11" fillcolor=pink]
		layer12_mha_qkt [label="Layer 12 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 11" fillcolor=lightcoral]
		layer12_mha_softmax [label="Layer 12 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 11" fillcolor=lightcoral]
		layer12_mha_apply [label="Layer 12 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 11" fillcolor=lightcoral]
		layer12_mha_out [label="Layer 12 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" fillcolor=pink]
		layer12_residual1 [label="Layer 12 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" fillcolor=lightblue shape=diamond]
		layer12_layernorm1 [label="Layer 12 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" fillcolor=lightgreen]
		layer12_mlp_gate [label="Layer 12 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 11" fillcolor=lightyellow]
		layer12_mlp_up [label="Layer 12 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 11" fillcolor=lightyellow]
		layer12_mlp_gelu [label="Layer 12 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 11" fillcolor=orange]
		layer12_mlp_multiply [label="Layer 12 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 11" fillcolor=orange]
		layer12_mlp_down [label="Layer 12 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" fillcolor=lightyellow]
		layer12_residual2 [label="Layer 12 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" fillcolor=lightblue shape=diamond]
		layer12_layernorm2 [label="Layer 12 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" fillcolor=lightgreen]
	}
	layer13_transfer [label="Layer 13 → 14 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 12
To: GPU 13" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_12 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 12 - Layer 13
100% SRAM/L2 Cache" style=filled
		layer13_input [label="Layer 13 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" fillcolor=lightyellow shape=parallelogram]
		layer13_mha_query [label="Layer 13 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 12" fillcolor=pink]
		layer13_mha_key [label="Layer 13 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 12" fillcolor=pink]
		layer13_mha_value [label="Layer 13 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 12" fillcolor=pink]
		layer13_mha_qkt [label="Layer 13 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 12" fillcolor=lightcoral]
		layer13_mha_softmax [label="Layer 13 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 12" fillcolor=lightcoral]
		layer13_mha_apply [label="Layer 13 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 12" fillcolor=lightcoral]
		layer13_mha_out [label="Layer 13 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" fillcolor=pink]
		layer13_residual1 [label="Layer 13 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" fillcolor=lightblue shape=diamond]
		layer13_layernorm1 [label="Layer 13 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" fillcolor=lightgreen]
		layer13_mlp_gate [label="Layer 13 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 12" fillcolor=lightyellow]
		layer13_mlp_up [label="Layer 13 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 12" fillcolor=lightyellow]
		layer13_mlp_gelu [label="Layer 13 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 12" fillcolor=orange]
		layer13_mlp_multiply [label="Layer 13 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 12" fillcolor=orange]
		layer13_mlp_down [label="Layer 13 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" fillcolor=lightyellow]
		layer13_residual2 [label="Layer 13 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" fillcolor=lightblue shape=diamond]
		layer13_layernorm2 [label="Layer 13 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" fillcolor=lightgreen]
	}
	layer14_transfer [label="Layer 14 → 15 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 13
To: GPU 14" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_13 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 13 - Layer 14
100% SRAM/L2 Cache" style=filled
		layer14_input [label="Layer 14 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" fillcolor=lightyellow shape=parallelogram]
		layer14_mha_query [label="Layer 14 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 13" fillcolor=pink]
		layer14_mha_key [label="Layer 14 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 13" fillcolor=pink]
		layer14_mha_value [label="Layer 14 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 13" fillcolor=pink]
		layer14_mha_qkt [label="Layer 14 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 13" fillcolor=lightcoral]
		layer14_mha_softmax [label="Layer 14 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 13" fillcolor=lightcoral]
		layer14_mha_apply [label="Layer 14 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 13" fillcolor=lightcoral]
		layer14_mha_out [label="Layer 14 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" fillcolor=pink]
		layer14_residual1 [label="Layer 14 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" fillcolor=lightblue shape=diamond]
		layer14_layernorm1 [label="Layer 14 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" fillcolor=lightgreen]
		layer14_mlp_gate [label="Layer 14 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 13" fillcolor=lightyellow]
		layer14_mlp_up [label="Layer 14 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 13" fillcolor=lightyellow]
		layer14_mlp_gelu [label="Layer 14 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 13" fillcolor=orange]
		layer14_mlp_multiply [label="Layer 14 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 13" fillcolor=orange]
		layer14_mlp_down [label="Layer 14 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" fillcolor=lightyellow]
		layer14_residual2 [label="Layer 14 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" fillcolor=lightblue shape=diamond]
		layer14_layernorm2 [label="Layer 14 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" fillcolor=lightgreen]
	}
	layer15_transfer [label="Layer 15 → 16 Transfer
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: GPU 14
To: GPU 15" fillcolor=gray shape=ellipse]
	subgraph cluster_gpu_14 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 14 - Layer 15
100% SRAM/L2 Cache" style=filled
		layer15_input [label="Layer 15 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" fillcolor=lightyellow shape=parallelogram]
		layer15_mha_query [label="Layer 15 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 14" fillcolor=pink]
		layer15_mha_key [label="Layer 15 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 14" fillcolor=pink]
		layer15_mha_value [label="Layer 15 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 14" fillcolor=pink]
		layer15_mha_qkt [label="Layer 15 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 14" fillcolor=lightcoral]
		layer15_mha_softmax [label="Layer 15 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 14" fillcolor=lightcoral]
		layer15_mha_apply [label="Layer 15 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 14" fillcolor=lightcoral]
		layer15_mha_out [label="Layer 15 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" fillcolor=pink]
		layer15_residual1 [label="Layer 15 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" fillcolor=lightblue shape=diamond]
		layer15_layernorm1 [label="Layer 15 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" fillcolor=lightgreen]
		layer15_mlp_gate [label="Layer 15 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 14" fillcolor=lightyellow]
		layer15_mlp_up [label="Layer 15 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 14" fillcolor=lightyellow]
		layer15_mlp_gelu [label="Layer 15 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 14" fillcolor=orange]
		layer15_mlp_multiply [label="Layer 15 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 14" fillcolor=orange]
		layer15_mlp_down [label="Layer 15 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" fillcolor=lightyellow]
		layer15_residual2 [label="Layer 15 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" fillcolor=lightblue shape=diamond]
		layer15_layernorm2 [label="Layer 15 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" fillcolor=lightgreen]
	}
	subgraph cluster_gpu_15 {
		color=lightgreen fillcolor=lightgreen1 label="GPU 15 - Layer 16
100% SRAM/L2 Cache" style=filled
		layer16_input [label="Layer 16 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" fillcolor=lightyellow shape=parallelogram]
		layer16_mha_query [label="Layer 16 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_q=512]
GPU: 15" fillcolor=pink]
		layer16_mha_key [label="Layer 16 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: 15" fillcolor=pink]
		layer16_mha_value [label="Layer 16 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_v=512]
GPU: 15" fillcolor=pink]
		layer16_mha_qkt [label="Layer 16 MHA QKT Calculation
Input: [batch_size=1024, heads=16, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 15" fillcolor=lightcoral]
		layer16_mha_softmax [label="Layer 16 MHA Softmax
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000]
GPU: 15" fillcolor=lightcoral]
		layer16_mha_apply [label="Layer 16 MHA Apply Attention
Input: [batch_size=1024, heads=16, seq_len=10000, seq_len=10000], [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
GPU: 15" fillcolor=lightcoral]
		layer16_mha_out [label="Layer 16 MHA Output Linear
Input: [batch_size=1024, heads=16, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" fillcolor=pink]
		layer16_residual1 [label="Layer 16 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" fillcolor=lightblue shape=diamond]
		layer16_layernorm1 [label="Layer 16 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" fillcolor=lightgreen]
		layer16_mlp_gate [label="Layer 16 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 15" fillcolor=lightyellow]
		layer16_mlp_up [label="Layer 16 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 15" fillcolor=lightyellow]
		layer16_mlp_gelu [label="Layer 16 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 15" fillcolor=orange]
		layer16_mlp_multiply [label="Layer 16 MLP Element-wise Multiply
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768], [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
GPU: 15" fillcolor=orange]
		layer16_mlp_down [label="Layer 16 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" fillcolor=lightyellow]
		layer16_residual2 [label="Layer 16 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" fillcolor=lightblue shape=diamond]
		layer16_layernorm2 [label="Layer 16 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" fillcolor=lightgreen]
	}
	input [label="Model Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Host → GPU 0" fillcolor=lightgreen shape=ellipse]
	output [label="Model Output
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: GPU 15 → Host" fillcolor=red shape=ellipse]
	input -> layer1_input
	layer1_input -> layer1_mha_query
	layer1_input -> layer1_mha_key
	layer1_input -> layer1_mha_value
	layer1_mha_query -> layer1_mha_qkt
	layer1_mha_key -> layer1_mha_qkt
	layer1_mha_qkt -> layer1_mha_softmax
	layer1_mha_softmax -> layer1_mha_apply
	layer1_mha_value -> layer1_mha_apply
	layer1_mha_apply -> layer1_mha_out
	layer1_mha_out -> layer1_residual1
	layer1_input -> layer1_residual1
	layer1_residual1 -> layer1_layernorm1
	layer1_layernorm1 -> layer1_mlp_gate
	layer1_layernorm1 -> layer1_mlp_up
	layer1_mlp_gate -> layer1_mlp_gelu
	layer1_mlp_up -> layer1_mlp_multiply
	layer1_mlp_gelu -> layer1_mlp_multiply
	layer1_mlp_multiply -> layer1_mlp_down
	layer1_mlp_down -> layer1_residual2
	layer1_layernorm1 -> layer1_residual2
	layer1_residual2 -> layer1_layernorm2
	layer1_layernorm2 -> layer1_transfer
	layer1_transfer -> layer2_input
	layer2_input -> layer2_mha_query
	layer2_input -> layer2_mha_key
	layer2_input -> layer2_mha_value
	layer2_mha_query -> layer2_mha_qkt
	layer2_mha_key -> layer2_mha_qkt
	layer2_mha_qkt -> layer2_mha_softmax
	layer2_mha_softmax -> layer2_mha_apply
	layer2_mha_value -> layer2_mha_apply
	layer2_mha_apply -> layer2_mha_out
	layer2_mha_out -> layer2_residual1
	layer2_input -> layer2_residual1
	layer2_residual1 -> layer2_layernorm1
	layer2_layernorm1 -> layer2_mlp_gate
	layer2_layernorm1 -> layer2_mlp_up
	layer2_mlp_gate -> layer2_mlp_gelu
	layer2_mlp_up -> layer2_mlp_multiply
	layer2_mlp_gelu -> layer2_mlp_multiply
	layer2_mlp_multiply -> layer2_mlp_down
	layer2_mlp_down -> layer2_residual2
	layer2_layernorm1 -> layer2_residual2
	layer2_residual2 -> layer2_layernorm2
	layer2_layernorm2 -> layer2_transfer
	layer2_transfer -> layer3_input
	layer3_input -> layer3_mha_query
	layer3_input -> layer3_mha_key
	layer3_input -> layer3_mha_value
	layer3_mha_query -> layer3_mha_qkt
	layer3_mha_key -> layer3_mha_qkt
	layer3_mha_qkt -> layer3_mha_softmax
	layer3_mha_softmax -> layer3_mha_apply
	layer3_mha_value -> layer3_mha_apply
	layer3_mha_apply -> layer3_mha_out
	layer3_mha_out -> layer3_residual1
	layer3_input -> layer3_residual1
	layer3_residual1 -> layer3_layernorm1
	layer3_layernorm1 -> layer3_mlp_gate
	layer3_layernorm1 -> layer3_mlp_up
	layer3_mlp_gate -> layer3_mlp_gelu
	layer3_mlp_up -> layer3_mlp_multiply
	layer3_mlp_gelu -> layer3_mlp_multiply
	layer3_mlp_multiply -> layer3_mlp_down
	layer3_mlp_down -> layer3_residual2
	layer3_layernorm1 -> layer3_residual2
	layer3_residual2 -> layer3_layernorm2
	layer3_layernorm2 -> layer3_transfer
	layer3_transfer -> layer4_input
	layer4_input -> layer4_mha_query
	layer4_input -> layer4_mha_key
	layer4_input -> layer4_mha_value
	layer4_mha_query -> layer4_mha_qkt
	layer4_mha_key -> layer4_mha_qkt
	layer4_mha_qkt -> layer4_mha_softmax
	layer4_mha_softmax -> layer4_mha_apply
	layer4_mha_value -> layer4_mha_apply
	layer4_mha_apply -> layer4_mha_out
	layer4_mha_out -> layer4_residual1
	layer4_input -> layer4_residual1
	layer4_residual1 -> layer4_layernorm1
	layer4_layernorm1 -> layer4_mlp_gate
	layer4_layernorm1 -> layer4_mlp_up
	layer4_mlp_gate -> layer4_mlp_gelu
	layer4_mlp_up -> layer4_mlp_multiply
	layer4_mlp_gelu -> layer4_mlp_multiply
	layer4_mlp_multiply -> layer4_mlp_down
	layer4_mlp_down -> layer4_residual2
	layer4_layernorm1 -> layer4_residual2
	layer4_residual2 -> layer4_layernorm2
	layer4_layernorm2 -> layer4_transfer
	layer4_transfer -> layer5_input
	layer5_input -> layer5_mha_query
	layer5_input -> layer5_mha_key
	layer5_input -> layer5_mha_value
	layer5_mha_query -> layer5_mha_qkt
	layer5_mha_key -> layer5_mha_qkt
	layer5_mha_qkt -> layer5_mha_softmax
	layer5_mha_softmax -> layer5_mha_apply
	layer5_mha_value -> layer5_mha_apply
	layer5_mha_apply -> layer5_mha_out
	layer5_mha_out -> layer5_residual1
	layer5_input -> layer5_residual1
	layer5_residual1 -> layer5_layernorm1
	layer5_layernorm1 -> layer5_mlp_gate
	layer5_layernorm1 -> layer5_mlp_up
	layer5_mlp_gate -> layer5_mlp_gelu
	layer5_mlp_up -> layer5_mlp_multiply
	layer5_mlp_gelu -> layer5_mlp_multiply
	layer5_mlp_multiply -> layer5_mlp_down
	layer5_mlp_down -> layer5_residual2
	layer5_layernorm1 -> layer5_residual2
	layer5_residual2 -> layer5_layernorm2
	layer5_layernorm2 -> layer5_transfer
	layer5_transfer -> layer6_input
	layer6_input -> layer6_mha_query
	layer6_input -> layer6_mha_key
	layer6_input -> layer6_mha_value
	layer6_mha_query -> layer6_mha_qkt
	layer6_mha_key -> layer6_mha_qkt
	layer6_mha_qkt -> layer6_mha_softmax
	layer6_mha_softmax -> layer6_mha_apply
	layer6_mha_value -> layer6_mha_apply
	layer6_mha_apply -> layer6_mha_out
	layer6_mha_out -> layer6_residual1
	layer6_input -> layer6_residual1
	layer6_residual1 -> layer6_layernorm1
	layer6_layernorm1 -> layer6_mlp_gate
	layer6_layernorm1 -> layer6_mlp_up
	layer6_mlp_gate -> layer6_mlp_gelu
	layer6_mlp_up -> layer6_mlp_multiply
	layer6_mlp_gelu -> layer6_mlp_multiply
	layer6_mlp_multiply -> layer6_mlp_down
	layer6_mlp_down -> layer6_residual2
	layer6_layernorm1 -> layer6_residual2
	layer6_residual2 -> layer6_layernorm2
	layer6_layernorm2 -> layer6_transfer
	layer6_transfer -> layer7_input
	layer7_input -> layer7_mha_query
	layer7_input -> layer7_mha_key
	layer7_input -> layer7_mha_value
	layer7_mha_query -> layer7_mha_qkt
	layer7_mha_key -> layer7_mha_qkt
	layer7_mha_qkt -> layer7_mha_softmax
	layer7_mha_softmax -> layer7_mha_apply
	layer7_mha_value -> layer7_mha_apply
	layer7_mha_apply -> layer7_mha_out
	layer7_mha_out -> layer7_residual1
	layer7_input -> layer7_residual1
	layer7_residual1 -> layer7_layernorm1
	layer7_layernorm1 -> layer7_mlp_gate
	layer7_layernorm1 -> layer7_mlp_up
	layer7_mlp_gate -> layer7_mlp_gelu
	layer7_mlp_up -> layer7_mlp_multiply
	layer7_mlp_gelu -> layer7_mlp_multiply
	layer7_mlp_multiply -> layer7_mlp_down
	layer7_mlp_down -> layer7_residual2
	layer7_layernorm1 -> layer7_residual2
	layer7_residual2 -> layer7_layernorm2
	layer7_layernorm2 -> layer7_transfer
	layer7_transfer -> layer8_input
	layer8_input -> layer8_mha_query
	layer8_input -> layer8_mha_key
	layer8_input -> layer8_mha_value
	layer8_mha_query -> layer8_mha_qkt
	layer8_mha_key -> layer8_mha_qkt
	layer8_mha_qkt -> layer8_mha_softmax
	layer8_mha_softmax -> layer8_mha_apply
	layer8_mha_value -> layer8_mha_apply
	layer8_mha_apply -> layer8_mha_out
	layer8_mha_out -> layer8_residual1
	layer8_input -> layer8_residual1
	layer8_residual1 -> layer8_layernorm1
	layer8_layernorm1 -> layer8_mlp_gate
	layer8_layernorm1 -> layer8_mlp_up
	layer8_mlp_gate -> layer8_mlp_gelu
	layer8_mlp_up -> layer8_mlp_multiply
	layer8_mlp_gelu -> layer8_mlp_multiply
	layer8_mlp_multiply -> layer8_mlp_down
	layer8_mlp_down -> layer8_residual2
	layer8_layernorm1 -> layer8_residual2
	layer8_residual2 -> layer8_layernorm2
	layer8_layernorm2 -> layer8_transfer
	layer8_transfer -> layer9_input
	layer9_input -> layer9_mha_query
	layer9_input -> layer9_mha_key
	layer9_input -> layer9_mha_value
	layer9_mha_query -> layer9_mha_qkt
	layer9_mha_key -> layer9_mha_qkt
	layer9_mha_qkt -> layer9_mha_softmax
	layer9_mha_softmax -> layer9_mha_apply
	layer9_mha_value -> layer9_mha_apply
	layer9_mha_apply -> layer9_mha_out
	layer9_mha_out -> layer9_residual1
	layer9_input -> layer9_residual1
	layer9_residual1 -> layer9_layernorm1
	layer9_layernorm1 -> layer9_mlp_gate
	layer9_layernorm1 -> layer9_mlp_up
	layer9_mlp_gate -> layer9_mlp_gelu
	layer9_mlp_up -> layer9_mlp_multiply
	layer9_mlp_gelu -> layer9_mlp_multiply
	layer9_mlp_multiply -> layer9_mlp_down
	layer9_mlp_down -> layer9_residual2
	layer9_layernorm1 -> layer9_residual2
	layer9_residual2 -> layer9_layernorm2
	layer9_layernorm2 -> layer9_transfer
	layer9_transfer -> layer10_input
	layer10_input -> layer10_mha_query
	layer10_input -> layer10_mha_key
	layer10_input -> layer10_mha_value
	layer10_mha_query -> layer10_mha_qkt
	layer10_mha_key -> layer10_mha_qkt
	layer10_mha_qkt -> layer10_mha_softmax
	layer10_mha_softmax -> layer10_mha_apply
	layer10_mha_value -> layer10_mha_apply
	layer10_mha_apply -> layer10_mha_out
	layer10_mha_out -> layer10_residual1
	layer10_input -> layer10_residual1
	layer10_residual1 -> layer10_layernorm1
	layer10_layernorm1 -> layer10_mlp_gate
	layer10_layernorm1 -> layer10_mlp_up
	layer10_mlp_gate -> layer10_mlp_gelu
	layer10_mlp_up -> layer10_mlp_multiply
	layer10_mlp_gelu -> layer10_mlp_multiply
	layer10_mlp_multiply -> layer10_mlp_down
	layer10_mlp_down -> layer10_residual2
	layer10_layernorm1 -> layer10_residual2
	layer10_residual2 -> layer10_layernorm2
	layer10_layernorm2 -> layer10_transfer
	layer10_transfer -> layer11_input
	layer11_input -> layer11_mha_query
	layer11_input -> layer11_mha_key
	layer11_input -> layer11_mha_value
	layer11_mha_query -> layer11_mha_qkt
	layer11_mha_key -> layer11_mha_qkt
	layer11_mha_qkt -> layer11_mha_softmax
	layer11_mha_softmax -> layer11_mha_apply
	layer11_mha_value -> layer11_mha_apply
	layer11_mha_apply -> layer11_mha_out
	layer11_mha_out -> layer11_residual1
	layer11_input -> layer11_residual1
	layer11_residual1 -> layer11_layernorm1
	layer11_layernorm1 -> layer11_mlp_gate
	layer11_layernorm1 -> layer11_mlp_up
	layer11_mlp_gate -> layer11_mlp_gelu
	layer11_mlp_up -> layer11_mlp_multiply
	layer11_mlp_gelu -> layer11_mlp_multiply
	layer11_mlp_multiply -> layer11_mlp_down
	layer11_mlp_down -> layer11_residual2
	layer11_layernorm1 -> layer11_residual2
	layer11_residual2 -> layer11_layernorm2
	layer11_layernorm2 -> layer11_transfer
	layer11_transfer -> layer12_input
	layer12_input -> layer12_mha_query
	layer12_input -> layer12_mha_key
	layer12_input -> layer12_mha_value
	layer12_mha_query -> layer12_mha_qkt
	layer12_mha_key -> layer12_mha_qkt
	layer12_mha_qkt -> layer12_mha_softmax
	layer12_mha_softmax -> layer12_mha_apply
	layer12_mha_value -> layer12_mha_apply
	layer12_mha_apply -> layer12_mha_out
	layer12_mha_out -> layer12_residual1
	layer12_input -> layer12_residual1
	layer12_residual1 -> layer12_layernorm1
	layer12_layernorm1 -> layer12_mlp_gate
	layer12_layernorm1 -> layer12_mlp_up
	layer12_mlp_gate -> layer12_mlp_gelu
	layer12_mlp_up -> layer12_mlp_multiply
	layer12_mlp_gelu -> layer12_mlp_multiply
	layer12_mlp_multiply -> layer12_mlp_down
	layer12_mlp_down -> layer12_residual2
	layer12_layernorm1 -> layer12_residual2
	layer12_residual2 -> layer12_layernorm2
	layer12_layernorm2 -> layer12_transfer
	layer12_transfer -> layer13_input
	layer13_input -> layer13_mha_query
	layer13_input -> layer13_mha_key
	layer13_input -> layer13_mha_value
	layer13_mha_query -> layer13_mha_qkt
	layer13_mha_key -> layer13_mha_qkt
	layer13_mha_qkt -> layer13_mha_softmax
	layer13_mha_softmax -> layer13_mha_apply
	layer13_mha_value -> layer13_mha_apply
	layer13_mha_apply -> layer13_mha_out
	layer13_mha_out -> layer13_residual1
	layer13_input -> layer13_residual1
	layer13_residual1 -> layer13_layernorm1
	layer13_layernorm1 -> layer13_mlp_gate
	layer13_layernorm1 -> layer13_mlp_up
	layer13_mlp_gate -> layer13_mlp_gelu
	layer13_mlp_up -> layer13_mlp_multiply
	layer13_mlp_gelu -> layer13_mlp_multiply
	layer13_mlp_multiply -> layer13_mlp_down
	layer13_mlp_down -> layer13_residual2
	layer13_layernorm1 -> layer13_residual2
	layer13_residual2 -> layer13_layernorm2
	layer13_layernorm2 -> layer13_transfer
	layer13_transfer -> layer14_input
	layer14_input -> layer14_mha_query
	layer14_input -> layer14_mha_key
	layer14_input -> layer14_mha_value
	layer14_mha_query -> layer14_mha_qkt
	layer14_mha_key -> layer14_mha_qkt
	layer14_mha_qkt -> layer14_mha_softmax
	layer14_mha_softmax -> layer14_mha_apply
	layer14_mha_value -> layer14_mha_apply
	layer14_mha_apply -> layer14_mha_out
	layer14_mha_out -> layer14_residual1
	layer14_input -> layer14_residual1
	layer14_residual1 -> layer14_layernorm1
	layer14_layernorm1 -> layer14_mlp_gate
	layer14_layernorm1 -> layer14_mlp_up
	layer14_mlp_gate -> layer14_mlp_gelu
	layer14_mlp_up -> layer14_mlp_multiply
	layer14_mlp_gelu -> layer14_mlp_multiply
	layer14_mlp_multiply -> layer14_mlp_down
	layer14_mlp_down -> layer14_residual2
	layer14_layernorm1 -> layer14_residual2
	layer14_residual2 -> layer14_layernorm2
	layer14_layernorm2 -> layer14_transfer
	layer14_transfer -> layer15_input
	layer15_input -> layer15_mha_query
	layer15_input -> layer15_mha_key
	layer15_input -> layer15_mha_value
	layer15_mha_query -> layer15_mha_qkt
	layer15_mha_key -> layer15_mha_qkt
	layer15_mha_qkt -> layer15_mha_softmax
	layer15_mha_softmax -> layer15_mha_apply
	layer15_mha_value -> layer15_mha_apply
	layer15_mha_apply -> layer15_mha_out
	layer15_mha_out -> layer15_residual1
	layer15_input -> layer15_residual1
	layer15_residual1 -> layer15_layernorm1
	layer15_layernorm1 -> layer15_mlp_gate
	layer15_layernorm1 -> layer15_mlp_up
	layer15_mlp_gate -> layer15_mlp_gelu
	layer15_mlp_up -> layer15_mlp_multiply
	layer15_mlp_gelu -> layer15_mlp_multiply
	layer15_mlp_multiply -> layer15_mlp_down
	layer15_mlp_down -> layer15_residual2
	layer15_layernorm1 -> layer15_residual2
	layer15_residual2 -> layer15_layernorm2
	layer15_layernorm2 -> layer15_transfer
	layer15_transfer -> layer16_input
	layer16_input -> layer16_mha_query
	layer16_input -> layer16_mha_key
	layer16_input -> layer16_mha_value
	layer16_mha_query -> layer16_mha_qkt
	layer16_mha_key -> layer16_mha_qkt
	layer16_mha_qkt -> layer16_mha_softmax
	layer16_mha_softmax -> layer16_mha_apply
	layer16_mha_value -> layer16_mha_apply
	layer16_mha_apply -> layer16_mha_out
	layer16_mha_out -> layer16_residual1
	layer16_input -> layer16_residual1
	layer16_residual1 -> layer16_layernorm1
	layer16_layernorm1 -> layer16_mlp_gate
	layer16_layernorm1 -> layer16_mlp_up
	layer16_mlp_gate -> layer16_mlp_gelu
	layer16_mlp_up -> layer16_mlp_multiply
	layer16_mlp_gelu -> layer16_mlp_multiply
	layer16_mlp_multiply -> layer16_mlp_down
	layer16_mlp_down -> layer16_residual2
	layer16_layernorm1 -> layer16_residual2
	layer16_residual2 -> layer16_layernorm2
	layer16_layernorm2 -> output
}
