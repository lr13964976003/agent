digraph baseline_tensor_pipeline_parallel {
	rankdir=TB size="100,200"
	node [fontname=Arial fontsize=10]
	node [shape=rectangle style=filled]
	edge [fontname=Arial fontsize=8]
	subgraph cluster_pipeline_stage_0 {
		color=lightblue fillcolor=lightblue1 label="Pipeline Stage 0 (Layers 1-8)
Devices 0-7 (TP=8)" style=filled
		input_stage0 [label="Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Host" fillcolor=lightgreen shape=ellipse]
		layer1_input_stage0 [label="Layer 1 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightyellow shape=parallelogram]
		layer1_mha_query_stage0 [label="Layer 1 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer1_mha_key_stage0 [label="Layer 1 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer1_mha_value_stage0 [label="Layer 1 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer1_mha_qkt_stage0 [label="Layer 1 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer1_mha_weights_stage0 [label="Layer 1 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer1_mha_apply_stage0 [label="Layer 1 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer1_mha_out_stage0 [label="Layer 1 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=pink]
		layer1_mha_allreduce_stage0 [label="Layer 1 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer1_residual1_stage0 [label="Layer 1 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer1_layernorm1_stage0 [label="Layer 1 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer1_mlp_gate_stage0 [label="Layer 1 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer1_mlp_up_stage0 [label="Layer 1 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer1_mlp_gelu_stage0 [label="Layer 1 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=orange]
		layer1_mlp_down_stage0 [label="Layer 1 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer1_mlp_allreduce_stage0 [label="Layer 1 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer1_residual2_stage0 [label="Layer 1 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer1_layernorm2_stage0 [label="Layer 1 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer2_input_stage0 [label="Layer 2 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightyellow shape=parallelogram]
		layer2_mha_query_stage0 [label="Layer 2 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer2_mha_key_stage0 [label="Layer 2 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer2_mha_value_stage0 [label="Layer 2 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer2_mha_qkt_stage0 [label="Layer 2 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer2_mha_weights_stage0 [label="Layer 2 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer2_mha_apply_stage0 [label="Layer 2 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer2_mha_out_stage0 [label="Layer 2 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=pink]
		layer2_mha_allreduce_stage0 [label="Layer 2 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer2_residual1_stage0 [label="Layer 2 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer2_layernorm1_stage0 [label="Layer 2 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer2_mlp_gate_stage0 [label="Layer 2 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer2_mlp_up_stage0 [label="Layer 2 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer2_mlp_gelu_stage0 [label="Layer 2 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=orange]
		layer2_mlp_down_stage0 [label="Layer 2 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer2_mlp_allreduce_stage0 [label="Layer 2 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer2_residual2_stage0 [label="Layer 2 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer2_layernorm2_stage0 [label="Layer 2 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer3_input_stage0 [label="Layer 3 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightyellow shape=parallelogram]
		layer3_mha_query_stage0 [label="Layer 3 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer3_mha_key_stage0 [label="Layer 3 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer3_mha_value_stage0 [label="Layer 3 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer3_mha_qkt_stage0 [label="Layer 3 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer3_mha_weights_stage0 [label="Layer 3 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer3_mha_apply_stage0 [label="Layer 3 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer3_mha_out_stage0 [label="Layer 3 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=pink]
		layer3_mha_allreduce_stage0 [label="Layer 3 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer3_residual1_stage0 [label="Layer 3 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer3_layernorm1_stage0 [label="Layer 3 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer3_mlp_gate_stage0 [label="Layer 3 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer3_mlp_up_stage0 [label="Layer 3 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer3_mlp_gelu_stage0 [label="Layer 3 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=orange]
		layer3_mlp_down_stage0 [label="Layer 3 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer3_mlp_allreduce_stage0 [label="Layer 3 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer3_residual2_stage0 [label="Layer 3 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer3_layernorm2_stage0 [label="Layer 3 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer4_input_stage0 [label="Layer 4 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightyellow shape=parallelogram]
		layer4_mha_query_stage0 [label="Layer 4 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer4_mha_key_stage0 [label="Layer 4 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer4_mha_value_stage0 [label="Layer 4 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer4_mha_qkt_stage0 [label="Layer 4 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer4_mha_weights_stage0 [label="Layer 4 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer4_mha_apply_stage0 [label="Layer 4 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer4_mha_out_stage0 [label="Layer 4 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=pink]
		layer4_mha_allreduce_stage0 [label="Layer 4 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer4_residual1_stage0 [label="Layer 4 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer4_layernorm1_stage0 [label="Layer 4 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer4_mlp_gate_stage0 [label="Layer 4 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer4_mlp_up_stage0 [label="Layer 4 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer4_mlp_gelu_stage0 [label="Layer 4 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=orange]
		layer4_mlp_down_stage0 [label="Layer 4 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer4_mlp_allreduce_stage0 [label="Layer 4 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer4_residual2_stage0 [label="Layer 4 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer4_layernorm2_stage0 [label="Layer 4 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer5_input_stage0 [label="Layer 5 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightyellow shape=parallelogram]
		layer5_mha_query_stage0 [label="Layer 5 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer5_mha_key_stage0 [label="Layer 5 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer5_mha_value_stage0 [label="Layer 5 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer5_mha_qkt_stage0 [label="Layer 5 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer5_mha_weights_stage0 [label="Layer 5 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer5_mha_apply_stage0 [label="Layer 5 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer5_mha_out_stage0 [label="Layer 5 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=pink]
		layer5_mha_allreduce_stage0 [label="Layer 5 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer5_residual1_stage0 [label="Layer 5 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer5_layernorm1_stage0 [label="Layer 5 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer5_mlp_gate_stage0 [label="Layer 5 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer5_mlp_up_stage0 [label="Layer 5 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer5_mlp_gelu_stage0 [label="Layer 5 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=orange]
		layer5_mlp_down_stage0 [label="Layer 5 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer5_mlp_allreduce_stage0 [label="Layer 5 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer5_residual2_stage0 [label="Layer 5 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer5_layernorm2_stage0 [label="Layer 5 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer6_input_stage0 [label="Layer 6 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightyellow shape=parallelogram]
		layer6_mha_query_stage0 [label="Layer 6 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer6_mha_key_stage0 [label="Layer 6 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer6_mha_value_stage0 [label="Layer 6 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer6_mha_qkt_stage0 [label="Layer 6 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer6_mha_weights_stage0 [label="Layer 6 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer6_mha_apply_stage0 [label="Layer 6 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer6_mha_out_stage0 [label="Layer 6 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=pink]
		layer6_mha_allreduce_stage0 [label="Layer 6 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer6_residual1_stage0 [label="Layer 6 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer6_layernorm1_stage0 [label="Layer 6 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer6_mlp_gate_stage0 [label="Layer 6 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer6_mlp_up_stage0 [label="Layer 6 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer6_mlp_gelu_stage0 [label="Layer 6 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=orange]
		layer6_mlp_down_stage0 [label="Layer 6 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer6_mlp_allreduce_stage0 [label="Layer 6 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer6_residual2_stage0 [label="Layer 6 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer6_layernorm2_stage0 [label="Layer 6 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer7_input_stage0 [label="Layer 7 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightyellow shape=parallelogram]
		layer7_mha_query_stage0 [label="Layer 7 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer7_mha_key_stage0 [label="Layer 7 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer7_mha_value_stage0 [label="Layer 7 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer7_mha_qkt_stage0 [label="Layer 7 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer7_mha_weights_stage0 [label="Layer 7 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer7_mha_apply_stage0 [label="Layer 7 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer7_mha_out_stage0 [label="Layer 7 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=pink]
		layer7_mha_allreduce_stage0 [label="Layer 7 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer7_residual1_stage0 [label="Layer 7 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer7_layernorm1_stage0 [label="Layer 7 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer7_mlp_gate_stage0 [label="Layer 7 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer7_mlp_up_stage0 [label="Layer 7 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer7_mlp_gelu_stage0 [label="Layer 7 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=orange]
		layer7_mlp_down_stage0 [label="Layer 7 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer7_mlp_allreduce_stage0 [label="Layer 7 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer7_residual2_stage0 [label="Layer 7 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer7_layernorm2_stage0 [label="Layer 7 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer8_input_stage0 [label="Layer 8 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightyellow shape=parallelogram]
		layer8_mha_query_stage0 [label="Layer 8 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer8_mha_key_stage0 [label="Layer 8 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer8_mha_value_stage0 [label="Layer 8 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 0-7" fillcolor=pink]
		layer8_mha_qkt_stage0 [label="Layer 8 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer8_mha_weights_stage0 [label="Layer 8 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer8_mha_apply_stage0 [label="Layer 8 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 0-7" fillcolor=lightcoral]
		layer8_mha_out_stage0 [label="Layer 8 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=pink]
		layer8_mha_allreduce_stage0 [label="Layer 8 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer8_residual1_stage0 [label="Layer 8 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer8_layernorm1_stage0 [label="Layer 8 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
		layer8_mlp_gate_stage0 [label="Layer 8 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer8_mlp_up_stage0 [label="Layer 8 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer8_mlp_gelu_stage0 [label="Layer 8 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 0-7" fillcolor=orange]
		layer8_mlp_down_stage0 [label="Layer 8 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 0-7" fillcolor=lightyellow]
		layer8_mlp_allreduce_stage0 [label="Layer 8 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgray shape=ellipse]
		layer8_residual2_stage0 [label="Layer 8 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightblue shape=diamond]
		layer8_layernorm2_stage0 [label="Layer 8 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 0-7" fillcolor=lightgreen]
	}
	subgraph cluster_pipeline_stage_1 {
		color=lightcoral fillcolor=lightcoral1 label="Pipeline Stage 1 (Layers 9-16)
Devices 8-15 (TP=8)" style=filled
		layer9_input_stage1 [label="Layer 9 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightyellow shape=parallelogram]
		layer9_mha_query_stage1 [label="Layer 9 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer9_mha_key_stage1 [label="Layer 9 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer9_mha_value_stage1 [label="Layer 9 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer9_mha_qkt_stage1 [label="Layer 9 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer9_mha_weights_stage1 [label="Layer 9 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer9_mha_apply_stage1 [label="Layer 9 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer9_mha_out_stage1 [label="Layer 9 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=pink]
		layer9_mha_allreduce_stage1 [label="Layer 9 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer9_residual1_stage1 [label="Layer 9 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer9_layernorm1_stage1 [label="Layer 9 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer9_mlp_gate_stage1 [label="Layer 9 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer9_mlp_up_stage1 [label="Layer 9 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer9_mlp_gelu_stage1 [label="Layer 9 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=orange]
		layer9_mlp_down_stage1 [label="Layer 9 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer9_mlp_allreduce_stage1 [label="Layer 9 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer9_residual2_stage1 [label="Layer 9 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer9_layernorm2_stage1 [label="Layer 9 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer10_input_stage1 [label="Layer 10 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightyellow shape=parallelogram]
		layer10_mha_query_stage1 [label="Layer 10 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer10_mha_key_stage1 [label="Layer 10 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer10_mha_value_stage1 [label="Layer 10 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer10_mha_qkt_stage1 [label="Layer 10 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer10_mha_weights_stage1 [label="Layer 10 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer10_mha_apply_stage1 [label="Layer 10 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer10_mha_out_stage1 [label="Layer 10 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=pink]
		layer10_mha_allreduce_stage1 [label="Layer 10 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer10_residual1_stage1 [label="Layer 10 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer10_layernorm1_stage1 [label="Layer 10 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer10_mlp_gate_stage1 [label="Layer 10 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer10_mlp_up_stage1 [label="Layer 10 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer10_mlp_gelu_stage1 [label="Layer 10 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=orange]
		layer10_mlp_down_stage1 [label="Layer 10 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer10_mlp_allreduce_stage1 [label="Layer 10 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer10_residual2_stage1 [label="Layer 10 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer10_layernorm2_stage1 [label="Layer 10 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer11_input_stage1 [label="Layer 11 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightyellow shape=parallelogram]
		layer11_mha_query_stage1 [label="Layer 11 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer11_mha_key_stage1 [label="Layer 11 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer11_mha_value_stage1 [label="Layer 11 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer11_mha_qkt_stage1 [label="Layer 11 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer11_mha_weights_stage1 [label="Layer 11 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer11_mha_apply_stage1 [label="Layer 11 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer11_mha_out_stage1 [label="Layer 11 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=pink]
		layer11_mha_allreduce_stage1 [label="Layer 11 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer11_residual1_stage1 [label="Layer 11 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer11_layernorm1_stage1 [label="Layer 11 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer11_mlp_gate_stage1 [label="Layer 11 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer11_mlp_up_stage1 [label="Layer 11 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer11_mlp_gelu_stage1 [label="Layer 11 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=orange]
		layer11_mlp_down_stage1 [label="Layer 11 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer11_mlp_allreduce_stage1 [label="Layer 11 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer11_residual2_stage1 [label="Layer 11 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer11_layernorm2_stage1 [label="Layer 11 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer12_input_stage1 [label="Layer 12 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightyellow shape=parallelogram]
		layer12_mha_query_stage1 [label="Layer 12 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer12_mha_key_stage1 [label="Layer 12 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer12_mha_value_stage1 [label="Layer 12 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer12_mha_qkt_stage1 [label="Layer 12 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer12_mha_weights_stage1 [label="Layer 12 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer12_mha_apply_stage1 [label="Layer 12 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer12_mha_out_stage1 [label="Layer 12 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=pink]
		layer12_mha_allreduce_stage1 [label="Layer 12 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer12_residual1_stage1 [label="Layer 12 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer12_layernorm1_stage1 [label="Layer 12 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer12_mlp_gate_stage1 [label="Layer 12 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer12_mlp_up_stage1 [label="Layer 12 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer12_mlp_gelu_stage1 [label="Layer 12 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=orange]
		layer12_mlp_down_stage1 [label="Layer 12 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer12_mlp_allreduce_stage1 [label="Layer 12 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer12_residual2_stage1 [label="Layer 12 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer12_layernorm2_stage1 [label="Layer 12 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer13_input_stage1 [label="Layer 13 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightyellow shape=parallelogram]
		layer13_mha_query_stage1 [label="Layer 13 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer13_mha_key_stage1 [label="Layer 13 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer13_mha_value_stage1 [label="Layer 13 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer13_mha_qkt_stage1 [label="Layer 13 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer13_mha_weights_stage1 [label="Layer 13 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer13_mha_apply_stage1 [label="Layer 13 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer13_mha_out_stage1 [label="Layer 13 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=pink]
		layer13_mha_allreduce_stage1 [label="Layer 13 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer13_residual1_stage1 [label="Layer 13 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer13_layernorm1_stage1 [label="Layer 13 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer13_mlp_gate_stage1 [label="Layer 13 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer13_mlp_up_stage1 [label="Layer 13 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer13_mlp_gelu_stage1 [label="Layer 13 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=orange]
		layer13_mlp_down_stage1 [label="Layer 13 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer13_mlp_allreduce_stage1 [label="Layer 13 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer13_residual2_stage1 [label="Layer 13 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer13_layernorm2_stage1 [label="Layer 13 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer14_input_stage1 [label="Layer 14 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightyellow shape=parallelogram]
		layer14_mha_query_stage1 [label="Layer 14 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer14_mha_key_stage1 [label="Layer 14 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer14_mha_value_stage1 [label="Layer 14 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer14_mha_qkt_stage1 [label="Layer 14 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer14_mha_weights_stage1 [label="Layer 14 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer14_mha_apply_stage1 [label="Layer 14 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer14_mha_out_stage1 [label="Layer 14 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=pink]
		layer14_mha_allreduce_stage1 [label="Layer 14 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer14_residual1_stage1 [label="Layer 14 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer14_layernorm1_stage1 [label="Layer 14 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer14_mlp_gate_stage1 [label="Layer 14 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer14_mlp_up_stage1 [label="Layer 14 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer14_mlp_gelu_stage1 [label="Layer 14 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=orange]
		layer14_mlp_down_stage1 [label="Layer 14 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer14_mlp_allreduce_stage1 [label="Layer 14 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer14_residual2_stage1 [label="Layer 14 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer14_layernorm2_stage1 [label="Layer 14 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer15_input_stage1 [label="Layer 15 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightyellow shape=parallelogram]
		layer15_mha_query_stage1 [label="Layer 15 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer15_mha_key_stage1 [label="Layer 15 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer15_mha_value_stage1 [label="Layer 15 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer15_mha_qkt_stage1 [label="Layer 15 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer15_mha_weights_stage1 [label="Layer 15 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer15_mha_apply_stage1 [label="Layer 15 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer15_mha_out_stage1 [label="Layer 15 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=pink]
		layer15_mha_allreduce_stage1 [label="Layer 15 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer15_residual1_stage1 [label="Layer 15 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer15_layernorm1_stage1 [label="Layer 15 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer15_mlp_gate_stage1 [label="Layer 15 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer15_mlp_up_stage1 [label="Layer 15 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer15_mlp_gelu_stage1 [label="Layer 15 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=orange]
		layer15_mlp_down_stage1 [label="Layer 15 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer15_mlp_allreduce_stage1 [label="Layer 15 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer15_residual2_stage1 [label="Layer 15 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer15_layernorm2_stage1 [label="Layer 15 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer16_input_stage1 [label="Layer 16 Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightyellow shape=parallelogram]
		layer16_mha_query_stage1 [label="Layer 16 MHA Query Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_q=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer16_mha_key_stage1 [label="Layer 16 MHA Key Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer16_mha_value_stage1 [label="Layer 16 MHA Value Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, heads=2, d_v=512]
GPU: Each shard 8-15" fillcolor=pink]
		layer16_mha_qkt_stage1 [label="Layer 16 MHA QKT Calculation
Input: [batch_size=1024, heads=2, seq_len=10000, d_k=512]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer16_mha_weights_stage1 [label="Layer 16 MHA Attention Weights
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer16_mha_apply_stage1 [label="Layer 16 MHA Apply Attention
Input: [batch_size=1024, heads=2, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
GPU: Each shard 8-15" fillcolor=lightcoral]
		layer16_mha_out_stage1 [label="Layer 16 MHA Output Linear
Input: [batch_size=1024, heads=2, seq_len=10000, d_v=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=pink]
		layer16_mha_allreduce_stage1 [label="Layer 16 MHA All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer16_residual1_stage1 [label="Layer 16 Residual Add 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer16_layernorm1_stage1 [label="Layer 16 Layer Norm 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
		layer16_mlp_gate_stage1 [label="Layer 16 MLP Gate Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer16_mlp_up_stage1 [label="Layer 16 MLP Up Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer16_mlp_gelu_stage1 [label="Layer 16 MLP GELU
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
GPU: Each shard 8-15" fillcolor=orange]
		layer16_mlp_down_stage1 [label="Layer 16 MLP Down Linear
Input: [batch_size=1024, seq_len=10000, mlp_hidden=4096]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: Each shard 8-15" fillcolor=lightyellow]
		layer16_mlp_allreduce_stage1 [label="Layer 16 MLP All-reduce
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgray shape=ellipse]
		layer16_residual2_stage1 [label="Layer 16 Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192], [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightblue shape=diamond]
		layer16_layernorm2_stage1 [label="Layer 16 Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=lightgreen]
	}
	pipeline_communication [label="Pipeline Communication
Transfer: [batch_size=1024, seq_len=10000, hidden_size=8192]
From: Devices 0-7
To: Devices 8-15" fillcolor=gray shape=ellipse]
	output [label="Final Output
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: All devices 8-15" fillcolor=red shape=ellipse]
	input_stage0 -> layer1_input_stage0
	layer1_input_stage0 -> layer1_mha_query_stage0
	layer1_input_stage0 -> layer1_mha_key_stage0
	layer1_input_stage0 -> layer1_mha_value_stage0
	layer1_mha_query_stage0 -> layer1_mha_qkt_stage0
	layer1_mha_key_stage0 -> layer1_mha_qkt_stage0
	layer1_mha_qkt_stage0 -> layer1_mha_weights_stage0
	layer1_mha_weights_stage0 -> layer1_mha_apply_stage0
	layer1_mha_value_stage0 -> layer1_mha_apply_stage0
	layer1_mha_apply_stage0 -> layer1_mha_out_stage0
	layer1_mha_out_stage0 -> layer1_mha_allreduce_stage0
	layer1_mha_allreduce_stage0 -> layer1_residual1_stage0
	layer1_input_stage0 -> layer1_residual1_stage0
	layer1_residual1_stage0 -> layer1_layernorm1_stage0
	layer1_layernorm1_stage0 -> layer1_mlp_gate_stage0
	layer1_layernorm1_stage0 -> layer1_mlp_up_stage0
	layer1_mlp_gate_stage0 -> layer1_mlp_gelu_stage0
	layer1_mlp_gelu_stage0 -> layer1_mlp_down_stage0
	layer1_mlp_up_stage0 -> layer1_mlp_down_stage0
	layer1_mlp_down_stage0 -> layer1_mlp_allreduce_stage0
	layer1_mlp_allreduce_stage0 -> layer1_residual2_stage0
	layer1_layernorm1_stage0 -> layer1_residual2_stage0
	layer1_residual2_stage0 -> layer1_layernorm2_stage0
	layer1_layernorm2_stage0 -> layer2_input_stage0
	layer2_input_stage0 -> layer2_mha_query_stage0
	layer2_input_stage0 -> layer2_mha_key_stage0
	layer2_input_stage0 -> layer2_mha_value_stage0
	layer2_mha_query_stage0 -> layer2_mha_qkt_stage0
	layer2_mha_key_stage0 -> layer2_mha_qkt_stage0
	layer2_mha_qkt_stage0 -> layer2_mha_weights_stage0
	layer2_mha_weights_stage0 -> layer2_mha_apply_stage0
	layer2_mha_value_stage0 -> layer2_mha_apply_stage0
	layer2_mha_apply_stage0 -> layer2_mha_out_stage0
	layer2_mha_out_stage0 -> layer2_mha_allreduce_stage0
	layer2_mha_allreduce_stage0 -> layer2_residual1_stage0
	layer2_input_stage0 -> layer2_residual1_stage0
	layer2_residual1_stage0 -> layer2_layernorm1_stage0
	layer2_layernorm1_stage0 -> layer2_mlp_gate_stage0
	layer2_layernorm1_stage0 -> layer2_mlp_up_stage0
	layer2_mlp_gate_stage0 -> layer2_mlp_gelu_stage0
	layer2_mlp_gelu_stage0 -> layer2_mlp_down_stage0
	layer2_mlp_up_stage0 -> layer2_mlp_down_stage0
	layer2_mlp_down_stage0 -> layer2_mlp_allreduce_stage0
	layer2_mlp_allreduce_stage0 -> layer2_residual2_stage0
	layer2_layernorm1_stage0 -> layer2_residual2_stage0
	layer2_residual2_stage0 -> layer2_layernorm2_stage0
	layer2_layernorm2_stage0 -> layer3_input_stage0
	layer3_input_stage0 -> layer3_mha_query_stage0
	layer3_input_stage0 -> layer3_mha_key_stage0
	layer3_input_stage0 -> layer3_mha_value_stage0
	layer3_mha_query_stage0 -> layer3_mha_qkt_stage0
	layer3_mha_key_stage0 -> layer3_mha_qkt_stage0
	layer3_mha_qkt_stage0 -> layer3_mha_weights_stage0
	layer3_mha_weights_stage0 -> layer3_mha_apply_stage0
	layer3_mha_value_stage0 -> layer3_mha_apply_stage0
	layer3_mha_apply_stage0 -> layer3_mha_out_stage0
	layer3_mha_out_stage0 -> layer3_mha_allreduce_stage0
	layer3_mha_allreduce_stage0 -> layer3_residual1_stage0
	layer3_input_stage0 -> layer3_residual1_stage0
	layer3_residual1_stage0 -> layer3_layernorm1_stage0
	layer3_layernorm1_stage0 -> layer3_mlp_gate_stage0
	layer3_layernorm1_stage0 -> layer3_mlp_up_stage0
	layer3_mlp_gate_stage0 -> layer3_mlp_gelu_stage0
	layer3_mlp_gelu_stage0 -> layer3_mlp_down_stage0
	layer3_mlp_up_stage0 -> layer3_mlp_down_stage0
	layer3_mlp_down_stage0 -> layer3_mlp_allreduce_stage0
	layer3_mlp_allreduce_stage0 -> layer3_residual2_stage0
	layer3_layernorm1_stage0 -> layer3_residual2_stage0
	layer3_residual2_stage0 -> layer3_layernorm2_stage0
	layer3_layernorm2_stage0 -> layer4_input_stage0
	layer4_input_stage0 -> layer4_mha_query_stage0
	layer4_input_stage0 -> layer4_mha_key_stage0
	layer4_input_stage0 -> layer4_mha_value_stage0
	layer4_mha_query_stage0 -> layer4_mha_qkt_stage0
	layer4_mha_key_stage0 -> layer4_mha_qkt_stage0
	layer4_mha_qkt_stage0 -> layer4_mha_weights_stage0
	layer4_mha_weights_stage0 -> layer4_mha_apply_stage0
	layer4_mha_value_stage0 -> layer4_mha_apply_stage0
	layer4_mha_apply_stage0 -> layer4_mha_out_stage0
	layer4_mha_out_stage0 -> layer4_mha_allreduce_stage0
	layer4_mha_allreduce_stage0 -> layer4_residual1_stage0
	layer4_input_stage0 -> layer4_residual1_stage0
	layer4_residual1_stage0 -> layer4_layernorm1_stage0
	layer4_layernorm1_stage0 -> layer4_mlp_gate_stage0
	layer4_layernorm1_stage0 -> layer4_mlp_up_stage0
	layer4_mlp_gate_stage0 -> layer4_mlp_gelu_stage0
	layer4_mlp_gelu_stage0 -> layer4_mlp_down_stage0
	layer4_mlp_up_stage0 -> layer4_mlp_down_stage0
	layer4_mlp_down_stage0 -> layer4_mlp_allreduce_stage0
	layer4_mlp_allreduce_stage0 -> layer4_residual2_stage0
	layer4_layernorm1_stage0 -> layer4_residual2_stage0
	layer4_residual2_stage0 -> layer4_layernorm2_stage0
	layer4_layernorm2_stage0 -> layer5_input_stage0
	layer5_input_stage0 -> layer5_mha_query_stage0
	layer5_input_stage0 -> layer5_mha_key_stage0
	layer5_input_stage0 -> layer5_mha_value_stage0
	layer5_mha_query_stage0 -> layer5_mha_qkt_stage0
	layer5_mha_key_stage0 -> layer5_mha_qkt_stage0
	layer5_mha_qkt_stage0 -> layer5_mha_weights_stage0
	layer5_mha_weights_stage0 -> layer5_mha_apply_stage0
	layer5_mha_value_stage0 -> layer5_mha_apply_stage0
	layer5_mha_apply_stage0 -> layer5_mha_out_stage0
	layer5_mha_out_stage0 -> layer5_mha_allreduce_stage0
	layer5_mha_allreduce_stage0 -> layer5_residual1_stage0
	layer5_input_stage0 -> layer5_residual1_stage0
	layer5_residual1_stage0 -> layer5_layernorm1_stage0
	layer5_layernorm1_stage0 -> layer5_mlp_gate_stage0
	layer5_layernorm1_stage0 -> layer5_mlp_up_stage0
	layer5_mlp_gate_stage0 -> layer5_mlp_gelu_stage0
	layer5_mlp_gelu_stage0 -> layer5_mlp_down_stage0
	layer5_mlp_up_stage0 -> layer5_mlp_down_stage0
	layer5_mlp_down_stage0 -> layer5_mlp_allreduce_stage0
	layer5_mlp_allreduce_stage0 -> layer5_residual2_stage0
	layer5_layernorm1_stage0 -> layer5_residual2_stage0
	layer5_residual2_stage0 -> layer5_layernorm2_stage0
	layer5_layernorm2_stage0 -> layer6_input_stage0
	layer6_input_stage0 -> layer6_mha_query_stage0
	layer6_input_stage0 -> layer6_mha_key_stage0
	layer6_input_stage0 -> layer6_mha_value_stage0
	layer6_mha_query_stage0 -> layer6_mha_qkt_stage0
	layer6_mha_key_stage0 -> layer6_mha_qkt_stage0
	layer6_mha_qkt_stage0 -> layer6_mha_weights_stage0
	layer6_mha_weights_stage0 -> layer6_mha_apply_stage0
	layer6_mha_value_stage0 -> layer6_mha_apply_stage0
	layer6_mha_apply_stage0 -> layer6_mha_out_stage0
	layer6_mha_out_stage0 -> layer6_mha_allreduce_stage0
	layer6_mha_allreduce_stage0 -> layer6_residual1_stage0
	layer6_input_stage0 -> layer6_residual1_stage0
	layer6_residual1_stage0 -> layer6_layernorm1_stage0
	layer6_layernorm1_stage0 -> layer6_mlp_gate_stage0
	layer6_layernorm1_stage0 -> layer6_mlp_up_stage0
	layer6_mlp_gate_stage0 -> layer6_mlp_gelu_stage0
	layer6_mlp_gelu_stage0 -> layer6_mlp_down_stage0
	layer6_mlp_up_stage0 -> layer6_mlp_down_stage0
	layer6_mlp_down_stage0 -> layer6_mlp_allreduce_stage0
	layer6_mlp_allreduce_stage0 -> layer6_residual2_stage0
	layer6_layernorm1_stage0 -> layer6_residual2_stage0
	layer6_residual2_stage0 -> layer6_layernorm2_stage0
	layer6_layernorm2_stage0 -> layer7_input_stage0
	layer7_input_stage0 -> layer7_mha_query_stage0
	layer7_input_stage0 -> layer7_mha_key_stage0
	layer7_input_stage0 -> layer7_mha_value_stage0
	layer7_mha_query_stage0 -> layer7_mha_qkt_stage0
	layer7_mha_key_stage0 -> layer7_mha_qkt_stage0
	layer7_mha_qkt_stage0 -> layer7_mha_weights_stage0
	layer7_mha_weights_stage0 -> layer7_mha_apply_stage0
	layer7_mha_value_stage0 -> layer7_mha_apply_stage0
	layer7_mha_apply_stage0 -> layer7_mha_out_stage0
	layer7_mha_out_stage0 -> layer7_mha_allreduce_stage0
	layer7_mha_allreduce_stage0 -> layer7_residual1_stage0
	layer7_input_stage0 -> layer7_residual1_stage0
	layer7_residual1_stage0 -> layer7_layernorm1_stage0
	layer7_layernorm1_stage0 -> layer7_mlp_gate_stage0
	layer7_layernorm1_stage0 -> layer7_mlp_up_stage0
	layer7_mlp_gate_stage0 -> layer7_mlp_gelu_stage0
	layer7_mlp_gelu_stage0 -> layer7_mlp_down_stage0
	layer7_mlp_up_stage0 -> layer7_mlp_down_stage0
	layer7_mlp_down_stage0 -> layer7_mlp_allreduce_stage0
	layer7_mlp_allreduce_stage0 -> layer7_residual2_stage0
	layer7_layernorm1_stage0 -> layer7_residual2_stage0
	layer7_residual2_stage0 -> layer7_layernorm2_stage0
	layer7_layernorm2_stage0 -> layer8_input_stage0
	layer8_input_stage0 -> layer8_mha_query_stage0
	layer8_input_stage0 -> layer8_mha_key_stage0
	layer8_input_stage0 -> layer8_mha_value_stage0
	layer8_mha_query_stage0 -> layer8_mha_qkt_stage0
	layer8_mha_key_stage0 -> layer8_mha_qkt_stage0
	layer8_mha_qkt_stage0 -> layer8_mha_weights_stage0
	layer8_mha_weights_stage0 -> layer8_mha_apply_stage0
	layer8_mha_value_stage0 -> layer8_mha_apply_stage0
	layer8_mha_apply_stage0 -> layer8_mha_out_stage0
	layer8_mha_out_stage0 -> layer8_mha_allreduce_stage0
	layer8_mha_allreduce_stage0 -> layer8_residual1_stage0
	layer8_input_stage0 -> layer8_residual1_stage0
	layer8_residual1_stage0 -> layer8_layernorm1_stage0
	layer8_layernorm1_stage0 -> layer8_mlp_gate_stage0
	layer8_layernorm1_stage0 -> layer8_mlp_up_stage0
	layer8_mlp_gate_stage0 -> layer8_mlp_gelu_stage0
	layer8_mlp_gelu_stage0 -> layer8_mlp_down_stage0
	layer8_mlp_up_stage0 -> layer8_mlp_down_stage0
	layer8_mlp_down_stage0 -> layer8_mlp_allreduce_stage0
	layer8_mlp_allreduce_stage0 -> layer8_residual2_stage0
	layer8_layernorm1_stage0 -> layer8_residual2_stage0
	layer8_residual2_stage0 -> layer8_layernorm2_stage0
	layer8_layernorm2_stage0 -> pipeline_communication
	pipeline_communication -> layer9_input_stage1
	layer9_input_stage1 -> layer9_mha_query_stage1
	layer9_input_stage1 -> layer9_mha_key_stage1
	layer9_input_stage1 -> layer9_mha_value_stage1
	layer9_mha_query_stage1 -> layer9_mha_qkt_stage1
	layer9_mha_key_stage1 -> layer9_mha_qkt_stage1
	layer9_mha_qkt_stage1 -> layer9_mha_weights_stage1
	layer9_mha_weights_stage1 -> layer9_mha_apply_stage1
	layer9_mha_value_stage1 -> layer9_mha_apply_stage1
	layer9_mha_apply_stage1 -> layer9_mha_out_stage1
	layer9_mha_out_stage1 -> layer9_mha_allreduce_stage1
	layer9_mha_allreduce_stage1 -> layer9_residual1_stage1
	layer9_input_stage1 -> layer9_residual1_stage1
	layer9_residual1_stage1 -> layer9_layernorm1_stage1
	layer9_layernorm1_stage1 -> layer9_mlp_gate_stage1
	layer9_layernorm1_stage1 -> layer9_mlp_up_stage1
	layer9_mlp_gate_stage1 -> layer9_mlp_gelu_stage1
	layer9_mlp_gelu_stage1 -> layer9_mlp_down_stage1
	layer9_mlp_up_stage1 -> layer9_mlp_down_stage1
	layer9_mlp_down_stage1 -> layer9_mlp_allreduce_stage1
	layer9_mlp_allreduce_stage1 -> layer9_residual2_stage1
	layer9_layernorm1_stage1 -> layer9_residual2_stage1
	layer9_residual2_stage1 -> layer9_layernorm2_stage1
	layer9_layernorm2_stage1 -> layer10_input_stage1
	layer10_input_stage1 -> layer10_mha_query_stage1
	layer10_input_stage1 -> layer10_mha_key_stage1
	layer10_input_stage1 -> layer10_mha_value_stage1
	layer10_mha_query_stage1 -> layer10_mha_qkt_stage1
	layer10_mha_key_stage1 -> layer10_mha_qkt_stage1
	layer10_mha_qkt_stage1 -> layer10_mha_weights_stage1
	layer10_mha_weights_stage1 -> layer10_mha_apply_stage1
	layer10_mha_value_stage1 -> layer10_mha_apply_stage1
	layer10_mha_apply_stage1 -> layer10_mha_out_stage1
	layer10_mha_out_stage1 -> layer10_mha_allreduce_stage1
	layer10_mha_allreduce_stage1 -> layer10_residual1_stage1
	layer10_input_stage1 -> layer10_residual1_stage1
	layer10_residual1_stage1 -> layer10_layernorm1_stage1
	layer10_layernorm1_stage1 -> layer10_mlp_gate_stage1
	layer10_layernorm1_stage1 -> layer10_mlp_up_stage1
	layer10_mlp_gate_stage1 -> layer10_mlp_gelu_stage1
	layer10_mlp_gelu_stage1 -> layer10_mlp_down_stage1
	layer10_mlp_up_stage1 -> layer10_mlp_down_stage1
	layer10_mlp_down_stage1 -> layer10_mlp_allreduce_stage1
	layer10_mlp_allreduce_stage1 -> layer10_residual2_stage1
	layer10_layernorm1_stage1 -> layer10_residual2_stage1
	layer10_residual2_stage1 -> layer10_layernorm2_stage1
	layer10_layernorm2_stage1 -> layer11_input_stage1
	layer11_input_stage1 -> layer11_mha_query_stage1
	layer11_input_stage1 -> layer11_mha_key_stage1
	layer11_input_stage1 -> layer11_mha_value_stage1
	layer11_mha_query_stage1 -> layer11_mha_qkt_stage1
	layer11_mha_key_stage1 -> layer11_mha_qkt_stage1
	layer11_mha_qkt_stage1 -> layer11_mha_weights_stage1
	layer11_mha_weights_stage1 -> layer11_mha_apply_stage1
	layer11_mha_value_stage1 -> layer11_mha_apply_stage1
	layer11_mha_apply_stage1 -> layer11_mha_out_stage1
	layer11_mha_out_stage1 -> layer11_mha_allreduce_stage1
	layer11_mha_allreduce_stage1 -> layer11_residual1_stage1
	layer11_input_stage1 -> layer11_residual1_stage1
	layer11_residual1_stage1 -> layer11_layernorm1_stage1
	layer11_layernorm1_stage1 -> layer11_mlp_gate_stage1
	layer11_layernorm1_stage1 -> layer11_mlp_up_stage1
	layer11_mlp_gate_stage1 -> layer11_mlp_gelu_stage1
	layer11_mlp_gelu_stage1 -> layer11_mlp_down_stage1
	layer11_mlp_up_stage1 -> layer11_mlp_down_stage1
	layer11_mlp_down_stage1 -> layer11_mlp_allreduce_stage1
	layer11_mlp_allreduce_stage1 -> layer11_residual2_stage1
	layer11_layernorm1_stage1 -> layer11_residual2_stage1
	layer11_residual2_stage1 -> layer11_layernorm2_stage1
	layer11_layernorm2_stage1 -> layer12_input_stage1
	layer12_input_stage1 -> layer12_mha_query_stage1
	layer12_input_stage1 -> layer12_mha_key_stage1
	layer12_input_stage1 -> layer12_mha_value_stage1
	layer12_mha_query_stage1 -> layer12_mha_qkt_stage1
	layer12_mha_key_stage1 -> layer12_mha_qkt_stage1
	layer12_mha_qkt_stage1 -> layer12_mha_weights_stage1
	layer12_mha_weights_stage1 -> layer12_mha_apply_stage1
	layer12_mha_value_stage1 -> layer12_mha_apply_stage1
	layer12_mha_apply_stage1 -> layer12_mha_out_stage1
	layer12_mha_out_stage1 -> layer12_mha_allreduce_stage1
	layer12_mha_allreduce_stage1 -> layer12_residual1_stage1
	layer12_input_stage1 -> layer12_residual1_stage1
	layer12_residual1_stage1 -> layer12_layernorm1_stage1
	layer12_layernorm1_stage1 -> layer12_mlp_gate_stage1
	layer12_layernorm1_stage1 -> layer12_mlp_up_stage1
	layer12_mlp_gate_stage1 -> layer12_mlp_gelu_stage1
	layer12_mlp_gelu_stage1 -> layer12_mlp_down_stage1
	layer12_mlp_up_stage1 -> layer12_mlp_down_stage1
	layer12_mlp_down_stage1 -> layer12_mlp_allreduce_stage1
	layer12_mlp_allreduce_stage1 -> layer12_residual2_stage1
	layer12_layernorm1_stage1 -> layer12_residual2_stage1
	layer12_residual2_stage1 -> layer12_layernorm2_stage1
	layer12_layernorm2_stage1 -> layer13_input_stage1
	layer13_input_stage1 -> layer13_mha_query_stage1
	layer13_input_stage1 -> layer13_mha_key_stage1
	layer13_input_stage1 -> layer13_mha_value_stage1
	layer13_mha_query_stage1 -> layer13_mha_qkt_stage1
	layer13_mha_key_stage1 -> layer13_mha_qkt_stage1
	layer13_mha_qkt_stage1 -> layer13_mha_weights_stage1
	layer13_mha_weights_stage1 -> layer13_mha_apply_stage1
	layer13_mha_value_stage1 -> layer13_mha_apply_stage1
	layer13_mha_apply_stage1 -> layer13_mha_out_stage1
	layer13_mha_out_stage1 -> layer13_mha_allreduce_stage1
	layer13_mha_allreduce_stage1 -> layer13_residual1_stage1
	layer13_input_stage1 -> layer13_residual1_stage1
	layer13_residual1_stage1 -> layer13_layernorm1_stage1
	layer13_layernorm1_stage1 -> layer13_mlp_gate_stage1
	layer13_layernorm1_stage1 -> layer13_mlp_up_stage1
	layer13_mlp_gate_stage1 -> layer13_mlp_gelu_stage1
	layer13_mlp_gelu_stage1 -> layer13_mlp_down_stage1
	layer13_mlp_up_stage1 -> layer13_mlp_down_stage1
	layer13_mlp_down_stage1 -> layer13_mlp_allreduce_stage1
	layer13_mlp_allreduce_stage1 -> layer13_residual2_stage1
	layer13_layernorm1_stage1 -> layer13_residual2_stage1
	layer13_residual2_stage1 -> layer13_layernorm2_stage1
	layer13_layernorm2_stage1 -> layer14_input_stage1
	layer14_input_stage1 -> layer14_mha_query_stage1
	layer14_input_stage1 -> layer14_mha_key_stage1
	layer14_input_stage1 -> layer14_mha_value_stage1
	layer14_mha_query_stage1 -> layer14_mha_qkt_stage1
	layer14_mha_key_stage1 -> layer14_mha_qkt_stage1
	layer14_mha_qkt_stage1 -> layer14_mha_weights_stage1
	layer14_mha_weights_stage1 -> layer14_mha_apply_stage1
	layer14_mha_value_stage1 -> layer14_mha_apply_stage1
	layer14_mha_apply_stage1 -> layer14_mha_out_stage1
	layer14_mha_out_stage1 -> layer14_mha_allreduce_stage1
	layer14_mha_allreduce_stage1 -> layer14_residual1_stage1
	layer14_input_stage1 -> layer14_residual1_stage1
	layer14_residual1_stage1 -> layer14_layernorm1_stage1
	layer14_layernorm1_stage1 -> layer14_mlp_gate_stage1
	layer14_layernorm1_stage1 -> layer14_mlp_up_stage1
	layer14_mlp_gate_stage1 -> layer14_mlp_gelu_stage1
	layer14_mlp_gelu_stage1 -> layer14_mlp_down_stage1
	layer14_mlp_up_stage1 -> layer14_mlp_down_stage1
	layer14_mlp_down_stage1 -> layer14_mlp_allreduce_stage1
	layer14_mlp_allreduce_stage1 -> layer14_residual2_stage1
	layer14_layernorm1_stage1 -> layer14_residual2_stage1
	layer14_residual2_stage1 -> layer14_layernorm2_stage1
	layer14_layernorm2_stage1 -> layer15_input_stage1
	layer15_input_stage1 -> layer15_mha_query_stage1
	layer15_input_stage1 -> layer15_mha_key_stage1
	layer15_input_stage1 -> layer15_mha_value_stage1
	layer15_mha_query_stage1 -> layer15_mha_qkt_stage1
	layer15_mha_key_stage1 -> layer15_mha_qkt_stage1
	layer15_mha_qkt_stage1 -> layer15_mha_weights_stage1
	layer15_mha_weights_stage1 -> layer15_mha_apply_stage1
	layer15_mha_value_stage1 -> layer15_mha_apply_stage1
	layer15_mha_apply_stage1 -> layer15_mha_out_stage1
	layer15_mha_out_stage1 -> layer15_mha_allreduce_stage1
	layer15_mha_allreduce_stage1 -> layer15_residual1_stage1
	layer15_input_stage1 -> layer15_residual1_stage1
	layer15_residual1_stage1 -> layer15_layernorm1_stage1
	layer15_layernorm1_stage1 -> layer15_mlp_gate_stage1
	layer15_layernorm1_stage1 -> layer15_mlp_up_stage1
	layer15_mlp_gate_stage1 -> layer15_mlp_gelu_stage1
	layer15_mlp_gelu_stage1 -> layer15_mlp_down_stage1
	layer15_mlp_up_stage1 -> layer15_mlp_down_stage1
	layer15_mlp_down_stage1 -> layer15_mlp_allreduce_stage1
	layer15_mlp_allreduce_stage1 -> layer15_residual2_stage1
	layer15_layernorm1_stage1 -> layer15_residual2_stage1
	layer15_residual2_stage1 -> layer15_layernorm2_stage1
	layer15_layernorm2_stage1 -> layer16_input_stage1
	layer16_input_stage1 -> layer16_mha_query_stage1
	layer16_input_stage1 -> layer16_mha_key_stage1
	layer16_input_stage1 -> layer16_mha_value_stage1
	layer16_mha_query_stage1 -> layer16_mha_qkt_stage1
	layer16_mha_key_stage1 -> layer16_mha_qkt_stage1
	layer16_mha_qkt_stage1 -> layer16_mha_weights_stage1
	layer16_mha_weights_stage1 -> layer16_mha_apply_stage1
	layer16_mha_value_stage1 -> layer16_mha_apply_stage1
	layer16_mha_apply_stage1 -> layer16_mha_out_stage1
	layer16_mha_out_stage1 -> layer16_mha_allreduce_stage1
	layer16_mha_allreduce_stage1 -> layer16_residual1_stage1
	layer16_input_stage1 -> layer16_residual1_stage1
	layer16_residual1_stage1 -> layer16_layernorm1_stage1
	layer16_layernorm1_stage1 -> layer16_mlp_gate_stage1
	layer16_layernorm1_stage1 -> layer16_mlp_up_stage1
	layer16_mlp_gate_stage1 -> layer16_mlp_gelu_stage1
	layer16_mlp_gelu_stage1 -> layer16_mlp_down_stage1
	layer16_mlp_up_stage1 -> layer16_mlp_down_stage1
	layer16_mlp_down_stage1 -> layer16_mlp_allreduce_stage1
	layer16_mlp_allreduce_stage1 -> layer16_residual2_stage1
	layer16_layernorm1_stage1 -> layer16_residual2_stage1
	layer16_residual2_stage1 -> layer16_layernorm2_stage1
	layer16_layernorm2_stage1 -> output
}
