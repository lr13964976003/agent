// MA Separation Complete Model DAG
digraph ma_separation_dag {
	rankdir=TB size="20,30"
	node [fontname=Arial fontsize=10]
	node [shape=ellipse]
	node [style=filled]
	input [label="Input\nInput: [batch_size=1024, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#e6f3ff" shape=ellipse]
	embedding [label="Embedding\nInput: [batch_size=1024, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
	subgraph cluster_layer_0 {
		color=gray label="Layer 0" style=dashed
		ln1_layer_0 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		qkv_layer_0_gpu_0 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		qkv_layer_0_gpu_1 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1" fillcolor="#0f8aff" shape=rectangle]
		qkv_layer_0_gpu_2 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2" fillcolor="#1e94ff" shape=rectangle]
		qkv_layer_0_gpu_3 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3" fillcolor="#2d9eff" shape=rectangle]
		qkv_layer_0_gpu_4 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4" fillcolor="#3ca8ff" shape=rectangle]
		qkv_layer_0_gpu_5 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5" fillcolor="#4bb2ff" shape=rectangle]
		qkv_layer_0_gpu_6 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6" fillcolor="#5abcff" shape=rectangle]
		qkv_layer_0_gpu_7 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7" fillcolor="#69c6ff" shape=rectangle]
		qkv_layer_0_gpu_8 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8" fillcolor="#78d0ff" shape=rectangle]
		qkv_layer_0_gpu_9 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9" fillcolor="#87daff" shape=rectangle]
		qkv_layer_0_gpu_10 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10" fillcolor="#96e4ff" shape=rectangle]
		qkv_layer_0_gpu_11 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11" fillcolor="#a5eeff" shape=rectangle]
		attention_layer_0_gpu_0 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		attention_layer_0_gpu_1 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1" fillcolor="#0f8aff" shape=rectangle]
		attention_layer_0_gpu_2 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2" fillcolor="#1e94ff" shape=rectangle]
		attention_layer_0_gpu_3 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3" fillcolor="#2d9eff" shape=rectangle]
		attention_layer_0_gpu_4 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4" fillcolor="#3ca8ff" shape=rectangle]
		attention_layer_0_gpu_5 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5" fillcolor="#4bb2ff" shape=rectangle]
		attention_layer_0_gpu_6 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6" fillcolor="#5abcff" shape=rectangle]
		attention_layer_0_gpu_7 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7" fillcolor="#69c6ff" shape=rectangle]
		attention_layer_0_gpu_8 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8" fillcolor="#78d0ff" shape=rectangle]
		attention_layer_0_gpu_9 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9" fillcolor="#87daff" shape=rectangle]
		attention_layer_0_gpu_10 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10" fillcolor="#96e4ff" shape=rectangle]
		attention_layer_0_gpu_11 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11" fillcolor="#a5eeff" shape=rectangle]
		attention_agg_layer_0 [label="Attention All-Reduce\nInput: [12× batch_size=1024, seq_len=2048, heads=2-3, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: all attention GPUs" fillcolor="#ffcccc" shape=parallelogram]
		out_proj_layer_0 [label="Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		residual1_layer_0 [label="Residual Add 1\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#ffffcc" shape=parallelogram]
		ln2_layer_0 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		gate_layer_0 [label="Gating Network\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: all MoE GPUs (12-15)" fillcolor="#ccffcc" shape=parallelogram style=dashed]
		expert_layer_0_expert_0_gpu_12 [label="Expert0\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_0_expert_1_gpu_12 [label="Expert1\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_0_expert_2_gpu_12 [label="Expert2\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_0_expert_3_gpu_12 [label="Expert3\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_0_expert_4_gpu_13 [label="Expert4\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_0_expert_5_gpu_13 [label="Expert5\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_0_expert_6_gpu_13 [label="Expert6\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_0_expert_7_gpu_13 [label="Expert7\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_0_expert_8_gpu_14 [label="Expert8\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_0_expert_9_gpu_14 [label="Expert9\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_0_expert_10_gpu_14 [label="Expert10\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_0_expert_11_gpu_14 [label="Expert11\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_0_expert_12_gpu_15 [label="Expert12\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_layer_0_expert_13_gpu_15 [label="Expert13\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_layer_0_expert_14_gpu_15 [label="Expert14\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_layer_0_expert_15_gpu_15 [label="Expert15\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_agg_layer_0 [label="Expert Aggregation\nInput: [16× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: all MoE GPUs" fillcolor="#ffcccc" shape=parallelogram]
		residual2_layer_0 [label="Residual Add 2\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#ffffcc" shape=parallelogram]
	}
	subgraph cluster_layer_1 {
		color=gray label="Layer 1" style=dashed
		ln1_layer_1 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		qkv_layer_1_gpu_0 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		qkv_layer_1_gpu_1 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1" fillcolor="#0f8aff" shape=rectangle]
		qkv_layer_1_gpu_2 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2" fillcolor="#1e94ff" shape=rectangle]
		qkv_layer_1_gpu_3 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3" fillcolor="#2d9eff" shape=rectangle]
		qkv_layer_1_gpu_4 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4" fillcolor="#3ca8ff" shape=rectangle]
		qkv_layer_1_gpu_5 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5" fillcolor="#4bb2ff" shape=rectangle]
		qkv_layer_1_gpu_6 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6" fillcolor="#5abcff" shape=rectangle]
		qkv_layer_1_gpu_7 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7" fillcolor="#69c6ff" shape=rectangle]
		qkv_layer_1_gpu_8 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8" fillcolor="#78d0ff" shape=rectangle]
		qkv_layer_1_gpu_9 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9" fillcolor="#87daff" shape=rectangle]
		qkv_layer_1_gpu_10 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10" fillcolor="#96e4ff" shape=rectangle]
		qkv_layer_1_gpu_11 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11" fillcolor="#a5eeff" shape=rectangle]
		attention_layer_1_gpu_0 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		attention_layer_1_gpu_1 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1" fillcolor="#0f8aff" shape=rectangle]
		attention_layer_1_gpu_2 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2" fillcolor="#1e94ff" shape=rectangle]
		attention_layer_1_gpu_3 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3" fillcolor="#2d9eff" shape=rectangle]
		attention_layer_1_gpu_4 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4" fillcolor="#3ca8ff" shape=rectangle]
		attention_layer_1_gpu_5 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5" fillcolor="#4bb2ff" shape=rectangle]
		attention_layer_1_gpu_6 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6" fillcolor="#5abcff" shape=rectangle]
		attention_layer_1_gpu_7 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7" fillcolor="#69c6ff" shape=rectangle]
		attention_layer_1_gpu_8 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8" fillcolor="#78d0ff" shape=rectangle]
		attention_layer_1_gpu_9 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9" fillcolor="#87daff" shape=rectangle]
		attention_layer_1_gpu_10 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10" fillcolor="#96e4ff" shape=rectangle]
		attention_layer_1_gpu_11 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11" fillcolor="#a5eeff" shape=rectangle]
		attention_agg_layer_1 [label="Attention All-Reduce\nInput: [12× batch_size=1024, seq_len=2048, heads=2-3, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: all attention GPUs" fillcolor="#ffcccc" shape=parallelogram]
		out_proj_layer_1 [label="Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		residual1_layer_1 [label="Residual Add 1\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#ffffcc" shape=parallelogram]
		ln2_layer_1 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		gate_layer_1 [label="Gating Network\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: all MoE GPUs (12-15)" fillcolor="#ccffcc" shape=parallelogram style=dashed]
		expert_layer_1_expert_0_gpu_12 [label="Expert0\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_1_expert_1_gpu_12 [label="Expert1\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_1_expert_2_gpu_12 [label="Expert2\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_1_expert_3_gpu_12 [label="Expert3\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_1_expert_4_gpu_13 [label="Expert4\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_1_expert_5_gpu_13 [label="Expert5\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_1_expert_6_gpu_13 [label="Expert6\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_1_expert_7_gpu_13 [label="Expert7\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_1_expert_8_gpu_14 [label="Expert8\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_1_expert_9_gpu_14 [label="Expert9\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_1_expert_10_gpu_14 [label="Expert10\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_1_expert_11_gpu_14 [label="Expert11\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_1_expert_12_gpu_15 [label="Expert12\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_layer_1_expert_13_gpu_15 [label="Expert13\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_layer_1_expert_14_gpu_15 [label="Expert14\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_layer_1_expert_15_gpu_15 [label="Expert15\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_agg_layer_1 [label="Expert Aggregation\nInput: [16× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: all MoE GPUs" fillcolor="#ffcccc" shape=parallelogram]
		residual2_layer_1 [label="Residual Add 2\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#ffffcc" shape=parallelogram]
	}
	subgraph cluster_layer_2 {
		color=gray label="Layer 2" style=dashed
		ln1_layer_2 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		qkv_layer_2_gpu_0 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		qkv_layer_2_gpu_1 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1" fillcolor="#0f8aff" shape=rectangle]
		qkv_layer_2_gpu_2 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2" fillcolor="#1e94ff" shape=rectangle]
		qkv_layer_2_gpu_3 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3" fillcolor="#2d9eff" shape=rectangle]
		qkv_layer_2_gpu_4 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4" fillcolor="#3ca8ff" shape=rectangle]
		qkv_layer_2_gpu_5 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5" fillcolor="#4bb2ff" shape=rectangle]
		qkv_layer_2_gpu_6 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6" fillcolor="#5abcff" shape=rectangle]
		qkv_layer_2_gpu_7 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7" fillcolor="#69c6ff" shape=rectangle]
		qkv_layer_2_gpu_8 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8" fillcolor="#78d0ff" shape=rectangle]
		qkv_layer_2_gpu_9 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9" fillcolor="#87daff" shape=rectangle]
		qkv_layer_2_gpu_10 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10" fillcolor="#96e4ff" shape=rectangle]
		qkv_layer_2_gpu_11 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11" fillcolor="#a5eeff" shape=rectangle]
		attention_layer_2_gpu_0 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		attention_layer_2_gpu_1 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1" fillcolor="#0f8aff" shape=rectangle]
		attention_layer_2_gpu_2 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2" fillcolor="#1e94ff" shape=rectangle]
		attention_layer_2_gpu_3 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3" fillcolor="#2d9eff" shape=rectangle]
		attention_layer_2_gpu_4 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4" fillcolor="#3ca8ff" shape=rectangle]
		attention_layer_2_gpu_5 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5" fillcolor="#4bb2ff" shape=rectangle]
		attention_layer_2_gpu_6 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6" fillcolor="#5abcff" shape=rectangle]
		attention_layer_2_gpu_7 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7" fillcolor="#69c6ff" shape=rectangle]
		attention_layer_2_gpu_8 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8" fillcolor="#78d0ff" shape=rectangle]
		attention_layer_2_gpu_9 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9" fillcolor="#87daff" shape=rectangle]
		attention_layer_2_gpu_10 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10" fillcolor="#96e4ff" shape=rectangle]
		attention_layer_2_gpu_11 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11" fillcolor="#a5eeff" shape=rectangle]
		attention_agg_layer_2 [label="Attention All-Reduce\nInput: [12× batch_size=1024, seq_len=2048, heads=2-3, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: all attention GPUs" fillcolor="#ffcccc" shape=parallelogram]
		out_proj_layer_2 [label="Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		residual1_layer_2 [label="Residual Add 1\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#ffffcc" shape=parallelogram]
		ln2_layer_2 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		gate_layer_2 [label="Gating Network\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: all MoE GPUs (12-15)" fillcolor="#ccffcc" shape=parallelogram style=dashed]
		expert_layer_2_expert_0_gpu_12 [label="Expert0\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_2_expert_1_gpu_12 [label="Expert1\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_2_expert_2_gpu_12 [label="Expert2\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_2_expert_3_gpu_12 [label="Expert3\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_2_expert_4_gpu_13 [label="Expert4\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_2_expert_5_gpu_13 [label="Expert5\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_2_expert_6_gpu_13 [label="Expert6\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_2_expert_7_gpu_13 [label="Expert7\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_2_expert_8_gpu_14 [label="Expert8\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_2_expert_9_gpu_14 [label="Expert9\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_2_expert_10_gpu_14 [label="Expert10\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_2_expert_11_gpu_14 [label="Expert11\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_2_expert_12_gpu_15 [label="Expert12\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_layer_2_expert_13_gpu_15 [label="Expert13\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_layer_2_expert_14_gpu_15 [label="Expert14\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_layer_2_expert_15_gpu_15 [label="Expert15\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_agg_layer_2 [label="Expert Aggregation\nInput: [16× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: all MoE GPUs" fillcolor="#ffcccc" shape=parallelogram]
		residual2_layer_2 [label="Residual Add 2\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#ffffcc" shape=parallelogram]
	}
	subgraph cluster_layer_3 {
		color=gray label="Layer 3" style=dashed
		ln1_layer_3 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		qkv_layer_3_gpu_0 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		qkv_layer_3_gpu_1 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1" fillcolor="#0f8aff" shape=rectangle]
		qkv_layer_3_gpu_2 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2" fillcolor="#1e94ff" shape=rectangle]
		qkv_layer_3_gpu_3 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3" fillcolor="#2d9eff" shape=rectangle]
		qkv_layer_3_gpu_4 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4" fillcolor="#3ca8ff" shape=rectangle]
		qkv_layer_3_gpu_5 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5" fillcolor="#4bb2ff" shape=rectangle]
		qkv_layer_3_gpu_6 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6" fillcolor="#5abcff" shape=rectangle]
		qkv_layer_3_gpu_7 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7" fillcolor="#69c6ff" shape=rectangle]
		qkv_layer_3_gpu_8 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8" fillcolor="#78d0ff" shape=rectangle]
		qkv_layer_3_gpu_9 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9" fillcolor="#87daff" shape=rectangle]
		qkv_layer_3_gpu_10 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10" fillcolor="#96e4ff" shape=rectangle]
		qkv_layer_3_gpu_11 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11" fillcolor="#a5eeff" shape=rectangle]
		attention_layer_3_gpu_0 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		attention_layer_3_gpu_1 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1" fillcolor="#0f8aff" shape=rectangle]
		attention_layer_3_gpu_2 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2" fillcolor="#1e94ff" shape=rectangle]
		attention_layer_3_gpu_3 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3" fillcolor="#2d9eff" shape=rectangle]
		attention_layer_3_gpu_4 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4" fillcolor="#3ca8ff" shape=rectangle]
		attention_layer_3_gpu_5 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5" fillcolor="#4bb2ff" shape=rectangle]
		attention_layer_3_gpu_6 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6" fillcolor="#5abcff" shape=rectangle]
		attention_layer_3_gpu_7 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7" fillcolor="#69c6ff" shape=rectangle]
		attention_layer_3_gpu_8 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8" fillcolor="#78d0ff" shape=rectangle]
		attention_layer_3_gpu_9 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9" fillcolor="#87daff" shape=rectangle]
		attention_layer_3_gpu_10 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10" fillcolor="#96e4ff" shape=rectangle]
		attention_layer_3_gpu_11 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11" fillcolor="#a5eeff" shape=rectangle]
		attention_agg_layer_3 [label="Attention All-Reduce\nInput: [12× batch_size=1024, seq_len=2048, heads=2-3, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: all attention GPUs" fillcolor="#ffcccc" shape=parallelogram]
		out_proj_layer_3 [label="Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		residual1_layer_3 [label="Residual Add 1\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#ffffcc" shape=parallelogram]
		ln2_layer_3 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
		gate_layer_3 [label="Gating Network\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: all MoE GPUs (12-15)" fillcolor="#ccffcc" shape=parallelogram style=dashed]
		expert_layer_3_expert_0_gpu_12 [label="Expert0\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_3_expert_1_gpu_12 [label="Expert1\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_3_expert_2_gpu_12 [label="Expert2\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_3_expert_3_gpu_12 [label="Expert3\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12" fillcolor="#ff0000" shape=rectangle]
		expert_layer_3_expert_4_gpu_13 [label="Expert4\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_3_expert_5_gpu_13 [label="Expert5\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_3_expert_6_gpu_13 [label="Expert6\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_3_expert_7_gpu_13 [label="Expert7\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13" fillcolor="#ff3232" shape=rectangle]
		expert_layer_3_expert_8_gpu_14 [label="Expert8\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_3_expert_9_gpu_14 [label="Expert9\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_3_expert_10_gpu_14 [label="Expert10\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_3_expert_11_gpu_14 [label="Expert11\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14" fillcolor="#ff6464" shape=rectangle]
		expert_layer_3_expert_12_gpu_15 [label="Expert12\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_layer_3_expert_13_gpu_15 [label="Expert13\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_layer_3_expert_14_gpu_15 [label="Expert14\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_layer_3_expert_15_gpu_15 [label="Expert15\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15" fillcolor="#ff9696" shape=rectangle]
		expert_agg_layer_3 [label="Expert Aggregation\nInput: [16× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: all MoE GPUs" fillcolor="#ffcccc" shape=parallelogram]
		residual2_layer_3 [label="Residual Add 2\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#ffffcc" shape=parallelogram]
	}
	final_layernorm [label="Final LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#0080ff" shape=rectangle]
	output [label="Output\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, vocab_size=50265]\nGPU: 0" fillcolor="#e6f3ff" shape=ellipse]
	input -> embedding
	embedding -> ln1_layer_0
	ln1_layer_0 -> qkv_layer_0_gpu_0
	qkv_layer_0_gpu_0 -> attention_layer_0_gpu_0
	qkv_layer_0_gpu_1 -> attention_layer_0_gpu_0 [color=blue style=dashed]
	qkv_layer_0_gpu_2 -> attention_layer_0_gpu_0 [color=blue style=dashed]
	qkv_layer_0_gpu_3 -> attention_layer_0_gpu_0 [color=blue style=dashed]
	qkv_layer_0_gpu_4 -> attention_layer_0_gpu_0 [color=blue style=dashed]
	qkv_layer_0_gpu_5 -> attention_layer_0_gpu_0 [color=blue style=dashed]
	qkv_layer_0_gpu_6 -> attention_layer_0_gpu_0 [color=blue style=dashed]
	qkv_layer_0_gpu_7 -> attention_layer_0_gpu_0 [color=blue style=dashed]
	qkv_layer_0_gpu_8 -> attention_layer_0_gpu_0 [color=blue style=dashed]
	qkv_layer_0_gpu_9 -> attention_layer_0_gpu_0 [color=blue style=dashed]
	qkv_layer_0_gpu_10 -> attention_layer_0_gpu_0 [color=blue style=dashed]
	qkv_layer_0_gpu_11 -> attention_layer_0_gpu_0 [color=blue style=dashed]
	ln1_layer_0 -> qkv_layer_0_gpu_1
	qkv_layer_0_gpu_1 -> attention_layer_0_gpu_1
	qkv_layer_0_gpu_0 -> attention_layer_0_gpu_1 [color=blue style=dashed]
	qkv_layer_0_gpu_2 -> attention_layer_0_gpu_1 [color=blue style=dashed]
	qkv_layer_0_gpu_3 -> attention_layer_0_gpu_1 [color=blue style=dashed]
	qkv_layer_0_gpu_4 -> attention_layer_0_gpu_1 [color=blue style=dashed]
	qkv_layer_0_gpu_5 -> attention_layer_0_gpu_1 [color=blue style=dashed]
	qkv_layer_0_gpu_6 -> attention_layer_0_gpu_1 [color=blue style=dashed]
	qkv_layer_0_gpu_7 -> attention_layer_0_gpu_1 [color=blue style=dashed]
	qkv_layer_0_gpu_8 -> attention_layer_0_gpu_1 [color=blue style=dashed]
	qkv_layer_0_gpu_9 -> attention_layer_0_gpu_1 [color=blue style=dashed]
	qkv_layer_0_gpu_10 -> attention_layer_0_gpu_1 [color=blue style=dashed]
	qkv_layer_0_gpu_11 -> attention_layer_0_gpu_1 [color=blue style=dashed]
	ln1_layer_0 -> qkv_layer_0_gpu_2
	qkv_layer_0_gpu_2 -> attention_layer_0_gpu_2
	qkv_layer_0_gpu_0 -> attention_layer_0_gpu_2 [color=blue style=dashed]
	qkv_layer_0_gpu_1 -> attention_layer_0_gpu_2 [color=blue style=dashed]
	qkv_layer_0_gpu_3 -> attention_layer_0_gpu_2 [color=blue style=dashed]
	qkv_layer_0_gpu_4 -> attention_layer_0_gpu_2 [color=blue style=dashed]
	qkv_layer_0_gpu_5 -> attention_layer_0_gpu_2 [color=blue style=dashed]
	qkv_layer_0_gpu_6 -> attention_layer_0_gpu_2 [color=blue style=dashed]
	qkv_layer_0_gpu_7 -> attention_layer_0_gpu_2 [color=blue style=dashed]
	qkv_layer_0_gpu_8 -> attention_layer_0_gpu_2 [color=blue style=dashed]
	qkv_layer_0_gpu_9 -> attention_layer_0_gpu_2 [color=blue style=dashed]
	qkv_layer_0_gpu_10 -> attention_layer_0_gpu_2 [color=blue style=dashed]
	qkv_layer_0_gpu_11 -> attention_layer_0_gpu_2 [color=blue style=dashed]
	ln1_layer_0 -> qkv_layer_0_gpu_3
	qkv_layer_0_gpu_3 -> attention_layer_0_gpu_3
	qkv_layer_0_gpu_0 -> attention_layer_0_gpu_3 [color=blue style=dashed]
	qkv_layer_0_gpu_1 -> attention_layer_0_gpu_3 [color=blue style=dashed]
	qkv_layer_0_gpu_2 -> attention_layer_0_gpu_3 [color=blue style=dashed]
	qkv_layer_0_gpu_4 -> attention_layer_0_gpu_3 [color=blue style=dashed]
	qkv_layer_0_gpu_5 -> attention_layer_0_gpu_3 [color=blue style=dashed]
	qkv_layer_0_gpu_6 -> attention_layer_0_gpu_3 [color=blue style=dashed]
	qkv_layer_0_gpu_7 -> attention_layer_0_gpu_3 [color=blue style=dashed]
	qkv_layer_0_gpu_8 -> attention_layer_0_gpu_3 [color=blue style=dashed]
	qkv_layer_0_gpu_9 -> attention_layer_0_gpu_3 [color=blue style=dashed]
	qkv_layer_0_gpu_10 -> attention_layer_0_gpu_3 [color=blue style=dashed]
	qkv_layer_0_gpu_11 -> attention_layer_0_gpu_3 [color=blue style=dashed]
	ln1_layer_0 -> qkv_layer_0_gpu_4
	qkv_layer_0_gpu_4 -> attention_layer_0_gpu_4
	qkv_layer_0_gpu_0 -> attention_layer_0_gpu_4 [color=blue style=dashed]
	qkv_layer_0_gpu_1 -> attention_layer_0_gpu_4 [color=blue style=dashed]
	qkv_layer_0_gpu_2 -> attention_layer_0_gpu_4 [color=blue style=dashed]
	qkv_layer_0_gpu_3 -> attention_layer_0_gpu_4 [color=blue style=dashed]
	qkv_layer_0_gpu_5 -> attention_layer_0_gpu_4 [color=blue style=dashed]
	qkv_layer_0_gpu_6 -> attention_layer_0_gpu_4 [color=blue style=dashed]
	qkv_layer_0_gpu_7 -> attention_layer_0_gpu_4 [color=blue style=dashed]
	qkv_layer_0_gpu_8 -> attention_layer_0_gpu_4 [color=blue style=dashed]
	qkv_layer_0_gpu_9 -> attention_layer_0_gpu_4 [color=blue style=dashed]
	qkv_layer_0_gpu_10 -> attention_layer_0_gpu_4 [color=blue style=dashed]
	qkv_layer_0_gpu_11 -> attention_layer_0_gpu_4 [color=blue style=dashed]
	ln1_layer_0 -> qkv_layer_0_gpu_5
	qkv_layer_0_gpu_5 -> attention_layer_0_gpu_5
	qkv_layer_0_gpu_0 -> attention_layer_0_gpu_5 [color=blue style=dashed]
	qkv_layer_0_gpu_1 -> attention_layer_0_gpu_5 [color=blue style=dashed]
	qkv_layer_0_gpu_2 -> attention_layer_0_gpu_5 [color=blue style=dashed]
	qkv_layer_0_gpu_3 -> attention_layer_0_gpu_5 [color=blue style=dashed]
	qkv_layer_0_gpu_4 -> attention_layer_0_gpu_5 [color=blue style=dashed]
	qkv_layer_0_gpu_6 -> attention_layer_0_gpu_5 [color=blue style=dashed]
	qkv_layer_0_gpu_7 -> attention_layer_0_gpu_5 [color=blue style=dashed]
	qkv_layer_0_gpu_8 -> attention_layer_0_gpu_5 [color=blue style=dashed]
	qkv_layer_0_gpu_9 -> attention_layer_0_gpu_5 [color=blue style=dashed]
	qkv_layer_0_gpu_10 -> attention_layer_0_gpu_5 [color=blue style=dashed]
	qkv_layer_0_gpu_11 -> attention_layer_0_gpu_5 [color=blue style=dashed]
	ln1_layer_0 -> qkv_layer_0_gpu_6
	qkv_layer_0_gpu_6 -> attention_layer_0_gpu_6
	qkv_layer_0_gpu_0 -> attention_layer_0_gpu_6 [color=blue style=dashed]
	qkv_layer_0_gpu_1 -> attention_layer_0_gpu_6 [color=blue style=dashed]
	qkv_layer_0_gpu_2 -> attention_layer_0_gpu_6 [color=blue style=dashed]
	qkv_layer_0_gpu_3 -> attention_layer_0_gpu_6 [color=blue style=dashed]
	qkv_layer_0_gpu_4 -> attention_layer_0_gpu_6 [color=blue style=dashed]
	qkv_layer_0_gpu_5 -> attention_layer_0_gpu_6 [color=blue style=dashed]
	qkv_layer_0_gpu_7 -> attention_layer_0_gpu_6 [color=blue style=dashed]
	qkv_layer_0_gpu_8 -> attention_layer_0_gpu_6 [color=blue style=dashed]
	qkv_layer_0_gpu_9 -> attention_layer_0_gpu_6 [color=blue style=dashed]
	qkv_layer_0_gpu_10 -> attention_layer_0_gpu_6 [color=blue style=dashed]
	qkv_layer_0_gpu_11 -> attention_layer_0_gpu_6 [color=blue style=dashed]
	ln1_layer_0 -> qkv_layer_0_gpu_7
	qkv_layer_0_gpu_7 -> attention_layer_0_gpu_7
	qkv_layer_0_gpu_0 -> attention_layer_0_gpu_7 [color=blue style=dashed]
	qkv_layer_0_gpu_1 -> attention_layer_0_gpu_7 [color=blue style=dashed]
	qkv_layer_0_gpu_2 -> attention_layer_0_gpu_7 [color=blue style=dashed]
	qkv_layer_0_gpu_3 -> attention_layer_0_gpu_7 [color=blue style=dashed]
	qkv_layer_0_gpu_4 -> attention_layer_0_gpu_7 [color=blue style=dashed]
	qkv_layer_0_gpu_5 -> attention_layer_0_gpu_7 [color=blue style=dashed]
	qkv_layer_0_gpu_6 -> attention_layer_0_gpu_7 [color=blue style=dashed]
	qkv_layer_0_gpu_8 -> attention_layer_0_gpu_7 [color=blue style=dashed]
	qkv_layer_0_gpu_9 -> attention_layer_0_gpu_7 [color=blue style=dashed]
	qkv_layer_0_gpu_10 -> attention_layer_0_gpu_7 [color=blue style=dashed]
	qkv_layer_0_gpu_11 -> attention_layer_0_gpu_7 [color=blue style=dashed]
	ln1_layer_0 -> qkv_layer_0_gpu_8
	qkv_layer_0_gpu_8 -> attention_layer_0_gpu_8
	qkv_layer_0_gpu_0 -> attention_layer_0_gpu_8 [color=blue style=dashed]
	qkv_layer_0_gpu_1 -> attention_layer_0_gpu_8 [color=blue style=dashed]
	qkv_layer_0_gpu_2 -> attention_layer_0_gpu_8 [color=blue style=dashed]
	qkv_layer_0_gpu_3 -> attention_layer_0_gpu_8 [color=blue style=dashed]
	qkv_layer_0_gpu_4 -> attention_layer_0_gpu_8 [color=blue style=dashed]
	qkv_layer_0_gpu_5 -> attention_layer_0_gpu_8 [color=blue style=dashed]
	qkv_layer_0_gpu_6 -> attention_layer_0_gpu_8 [color=blue style=dashed]
	qkv_layer_0_gpu_7 -> attention_layer_0_gpu_8 [color=blue style=dashed]
	qkv_layer_0_gpu_9 -> attention_layer_0_gpu_8 [color=blue style=dashed]
	qkv_layer_0_gpu_10 -> attention_layer_0_gpu_8 [color=blue style=dashed]
	qkv_layer_0_gpu_11 -> attention_layer_0_gpu_8 [color=blue style=dashed]
	ln1_layer_0 -> qkv_layer_0_gpu_9
	qkv_layer_0_gpu_9 -> attention_layer_0_gpu_9
	qkv_layer_0_gpu_0 -> attention_layer_0_gpu_9 [color=blue style=dashed]
	qkv_layer_0_gpu_1 -> attention_layer_0_gpu_9 [color=blue style=dashed]
	qkv_layer_0_gpu_2 -> attention_layer_0_gpu_9 [color=blue style=dashed]
	qkv_layer_0_gpu_3 -> attention_layer_0_gpu_9 [color=blue style=dashed]
	qkv_layer_0_gpu_4 -> attention_layer_0_gpu_9 [color=blue style=dashed]
	qkv_layer_0_gpu_5 -> attention_layer_0_gpu_9 [color=blue style=dashed]
	qkv_layer_0_gpu_6 -> attention_layer_0_gpu_9 [color=blue style=dashed]
	qkv_layer_0_gpu_7 -> attention_layer_0_gpu_9 [color=blue style=dashed]
	qkv_layer_0_gpu_8 -> attention_layer_0_gpu_9 [color=blue style=dashed]
	qkv_layer_0_gpu_10 -> attention_layer_0_gpu_9 [color=blue style=dashed]
	qkv_layer_0_gpu_11 -> attention_layer_0_gpu_9 [color=blue style=dashed]
	ln1_layer_0 -> qkv_layer_0_gpu_10
	qkv_layer_0_gpu_10 -> attention_layer_0_gpu_10
	qkv_layer_0_gpu_0 -> attention_layer_0_gpu_10 [color=blue style=dashed]
	qkv_layer_0_gpu_1 -> attention_layer_0_gpu_10 [color=blue style=dashed]
	qkv_layer_0_gpu_2 -> attention_layer_0_gpu_10 [color=blue style=dashed]
	qkv_layer_0_gpu_3 -> attention_layer_0_gpu_10 [color=blue style=dashed]
	qkv_layer_0_gpu_4 -> attention_layer_0_gpu_10 [color=blue style=dashed]
	qkv_layer_0_gpu_5 -> attention_layer_0_gpu_10 [color=blue style=dashed]
	qkv_layer_0_gpu_6 -> attention_layer_0_gpu_10 [color=blue style=dashed]
	qkv_layer_0_gpu_7 -> attention_layer_0_gpu_10 [color=blue style=dashed]
	qkv_layer_0_gpu_8 -> attention_layer_0_gpu_10 [color=blue style=dashed]
	qkv_layer_0_gpu_9 -> attention_layer_0_gpu_10 [color=blue style=dashed]
	qkv_layer_0_gpu_11 -> attention_layer_0_gpu_10 [color=blue style=dashed]
	ln1_layer_0 -> qkv_layer_0_gpu_11
	qkv_layer_0_gpu_11 -> attention_layer_0_gpu_11
	qkv_layer_0_gpu_0 -> attention_layer_0_gpu_11 [color=blue style=dashed]
	qkv_layer_0_gpu_1 -> attention_layer_0_gpu_11 [color=blue style=dashed]
	qkv_layer_0_gpu_2 -> attention_layer_0_gpu_11 [color=blue style=dashed]
	qkv_layer_0_gpu_3 -> attention_layer_0_gpu_11 [color=blue style=dashed]
	qkv_layer_0_gpu_4 -> attention_layer_0_gpu_11 [color=blue style=dashed]
	qkv_layer_0_gpu_5 -> attention_layer_0_gpu_11 [color=blue style=dashed]
	qkv_layer_0_gpu_6 -> attention_layer_0_gpu_11 [color=blue style=dashed]
	qkv_layer_0_gpu_7 -> attention_layer_0_gpu_11 [color=blue style=dashed]
	qkv_layer_0_gpu_8 -> attention_layer_0_gpu_11 [color=blue style=dashed]
	qkv_layer_0_gpu_9 -> attention_layer_0_gpu_11 [color=blue style=dashed]
	qkv_layer_0_gpu_10 -> attention_layer_0_gpu_11 [color=blue style=dashed]
	attention_layer_0_gpu_0 -> attention_agg_layer_0
	attention_layer_0_gpu_1 -> attention_agg_layer_0
	attention_layer_0_gpu_2 -> attention_agg_layer_0
	attention_layer_0_gpu_3 -> attention_agg_layer_0
	attention_layer_0_gpu_4 -> attention_agg_layer_0
	attention_layer_0_gpu_5 -> attention_agg_layer_0
	attention_layer_0_gpu_6 -> attention_agg_layer_0
	attention_layer_0_gpu_7 -> attention_agg_layer_0
	attention_layer_0_gpu_8 -> attention_agg_layer_0
	attention_layer_0_gpu_9 -> attention_agg_layer_0
	attention_layer_0_gpu_10 -> attention_agg_layer_0
	attention_layer_0_gpu_11 -> attention_agg_layer_0
	attention_agg_layer_0 -> out_proj_layer_0
	out_proj_layer_0 -> residual1_layer_0
	embedding -> residual1_layer_0
	residual1_layer_0 -> ln2_layer_0
	ln2_layer_0 -> gate_layer_0 [style=dashed]
	gate_layer_0 -> expert_layer_0_expert_0_gpu_12 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_0_gpu_12
	gate_layer_0 -> expert_layer_0_expert_1_gpu_12 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_1_gpu_12
	gate_layer_0 -> expert_layer_0_expert_2_gpu_12 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_2_gpu_12
	gate_layer_0 -> expert_layer_0_expert_3_gpu_12 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_3_gpu_12
	gate_layer_0 -> expert_layer_0_expert_4_gpu_13 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_4_gpu_13
	gate_layer_0 -> expert_layer_0_expert_5_gpu_13 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_5_gpu_13
	gate_layer_0 -> expert_layer_0_expert_6_gpu_13 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_6_gpu_13
	gate_layer_0 -> expert_layer_0_expert_7_gpu_13 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_7_gpu_13
	gate_layer_0 -> expert_layer_0_expert_8_gpu_14 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_8_gpu_14
	gate_layer_0 -> expert_layer_0_expert_9_gpu_14 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_9_gpu_14
	gate_layer_0 -> expert_layer_0_expert_10_gpu_14 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_10_gpu_14
	gate_layer_0 -> expert_layer_0_expert_11_gpu_14 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_11_gpu_14
	gate_layer_0 -> expert_layer_0_expert_12_gpu_15 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_12_gpu_15
	gate_layer_0 -> expert_layer_0_expert_13_gpu_15 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_13_gpu_15
	gate_layer_0 -> expert_layer_0_expert_14_gpu_15 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_14_gpu_15
	gate_layer_0 -> expert_layer_0_expert_15_gpu_15 [color=red style=dashed]
	ln2_layer_0 -> expert_layer_0_expert_15_gpu_15
	expert_layer_0_expert_0_gpu_12 -> expert_agg_layer_0
	expert_layer_0_expert_1_gpu_12 -> expert_agg_layer_0
	expert_layer_0_expert_2_gpu_12 -> expert_agg_layer_0
	expert_layer_0_expert_3_gpu_12 -> expert_agg_layer_0
	expert_layer_0_expert_4_gpu_13 -> expert_agg_layer_0
	expert_layer_0_expert_5_gpu_13 -> expert_agg_layer_0
	expert_layer_0_expert_6_gpu_13 -> expert_agg_layer_0
	expert_layer_0_expert_7_gpu_13 -> expert_agg_layer_0
	expert_layer_0_expert_8_gpu_14 -> expert_agg_layer_0
	expert_layer_0_expert_9_gpu_14 -> expert_agg_layer_0
	expert_layer_0_expert_10_gpu_14 -> expert_agg_layer_0
	expert_layer_0_expert_11_gpu_14 -> expert_agg_layer_0
	expert_layer_0_expert_12_gpu_15 -> expert_agg_layer_0
	expert_layer_0_expert_13_gpu_15 -> expert_agg_layer_0
	expert_layer_0_expert_14_gpu_15 -> expert_agg_layer_0
	expert_layer_0_expert_15_gpu_15 -> expert_agg_layer_0
	expert_agg_layer_0 -> residual2_layer_0
	residual1_layer_0 -> residual2_layer_0
	residual2_layer_0 -> ln1_layer_1
	ln1_layer_1 -> qkv_layer_1_gpu_0
	qkv_layer_1_gpu_0 -> attention_layer_1_gpu_0
	qkv_layer_1_gpu_1 -> attention_layer_1_gpu_0 [color=blue style=dashed]
	qkv_layer_1_gpu_2 -> attention_layer_1_gpu_0 [color=blue style=dashed]
	qkv_layer_1_gpu_3 -> attention_layer_1_gpu_0 [color=blue style=dashed]
	qkv_layer_1_gpu_4 -> attention_layer_1_gpu_0 [color=blue style=dashed]
	qkv_layer_1_gpu_5 -> attention_layer_1_gpu_0 [color=blue style=dashed]
	qkv_layer_1_gpu_6 -> attention_layer_1_gpu_0 [color=blue style=dashed]
	qkv_layer_1_gpu_7 -> attention_layer_1_gpu_0 [color=blue style=dashed]
	qkv_layer_1_gpu_8 -> attention_layer_1_gpu_0 [color=blue style=dashed]
	qkv_layer_1_gpu_9 -> attention_layer_1_gpu_0 [color=blue style=dashed]
	qkv_layer_1_gpu_10 -> attention_layer_1_gpu_0 [color=blue style=dashed]
	qkv_layer_1_gpu_11 -> attention_layer_1_gpu_0 [color=blue style=dashed]
	ln1_layer_1 -> qkv_layer_1_gpu_1
	qkv_layer_1_gpu_1 -> attention_layer_1_gpu_1
	qkv_layer_1_gpu_0 -> attention_layer_1_gpu_1 [color=blue style=dashed]
	qkv_layer_1_gpu_2 -> attention_layer_1_gpu_1 [color=blue style=dashed]
	qkv_layer_1_gpu_3 -> attention_layer_1_gpu_1 [color=blue style=dashed]
	qkv_layer_1_gpu_4 -> attention_layer_1_gpu_1 [color=blue style=dashed]
	qkv_layer_1_gpu_5 -> attention_layer_1_gpu_1 [color=blue style=dashed]
	qkv_layer_1_gpu_6 -> attention_layer_1_gpu_1 [color=blue style=dashed]
	qkv_layer_1_gpu_7 -> attention_layer_1_gpu_1 [color=blue style=dashed]
	qkv_layer_1_gpu_8 -> attention_layer_1_gpu_1 [color=blue style=dashed]
	qkv_layer_1_gpu_9 -> attention_layer_1_gpu_1 [color=blue style=dashed]
	qkv_layer_1_gpu_10 -> attention_layer_1_gpu_1 [color=blue style=dashed]
	qkv_layer_1_gpu_11 -> attention_layer_1_gpu_1 [color=blue style=dashed]
	ln1_layer_1 -> qkv_layer_1_gpu_2
	qkv_layer_1_gpu_2 -> attention_layer_1_gpu_2
	qkv_layer_1_gpu_0 -> attention_layer_1_gpu_2 [color=blue style=dashed]
	qkv_layer_1_gpu_1 -> attention_layer_1_gpu_2 [color=blue style=dashed]
	qkv_layer_1_gpu_3 -> attention_layer_1_gpu_2 [color=blue style=dashed]
	qkv_layer_1_gpu_4 -> attention_layer_1_gpu_2 [color=blue style=dashed]
	qkv_layer_1_gpu_5 -> attention_layer_1_gpu_2 [color=blue style=dashed]
	qkv_layer_1_gpu_6 -> attention_layer_1_gpu_2 [color=blue style=dashed]
	qkv_layer_1_gpu_7 -> attention_layer_1_gpu_2 [color=blue style=dashed]
	qkv_layer_1_gpu_8 -> attention_layer_1_gpu_2 [color=blue style=dashed]
	qkv_layer_1_gpu_9 -> attention_layer_1_gpu_2 [color=blue style=dashed]
	qkv_layer_1_gpu_10 -> attention_layer_1_gpu_2 [color=blue style=dashed]
	qkv_layer_1_gpu_11 -> attention_layer_1_gpu_2 [color=blue style=dashed]
	ln1_layer_1 -> qkv_layer_1_gpu_3
	qkv_layer_1_gpu_3 -> attention_layer_1_gpu_3
	qkv_layer_1_gpu_0 -> attention_layer_1_gpu_3 [color=blue style=dashed]
	qkv_layer_1_gpu_1 -> attention_layer_1_gpu_3 [color=blue style=dashed]
	qkv_layer_1_gpu_2 -> attention_layer_1_gpu_3 [color=blue style=dashed]
	qkv_layer_1_gpu_4 -> attention_layer_1_gpu_3 [color=blue style=dashed]
	qkv_layer_1_gpu_5 -> attention_layer_1_gpu_3 [color=blue style=dashed]
	qkv_layer_1_gpu_6 -> attention_layer_1_gpu_3 [color=blue style=dashed]
	qkv_layer_1_gpu_7 -> attention_layer_1_gpu_3 [color=blue style=dashed]
	qkv_layer_1_gpu_8 -> attention_layer_1_gpu_3 [color=blue style=dashed]
	qkv_layer_1_gpu_9 -> attention_layer_1_gpu_3 [color=blue style=dashed]
	qkv_layer_1_gpu_10 -> attention_layer_1_gpu_3 [color=blue style=dashed]
	qkv_layer_1_gpu_11 -> attention_layer_1_gpu_3 [color=blue style=dashed]
	ln1_layer_1 -> qkv_layer_1_gpu_4
	qkv_layer_1_gpu_4 -> attention_layer_1_gpu_4
	qkv_layer_1_gpu_0 -> attention_layer_1_gpu_4 [color=blue style=dashed]
	qkv_layer_1_gpu_1 -> attention_layer_1_gpu_4 [color=blue style=dashed]
	qkv_layer_1_gpu_2 -> attention_layer_1_gpu_4 [color=blue style=dashed]
	qkv_layer_1_gpu_3 -> attention_layer_1_gpu_4 [color=blue style=dashed]
	qkv_layer_1_gpu_5 -> attention_layer_1_gpu_4 [color=blue style=dashed]
	qkv_layer_1_gpu_6 -> attention_layer_1_gpu_4 [color=blue style=dashed]
	qkv_layer_1_gpu_7 -> attention_layer_1_gpu_4 [color=blue style=dashed]
	qkv_layer_1_gpu_8 -> attention_layer_1_gpu_4 [color=blue style=dashed]
	qkv_layer_1_gpu_9 -> attention_layer_1_gpu_4 [color=blue style=dashed]
	qkv_layer_1_gpu_10 -> attention_layer_1_gpu_4 [color=blue style=dashed]
	qkv_layer_1_gpu_11 -> attention_layer_1_gpu_4 [color=blue style=dashed]
	ln1_layer_1 -> qkv_layer_1_gpu_5
	qkv_layer_1_gpu_5 -> attention_layer_1_gpu_5
	qkv_layer_1_gpu_0 -> attention_layer_1_gpu_5 [color=blue style=dashed]
	qkv_layer_1_gpu_1 -> attention_layer_1_gpu_5 [color=blue style=dashed]
	qkv_layer_1_gpu_2 -> attention_layer_1_gpu_5 [color=blue style=dashed]
	qkv_layer_1_gpu_3 -> attention_layer_1_gpu_5 [color=blue style=dashed]
	qkv_layer_1_gpu_4 -> attention_layer_1_gpu_5 [color=blue style=dashed]
	qkv_layer_1_gpu_6 -> attention_layer_1_gpu_5 [color=blue style=dashed]
	qkv_layer_1_gpu_7 -> attention_layer_1_gpu_5 [color=blue style=dashed]
	qkv_layer_1_gpu_8 -> attention_layer_1_gpu_5 [color=blue style=dashed]
	qkv_layer_1_gpu_9 -> attention_layer_1_gpu_5 [color=blue style=dashed]
	qkv_layer_1_gpu_10 -> attention_layer_1_gpu_5 [color=blue style=dashed]
	qkv_layer_1_gpu_11 -> attention_layer_1_gpu_5 [color=blue style=dashed]
	ln1_layer_1 -> qkv_layer_1_gpu_6
	qkv_layer_1_gpu_6 -> attention_layer_1_gpu_6
	qkv_layer_1_gpu_0 -> attention_layer_1_gpu_6 [color=blue style=dashed]
	qkv_layer_1_gpu_1 -> attention_layer_1_gpu_6 [color=blue style=dashed]
	qkv_layer_1_gpu_2 -> attention_layer_1_gpu_6 [color=blue style=dashed]
	qkv_layer_1_gpu_3 -> attention_layer_1_gpu_6 [color=blue style=dashed]
	qkv_layer_1_gpu_4 -> attention_layer_1_gpu_6 [color=blue style=dashed]
	qkv_layer_1_gpu_5 -> attention_layer_1_gpu_6 [color=blue style=dashed]
	qkv_layer_1_gpu_7 -> attention_layer_1_gpu_6 [color=blue style=dashed]
	qkv_layer_1_gpu_8 -> attention_layer_1_gpu_6 [color=blue style=dashed]
	qkv_layer_1_gpu_9 -> attention_layer_1_gpu_6 [color=blue style=dashed]
	qkv_layer_1_gpu_10 -> attention_layer_1_gpu_6 [color=blue style=dashed]
	qkv_layer_1_gpu_11 -> attention_layer_1_gpu_6 [color=blue style=dashed]
	ln1_layer_1 -> qkv_layer_1_gpu_7
	qkv_layer_1_gpu_7 -> attention_layer_1_gpu_7
	qkv_layer_1_gpu_0 -> attention_layer_1_gpu_7 [color=blue style=dashed]
	qkv_layer_1_gpu_1 -> attention_layer_1_gpu_7 [color=blue style=dashed]
	qkv_layer_1_gpu_2 -> attention_layer_1_gpu_7 [color=blue style=dashed]
	qkv_layer_1_gpu_3 -> attention_layer_1_gpu_7 [color=blue style=dashed]
	qkv_layer_1_gpu_4 -> attention_layer_1_gpu_7 [color=blue style=dashed]
	qkv_layer_1_gpu_5 -> attention_layer_1_gpu_7 [color=blue style=dashed]
	qkv_layer_1_gpu_6 -> attention_layer_1_gpu_7 [color=blue style=dashed]
	qkv_layer_1_gpu_8 -> attention_layer_1_gpu_7 [color=blue style=dashed]
	qkv_layer_1_gpu_9 -> attention_layer_1_gpu_7 [color=blue style=dashed]
	qkv_layer_1_gpu_10 -> attention_layer_1_gpu_7 [color=blue style=dashed]
	qkv_layer_1_gpu_11 -> attention_layer_1_gpu_7 [color=blue style=dashed]
	ln1_layer_1 -> qkv_layer_1_gpu_8
	qkv_layer_1_gpu_8 -> attention_layer_1_gpu_8
	qkv_layer_1_gpu_0 -> attention_layer_1_gpu_8 [color=blue style=dashed]
	qkv_layer_1_gpu_1 -> attention_layer_1_gpu_8 [color=blue style=dashed]
	qkv_layer_1_gpu_2 -> attention_layer_1_gpu_8 [color=blue style=dashed]
	qkv_layer_1_gpu_3 -> attention_layer_1_gpu_8 [color=blue style=dashed]
	qkv_layer_1_gpu_4 -> attention_layer_1_gpu_8 [color=blue style=dashed]
	qkv_layer_1_gpu_5 -> attention_layer_1_gpu_8 [color=blue style=dashed]
	qkv_layer_1_gpu_6 -> attention_layer_1_gpu_8 [color=blue style=dashed]
	qkv_layer_1_gpu_7 -> attention_layer_1_gpu_8 [color=blue style=dashed]
	qkv_layer_1_gpu_9 -> attention_layer_1_gpu_8 [color=blue style=dashed]
	qkv_layer_1_gpu_10 -> attention_layer_1_gpu_8 [color=blue style=dashed]
	qkv_layer_1_gpu_11 -> attention_layer_1_gpu_8 [color=blue style=dashed]
	ln1_layer_1 -> qkv_layer_1_gpu_9
	qkv_layer_1_gpu_9 -> attention_layer_1_gpu_9
	qkv_layer_1_gpu_0 -> attention_layer_1_gpu_9 [color=blue style=dashed]
	qkv_layer_1_gpu_1 -> attention_layer_1_gpu_9 [color=blue style=dashed]
	qkv_layer_1_gpu_2 -> attention_layer_1_gpu_9 [color=blue style=dashed]
	qkv_layer_1_gpu_3 -> attention_layer_1_gpu_9 [color=blue style=dashed]
	qkv_layer_1_gpu_4 -> attention_layer_1_gpu_9 [color=blue style=dashed]
	qkv_layer_1_gpu_5 -> attention_layer_1_gpu_9 [color=blue style=dashed]
	qkv_layer_1_gpu_6 -> attention_layer_1_gpu_9 [color=blue style=dashed]
	qkv_layer_1_gpu_7 -> attention_layer_1_gpu_9 [color=blue style=dashed]
	qkv_layer_1_gpu_8 -> attention_layer_1_gpu_9 [color=blue style=dashed]
	qkv_layer_1_gpu_10 -> attention_layer_1_gpu_9 [color=blue style=dashed]
	qkv_layer_1_gpu_11 -> attention_layer_1_gpu_9 [color=blue style=dashed]
	ln1_layer_1 -> qkv_layer_1_gpu_10
	qkv_layer_1_gpu_10 -> attention_layer_1_gpu_10
	qkv_layer_1_gpu_0 -> attention_layer_1_gpu_10 [color=blue style=dashed]
	qkv_layer_1_gpu_1 -> attention_layer_1_gpu_10 [color=blue style=dashed]
	qkv_layer_1_gpu_2 -> attention_layer_1_gpu_10 [color=blue style=dashed]
	qkv_layer_1_gpu_3 -> attention_layer_1_gpu_10 [color=blue style=dashed]
	qkv_layer_1_gpu_4 -> attention_layer_1_gpu_10 [color=blue style=dashed]
	qkv_layer_1_gpu_5 -> attention_layer_1_gpu_10 [color=blue style=dashed]
	qkv_layer_1_gpu_6 -> attention_layer_1_gpu_10 [color=blue style=dashed]
	qkv_layer_1_gpu_7 -> attention_layer_1_gpu_10 [color=blue style=dashed]
	qkv_layer_1_gpu_8 -> attention_layer_1_gpu_10 [color=blue style=dashed]
	qkv_layer_1_gpu_9 -> attention_layer_1_gpu_10 [color=blue style=dashed]
	qkv_layer_1_gpu_11 -> attention_layer_1_gpu_10 [color=blue style=dashed]
	ln1_layer_1 -> qkv_layer_1_gpu_11
	qkv_layer_1_gpu_11 -> attention_layer_1_gpu_11
	qkv_layer_1_gpu_0 -> attention_layer_1_gpu_11 [color=blue style=dashed]
	qkv_layer_1_gpu_1 -> attention_layer_1_gpu_11 [color=blue style=dashed]
	qkv_layer_1_gpu_2 -> attention_layer_1_gpu_11 [color=blue style=dashed]
	qkv_layer_1_gpu_3 -> attention_layer_1_gpu_11 [color=blue style=dashed]
	qkv_layer_1_gpu_4 -> attention_layer_1_gpu_11 [color=blue style=dashed]
	qkv_layer_1_gpu_5 -> attention_layer_1_gpu_11 [color=blue style=dashed]
	qkv_layer_1_gpu_6 -> attention_layer_1_gpu_11 [color=blue style=dashed]
	qkv_layer_1_gpu_7 -> attention_layer_1_gpu_11 [color=blue style=dashed]
	qkv_layer_1_gpu_8 -> attention_layer_1_gpu_11 [color=blue style=dashed]
	qkv_layer_1_gpu_9 -> attention_layer_1_gpu_11 [color=blue style=dashed]
	qkv_layer_1_gpu_10 -> attention_layer_1_gpu_11 [color=blue style=dashed]
	attention_layer_1_gpu_0 -> attention_agg_layer_1
	attention_layer_1_gpu_1 -> attention_agg_layer_1
	attention_layer_1_gpu_2 -> attention_agg_layer_1
	attention_layer_1_gpu_3 -> attention_agg_layer_1
	attention_layer_1_gpu_4 -> attention_agg_layer_1
	attention_layer_1_gpu_5 -> attention_agg_layer_1
	attention_layer_1_gpu_6 -> attention_agg_layer_1
	attention_layer_1_gpu_7 -> attention_agg_layer_1
	attention_layer_1_gpu_8 -> attention_agg_layer_1
	attention_layer_1_gpu_9 -> attention_agg_layer_1
	attention_layer_1_gpu_10 -> attention_agg_layer_1
	attention_layer_1_gpu_11 -> attention_agg_layer_1
	attention_agg_layer_1 -> out_proj_layer_1
	out_proj_layer_1 -> residual1_layer_1
	residual2_layer_0 -> residual1_layer_1
	residual1_layer_1 -> ln2_layer_1
	ln2_layer_1 -> gate_layer_1 [style=dashed]
	gate_layer_1 -> expert_layer_1_expert_0_gpu_12 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_0_gpu_12
	gate_layer_1 -> expert_layer_1_expert_1_gpu_12 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_1_gpu_12
	gate_layer_1 -> expert_layer_1_expert_2_gpu_12 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_2_gpu_12
	gate_layer_1 -> expert_layer_1_expert_3_gpu_12 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_3_gpu_12
	gate_layer_1 -> expert_layer_1_expert_4_gpu_13 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_4_gpu_13
	gate_layer_1 -> expert_layer_1_expert_5_gpu_13 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_5_gpu_13
	gate_layer_1 -> expert_layer_1_expert_6_gpu_13 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_6_gpu_13
	gate_layer_1 -> expert_layer_1_expert_7_gpu_13 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_7_gpu_13
	gate_layer_1 -> expert_layer_1_expert_8_gpu_14 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_8_gpu_14
	gate_layer_1 -> expert_layer_1_expert_9_gpu_14 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_9_gpu_14
	gate_layer_1 -> expert_layer_1_expert_10_gpu_14 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_10_gpu_14
	gate_layer_1 -> expert_layer_1_expert_11_gpu_14 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_11_gpu_14
	gate_layer_1 -> expert_layer_1_expert_12_gpu_15 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_12_gpu_15
	gate_layer_1 -> expert_layer_1_expert_13_gpu_15 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_13_gpu_15
	gate_layer_1 -> expert_layer_1_expert_14_gpu_15 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_14_gpu_15
	gate_layer_1 -> expert_layer_1_expert_15_gpu_15 [color=red style=dashed]
	ln2_layer_1 -> expert_layer_1_expert_15_gpu_15
	expert_layer_1_expert_0_gpu_12 -> expert_agg_layer_1
	expert_layer_1_expert_1_gpu_12 -> expert_agg_layer_1
	expert_layer_1_expert_2_gpu_12 -> expert_agg_layer_1
	expert_layer_1_expert_3_gpu_12 -> expert_agg_layer_1
	expert_layer_1_expert_4_gpu_13 -> expert_agg_layer_1
	expert_layer_1_expert_5_gpu_13 -> expert_agg_layer_1
	expert_layer_1_expert_6_gpu_13 -> expert_agg_layer_1
	expert_layer_1_expert_7_gpu_13 -> expert_agg_layer_1
	expert_layer_1_expert_8_gpu_14 -> expert_agg_layer_1
	expert_layer_1_expert_9_gpu_14 -> expert_agg_layer_1
	expert_layer_1_expert_10_gpu_14 -> expert_agg_layer_1
	expert_layer_1_expert_11_gpu_14 -> expert_agg_layer_1
	expert_layer_1_expert_12_gpu_15 -> expert_agg_layer_1
	expert_layer_1_expert_13_gpu_15 -> expert_agg_layer_1
	expert_layer_1_expert_14_gpu_15 -> expert_agg_layer_1
	expert_layer_1_expert_15_gpu_15 -> expert_agg_layer_1
	expert_agg_layer_1 -> residual2_layer_1
	residual1_layer_1 -> residual2_layer_1
	residual2_layer_1 -> ln1_layer_2
	ln1_layer_2 -> qkv_layer_2_gpu_0
	qkv_layer_2_gpu_0 -> attention_layer_2_gpu_0
	qkv_layer_2_gpu_1 -> attention_layer_2_gpu_0 [color=blue style=dashed]
	qkv_layer_2_gpu_2 -> attention_layer_2_gpu_0 [color=blue style=dashed]
	qkv_layer_2_gpu_3 -> attention_layer_2_gpu_0 [color=blue style=dashed]
	qkv_layer_2_gpu_4 -> attention_layer_2_gpu_0 [color=blue style=dashed]
	qkv_layer_2_gpu_5 -> attention_layer_2_gpu_0 [color=blue style=dashed]
	qkv_layer_2_gpu_6 -> attention_layer_2_gpu_0 [color=blue style=dashed]
	qkv_layer_2_gpu_7 -> attention_layer_2_gpu_0 [color=blue style=dashed]
	qkv_layer_2_gpu_8 -> attention_layer_2_gpu_0 [color=blue style=dashed]
	qkv_layer_2_gpu_9 -> attention_layer_2_gpu_0 [color=blue style=dashed]
	qkv_layer_2_gpu_10 -> attention_layer_2_gpu_0 [color=blue style=dashed]
	qkv_layer_2_gpu_11 -> attention_layer_2_gpu_0 [color=blue style=dashed]
	ln1_layer_2 -> qkv_layer_2_gpu_1
	qkv_layer_2_gpu_1 -> attention_layer_2_gpu_1
	qkv_layer_2_gpu_0 -> attention_layer_2_gpu_1 [color=blue style=dashed]
	qkv_layer_2_gpu_2 -> attention_layer_2_gpu_1 [color=blue style=dashed]
	qkv_layer_2_gpu_3 -> attention_layer_2_gpu_1 [color=blue style=dashed]
	qkv_layer_2_gpu_4 -> attention_layer_2_gpu_1 [color=blue style=dashed]
	qkv_layer_2_gpu_5 -> attention_layer_2_gpu_1 [color=blue style=dashed]
	qkv_layer_2_gpu_6 -> attention_layer_2_gpu_1 [color=blue style=dashed]
	qkv_layer_2_gpu_7 -> attention_layer_2_gpu_1 [color=blue style=dashed]
	qkv_layer_2_gpu_8 -> attention_layer_2_gpu_1 [color=blue style=dashed]
	qkv_layer_2_gpu_9 -> attention_layer_2_gpu_1 [color=blue style=dashed]
	qkv_layer_2_gpu_10 -> attention_layer_2_gpu_1 [color=blue style=dashed]
	qkv_layer_2_gpu_11 -> attention_layer_2_gpu_1 [color=blue style=dashed]
	ln1_layer_2 -> qkv_layer_2_gpu_2
	qkv_layer_2_gpu_2 -> attention_layer_2_gpu_2
	qkv_layer_2_gpu_0 -> attention_layer_2_gpu_2 [color=blue style=dashed]
	qkv_layer_2_gpu_1 -> attention_layer_2_gpu_2 [color=blue style=dashed]
	qkv_layer_2_gpu_3 -> attention_layer_2_gpu_2 [color=blue style=dashed]
	qkv_layer_2_gpu_4 -> attention_layer_2_gpu_2 [color=blue style=dashed]
	qkv_layer_2_gpu_5 -> attention_layer_2_gpu_2 [color=blue style=dashed]
	qkv_layer_2_gpu_6 -> attention_layer_2_gpu_2 [color=blue style=dashed]
	qkv_layer_2_gpu_7 -> attention_layer_2_gpu_2 [color=blue style=dashed]
	qkv_layer_2_gpu_8 -> attention_layer_2_gpu_2 [color=blue style=dashed]
	qkv_layer_2_gpu_9 -> attention_layer_2_gpu_2 [color=blue style=dashed]
	qkv_layer_2_gpu_10 -> attention_layer_2_gpu_2 [color=blue style=dashed]
	qkv_layer_2_gpu_11 -> attention_layer_2_gpu_2 [color=blue style=dashed]
	ln1_layer_2 -> qkv_layer_2_gpu_3
	qkv_layer_2_gpu_3 -> attention_layer_2_gpu_3
	qkv_layer_2_gpu_0 -> attention_layer_2_gpu_3 [color=blue style=dashed]
	qkv_layer_2_gpu_1 -> attention_layer_2_gpu_3 [color=blue style=dashed]
	qkv_layer_2_gpu_2 -> attention_layer_2_gpu_3 [color=blue style=dashed]
	qkv_layer_2_gpu_4 -> attention_layer_2_gpu_3 [color=blue style=dashed]
	qkv_layer_2_gpu_5 -> attention_layer_2_gpu_3 [color=blue style=dashed]
	qkv_layer_2_gpu_6 -> attention_layer_2_gpu_3 [color=blue style=dashed]
	qkv_layer_2_gpu_7 -> attention_layer_2_gpu_3 [color=blue style=dashed]
	qkv_layer_2_gpu_8 -> attention_layer_2_gpu_3 [color=blue style=dashed]
	qkv_layer_2_gpu_9 -> attention_layer_2_gpu_3 [color=blue style=dashed]
	qkv_layer_2_gpu_10 -> attention_layer_2_gpu_3 [color=blue style=dashed]
	qkv_layer_2_gpu_11 -> attention_layer_2_gpu_3 [color=blue style=dashed]
	ln1_layer_2 -> qkv_layer_2_gpu_4
	qkv_layer_2_gpu_4 -> attention_layer_2_gpu_4
	qkv_layer_2_gpu_0 -> attention_layer_2_gpu_4 [color=blue style=dashed]
	qkv_layer_2_gpu_1 -> attention_layer_2_gpu_4 [color=blue style=dashed]
	qkv_layer_2_gpu_2 -> attention_layer_2_gpu_4 [color=blue style=dashed]
	qkv_layer_2_gpu_3 -> attention_layer_2_gpu_4 [color=blue style=dashed]
	qkv_layer_2_gpu_5 -> attention_layer_2_gpu_4 [color=blue style=dashed]
	qkv_layer_2_gpu_6 -> attention_layer_2_gpu_4 [color=blue style=dashed]
	qkv_layer_2_gpu_7 -> attention_layer_2_gpu_4 [color=blue style=dashed]
	qkv_layer_2_gpu_8 -> attention_layer_2_gpu_4 [color=blue style=dashed]
	qkv_layer_2_gpu_9 -> attention_layer_2_gpu_4 [color=blue style=dashed]
	qkv_layer_2_gpu_10 -> attention_layer_2_gpu_4 [color=blue style=dashed]
	qkv_layer_2_gpu_11 -> attention_layer_2_gpu_4 [color=blue style=dashed]
	ln1_layer_2 -> qkv_layer_2_gpu_5
	qkv_layer_2_gpu_5 -> attention_layer_2_gpu_5
	qkv_layer_2_gpu_0 -> attention_layer_2_gpu_5 [color=blue style=dashed]
	qkv_layer_2_gpu_1 -> attention_layer_2_gpu_5 [color=blue style=dashed]
	qkv_layer_2_gpu_2 -> attention_layer_2_gpu_5 [color=blue style=dashed]
	qkv_layer_2_gpu_3 -> attention_layer_2_gpu_5 [color=blue style=dashed]
	qkv_layer_2_gpu_4 -> attention_layer_2_gpu_5 [color=blue style=dashed]
	qkv_layer_2_gpu_6 -> attention_layer_2_gpu_5 [color=blue style=dashed]
	qkv_layer_2_gpu_7 -> attention_layer_2_gpu_5 [color=blue style=dashed]
	qkv_layer_2_gpu_8 -> attention_layer_2_gpu_5 [color=blue style=dashed]
	qkv_layer_2_gpu_9 -> attention_layer_2_gpu_5 [color=blue style=dashed]
	qkv_layer_2_gpu_10 -> attention_layer_2_gpu_5 [color=blue style=dashed]
	qkv_layer_2_gpu_11 -> attention_layer_2_gpu_5 [color=blue style=dashed]
	ln1_layer_2 -> qkv_layer_2_gpu_6
	qkv_layer_2_gpu_6 -> attention_layer_2_gpu_6
	qkv_layer_2_gpu_0 -> attention_layer_2_gpu_6 [color=blue style=dashed]
	qkv_layer_2_gpu_1 -> attention_layer_2_gpu_6 [color=blue style=dashed]
	qkv_layer_2_gpu_2 -> attention_layer_2_gpu_6 [color=blue style=dashed]
	qkv_layer_2_gpu_3 -> attention_layer_2_gpu_6 [color=blue style=dashed]
	qkv_layer_2_gpu_4 -> attention_layer_2_gpu_6 [color=blue style=dashed]
	qkv_layer_2_gpu_5 -> attention_layer_2_gpu_6 [color=blue style=dashed]
	qkv_layer_2_gpu_7 -> attention_layer_2_gpu_6 [color=blue style=dashed]
	qkv_layer_2_gpu_8 -> attention_layer_2_gpu_6 [color=blue style=dashed]
	qkv_layer_2_gpu_9 -> attention_layer_2_gpu_6 [color=blue style=dashed]
	qkv_layer_2_gpu_10 -> attention_layer_2_gpu_6 [color=blue style=dashed]
	qkv_layer_2_gpu_11 -> attention_layer_2_gpu_6 [color=blue style=dashed]
	ln1_layer_2 -> qkv_layer_2_gpu_7
	qkv_layer_2_gpu_7 -> attention_layer_2_gpu_7
	qkv_layer_2_gpu_0 -> attention_layer_2_gpu_7 [color=blue style=dashed]
	qkv_layer_2_gpu_1 -> attention_layer_2_gpu_7 [color=blue style=dashed]
	qkv_layer_2_gpu_2 -> attention_layer_2_gpu_7 [color=blue style=dashed]
	qkv_layer_2_gpu_3 -> attention_layer_2_gpu_7 [color=blue style=dashed]
	qkv_layer_2_gpu_4 -> attention_layer_2_gpu_7 [color=blue style=dashed]
	qkv_layer_2_gpu_5 -> attention_layer_2_gpu_7 [color=blue style=dashed]
	qkv_layer_2_gpu_6 -> attention_layer_2_gpu_7 [color=blue style=dashed]
	qkv_layer_2_gpu_8 -> attention_layer_2_gpu_7 [color=blue style=dashed]
	qkv_layer_2_gpu_9 -> attention_layer_2_gpu_7 [color=blue style=dashed]
	qkv_layer_2_gpu_10 -> attention_layer_2_gpu_7 [color=blue style=dashed]
	qkv_layer_2_gpu_11 -> attention_layer_2_gpu_7 [color=blue style=dashed]
	ln1_layer_2 -> qkv_layer_2_gpu_8
	qkv_layer_2_gpu_8 -> attention_layer_2_gpu_8
	qkv_layer_2_gpu_0 -> attention_layer_2_gpu_8 [color=blue style=dashed]
	qkv_layer_2_gpu_1 -> attention_layer_2_gpu_8 [color=blue style=dashed]
	qkv_layer_2_gpu_2 -> attention_layer_2_gpu_8 [color=blue style=dashed]
	qkv_layer_2_gpu_3 -> attention_layer_2_gpu_8 [color=blue style=dashed]
	qkv_layer_2_gpu_4 -> attention_layer_2_gpu_8 [color=blue style=dashed]
	qkv_layer_2_gpu_5 -> attention_layer_2_gpu_8 [color=blue style=dashed]
	qkv_layer_2_gpu_6 -> attention_layer_2_gpu_8 [color=blue style=dashed]
	qkv_layer_2_gpu_7 -> attention_layer_2_gpu_8 [color=blue style=dashed]
	qkv_layer_2_gpu_9 -> attention_layer_2_gpu_8 [color=blue style=dashed]
	qkv_layer_2_gpu_10 -> attention_layer_2_gpu_8 [color=blue style=dashed]
	qkv_layer_2_gpu_11 -> attention_layer_2_gpu_8 [color=blue style=dashed]
	ln1_layer_2 -> qkv_layer_2_gpu_9
	qkv_layer_2_gpu_9 -> attention_layer_2_gpu_9
	qkv_layer_2_gpu_0 -> attention_layer_2_gpu_9 [color=blue style=dashed]
	qkv_layer_2_gpu_1 -> attention_layer_2_gpu_9 [color=blue style=dashed]
	qkv_layer_2_gpu_2 -> attention_layer_2_gpu_9 [color=blue style=dashed]
	qkv_layer_2_gpu_3 -> attention_layer_2_gpu_9 [color=blue style=dashed]
	qkv_layer_2_gpu_4 -> attention_layer_2_gpu_9 [color=blue style=dashed]
	qkv_layer_2_gpu_5 -> attention_layer_2_gpu_9 [color=blue style=dashed]
	qkv_layer_2_gpu_6 -> attention_layer_2_gpu_9 [color=blue style=dashed]
	qkv_layer_2_gpu_7 -> attention_layer_2_gpu_9 [color=blue style=dashed]
	qkv_layer_2_gpu_8 -> attention_layer_2_gpu_9 [color=blue style=dashed]
	qkv_layer_2_gpu_10 -> attention_layer_2_gpu_9 [color=blue style=dashed]
	qkv_layer_2_gpu_11 -> attention_layer_2_gpu_9 [color=blue style=dashed]
	ln1_layer_2 -> qkv_layer_2_gpu_10
	qkv_layer_2_gpu_10 -> attention_layer_2_gpu_10
	qkv_layer_2_gpu_0 -> attention_layer_2_gpu_10 [color=blue style=dashed]
	qkv_layer_2_gpu_1 -> attention_layer_2_gpu_10 [color=blue style=dashed]
	qkv_layer_2_gpu_2 -> attention_layer_2_gpu_10 [color=blue style=dashed]
	qkv_layer_2_gpu_3 -> attention_layer_2_gpu_10 [color=blue style=dashed]
	qkv_layer_2_gpu_4 -> attention_layer_2_gpu_10 [color=blue style=dashed]
	qkv_layer_2_gpu_5 -> attention_layer_2_gpu_10 [color=blue style=dashed]
	qkv_layer_2_gpu_6 -> attention_layer_2_gpu_10 [color=blue style=dashed]
	qkv_layer_2_gpu_7 -> attention_layer_2_gpu_10 [color=blue style=dashed]
	qkv_layer_2_gpu_8 -> attention_layer_2_gpu_10 [color=blue style=dashed]
	qkv_layer_2_gpu_9 -> attention_layer_2_gpu_10 [color=blue style=dashed]
	qkv_layer_2_gpu_11 -> attention_layer_2_gpu_10 [color=blue style=dashed]
	ln1_layer_2 -> qkv_layer_2_gpu_11
	qkv_layer_2_gpu_11 -> attention_layer_2_gpu_11
	qkv_layer_2_gpu_0 -> attention_layer_2_gpu_11 [color=blue style=dashed]
	qkv_layer_2_gpu_1 -> attention_layer_2_gpu_11 [color=blue style=dashed]
	qkv_layer_2_gpu_2 -> attention_layer_2_gpu_11 [color=blue style=dashed]
	qkv_layer_2_gpu_3 -> attention_layer_2_gpu_11 [color=blue style=dashed]
	qkv_layer_2_gpu_4 -> attention_layer_2_gpu_11 [color=blue style=dashed]
	qkv_layer_2_gpu_5 -> attention_layer_2_gpu_11 [color=blue style=dashed]
	qkv_layer_2_gpu_6 -> attention_layer_2_gpu_11 [color=blue style=dashed]
	qkv_layer_2_gpu_7 -> attention_layer_2_gpu_11 [color=blue style=dashed]
	qkv_layer_2_gpu_8 -> attention_layer_2_gpu_11 [color=blue style=dashed]
	qkv_layer_2_gpu_9 -> attention_layer_2_gpu_11 [color=blue style=dashed]
	qkv_layer_2_gpu_10 -> attention_layer_2_gpu_11 [color=blue style=dashed]
	attention_layer_2_gpu_0 -> attention_agg_layer_2
	attention_layer_2_gpu_1 -> attention_agg_layer_2
	attention_layer_2_gpu_2 -> attention_agg_layer_2
	attention_layer_2_gpu_3 -> attention_agg_layer_2
	attention_layer_2_gpu_4 -> attention_agg_layer_2
	attention_layer_2_gpu_5 -> attention_agg_layer_2
	attention_layer_2_gpu_6 -> attention_agg_layer_2
	attention_layer_2_gpu_7 -> attention_agg_layer_2
	attention_layer_2_gpu_8 -> attention_agg_layer_2
	attention_layer_2_gpu_9 -> attention_agg_layer_2
	attention_layer_2_gpu_10 -> attention_agg_layer_2
	attention_layer_2_gpu_11 -> attention_agg_layer_2
	attention_agg_layer_2 -> out_proj_layer_2
	out_proj_layer_2 -> residual1_layer_2
	residual2_layer_1 -> residual1_layer_2
	residual1_layer_2 -> ln2_layer_2
	ln2_layer_2 -> gate_layer_2 [style=dashed]
	gate_layer_2 -> expert_layer_2_expert_0_gpu_12 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_0_gpu_12
	gate_layer_2 -> expert_layer_2_expert_1_gpu_12 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_1_gpu_12
	gate_layer_2 -> expert_layer_2_expert_2_gpu_12 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_2_gpu_12
	gate_layer_2 -> expert_layer_2_expert_3_gpu_12 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_3_gpu_12
	gate_layer_2 -> expert_layer_2_expert_4_gpu_13 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_4_gpu_13
	gate_layer_2 -> expert_layer_2_expert_5_gpu_13 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_5_gpu_13
	gate_layer_2 -> expert_layer_2_expert_6_gpu_13 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_6_gpu_13
	gate_layer_2 -> expert_layer_2_expert_7_gpu_13 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_7_gpu_13
	gate_layer_2 -> expert_layer_2_expert_8_gpu_14 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_8_gpu_14
	gate_layer_2 -> expert_layer_2_expert_9_gpu_14 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_9_gpu_14
	gate_layer_2 -> expert_layer_2_expert_10_gpu_14 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_10_gpu_14
	gate_layer_2 -> expert_layer_2_expert_11_gpu_14 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_11_gpu_14
	gate_layer_2 -> expert_layer_2_expert_12_gpu_15 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_12_gpu_15
	gate_layer_2 -> expert_layer_2_expert_13_gpu_15 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_13_gpu_15
	gate_layer_2 -> expert_layer_2_expert_14_gpu_15 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_14_gpu_15
	gate_layer_2 -> expert_layer_2_expert_15_gpu_15 [color=red style=dashed]
	ln2_layer_2 -> expert_layer_2_expert_15_gpu_15
	expert_layer_2_expert_0_gpu_12 -> expert_agg_layer_2
	expert_layer_2_expert_1_gpu_12 -> expert_agg_layer_2
	expert_layer_2_expert_2_gpu_12 -> expert_agg_layer_2
	expert_layer_2_expert_3_gpu_12 -> expert_agg_layer_2
	expert_layer_2_expert_4_gpu_13 -> expert_agg_layer_2
	expert_layer_2_expert_5_gpu_13 -> expert_agg_layer_2
	expert_layer_2_expert_6_gpu_13 -> expert_agg_layer_2
	expert_layer_2_expert_7_gpu_13 -> expert_agg_layer_2
	expert_layer_2_expert_8_gpu_14 -> expert_agg_layer_2
	expert_layer_2_expert_9_gpu_14 -> expert_agg_layer_2
	expert_layer_2_expert_10_gpu_14 -> expert_agg_layer_2
	expert_layer_2_expert_11_gpu_14 -> expert_agg_layer_2
	expert_layer_2_expert_12_gpu_15 -> expert_agg_layer_2
	expert_layer_2_expert_13_gpu_15 -> expert_agg_layer_2
	expert_layer_2_expert_14_gpu_15 -> expert_agg_layer_2
	expert_layer_2_expert_15_gpu_15 -> expert_agg_layer_2
	expert_agg_layer_2 -> residual2_layer_2
	residual1_layer_2 -> residual2_layer_2
	residual2_layer_2 -> ln1_layer_3
	ln1_layer_3 -> qkv_layer_3_gpu_0
	qkv_layer_3_gpu_0 -> attention_layer_3_gpu_0
	qkv_layer_3_gpu_1 -> attention_layer_3_gpu_0 [color=blue style=dashed]
	qkv_layer_3_gpu_2 -> attention_layer_3_gpu_0 [color=blue style=dashed]
	qkv_layer_3_gpu_3 -> attention_layer_3_gpu_0 [color=blue style=dashed]
	qkv_layer_3_gpu_4 -> attention_layer_3_gpu_0 [color=blue style=dashed]
	qkv_layer_3_gpu_5 -> attention_layer_3_gpu_0 [color=blue style=dashed]
	qkv_layer_3_gpu_6 -> attention_layer_3_gpu_0 [color=blue style=dashed]
	qkv_layer_3_gpu_7 -> attention_layer_3_gpu_0 [color=blue style=dashed]
	qkv_layer_3_gpu_8 -> attention_layer_3_gpu_0 [color=blue style=dashed]
	qkv_layer_3_gpu_9 -> attention_layer_3_gpu_0 [color=blue style=dashed]
	qkv_layer_3_gpu_10 -> attention_layer_3_gpu_0 [color=blue style=dashed]
	qkv_layer_3_gpu_11 -> attention_layer_3_gpu_0 [color=blue style=dashed]
	ln1_layer_3 -> qkv_layer_3_gpu_1
	qkv_layer_3_gpu_1 -> attention_layer_3_gpu_1
	qkv_layer_3_gpu_0 -> attention_layer_3_gpu_1 [color=blue style=dashed]
	qkv_layer_3_gpu_2 -> attention_layer_3_gpu_1 [color=blue style=dashed]
	qkv_layer_3_gpu_3 -> attention_layer_3_gpu_1 [color=blue style=dashed]
	qkv_layer_3_gpu_4 -> attention_layer_3_gpu_1 [color=blue style=dashed]
	qkv_layer_3_gpu_5 -> attention_layer_3_gpu_1 [color=blue style=dashed]
	qkv_layer_3_gpu_6 -> attention_layer_3_gpu_1 [color=blue style=dashed]
	qkv_layer_3_gpu_7 -> attention_layer_3_gpu_1 [color=blue style=dashed]
	qkv_layer_3_gpu_8 -> attention_layer_3_gpu_1 [color=blue style=dashed]
	qkv_layer_3_gpu_9 -> attention_layer_3_gpu_1 [color=blue style=dashed]
	qkv_layer_3_gpu_10 -> attention_layer_3_gpu_1 [color=blue style=dashed]
	qkv_layer_3_gpu_11 -> attention_layer_3_gpu_1 [color=blue style=dashed]
	ln1_layer_3 -> qkv_layer_3_gpu_2
	qkv_layer_3_gpu_2 -> attention_layer_3_gpu_2
	qkv_layer_3_gpu_0 -> attention_layer_3_gpu_2 [color=blue style=dashed]
	qkv_layer_3_gpu_1 -> attention_layer_3_gpu_2 [color=blue style=dashed]
	qkv_layer_3_gpu_3 -> attention_layer_3_gpu_2 [color=blue style=dashed]
	qkv_layer_3_gpu_4 -> attention_layer_3_gpu_2 [color=blue style=dashed]
	qkv_layer_3_gpu_5 -> attention_layer_3_gpu_2 [color=blue style=dashed]
	qkv_layer_3_gpu_6 -> attention_layer_3_gpu_2 [color=blue style=dashed]
	qkv_layer_3_gpu_7 -> attention_layer_3_gpu_2 [color=blue style=dashed]
	qkv_layer_3_gpu_8 -> attention_layer_3_gpu_2 [color=blue style=dashed]
	qkv_layer_3_gpu_9 -> attention_layer_3_gpu_2 [color=blue style=dashed]
	qkv_layer_3_gpu_10 -> attention_layer_3_gpu_2 [color=blue style=dashed]
	qkv_layer_3_gpu_11 -> attention_layer_3_gpu_2 [color=blue style=dashed]
	ln1_layer_3 -> qkv_layer_3_gpu_3
	qkv_layer_3_gpu_3 -> attention_layer_3_gpu_3
	qkv_layer_3_gpu_0 -> attention_layer_3_gpu_3 [color=blue style=dashed]
	qkv_layer_3_gpu_1 -> attention_layer_3_gpu_3 [color=blue style=dashed]
	qkv_layer_3_gpu_2 -> attention_layer_3_gpu_3 [color=blue style=dashed]
	qkv_layer_3_gpu_4 -> attention_layer_3_gpu_3 [color=blue style=dashed]
	qkv_layer_3_gpu_5 -> attention_layer_3_gpu_3 [color=blue style=dashed]
	qkv_layer_3_gpu_6 -> attention_layer_3_gpu_3 [color=blue style=dashed]
	qkv_layer_3_gpu_7 -> attention_layer_3_gpu_3 [color=blue style=dashed]
	qkv_layer_3_gpu_8 -> attention_layer_3_gpu_3 [color=blue style=dashed]
	qkv_layer_3_gpu_9 -> attention_layer_3_gpu_3 [color=blue style=dashed]
	qkv_layer_3_gpu_10 -> attention_layer_3_gpu_3 [color=blue style=dashed]
	qkv_layer_3_gpu_11 -> attention_layer_3_gpu_3 [color=blue style=dashed]
	ln1_layer_3 -> qkv_layer_3_gpu_4
	qkv_layer_3_gpu_4 -> attention_layer_3_gpu_4
	qkv_layer_3_gpu_0 -> attention_layer_3_gpu_4 [color=blue style=dashed]
	qkv_layer_3_gpu_1 -> attention_layer_3_gpu_4 [color=blue style=dashed]
	qkv_layer_3_gpu_2 -> attention_layer_3_gpu_4 [color=blue style=dashed]
	qkv_layer_3_gpu_3 -> attention_layer_3_gpu_4 [color=blue style=dashed]
	qkv_layer_3_gpu_5 -> attention_layer_3_gpu_4 [color=blue style=dashed]
	qkv_layer_3_gpu_6 -> attention_layer_3_gpu_4 [color=blue style=dashed]
	qkv_layer_3_gpu_7 -> attention_layer_3_gpu_4 [color=blue style=dashed]
	qkv_layer_3_gpu_8 -> attention_layer_3_gpu_4 [color=blue style=dashed]
	qkv_layer_3_gpu_9 -> attention_layer_3_gpu_4 [color=blue style=dashed]
	qkv_layer_3_gpu_10 -> attention_layer_3_gpu_4 [color=blue style=dashed]
	qkv_layer_3_gpu_11 -> attention_layer_3_gpu_4 [color=blue style=dashed]
	ln1_layer_3 -> qkv_layer_3_gpu_5
	qkv_layer_3_gpu_5 -> attention_layer_3_gpu_5
	qkv_layer_3_gpu_0 -> attention_layer_3_gpu_5 [color=blue style=dashed]
	qkv_layer_3_gpu_1 -> attention_layer_3_gpu_5 [color=blue style=dashed]
	qkv_layer_3_gpu_2 -> attention_layer_3_gpu_5 [color=blue style=dashed]
	qkv_layer_3_gpu_3 -> attention_layer_3_gpu_5 [color=blue style=dashed]
	qkv_layer_3_gpu_4 -> attention_layer_3_gpu_5 [color=blue style=dashed]
	qkv_layer_3_gpu_6 -> attention_layer_3_gpu_5 [color=blue style=dashed]
	qkv_layer_3_gpu_7 -> attention_layer_3_gpu_5 [color=blue style=dashed]
	qkv_layer_3_gpu_8 -> attention_layer_3_gpu_5 [color=blue style=dashed]
	qkv_layer_3_gpu_9 -> attention_layer_3_gpu_5 [color=blue style=dashed]
	qkv_layer_3_gpu_10 -> attention_layer_3_gpu_5 [color=blue style=dashed]
	qkv_layer_3_gpu_11 -> attention_layer_3_gpu_5 [color=blue style=dashed]
	ln1_layer_3 -> qkv_layer_3_gpu_6
	qkv_layer_3_gpu_6 -> attention_layer_3_gpu_6
	qkv_layer_3_gpu_0 -> attention_layer_3_gpu_6 [color=blue style=dashed]
	qkv_layer_3_gpu_1 -> attention_layer_3_gpu_6 [color=blue style=dashed]
	qkv_layer_3_gpu_2 -> attention_layer_3_gpu_6 [color=blue style=dashed]
	qkv_layer_3_gpu_3 -> attention_layer_3_gpu_6 [color=blue style=dashed]
	qkv_layer_3_gpu_4 -> attention_layer_3_gpu_6 [color=blue style=dashed]
	qkv_layer_3_gpu_5 -> attention_layer_3_gpu_6 [color=blue style=dashed]
	qkv_layer_3_gpu_7 -> attention_layer_3_gpu_6 [color=blue style=dashed]
	qkv_layer_3_gpu_8 -> attention_layer_3_gpu_6 [color=blue style=dashed]
	qkv_layer_3_gpu_9 -> attention_layer_3_gpu_6 [color=blue style=dashed]
	qkv_layer_3_gpu_10 -> attention_layer_3_gpu_6 [color=blue style=dashed]
	qkv_layer_3_gpu_11 -> attention_layer_3_gpu_6 [color=blue style=dashed]
	ln1_layer_3 -> qkv_layer_3_gpu_7
	qkv_layer_3_gpu_7 -> attention_layer_3_gpu_7
	qkv_layer_3_gpu_0 -> attention_layer_3_gpu_7 [color=blue style=dashed]
	qkv_layer_3_gpu_1 -> attention_layer_3_gpu_7 [color=blue style=dashed]
	qkv_layer_3_gpu_2 -> attention_layer_3_gpu_7 [color=blue style=dashed]
	qkv_layer_3_gpu_3 -> attention_layer_3_gpu_7 [color=blue style=dashed]
	qkv_layer_3_gpu_4 -> attention_layer_3_gpu_7 [color=blue style=dashed]
	qkv_layer_3_gpu_5 -> attention_layer_3_gpu_7 [color=blue style=dashed]
	qkv_layer_3_gpu_6 -> attention_layer_3_gpu_7 [color=blue style=dashed]
	qkv_layer_3_gpu_8 -> attention_layer_3_gpu_7 [color=blue style=dashed]
	qkv_layer_3_gpu_9 -> attention_layer_3_gpu_7 [color=blue style=dashed]
	qkv_layer_3_gpu_10 -> attention_layer_3_gpu_7 [color=blue style=dashed]
	qkv_layer_3_gpu_11 -> attention_layer_3_gpu_7 [color=blue style=dashed]
	ln1_layer_3 -> qkv_layer_3_gpu_8
	qkv_layer_3_gpu_8 -> attention_layer_3_gpu_8
	qkv_layer_3_gpu_0 -> attention_layer_3_gpu_8 [color=blue style=dashed]
	qkv_layer_3_gpu_1 -> attention_layer_3_gpu_8 [color=blue style=dashed]
	qkv_layer_3_gpu_2 -> attention_layer_3_gpu_8 [color=blue style=dashed]
	qkv_layer_3_gpu_3 -> attention_layer_3_gpu_8 [color=blue style=dashed]
	qkv_layer_3_gpu_4 -> attention_layer_3_gpu_8 [color=blue style=dashed]
	qkv_layer_3_gpu_5 -> attention_layer_3_gpu_8 [color=blue style=dashed]
	qkv_layer_3_gpu_6 -> attention_layer_3_gpu_8 [color=blue style=dashed]
	qkv_layer_3_gpu_7 -> attention_layer_3_gpu_8 [color=blue style=dashed]
	qkv_layer_3_gpu_9 -> attention_layer_3_gpu_8 [color=blue style=dashed]
	qkv_layer_3_gpu_10 -> attention_layer_3_gpu_8 [color=blue style=dashed]
	qkv_layer_3_gpu_11 -> attention_layer_3_gpu_8 [color=blue style=dashed]
	ln1_layer_3 -> qkv_layer_3_gpu_9
	qkv_layer_3_gpu_9 -> attention_layer_3_gpu_9
	qkv_layer_3_gpu_0 -> attention_layer_3_gpu_9 [color=blue style=dashed]
	qkv_layer_3_gpu_1 -> attention_layer_3_gpu_9 [color=blue style=dashed]
	qkv_layer_3_gpu_2 -> attention_layer_3_gpu_9 [color=blue style=dashed]
	qkv_layer_3_gpu_3 -> attention_layer_3_gpu_9 [color=blue style=dashed]
	qkv_layer_3_gpu_4 -> attention_layer_3_gpu_9 [color=blue style=dashed]
	qkv_layer_3_gpu_5 -> attention_layer_3_gpu_9 [color=blue style=dashed]
	qkv_layer_3_gpu_6 -> attention_layer_3_gpu_9 [color=blue style=dashed]
	qkv_layer_3_gpu_7 -> attention_layer_3_gpu_9 [color=blue style=dashed]
	qkv_layer_3_gpu_8 -> attention_layer_3_gpu_9 [color=blue style=dashed]
	qkv_layer_3_gpu_10 -> attention_layer_3_gpu_9 [color=blue style=dashed]
	qkv_layer_3_gpu_11 -> attention_layer_3_gpu_9 [color=blue style=dashed]
	ln1_layer_3 -> qkv_layer_3_gpu_10
	qkv_layer_3_gpu_10 -> attention_layer_3_gpu_10
	qkv_layer_3_gpu_0 -> attention_layer_3_gpu_10 [color=blue style=dashed]
	qkv_layer_3_gpu_1 -> attention_layer_3_gpu_10 [color=blue style=dashed]
	qkv_layer_3_gpu_2 -> attention_layer_3_gpu_10 [color=blue style=dashed]
	qkv_layer_3_gpu_3 -> attention_layer_3_gpu_10 [color=blue style=dashed]
	qkv_layer_3_gpu_4 -> attention_layer_3_gpu_10 [color=blue style=dashed]
	qkv_layer_3_gpu_5 -> attention_layer_3_gpu_10 [color=blue style=dashed]
	qkv_layer_3_gpu_6 -> attention_layer_3_gpu_10 [color=blue style=dashed]
	qkv_layer_3_gpu_7 -> attention_layer_3_gpu_10 [color=blue style=dashed]
	qkv_layer_3_gpu_8 -> attention_layer_3_gpu_10 [color=blue style=dashed]
	qkv_layer_3_gpu_9 -> attention_layer_3_gpu_10 [color=blue style=dashed]
	qkv_layer_3_gpu_11 -> attention_layer_3_gpu_10 [color=blue style=dashed]
	ln1_layer_3 -> qkv_layer_3_gpu_11
	qkv_layer_3_gpu_11 -> attention_layer_3_gpu_11
	qkv_layer_3_gpu_0 -> attention_layer_3_gpu_11 [color=blue style=dashed]
	qkv_layer_3_gpu_1 -> attention_layer_3_gpu_11 [color=blue style=dashed]
	qkv_layer_3_gpu_2 -> attention_layer_3_gpu_11 [color=blue style=dashed]
	qkv_layer_3_gpu_3 -> attention_layer_3_gpu_11 [color=blue style=dashed]
	qkv_layer_3_gpu_4 -> attention_layer_3_gpu_11 [color=blue style=dashed]
	qkv_layer_3_gpu_5 -> attention_layer_3_gpu_11 [color=blue style=dashed]
	qkv_layer_3_gpu_6 -> attention_layer_3_gpu_11 [color=blue style=dashed]
	qkv_layer_3_gpu_7 -> attention_layer_3_gpu_11 [color=blue style=dashed]
	qkv_layer_3_gpu_8 -> attention_layer_3_gpu_11 [color=blue style=dashed]
	qkv_layer_3_gpu_9 -> attention_layer_3_gpu_11 [color=blue style=dashed]
	qkv_layer_3_gpu_10 -> attention_layer_3_gpu_11 [color=blue style=dashed]
	attention_layer_3_gpu_0 -> attention_agg_layer_3
	attention_layer_3_gpu_1 -> attention_agg_layer_3
	attention_layer_3_gpu_2 -> attention_agg_layer_3
	attention_layer_3_gpu_3 -> attention_agg_layer_3
	attention_layer_3_gpu_4 -> attention_agg_layer_3
	attention_layer_3_gpu_5 -> attention_agg_layer_3
	attention_layer_3_gpu_6 -> attention_agg_layer_3
	attention_layer_3_gpu_7 -> attention_agg_layer_3
	attention_layer_3_gpu_8 -> attention_agg_layer_3
	attention_layer_3_gpu_9 -> attention_agg_layer_3
	attention_layer_3_gpu_10 -> attention_agg_layer_3
	attention_layer_3_gpu_11 -> attention_agg_layer_3
	attention_agg_layer_3 -> out_proj_layer_3
	out_proj_layer_3 -> residual1_layer_3
	residual2_layer_2 -> residual1_layer_3
	residual1_layer_3 -> ln2_layer_3
	ln2_layer_3 -> gate_layer_3 [style=dashed]
	gate_layer_3 -> expert_layer_3_expert_0_gpu_12 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_0_gpu_12
	gate_layer_3 -> expert_layer_3_expert_1_gpu_12 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_1_gpu_12
	gate_layer_3 -> expert_layer_3_expert_2_gpu_12 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_2_gpu_12
	gate_layer_3 -> expert_layer_3_expert_3_gpu_12 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_3_gpu_12
	gate_layer_3 -> expert_layer_3_expert_4_gpu_13 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_4_gpu_13
	gate_layer_3 -> expert_layer_3_expert_5_gpu_13 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_5_gpu_13
	gate_layer_3 -> expert_layer_3_expert_6_gpu_13 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_6_gpu_13
	gate_layer_3 -> expert_layer_3_expert_7_gpu_13 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_7_gpu_13
	gate_layer_3 -> expert_layer_3_expert_8_gpu_14 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_8_gpu_14
	gate_layer_3 -> expert_layer_3_expert_9_gpu_14 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_9_gpu_14
	gate_layer_3 -> expert_layer_3_expert_10_gpu_14 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_10_gpu_14
	gate_layer_3 -> expert_layer_3_expert_11_gpu_14 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_11_gpu_14
	gate_layer_3 -> expert_layer_3_expert_12_gpu_15 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_12_gpu_15
	gate_layer_3 -> expert_layer_3_expert_13_gpu_15 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_13_gpu_15
	gate_layer_3 -> expert_layer_3_expert_14_gpu_15 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_14_gpu_15
	gate_layer_3 -> expert_layer_3_expert_15_gpu_15 [color=red style=dashed]
	ln2_layer_3 -> expert_layer_3_expert_15_gpu_15
	expert_layer_3_expert_0_gpu_12 -> expert_agg_layer_3
	expert_layer_3_expert_1_gpu_12 -> expert_agg_layer_3
	expert_layer_3_expert_2_gpu_12 -> expert_agg_layer_3
	expert_layer_3_expert_3_gpu_12 -> expert_agg_layer_3
	expert_layer_3_expert_4_gpu_13 -> expert_agg_layer_3
	expert_layer_3_expert_5_gpu_13 -> expert_agg_layer_3
	expert_layer_3_expert_6_gpu_13 -> expert_agg_layer_3
	expert_layer_3_expert_7_gpu_13 -> expert_agg_layer_3
	expert_layer_3_expert_8_gpu_14 -> expert_agg_layer_3
	expert_layer_3_expert_9_gpu_14 -> expert_agg_layer_3
	expert_layer_3_expert_10_gpu_14 -> expert_agg_layer_3
	expert_layer_3_expert_11_gpu_14 -> expert_agg_layer_3
	expert_layer_3_expert_12_gpu_15 -> expert_agg_layer_3
	expert_layer_3_expert_13_gpu_15 -> expert_agg_layer_3
	expert_layer_3_expert_14_gpu_15 -> expert_agg_layer_3
	expert_layer_3_expert_15_gpu_15 -> expert_agg_layer_3
	expert_agg_layer_3 -> residual2_layer_3
	residual1_layer_3 -> residual2_layer_3
	residual2_layer_3 -> final_layernorm
	final_layernorm -> output
}
