digraph ma_separation_complete {
    graph [comment="MA Separation Complete Model DAG", rankdir=TB, size="20,30"]
    node [fontname="Arial", fontsize=10, shape=ellipse, style=filled]
    
    // Input and Output nodes
    input [fillcolor="#e6f3ff", label="Input\nInput: [batch_size=1024, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048]\nGPU: 0", shape=ellipse]
    output [fillcolor="#e6f3ff", label="Output\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, vocab_size=50265]\nGPU: 0", shape=ellipse]
    
    // Embedding layer
    embedding [fillcolor="#000080", label="Embedding\nInput: [batch_size=1024, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=rectangle]
    
    // Layer 0 components
    ln1_layer_0 [fillcolor="#000080", label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=rectangle]
    
    // QKV projections for Layer 0 across 12 attention GPUs
    qkv_layer_0_gpu_0 [fillcolor="#0014ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0", shape=rectangle]
    qkv_layer_0_gpu_1 [fillcolor="#001eff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1", shape=rectangle]
    qkv_layer_0_gpu_2 [fillcolor="#0028ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2", shape=rectangle]
    qkv_layer_0_gpu_3 [fillcolor="#0032ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3", shape=rectangle]
    qkv_layer_0_gpu_4 [fillcolor="#003cff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4", shape=rectangle]
    qkv_layer_0_gpu_5 [fillcolor="#0046ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5", shape=rectangle]
    qkv_layer_0_gpu_6 [fillcolor="#0050ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6", shape=rectangle]
    qkv_layer_0_gpu_7 [fillcolor="#005aff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7", shape=rectangle]
    qkv_layer_0_gpu_8 [fillcolor="#0064ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8", shape=rectangle]
    qkv_layer_0_gpu_9 [fillcolor="#006eff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9", shape=rectangle]
    qkv_layer_0_gpu_10 [fillcolor="#0078ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10", shape=rectangle]
    qkv_layer_0_gpu_11 [fillcolor="#0082ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11", shape=rectangle]

    // Attention computation nodes for Layer 0
    attention_layer_0_gpu_0 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0", shape=rectangle]
    attention_layer_0_gpu_1 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1", shape=rectangle]
    attention_layer_0_gpu_2 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2", shape=rectangle]
    attention_layer_0_gpu_3 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3", shape=rectangle]
    attention_layer_0_gpu_4 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4", shape=rectangle]
    attention_layer_0_gpu_5 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5", shape=rectangle]
    attention_layer_0_gpu_6 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6", shape=rectangle]
    attention_layer_0_gpu_7 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7", shape=rectangle]
    attention_layer_0_gpu_8 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8", shape=rectangle]
    attention_layer_0_gpu_9 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9", shape=rectangle]
    attention_layer_0_gpu_10 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10", shape=rectangle]
    attention_layer_0_gpu_11 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11", shape=rectangle]

    // Attention aggregation and output projection
    attention_agg_layer_0 [fillcolor="#ffffcc", label="All-Reduce Attention Aggregation\nInput: [12Ã— batch_size=1024, seq_len=2048, partial_heads, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]
    out_proj_layer_0 [fillcolor="#000080", label="Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=rectangle]
    residual1_layer_0 [fillcolor="#ffffcc", label="Residual Add\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096], [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]

    // MoE components for Layer 0
    ln2_layer_0 [fillcolor="#000080", label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=rectangle]
    gate_layer_0 [fillcolor="#00ff00", label="Gating Network\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 0", shape=parallelogram, style=dashed]

    // Expert nodes distributed across GPUs 12-15
    expert_0_gpu_12 [fillcolor="#ff8800", label="Expert 0\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12", shape=rectangle]
    expert_1_gpu_12 [fillcolor="#ff8800", label="Expert 1\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12", shape=rectangle]
    expert_2_gpu_12 [fillcolor="#ff8800", label="Expert 2\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12", shape=rectangle]
    expert_3_gpu_12 [fillcolor="#ff8800", label="Expert 3\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 12", shape=rectangle]
    expert_4_gpu_13 [fillcolor="#ff8800", label="Expert 4\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13", shape=rectangle]
    expert_5_gpu_13 [fillcolor="#ff8800", label="Expert 5\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13", shape=rectangle]
    expert_6_gpu_13 [fillcolor="#ff8800", label="Expert 6\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13", shape=rectangle]
    expert_7_gpu_13 [fillcolor="#ff8800", label="Expert 7\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 13", shape=rectangle]
    expert_8_gpu_14 [fillcolor="#ff8800", label="Expert 8\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14", shape=rectangle]
    expert_9_gpu_14 [fillcolor="#ff8800", label="Expert 9\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14", shape=rectangle]
    expert_10_gpu_14 [fillcolor="#ff8800", label="Expert 10\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14", shape=rectangle]
    expert_11_gpu_14 [fillcolor="#ff8800", label="Expert 11\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 14", shape=rectangle]
    expert_12_gpu_15 [fillcolor="#ff8800", label="Expert 12\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15", shape=rectangle]
    expert_13_gpu_15 [fillcolor="#ff8800", label="Expert 13\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15", shape=rectangle]
    expert_14_gpu_15 [fillcolor="#ff8800", label="Expert 14\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15", shape=rectangle]
    expert_15_gpu_15 [fillcolor="#ff8800", label="Expert 15\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 15", shape=rectangle]

    expert_agg_layer_0 [fillcolor="#ffffcc", label="Expert Aggregation\nInput: [16Ã— batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]
    residual2_layer_0 [fillcolor="#ffffcc", label="Residual Add\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096], [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]

    // Layer 1 components
    ln1_layer_1 [fillcolor="#000080", label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=rectangle]
    
    // QKV projections for Layer 1 (same pattern as Layer 0)
    qkv_layer_1_gpu_0 [fillcolor="#0014ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0", shape=rectangle]
    qkv_layer_1_gpu_1 [fillcolor="#001eff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1", shape=rectangle]
    qkv_layer_1_gpu_2 [fillcolor="#0028ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2", shape=rectangle]
    qkv_layer_1_gpu_3 [fillcolor="#0032ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3", shape=rectangle]
    qkv_layer_1_gpu_4 [fillcolor="#003cff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4", shape=rectangle]
    qkv_layer_1_gpu_5 [fillcolor="#0046ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5", shape=rectangle]
    qkv_layer_1_gpu_6 [fillcolor="#0050ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6", shape=rectangle]
    qkv_layer_1_gpu_7 [fillcolor="#005aff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7", shape=rectangle]
    qkv_layer_1_gpu_8 [fillcolor="#0064ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8", shape=rectangle]
    qkv_layer_1_gpu_9 [fillcolor="#006eff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9", shape=rectangle]
    qkv_layer_1_gpu_10 [fillcolor="#0078ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10", shape=rectangle]
    qkv_layer_1_gpu_11 [fillcolor="#0082ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11", shape=rectangle]

    // Attention computation for Layer 1
    attention_layer_1_gpu_0 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0", shape=rectangle]
    attention_layer_1_gpu_1 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1", shape=rectangle]
    attention_layer_1_gpu_2 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2", shape=rectangle]
    attention_layer_1_gpu_3 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3", shape=rectangle]
    attention_layer_1_gpu_4 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4", shape=rectangle]
    attention_layer_1_gpu_5 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5", shape=rectangle]
    attention_layer_1_gpu_6 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6", shape=rectangle]
    attention_layer_1_gpu_7 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7", shape=rectangle]
    attention_layer_1_gpu_8 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8", shape=rectangle]
    attention_layer_1_gpu_9 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9", shape=rectangle]
    attention_layer_1_gpu_10 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10", shape=rectangle]
    attention_layer_1_gpu_11 [fillcolor="#ff0000", label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=32, d_k=128], V:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11", shape=rectangle]

    attention_agg_layer_1 [fillcolor="#ffffcc", label="All-Reduce Attention Aggregation\nInput: [12Ã— batch_size=1024, seq_len=2048, partial_heads, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]
    out_proj_layer_1 [fillcolor="#000080", label="Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=rectangle]
    residual1_layer_1 [fillcolor="#ffffcc", label="Residual Add\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096], [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]

    // MoE components for Layer 1
    ln2_layer_1 [fillcolor="#000080", label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=rectangle]
    gate_layer_1 [fillcolor="#00ff00", label="Gating Network\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 0", shape=parallelogram, style=dashed]
    expert_agg_layer_1 [fillcolor="#ffffcc", label="Expert Aggregation\nInput: [16Ã— batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]
    residual2_layer_1 [fillcolor="#ffffcc", label="Residual Add\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096], [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]

    // Layer 2 components (same structure as Layer 1)
    ln1_layer_2 [fillcolor="#000080", label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=rectangle]
    qkv_layer_2_gpu_0 [fillcolor="#0014ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0", shape=rectangle]
    qkv_layer_2_gpu_1 [fillcolor="#001eff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1", shape=rectangle]
    qkv_layer_2_gpu_2 [fillcolor="#0028ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2", shape=rectangle]
    qkv_layer_2_gpu_3 [fillcolor="#0032ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3", shape=rectangle]
    qkv_layer_2_gpu_4 [fillcolor="#003cff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4", shape=rectangle]
    qkv_layer_2_gpu_5 [fillcolor="#0046ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5", shape=rectangle]
    qkv_layer_2_gpu_6 [fillcolor="#0050ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6", shape=rectangle]
    qkv_layer_2_gpu_7 [fillcolor="#005aff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7", shape=rectangle]
    qkv_layer_2_gpu_8 [fillcolor="#0064ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8", shape=rectangle]
    qkv_layer_2_gpu_9 [fillcolor="#006eff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9", shape=rectangle]
    qkv_layer_2_gpu_10 [fillcolor="#0078ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10", shape=rectangle]
    qkv_layer_2_gpu_11 [fillcolor="#0082ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11", shape=rectangle]

    attention_agg_layer_2 [fillcolor="#ffffcc", label="All-Reduce Attention Aggregation\nInput: [12Ã— batch_size=1024, seq_len=2048, partial_heads, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]
    out_proj_layer_2 [fillcolor="#000080", label="Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=rectangle]
    residual1_layer_2 [fillcolor="#ffffcc", label="Residual Add\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096], [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]
    ln2_layer_2 [fillcolor="#000080", label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=rectangle]
    gate_layer_2 [fillcolor="#00ff00", label="Gating Network\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 0", shape=parallelogram, style=dashed]
    expert_agg_layer_2 [fillcolor="#ffffcc", label="Expert Aggregation\nInput: [16Ã— batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]
    residual2_layer_2 [fillcolor="#ffffcc", label="Residual Add\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096], [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]

    // Layer 3 components (final layer)
    ln1_layer_3 [fillcolor="#000080", label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=rectangle]
    qkv_layer_3_gpu_0 [fillcolor="#0014ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 0", shape=rectangle]
    qkv_layer_3_gpu_1 [fillcolor="#001eff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 1", shape=rectangle]
    qkv_layer_3_gpu_2 [fillcolor="#0028ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 2", shape=rectangle]
    qkv_layer_3_gpu_3 [fillcolor="#0032ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 3", shape=rectangle]
    qkv_layer_3_gpu_4 [fillcolor="#003cff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 4", shape=rectangle]
    qkv_layer_3_gpu_5 [fillcolor="#0046ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 5", shape=rectangle]
    qkv_layer_3_gpu_6 [fillcolor="#0050ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 6", shape=rectangle]
    qkv_layer_3_gpu_7 [fillcolor="#005aff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=3, d_k=128], K:[batch_size=1024, seq_len=2048, heads=3, d_k=128], V:[batch_size=1024, seq_len=2048, heads=3, d_k=128]\nGPU: 7", shape=rectangle]
    qkv_layer_3_gpu_8 [fillcolor="#0064ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 8", shape=rectangle]
    qkv_layer_3_gpu_9 [fillcolor="#006eff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 9", shape=rectangle]
    qkv_layer_3_gpu_10 [fillcolor="#0078ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 10", shape=rectangle]
    qkv_layer_3_gpu_11 [fillcolor="#0082ff", label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=2, d_k=128], K:[batch_size=1024, seq_len=2048, heads=2, d_k=128], V:[batch_size=1024, seq_len=2048, heads=2, d_k=128]\nGPU: 11", shape=rectangle]

    attention_agg_layer_3 [fillcolor="#ffffcc", label="All-Reduce Attention Aggregation\nInput: [12Ã— batch_size=1024, seq_len=2048, partial_heads, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]
    out_proj_layer_3 [fillcolor="#000080", label="Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=rectangle]
    residual1_layer_3 [fillcolor="#ffffcc", label="Residual Add\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096], [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]
    ln2_layer_3 [fillcolor="#000080", label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=rectangle]
    gate_layer_3 [fillcolor="#00ff00", label="Gating Network\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 0", shape=parallelogram, style=dashed]
    expert_agg_layer_3 [fillcolor="#ffffcc", label="Expert Aggregation\nInput: [16Ã— batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]
    residual2_layer_3 [fillcolor="#ffffcc", label="Residual Add\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096], [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0", shape=parallelogram]

    // Edges - Complete flow from input to output
    input -> embedding
    embedding -> ln1_layer_0

    // Layer 0 attention flow
    ln1_layer_0 -> qkv_layer_0_gpu_0
    ln1_layer_0 -> qkv_layer_0_gpu_1
    ln1_layer_0 -> qkv_layer_0_gpu_2
    ln1_layer_0 -> qkv_layer_0_gpu_3
    ln1_layer_0 -> qkv_layer_0_gpu_4
    ln1_layer_0 -> qkv_layer_0_gpu_5
    ln1_layer_0 -> qkv_layer_0_gpu_6
    ln1_layer_0 -> qkv_layer_0_gpu_7
    ln1_layer_0 -> qkv_layer_0_gpu_8
    ln1_layer_0 -> qkv_layer_0_gpu_9
    ln1_layer_0 -> qkv_layer_0_gpu_10
    ln1_layer_0 -> qkv_layer_0_gpu_11

    qkv_layer_0_gpu_0 -> attention_layer_0_gpu_0
    qkv_layer_0_gpu_1 -> attention_layer_0_gpu_0
    qkv_layer_0_gpu_2 -> attention_layer_0_gpu_0
    qkv_layer_0_gpu_3 -> attention_layer_0_gpu_0
    qkv_layer_0_gpu_4 -> attention_layer_0_gpu_0
    qkv_layer_0_gpu_5 -> attention_layer_0_gpu_0
    qkv_layer_0_gpu_6 -> attention_layer_0_gpu_0
    qkv_layer_0_gpu_7 -> attention_layer_0_gpu_0
    qkv_layer_0_gpu_8 -> attention_layer_0_gpu_0
    qkv_layer_0_gpu_9 -> attention_layer_0_gpu_0
    qkv_layer_0_gpu_10 -> attention_layer_0_gpu_0
    qkv_layer_0_gpu_11 -> attention_layer_0_gpu_0

    attention_layer_0_gpu_0 -> attention_agg_layer_0
    attention_layer_0_gpu_1 -> attention_agg_layer_0
    attention_layer_0_gpu_2 -> attention_agg_layer_0
    attention_layer_0_gpu_3 -> attention_agg_layer_0
    attention_layer_0_gpu_4 -> attention_agg_layer_0
    attention_layer_0_gpu_5 -> attention_agg_layer_0
    attention_layer_0_gpu_6 -> attention_agg_layer_0
    attention_layer_0_gpu_7 -> attention_agg_layer_0
    attention_layer_0_gpu_8 -> attention_agg_layer_0
    attention_layer_0_gpu_9 -> attention_agg_layer_0
    attention_layer_0_gpu_10 -> attention_agg_layer_0
    attention_layer_0_gpu_11 -> attention_agg_layer_0

    attention_agg_layer_0 -> out_proj_layer_0
    out_proj_layer_0 -> residual1_layer_0
    embedding -> residual1_layer_0
    residual1_layer_0 -> ln2_layer_0

    // Layer 0 MoE flow
    ln2_layer_0 -> gate_layer_0
    ln2_layer_0 -> expert_0_gpu_12
    ln2_layer_0 -> expert_1_gpu_12
    ln2_layer_0 -> expert_2_gpu_12
    ln2_layer_0 -> expert_3_gpu_12
    ln2_layer_0 -> expert_4_gpu_13
    ln2_layer_0 -> expert_5_gpu_13
    ln2_layer_0 -> expert_6_gpu_13
    ln2_layer_0 -> expert_7_gpu_13
    ln2_layer_0 -> expert_8_gpu_14
    ln2_layer_0 -> expert_9_gpu_14
    ln2_layer_0 -> expert_10_gpu_14
    ln2_layer_0 -> expert_11_gpu_14
    ln2_layer_0 -> expert_12_gpu_15
    ln2_layer_0 -> expert_13_gpu_15
    ln2_layer_0 -> expert_14_gpu_15
    ln2_layer_0 -> expert_15_gpu_15

    gate_layer_0 -> expert_0_gpu_12 [style=dashed]
    gate_layer_0 -> expert_1_gpu_12 [style=dashed]
    gate_layer_0 -> expert_2_gpu_12 [style=dashed]
    gate_layer_0 -> expert_3_gpu_12 [style=dashed]
    gate_layer_0 -> expert_4_gpu_13 [style=dashed]
    gate_layer_0 -> expert_5_gpu_13 [style=dashed]
    gate_layer_0 -> expert_6_gpu_13 [style=dashed]
    gate_layer_0 -> expert_7_gpu_13 [style=dashed]
    gate_layer_0 -> expert_8_gpu_14 [style=dashed]
    gate_layer_0 -> expert_9_gpu_14 [style=dashed]
    gate_layer_0 -> expert_10_gpu_14 [style=dashed]
    gate_layer_0 -> expert_11_gpu_14 [style=dashed]
    gate_layer_0 -> expert_12_gpu_15 [style=dashed]
    gate_layer_0 -> expert_13_gpu_15 [style=dashed]
    gate_layer_0 -> expert_14_gpu_15 [style=dashed]
    gate_layer_0 -> expert_15_gpu_15 [style=dashed]

    expert_0_gpu_12 -> expert_agg_layer_0
    expert_1_gpu_12 -> expert_agg_layer_0
    expert_2_gpu_12 -> expert_agg_layer_0
    expert_3_gpu_12 -> expert_agg_layer_0
    expert_4_gpu_13 -> expert_agg_layer_0
    expert_5_gpu_13 -> expert_agg_layer_0
    expert_6_gpu_13 -> expert_agg_layer_0
    expert_7_gpu_13 -> expert_agg_layer_0
    expert_8_gpu_14 -> expert_agg_layer_0
    expert_9_gpu_14 -> expert_agg_layer_0
    expert_10_gpu_14 -> expert_agg_layer_0
    expert_11_gpu_14 -> expert_agg_layer_0
    expert_12_gpu_15 -> expert_agg_layer_0
    expert_13_gpu_15 -> expert_agg_layer_0
    expert_14_gpu_15 -> expert_agg_layer_0
    expert_15_gpu_15 -> expert_agg_layer_0

    expert_agg_layer_0 -> residual2_layer_0
    residual1_layer_0 -> residual2_layer_0
    residual2_layer_0 -> ln1_layer_1

    // Layer 1 attention flow
    ln1_layer_1 -> qkv_layer_1_gpu_0
    ln1_layer_1 -> qkv_layer_1_gpu_1
    ln1_layer_1 -> qkv_layer_1_gpu_2
    ln1_layer_1 -> qkv_layer_1_gpu_3
    ln1_layer_1 -> qkv_layer_1_gpu_4
    ln1_layer_1 -> qkv_layer_1_gpu_5
    ln1_layer_1 -> qkv_layer_1_gpu_6
    ln1_layer_1 -> qkv_layer_1_gpu_7
    ln1_layer_1 -> qkv_layer_1_gpu_8
    ln1_layer_1 -> qkv_layer_1_gpu_9
    ln1_layer_1 -> qkv_layer_1_gpu_10
    ln1_layer_1 -> qkv_layer_1_gpu_11

    qkv_layer_1_gpu_0 -> attention_layer_1_gpu_0
    qkv_layer_1_gpu_1 -> attention_layer_1_gpu_1
    qkv_layer_1_gpu_2 -> attention_layer_1_gpu_2
    qkv_layer_1_gpu_3 -> attention_layer_1_gpu_3
    qkv_layer_1_gpu_4 -> attention_layer_1_gpu_4
    qkv_layer_1_gpu_5 -> attention_layer_1_gpu_5
    qkv_layer_1_gpu_6 -> attention_layer_1_gpu_6
    qkv_layer_1_gpu_7 -> attention_layer_1_gpu_7
    qkv_layer_1_gpu_8 -> attention_layer_1_gpu_8
    qkv_layer_1_gpu_9 -> attention_layer_1_gpu_9
    qkv_layer_1_gpu_10 -> attention_layer_1_gpu_10
    qkv_layer_1_gpu_11 -> attention_layer_1_gpu_11

    attention_layer_1_gpu_0 -> attention_agg_layer_1
    attention_layer_1_gpu_1 -> attention_agg_layer_1
    attention_layer_1_gpu_2 -> attention_agg_layer_1
    attention_layer_1_gpu_3 -> attention_agg_layer_1
    attention_layer_1_gpu_4 -> attention_agg_layer_1
    attention_layer_1_gpu_5 -> attention_agg_layer_1
    attention_layer_1_gpu_6 -> attention_agg_layer_1
    attention_layer_1_gpu_7 -> attention_agg_layer_1
    attention_layer_1_gpu_8 -> attention_agg_layer_1
    attention_layer_1_gpu_9 -> attention_agg_layer_1
    attention_layer_1_gpu_10 -> attention_agg_layer_1
    attention_layer_1_gpu_11 -> attention_agg_layer_1

    attention_agg_layer_1 -> out_proj_layer_1
    out_proj_layer_1 -> residual1_layer_1
    residual2_layer_0 -> residual1_layer_1
    residual1_layer_1 -> ln2_layer_1

    // Layer 1 MoE flow (reusing expert nodes)
    ln2_layer_1 -> gate_layer_1
    ln2_layer_1 -> expert_0_gpu_12
    ln2_layer_1 -> expert_1_gpu_12
    ln2_layer_1 -> expert_2_gpu_12
    ln2_layer_1 -> expert_3_gpu_12
    ln2_layer_1 -> expert_4_gpu_13
    ln2_layer_1 -> expert_5_gpu_13
    ln2_layer_1 -> expert_6_gpu_13
    ln2_layer_1 -> expert_7_gpu_13
    ln2_layer_1 -> expert_8_gpu_14
    ln2_layer_1 -> expert_9_gpu_14
    ln2_layer_1 -> expert_10_gpu_14
    ln2_layer_1 -> expert_11_gpu_14
    ln2_layer_1 -> expert_12_gpu_15
    ln2_layer_1 -> expert_13_gpu_15
    ln2_layer_1 -> expert_14_gpu_15
    ln2_layer_1 -> expert_15_gpu_15

    gate_layer_1 -> expert_0_gpu_12 [style=dashed]
    gate_layer_1 -> expert_1_gpu_12 [style=dashed]
    gate_layer_1 -> expert_2_gpu_12 [style=dashed]
    gate_layer_1 -> expert_3_gpu_12 [style=dashed]
    gate_layer_1 -> expert_4_gpu_13 [style=dashed]
    gate_layer_1 -> expert_5_gpu_13 [style=dashed]
    gate_layer_1 -> expert_6_gpu_13 [style=dashed]
    gate_layer_1 -> expert_7_gpu_13 [style=dashed]
    gate_layer_1 -> expert_8_gpu_14 [