// Baseline Hybrid TP=8 PP=2 Complete Model DAG
digraph baseline_hybrid_tp_pp_dag {
	rankdir=TB size="20,30"
	node [fontname=Arial fontsize=10]
	node [shape=ellipse]
	node [style=filled]
	input [label="Input\nInput: [batch_size=1024, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#e6f3ff" shape=ellipse]
	embedding [label="Embedding\nInput: [batch_size=1024, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0" fillcolor="#ff0080" shape=rectangle]
	subgraph cluster_stage_0 {
		color=orange label="Stage 0 (GPUs 0-7)" style=dotted
		subgraph cluster_stage0_layer_0 {
			color=orange label="Layer 0" style=dashed
			stage0_ln1_layer_0 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (sharded)" fillcolor="#ff0080" shape=rectangle]
			stage0_qkv_layer_0_gpu_0 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 0" fillcolor="#ff0080" shape=rectangle]
			stage0_qkv_layer_0_gpu_1 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 1" fillcolor="#ff1480" shape=rectangle]
			stage0_qkv_layer_0_gpu_2 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 2" fillcolor="#ff2880" shape=rectangle]
			stage0_qkv_layer_0_gpu_3 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 3" fillcolor="#ff3c80" shape=rectangle]
			stage0_qkv_layer_0_gpu_4 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 4" fillcolor="#ff5080" shape=rectangle]
			stage0_qkv_layer_0_gpu_5 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 5" fillcolor="#ff6480" shape=rectangle]
			stage0_qkv_layer_0_gpu_6 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 6" fillcolor="#ff7880" shape=rectangle]
			stage0_qkv_layer_0_gpu_7 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 7" fillcolor="#ff8c80" shape=rectangle]
			stage0_attention_layer_0_gpu_0 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 0" fillcolor="#ff0080" shape=rectangle]
			stage0_attention_layer_0_gpu_1 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 1" fillcolor="#ff1480" shape=rectangle]
			stage0_attention_layer_0_gpu_2 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 2" fillcolor="#ff2880" shape=rectangle]
			stage0_attention_layer_0_gpu_3 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 3" fillcolor="#ff3c80" shape=rectangle]
			stage0_attention_layer_0_gpu_4 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 4" fillcolor="#ff5080" shape=rectangle]
			stage0_attention_layer_0_gpu_5 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 5" fillcolor="#ff6480" shape=rectangle]
			stage0_attention_layer_0_gpu_6 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 6" fillcolor="#ff7880" shape=rectangle]
			stage0_attention_layer_0_gpu_7 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 7" fillcolor="#ff8c80" shape=rectangle]
			stage0_attention_reduce_layer_0 [label="Attention All-Reduce\nInput: [8× batch_size=1024, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (all-reduce)" fillcolor="#ffcccc" shape=parallelogram]
			stage0_out_proj_layer_0 [label="Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (sharded)" fillcolor="#ff0080" shape=rectangle]
			stage0_residual1_layer_0 [label="Residual Add 1\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (all-reduce)" fillcolor="#ffffcc" shape=parallelogram]
			stage0_ln2_layer_0 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (sharded)" fillcolor="#ff0080" shape=rectangle]
			stage0_gate_layer_0 [label="Gating Network\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 0-7 (sharded)" fillcolor="#ccffcc" shape=parallelogram style=dashed]
			stage0_expert_layer_0_expert_0_gpu_0 [label="Expert0\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 0" fillcolor="#ff0080" shape=rectangle]
			stage0_expert_layer_0_expert_1_gpu_0 [label="Expert1\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 0" fillcolor="#ff0080" shape=rectangle]
			stage0_expert_layer_0_expert_2_gpu_1 [label="Expert2\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 1" fillcolor="#ff1480" shape=rectangle]
			stage0_expert_layer_0_expert_3_gpu_1 [label="Expert3\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 1" fillcolor="#ff1480" shape=rectangle]
			stage0_expert_layer_0_expert_4_gpu_2 [label="Expert4\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 2" fillcolor="#ff2880" shape=rectangle]
			stage0_expert_layer_0_expert_5_gpu_2 [label="Expert5\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 2" fillcolor="#ff2880" shape=rectangle]
			stage0_expert_layer_0_expert_6_gpu_3 [label="Expert6\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 3" fillcolor="#ff3c80" shape=rectangle]
			stage0_expert_layer_0_expert_7_gpu_3 [label="Expert7\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 3" fillcolor="#ff3c80" shape=rectangle]
			stage0_expert_layer_0_expert_8_gpu_4 [label="Expert8\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 4" fillcolor="#ff5080" shape=rectangle]
			stage0_expert_layer_0_expert_9_gpu_4 [label="Expert9\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 4" fillcolor="#ff5080" shape=rectangle]
			stage0_expert_layer_0_expert_10_gpu_5 [label="Expert10\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 5" fillcolor="#ff6480" shape=rectangle]
			stage0_expert_layer_0_expert_11_gpu_5 [label="Expert11\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 5" fillcolor="#ff6480" shape=rectangle]
			stage0_expert_layer_0_expert_12_gpu_6 [label="Expert12\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 6" fillcolor="#ff7880" shape=rectangle]
			stage0_expert_layer_0_expert_13_gpu_6 [label="Expert13\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 6" fillcolor="#ff7880" shape=rectangle]
			stage0_expert_layer_0_expert_14_gpu_7 [label="Expert14\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 7" fillcolor="#ff8c80" shape=rectangle]
			stage0_expert_layer_0_expert_15_gpu_7 [label="Expert15\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 7" fillcolor="#ff8c80" shape=rectangle]
			stage0_expert_agg_layer_0 [label="Expert Aggregation\nInput: [8× batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (all-reduce)" fillcolor="#ffcccc" shape=parallelogram]
			stage0_residual2_layer_0 [label="Residual Add 2\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (all-reduce)" fillcolor="#ffffcc" shape=parallelogram]
		}
		subgraph cluster_stage0_layer_1 {
			color=orange label="Layer 1" style=dashed
			stage0_ln1_layer_1 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (sharded)" fillcolor="#ff0080" shape=rectangle]
			stage0_qkv_layer_1_gpu_0 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 0" fillcolor="#ff0080" shape=rectangle]
			stage0_qkv_layer_1_gpu_1 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 1" fillcolor="#ff1480" shape=rectangle]
			stage0_qkv_layer_1_gpu_2 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 2" fillcolor="#ff2880" shape=rectangle]
			stage0_qkv_layer_1_gpu_3 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 3" fillcolor="#ff3c80" shape=rectangle]
			stage0_qkv_layer_1_gpu_4 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 4" fillcolor="#ff5080" shape=rectangle]
			stage0_qkv_layer_1_gpu_5 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 5" fillcolor="#ff6480" shape=rectangle]
			stage0_qkv_layer_1_gpu_6 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 6" fillcolor="#ff7880" shape=rectangle]
			stage0_qkv_layer_1_gpu_7 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 7" fillcolor="#ff8c80" shape=rectangle]
			stage0_attention_layer_1_gpu_0 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 0" fillcolor="#ff0080" shape=rectangle]
			stage0_attention_layer_1_gpu_1 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 1" fillcolor="#ff1480" shape=rectangle]
			stage0_attention_layer_1_gpu_2 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 2" fillcolor="#ff2880" shape=rectangle]
			stage0_attention_layer_1_gpu_3 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 3" fillcolor="#ff3c80" shape=rectangle]
			stage0_attention_layer_1_gpu_4 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 4" fillcolor="#ff5080" shape=rectangle]
			stage0_attention_layer_1_gpu_5 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 5" fillcolor="#ff6480" shape=rectangle]
			stage0_attention_layer_1_gpu_6 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 6" fillcolor="#ff7880" shape=rectangle]
			stage0_attention_layer_1_gpu_7 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 7" fillcolor="#ff8c80" shape=rectangle]
			stage0_attention_reduce_layer_1 [label="Attention All-Reduce\nInput: [8× batch_size=1024, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (all-reduce)" fillcolor="#ffcccc" shape=parallelogram]
			stage0_out_proj_layer_1 [label="Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (sharded)" fillcolor="#ff0080" shape=rectangle]
			stage0_residual1_layer_1 [label="Residual Add 1\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (all-reduce)" fillcolor="#ffffcc" shape=parallelogram]
			stage0_ln2_layer_1 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (sharded)" fillcolor="#ff0080" shape=rectangle]
			stage0_gate_layer_1 [label="Gating Network\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 0-7 (sharded)" fillcolor="#ccffcc" shape=parallelogram style=dashed]
			stage0_expert_layer_1_expert_0_gpu_0 [label="Expert0\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 0" fillcolor="#ff0080" shape=rectangle]
			stage0_expert_layer_1_expert_1_gpu_0 [label="Expert1\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 0" fillcolor="#ff0080" shape=rectangle]
			stage0_expert_layer_1_expert_2_gpu_1 [label="Expert2\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 1" fillcolor="#ff1480" shape=rectangle]
			stage0_expert_layer_1_expert_3_gpu_1 [label="Expert3\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 1" fillcolor="#ff1480" shape=rectangle]
			stage0_expert_layer_1_expert_4_gpu_2 [label="Expert4\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 2" fillcolor="#ff2880" shape=rectangle]
			stage0_expert_layer_1_expert_5_gpu_2 [label="Expert5\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 2" fillcolor="#ff2880" shape=rectangle]
			stage0_expert_layer_1_expert_6_gpu_3 [label="Expert6\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 3" fillcolor="#ff3c80" shape=rectangle]
			stage0_expert_layer_1_expert_7_gpu_3 [label="Expert7\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 3" fillcolor="#ff3c80" shape=rectangle]
			stage0_expert_layer_1_expert_8_gpu_4 [label="Expert8\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 4" fillcolor="#ff5080" shape=rectangle]
			stage0_expert_layer_1_expert_9_gpu_4 [label="Expert9\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 4" fillcolor="#ff5080" shape=rectangle]
			stage0_expert_layer_1_expert_10_gpu_5 [label="Expert10\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 5" fillcolor="#ff6480" shape=rectangle]
			stage0_expert_layer_1_expert_11_gpu_5 [label="Expert11\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 5" fillcolor="#ff6480" shape=rectangle]
			stage0_expert_layer_1_expert_12_gpu_6 [label="Expert12\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 6" fillcolor="#ff7880" shape=rectangle]
			stage0_expert_layer_1_expert_13_gpu_6 [label="Expert13\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 6" fillcolor="#ff7880" shape=rectangle]
			stage0_expert_layer_1_expert_14_gpu_7 [label="Expert14\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 7" fillcolor="#ff8c80" shape=rectangle]
			stage0_expert_layer_1_expert_15_gpu_7 [label="Expert15\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 7" fillcolor="#ff8c80" shape=rectangle]
			stage0_expert_agg_layer_1 [label="Expert Aggregation\nInput: [8× batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (all-reduce)" fillcolor="#ffcccc" shape=parallelogram]
			stage0_residual2_layer_1 [label="Residual Add 2\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 0-7 (all-reduce)" fillcolor="#ffffcc" shape=parallelogram]
		}
	}
	subgraph cluster_stage_1 {
		color=purple label="Stage 1 (GPUs 8-15)" style=dotted
		subgraph cluster_stage1_layer_2 {
			color=purple label="Layer 2" style=dashed
			stage1_ln1_layer_2 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (sharded)" fillcolor="#0080ff" shape=rectangle]
			stage1_qkv_layer_2_gpu_8 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 8" fillcolor="#0080ff" shape=rectangle]
			stage1_qkv_layer_2_gpu_9 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 9" fillcolor="#1e80ff" shape=rectangle]
			stage1_qkv_layer_2_gpu_10 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 10" fillcolor="#3c80ff" shape=rectangle]
			stage1_qkv_layer_2_gpu_11 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 11" fillcolor="#5a80ff" shape=rectangle]
			stage1_qkv_layer_2_gpu_12 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 12" fillcolor="#7880ff" shape=rectangle]
			stage1_qkv_layer_2_gpu_13 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 13" fillcolor="#9680ff" shape=rectangle]
			stage1_qkv_layer_2_gpu_14 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 14" fillcolor="#b480ff" shape=rectangle]
			stage1_qkv_layer_2_gpu_15 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 15" fillcolor="#d280ff" shape=rectangle]
			stage1_attention_layer_2_gpu_8 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 8" fillcolor="#0080ff" shape=rectangle]
			stage1_attention_layer_2_gpu_9 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 9" fillcolor="#1e80ff" shape=rectangle]
			stage1_attention_layer_2_gpu_10 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 10" fillcolor="#3c80ff" shape=rectangle]
			stage1_attention_layer_2_gpu_11 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 11" fillcolor="#5a80ff" shape=rectangle]
			stage1_attention_layer_2_gpu_12 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 12" fillcolor="#7880ff" shape=rectangle]
			stage1_attention_layer_2_gpu_13 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 13" fillcolor="#9680ff" shape=rectangle]
			stage1_attention_layer_2_gpu_14 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 14" fillcolor="#b480ff" shape=rectangle]
			stage1_attention_layer_2_gpu_15 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 15" fillcolor="#d280ff" shape=rectangle]
			stage1_attention_reduce_layer_2 [label="Attention All-Reduce\nInput: [8× batch_size=1024, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (all-reduce)" fillcolor="#ffcccc" shape=parallelogram]
			stage1_out_proj_layer_2 [label="Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (sharded)" fillcolor="#0080ff" shape=rectangle]
			stage1_residual1_layer_2 [label="Residual Add 1\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (all-reduce)" fillcolor="#ffffcc" shape=parallelogram]
			stage1_ln2_layer_2 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (sharded)" fillcolor="#0080ff" shape=rectangle]
			stage1_gate_layer_2 [label="Gating Network\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 8-15 (sharded)" fillcolor="#ccffcc" shape=parallelogram style=dashed]
			stage1_expert_layer_2_expert_0_gpu_8 [label="Expert0\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 8" fillcolor="#0080ff" shape=rectangle]
			stage1_expert_layer_2_expert_1_gpu_8 [label="Expert1\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 8" fillcolor="#0080ff" shape=rectangle]
			stage1_expert_layer_2_expert_2_gpu_9 [label="Expert2\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 9" fillcolor="#1e80ff" shape=rectangle]
			stage1_expert_layer_2_expert_3_gpu_9 [label="Expert3\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 9" fillcolor="#1e80ff" shape=rectangle]
			stage1_expert_layer_2_expert_4_gpu_10 [label="Expert4\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 10" fillcolor="#3c80ff" shape=rectangle]
			stage1_expert_layer_2_expert_5_gpu_10 [label="Expert5\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 10" fillcolor="#3c80ff" shape=rectangle]
			stage1_expert_layer_2_expert_6_gpu_11 [label="Expert6\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 11" fillcolor="#5a80ff" shape=rectangle]
			stage1_expert_layer_2_expert_7_gpu_11 [label="Expert7\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 11" fillcolor="#5a80ff" shape=rectangle]
			stage1_expert_layer_2_expert_8_gpu_12 [label="Expert8\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 12" fillcolor="#7880ff" shape=rectangle]
			stage1_expert_layer_2_expert_9_gpu_12 [label="Expert9\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 12" fillcolor="#7880ff" shape=rectangle]
			stage1_expert_layer_2_expert_10_gpu_13 [label="Expert10\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 13" fillcolor="#9680ff" shape=rectangle]
			stage1_expert_layer_2_expert_11_gpu_13 [label="Expert11\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 13" fillcolor="#9680ff" shape=rectangle]
			stage1_expert_layer_2_expert_12_gpu_14 [label="Expert12\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 14" fillcolor="#b480ff" shape=rectangle]
			stage1_expert_layer_2_expert_13_gpu_14 [label="Expert13\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 14" fillcolor="#b480ff" shape=rectangle]
			stage1_expert_layer_2_expert_14_gpu_15 [label="Expert14\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 15" fillcolor="#d280ff" shape=rectangle]
			stage1_expert_layer_2_expert_15_gpu_15 [label="Expert15\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 15" fillcolor="#d280ff" shape=rectangle]
			stage1_expert_agg_layer_2 [label="Expert Aggregation\nInput: [8× batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (all-reduce)" fillcolor="#ffcccc" shape=parallelogram]
			stage1_residual2_layer_2 [label="Residual Add 2\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (all-reduce)" fillcolor="#ffffcc" shape=parallelogram]
		}
		subgraph cluster_stage1_layer_3 {
			color=purple label="Layer 3" style=dashed
			stage1_ln1_layer_3 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (sharded)" fillcolor="#0080ff" shape=rectangle]
			stage1_qkv_layer_3_gpu_8 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 8" fillcolor="#0080ff" shape=rectangle]
			stage1_qkv_layer_3_gpu_9 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 9" fillcolor="#1e80ff" shape=rectangle]
			stage1_qkv_layer_3_gpu_10 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 10" fillcolor="#3c80ff" shape=rectangle]
			stage1_qkv_layer_3_gpu_11 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 11" fillcolor="#5a80ff" shape=rectangle]
			stage1_qkv_layer_3_gpu_12 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 12" fillcolor="#7880ff" shape=rectangle]
			stage1_qkv_layer_3_gpu_13 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 13" fillcolor="#9680ff" shape=rectangle]
			stage1_qkv_layer_3_gpu_14 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 14" fillcolor="#b480ff" shape=rectangle]
			stage1_qkv_layer_3_gpu_15 [label="QKV Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nV:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 15" fillcolor="#d280ff" shape=rectangle]
			stage1_attention_layer_3_gpu_8 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 8" fillcolor="#0080ff" shape=rectangle]
			stage1_attention_layer_3_gpu_9 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 9" fillcolor="#1e80ff" shape=rectangle]
			stage1_attention_layer_3_gpu_10 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 10" fillcolor="#3c80ff" shape=rectangle]
			stage1_attention_layer_3_gpu_11 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 11" fillcolor="#5a80ff" shape=rectangle]
			stage1_attention_layer_3_gpu_12 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 12" fillcolor="#7880ff" shape=rectangle]
			stage1_attention_layer_3_gpu_13 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 13" fillcolor="#9680ff" shape=rectangle]
			stage1_attention_layer_3_gpu_14 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 14" fillcolor="#b480ff" shape=rectangle]
			stage1_attention_layer_3_gpu_15 [label="Multi-Head Attention\nInput: Q:[batch_size=1024, seq_len=2048, heads=4, d_k=128]\nK_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nV_all:[batch_size=1024, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, d_k=128]\nGPU: 15" fillcolor="#d280ff" shape=rectangle]
			stage1_attention_reduce_layer_3 [label="Attention All-Reduce\nInput: [8× batch_size=1024, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (all-reduce)" fillcolor="#ffcccc" shape=parallelogram]
			stage1_out_proj_layer_3 [label="Output Projection\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (sharded)" fillcolor="#0080ff" shape=rectangle]
			stage1_residual1_layer_3 [label="Residual Add 1\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (all-reduce)" fillcolor="#ffffcc" shape=parallelogram]
			stage1_ln2_layer_3 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (sharded)" fillcolor="#0080ff" shape=rectangle]
			stage1_gate_layer_3 [label="Gating Network\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 8-15 (sharded)" fillcolor="#ccffcc" shape=parallelogram style=dashed]
			stage1_expert_layer_3_expert_0_gpu_8 [label="Expert0\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 8" fillcolor="#0080ff" shape=rectangle]
			stage1_expert_layer_3_expert_1_gpu_8 [label="Expert1\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 8" fillcolor="#0080ff" shape=rectangle]
			stage1_expert_layer_3_expert_2_gpu_9 [label="Expert2\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 9" fillcolor="#1e80ff" shape=rectangle]
			stage1_expert_layer_3_expert_3_gpu_9 [label="Expert3\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 9" fillcolor="#1e80ff" shape=rectangle]
			stage1_expert_layer_3_expert_4_gpu_10 [label="Expert4\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 10" fillcolor="#3c80ff" shape=rectangle]
			stage1_expert_layer_3_expert_5_gpu_10 [label="Expert5\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 10" fillcolor="#3c80ff" shape=rectangle]
			stage1_expert_layer_3_expert_6_gpu_11 [label="Expert6\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 11" fillcolor="#5a80ff" shape=rectangle]
			stage1_expert_layer_3_expert_7_gpu_11 [label="Expert7\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 11" fillcolor="#5a80ff" shape=rectangle]
			stage1_expert_layer_3_expert_8_gpu_12 [label="Expert8\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 12" fillcolor="#7880ff" shape=rectangle]
			stage1_expert_layer_3_expert_9_gpu_12 [label="Expert9\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 12" fillcolor="#7880ff" shape=rectangle]
			stage1_expert_layer_3_expert_10_gpu_13 [label="Expert10\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 13" fillcolor="#9680ff" shape=rectangle]
			stage1_expert_layer_3_expert_11_gpu_13 [label="Expert11\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 13" fillcolor="#9680ff" shape=rectangle]
			stage1_expert_layer_3_expert_12_gpu_14 [label="Expert12\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 14" fillcolor="#b480ff" shape=rectangle]
			stage1_expert_layer_3_expert_13_gpu_14 [label="Expert13\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 14" fillcolor="#b480ff" shape=rectangle]
			stage1_expert_layer_3_expert_14_gpu_15 [label="Expert14\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 15" fillcolor="#d280ff" shape=rectangle]
			stage1_expert_layer_3_expert_15_gpu_15 [label="Expert15\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096/8]\nGPU: 15" fillcolor="#d280ff" shape=rectangle]
			stage1_expert_agg_layer_3 [label="Expert Aggregation\nInput: [8× batch_size=1024, seq_len=2048, hidden_size=4096/8]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (all-reduce)" fillcolor="#ffcccc" shape=parallelogram]
			stage1_residual2_layer_3 [label="Residual Add 2\nInput: [2× batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15 (all-reduce)" fillcolor="#ffffcc" shape=parallelogram]
		}
	}
	final_layernorm [label="Final LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nGPU: 8-15" fillcolor="#0080ff" shape=rectangle]
	output [label="Output\nInput: [batch_size=1024, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=2048, vocab_size=50265]\nGPU: 8-15" fillcolor="#e6f3ff" shape=ellipse]
	input -> embedding
	embedding -> stage0_ln1_layer_0
	stage0_ln1_layer_0 -> stage0_qkv_layer_0_gpu_0
	stage0_qkv_layer_0_gpu_0 -> stage0_attention_layer_0_gpu_0
	stage0_qkv_layer_0_gpu_1 -> stage0_attention_layer_0_gpu_0 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_2 -> stage0_attention_layer_0_gpu_0 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_3 -> stage0_attention_layer_0_gpu_0 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_4 -> stage0_attention_layer_0_gpu_0 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_5 -> stage0_attention_layer_0_gpu_0 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_6 -> stage0_attention_layer_0_gpu_0 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_7 -> stage0_attention_layer_0_gpu_0 [color=blue style=dashed]
	stage0_ln1_layer_0 -> stage0_qkv_layer_0_gpu_1
	stage0_qkv_layer_0_gpu_1 -> stage0_attention_layer_0_gpu_1
	stage0_qkv_layer_0_gpu_0 -> stage0_attention_layer_0_gpu_1 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_2 -> stage0_attention_layer_0_gpu_1 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_3 -> stage0_attention_layer_0_gpu_1 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_4 -> stage0_attention_layer_0_gpu_1 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_5 -> stage0_attention_layer_0_gpu_1 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_6 -> stage0_attention_layer_0_gpu_1 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_7 -> stage0_attention_layer_0_gpu_1 [color=blue style=dashed]
	stage0_ln1_layer_0 -> stage0_qkv_layer_0_gpu_2
	stage0_qkv_layer_0_gpu_2 -> stage0_attention_layer_0_gpu_2
	stage0_qkv_layer_0_gpu_0 -> stage0_attention_layer_0_gpu_2 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_1 -> stage0_attention_layer_0_gpu_2 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_3 -> stage0_attention_layer_0_gpu_2 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_4 -> stage0_attention_layer_0_gpu_2 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_5 -> stage0_attention_layer_0_gpu_2 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_6 -> stage0_attention_layer_0_gpu_2 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_7 -> stage0_attention_layer_0_gpu_2 [color=blue style=dashed]
	stage0_ln1_layer_0 -> stage0_qkv_layer_0_gpu_3
	stage0_qkv_layer_0_gpu_3 -> stage0_attention_layer_0_gpu_3
	stage0_qkv_layer_0_gpu_0 -> stage0_attention_layer_0_gpu_3 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_1 -> stage0_attention_layer_0_gpu_3 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_2 -> stage0_attention_layer_0_gpu_3 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_4 -> stage0_attention_layer_0_gpu_3 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_5 -> stage0_attention_layer_0_gpu_3 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_6 -> stage0_attention_layer_0_gpu_3 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_7 -> stage0_attention_layer_0_gpu_3 [color=blue style=dashed]
	stage0_ln1_layer_0 -> stage0_qkv_layer_0_gpu_4
	stage0_qkv_layer_0_gpu_4 -> stage0_attention_layer_0_gpu_4
	stage0_qkv_layer_0_gpu_0 -> stage0_attention_layer_0_gpu_4 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_1 -> stage0_attention_layer_0_gpu_4 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_2 -> stage0_attention_layer_0_gpu_4 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_3 -> stage0_attention_layer_0_gpu_4 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_5 -> stage0_attention_layer_0_gpu_4 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_6 -> stage0_attention_layer_0_gpu_4 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_7 -> stage0_attention_layer_0_gpu_4 [color=blue style=dashed]
	stage0_ln1_layer_0 -> stage0_qkv_layer_0_gpu_5
	stage0_qkv_layer_0_gpu_5 -> stage0_attention_layer_0_gpu_5
	stage0_qkv_layer_0_gpu_0 -> stage0_attention_layer_0_gpu_5 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_1 -> stage0_attention_layer_0_gpu_5 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_2 -> stage0_attention_layer_0_gpu_5 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_3 -> stage0_attention_layer_0_gpu_5 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_4 -> stage0_attention_layer_0_gpu_5 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_6 -> stage0_attention_layer_0_gpu_5 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_7 -> stage0_attention_layer_0_gpu_5 [color=blue style=dashed]
	stage0_ln1_layer_0 -> stage0_qkv_layer_0_gpu_6
	stage0_qkv_layer_0_gpu_6 -> stage0_attention_layer_0_gpu_6
	stage0_qkv_layer_0_gpu_0 -> stage0_attention_layer_0_gpu_6 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_1 -> stage0_attention_layer_0_gpu_6 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_2 -> stage0_attention_layer_0_gpu_6 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_3 -> stage0_attention_layer_0_gpu_6 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_4 -> stage0_attention_layer_0_gpu_6 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_5 -> stage0_attention_layer_0_gpu_6 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_7 -> stage0_attention_layer_0_gpu_6 [color=blue style=dashed]
	stage0_ln1_layer_0 -> stage0_qkv_layer_0_gpu_7
	stage0_qkv_layer_0_gpu_7 -> stage0_attention_layer_0_gpu_7
	stage0_qkv_layer_0_gpu_0 -> stage0_attention_layer_0_gpu_7 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_1 -> stage0_attention_layer_0_gpu_7 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_2 -> stage0_attention_layer_0_gpu_7 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_3 -> stage0_attention_layer_0_gpu_7 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_4 -> stage0_attention_layer_0_gpu_7 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_5 -> stage0_attention_layer_0_gpu_7 [color=blue style=dashed]
	stage0_qkv_layer_0_gpu_6 -> stage0_attention_layer_0_gpu_7 [color=blue style=dashed]
	stage0_attention_layer_0_gpu_0 -> stage0_attention_reduce_layer_0
	stage0_attention_layer_0_gpu_1 -> stage0_attention_reduce_layer_0
	stage0_attention_layer_0_gpu_2 -> stage0_attention_reduce_layer_0
	stage0_attention_layer_0_gpu_3 -> stage0_attention_reduce_layer_0
	stage0_attention_layer_0_gpu_4 -> stage0_attention_reduce_layer_0
	stage0_attention_layer_0_gpu_5 -> stage0_attention_reduce_layer_0
	stage0_attention_layer_0_gpu_6 -> stage0_attention_reduce_layer_0
	stage0_attention_layer_0_gpu_7 -> stage0_attention_reduce_layer_0
	stage0_attention_reduce_layer_0 -> stage0_out_proj_layer_0
	stage0_out_proj_layer_0 -> stage0_residual1_layer_0
	embedding -> stage0_residual1_layer_0
	stage0_residual1_layer_0 -> stage0_ln2_layer_0
	stage0_ln2_layer_0 -> stage0_gate_layer_0 [style=dashed]
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_0_gpu_0 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_0_gpu_0
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_1_gpu_0 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_1_gpu_0
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_2_gpu_1 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_2_gpu_1
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_3_gpu_1 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_3_gpu_1
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_4_gpu_2 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_4_gpu_2
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_5_gpu_2 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_5_gpu_2
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_6_gpu_3 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_6_gpu_3
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_7_gpu_3 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_7_gpu_3
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_8_gpu_4 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_8_gpu_4
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_9_gpu_4 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_9_gpu_4
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_10_gpu_5 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_10_gpu_5
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_11_gpu_5 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_11_gpu_5
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_12_gpu_6 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_12_gpu_6
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_13_gpu_6 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_13_gpu_6
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_14_gpu_7 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_14_gpu_7
	stage0_gate_layer_0 -> stage0_expert_layer_0_expert_15_gpu_7 [color=red style=dashed]
	stage0_ln2_layer_0 -> stage0_expert_layer_0_expert_15_gpu_7
	stage0_expert_layer_0_expert_0_gpu_0 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_1_gpu_0 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_2_gpu_1 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_3_gpu_1 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_4_gpu_2 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_5_gpu_2 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_6_gpu_3 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_7_gpu_3 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_8_gpu_4 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_9_gpu_4 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_10_gpu_5 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_11_gpu_5 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_12_gpu_6 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_13_gpu_6 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_14_gpu_7 -> stage0_expert_agg_layer_0
	stage0_expert_layer_0_expert_15_gpu_7 -> stage0_expert_agg_layer_0
	stage0_expert_agg_layer_0 -> stage0_residual2_layer_0
	stage0_residual1_layer_0 -> stage0_residual2_layer_0
	stage0_residual2_layer_0 -> stage0_ln1_layer_1
	stage0_ln1_layer_1 -> stage0_qkv_layer_1_gpu_0
	stage0_qkv_layer_1_gpu_0 -> stage0_attention_layer_1_gpu_0
	stage0_qkv_layer_1_gpu_1 -> stage0_attention_layer_1_gpu_0 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_2 -> stage0_attention_layer_1_gpu_0 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_3 -> stage0_attention_layer_1_gpu_0 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_4 -> stage0_attention_layer_1_gpu_0 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_5 -> stage0_attention_layer_1_gpu_0 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_6 -> stage0_attention_layer_1_gpu_0 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_7 -> stage0_attention_layer_1_gpu_0 [color=blue style=dashed]
	stage0_ln1_layer_1 -> stage0_qkv_layer_1_gpu_1
	stage0_qkv_layer_1_gpu_1 -> stage0_attention_layer_1_gpu_1
	stage0_qkv_layer_1_gpu_0 -> stage0_attention_layer_1_gpu_1 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_2 -> stage0_attention_layer_1_gpu_1 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_3 -> stage0_attention_layer_1_gpu_1 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_4 -> stage0_attention_layer_1_gpu_1 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_5 -> stage0_attention_layer_1_gpu_1 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_6 -> stage0_attention_layer_1_gpu_1 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_7 -> stage0_attention_layer_1_gpu_1 [color=blue style=dashed]
	stage0_ln1_layer_1 -> stage0_qkv_layer_1_gpu_2
	stage0_qkv_layer_1_gpu_2 -> stage0_attention_layer_1_gpu_2
	stage0_qkv_layer_1_gpu_0 -> stage0_attention_layer_1_gpu_2 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_1 -> stage0_attention_layer_1_gpu_2 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_3 -> stage0_attention_layer_1_gpu_2 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_4 -> stage0_attention_layer_1_gpu_2 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_5 -> stage0_attention_layer_1_gpu_2 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_6 -> stage0_attention_layer_1_gpu_2 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_7 -> stage0_attention_layer_1_gpu_2 [color=blue style=dashed]
	stage0_ln1_layer_1 -> stage0_qkv_layer_1_gpu_3
	stage0_qkv_layer_1_gpu_3 -> stage0_attention_layer_1_gpu_3
	stage0_qkv_layer_1_gpu_0 -> stage0_attention_layer_1_gpu_3 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_1 -> stage0_attention_layer_1_gpu_3 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_2 -> stage0_attention_layer_1_gpu_3 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_4 -> stage0_attention_layer_1_gpu_3 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_5 -> stage0_attention_layer_1_gpu_3 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_6 -> stage0_attention_layer_1_gpu_3 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_7 -> stage0_attention_layer_1_gpu_3 [color=blue style=dashed]
	stage0_ln1_layer_1 -> stage0_qkv_layer_1_gpu_4
	stage0_qkv_layer_1_gpu_4 -> stage0_attention_layer_1_gpu_4
	stage0_qkv_layer_1_gpu_0 -> stage0_attention_layer_1_gpu_4 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_1 -> stage0_attention_layer_1_gpu_4 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_2 -> stage0_attention_layer_1_gpu_4 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_3 -> stage0_attention_layer_1_gpu_4 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_5 -> stage0_attention_layer_1_gpu_4 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_6 -> stage0_attention_layer_1_gpu_4 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_7 -> stage0_attention_layer_1_gpu_4 [color=blue style=dashed]
	stage0_ln1_layer_1 -> stage0_qkv_layer_1_gpu_5
	stage0_qkv_layer_1_gpu_5 -> stage0_attention_layer_1_gpu_5
	stage0_qkv_layer_1_gpu_0 -> stage0_attention_layer_1_gpu_5 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_1 -> stage0_attention_layer_1_gpu_5 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_2 -> stage0_attention_layer_1_gpu_5 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_3 -> stage0_attention_layer_1_gpu_5 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_4 -> stage0_attention_layer_1_gpu_5 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_6 -> stage0_attention_layer_1_gpu_5 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_7 -> stage0_attention_layer_1_gpu_5 [color=blue style=dashed]
	stage0_ln1_layer_1 -> stage0_qkv_layer_1_gpu_6
	stage0_qkv_layer_1_gpu_6 -> stage0_attention_layer_1_gpu_6
	stage0_qkv_layer_1_gpu_0 -> stage0_attention_layer_1_gpu_6 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_1 -> stage0_attention_layer_1_gpu_6 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_2 -> stage0_attention_layer_1_gpu_6 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_3 -> stage0_attention_layer_1_gpu_6 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_4 -> stage0_attention_layer_1_gpu_6 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_5 -> stage0_attention_layer_1_gpu_6 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_7 -> stage0_attention_layer_1_gpu_6 [color=blue style=dashed]
	stage0_ln1_layer_1 -> stage0_qkv_layer_1_gpu_7
	stage0_qkv_layer_1_gpu_7 -> stage0_attention_layer_1_gpu_7
	stage0_qkv_layer_1_gpu_0 -> stage0_attention_layer_1_gpu_7 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_1 -> stage0_attention_layer_1_gpu_7 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_2 -> stage0_attention_layer_1_gpu_7 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_3 -> stage0_attention_layer_1_gpu_7 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_4 -> stage0_attention_layer_1_gpu_7 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_5 -> stage0_attention_layer_1_gpu_7 [color=blue style=dashed]
	stage0_qkv_layer_1_gpu_6 -> stage0_attention_layer_1_gpu_7 [color=blue style=dashed]
	stage0_attention_layer_1_gpu_0 -> stage0_attention_reduce_layer_1
	stage0_attention_layer_1_gpu_1 -> stage0_attention_reduce_layer_1
	stage0_attention_layer_1_gpu_2 -> stage0_attention_reduce_layer_1
	stage0_attention_layer_1_gpu_3 -> stage0_attention_reduce_layer_1
	stage0_attention_layer_1_gpu_4 -> stage0_attention_reduce_layer_1
	stage0_attention_layer_1_gpu_5 -> stage0_attention_reduce_layer_1
	stage0_attention_layer_1_gpu_6 -> stage0_attention_reduce_layer_1
	stage0_attention_layer_1_gpu_7 -> stage0_attention_reduce_layer_1
	stage0_attention_reduce_layer_1 -> stage0_out_proj_layer_1
	stage0_out_proj_layer_1 -> stage0_residual1_layer_1
	stage0_residual2_layer_0 -> stage0_residual1_layer_1
	stage0_residual1_layer_1 -> stage0_ln2_layer_1
	stage0_ln2_layer_1 -> stage0_gate_layer_1 [style=dashed]
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_0_gpu_0 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_0_gpu_0
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_1_gpu_0 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_1_gpu_0
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_2_gpu_1 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_2_gpu_1
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_3_gpu_1 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_3_gpu_1
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_4_gpu_2 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_4_gpu_2
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_5_gpu_2 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_5_gpu_2
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_6_gpu_3 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_6_gpu_3
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_7_gpu_3 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_7_gpu_3
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_8_gpu_4 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_8_gpu_4
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_9_gpu_4 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_9_gpu_4
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_10_gpu_5 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_10_gpu_5
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_11_gpu_5 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_11_gpu_5
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_12_gpu_6 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_12_gpu_6
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_13_gpu_6 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_13_gpu_6
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_14_gpu_7 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_14_gpu_7
	stage0_gate_layer_1 -> stage0_expert_layer_1_expert_15_gpu_7 [color=red style=dashed]
	stage0_ln2_layer_1 -> stage0_expert_layer_1_expert_15_gpu_7
	stage0_expert_layer_1_expert_0_gpu_0 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_1_gpu_0 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_2_gpu_1 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_3_gpu_1 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_4_gpu_2 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_5_gpu_2 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_6_gpu_3 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_7_gpu_3 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_8_gpu_4 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_9_gpu_4 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_10_gpu_5 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_11_gpu_5 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_12_gpu_6 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_13_gpu_6 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_14_gpu_7 -> stage0_expert_agg_layer_1
	stage0_expert_layer_1_expert_15_gpu_7 -> stage0_expert_agg_layer_1
	stage0_expert_agg_layer_1 -> stage0_residual2_layer_1
	stage0_residual1_layer_1 -> stage0_residual2_layer_1
	stage0_residual2_layer_1 -> stage1_ln1_layer_2 [label="Send/Recv
Stage 0 → 1" color=purple style=bold]
	stage1_ln1_layer_2 -> stage1_qkv_layer_2_gpu_8
	stage1_qkv_layer_2_gpu_8 -> stage1_attention_layer_2_gpu_8
	stage1_qkv_layer_2_gpu_9 -> stage1_attention_layer_2_gpu_8 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_10 -> stage1_attention_layer_2_gpu_8 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_11 -> stage1_attention_layer_2_gpu_8 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_12 -> stage1_attention_layer_2_gpu_8 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_13 -> stage1_attention_layer_2_gpu_8 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_14 -> stage1_attention_layer_2_gpu_8 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_15 -> stage1_attention_layer_2_gpu_8 [color=blue style=dashed]
	stage1_ln1_layer_2 -> stage1_qkv_layer_2_gpu_9
	stage1_qkv_layer_2_gpu_9 -> stage1_attention_layer_2_gpu_9
	stage1_qkv_layer_2_gpu_8 -> stage1_attention_layer_2_gpu_9 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_10 -> stage1_attention_layer_2_gpu_9 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_11 -> stage1_attention_layer_2_gpu_9 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_12 -> stage1_attention_layer_2_gpu_9 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_13 -> stage1_attention_layer_2_gpu_9 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_14 -> stage1_attention_layer_2_gpu_9 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_15 -> stage1_attention_layer_2_gpu_9 [color=blue style=dashed]
	stage1_ln1_layer_2 -> stage1_qkv_layer_2_gpu_10
	stage1_qkv_layer_2_gpu_10 -> stage1_attention_layer_2_gpu_10
	stage1_qkv_layer_2_gpu_8 -> stage1_attention_layer_2_gpu_10 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_9 -> stage1_attention_layer_2_gpu_10 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_11 -> stage1_attention_layer_2_gpu_10 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_12 -> stage1_attention_layer_2_gpu_10 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_13 -> stage1_attention_layer_2_gpu_10 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_14 -> stage1_attention_layer_2_gpu_10 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_15 -> stage1_attention_layer_2_gpu_10 [color=blue style=dashed]
	stage1_ln1_layer_2 -> stage1_qkv_layer_2_gpu_11
	stage1_qkv_layer_2_gpu_11 -> stage1_attention_layer_2_gpu_11
	stage1_qkv_layer_2_gpu_8 -> stage1_attention_layer_2_gpu_11 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_9 -> stage1_attention_layer_2_gpu_11 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_10 -> stage1_attention_layer_2_gpu_11 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_12 -> stage1_attention_layer_2_gpu_11 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_13 -> stage1_attention_layer_2_gpu_11 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_14 -> stage1_attention_layer_2_gpu_11 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_15 -> stage1_attention_layer_2_gpu_11 [color=blue style=dashed]
	stage1_ln1_layer_2 -> stage1_qkv_layer_2_gpu_12
	stage1_qkv_layer_2_gpu_12 -> stage1_attention_layer_2_gpu_12
	stage1_qkv_layer_2_gpu_8 -> stage1_attention_layer_2_gpu_12 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_9 -> stage1_attention_layer_2_gpu_12 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_10 -> stage1_attention_layer_2_gpu_12 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_11 -> stage1_attention_layer_2_gpu_12 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_13 -> stage1_attention_layer_2_gpu_12 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_14 -> stage1_attention_layer_2_gpu_12 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_15 -> stage1_attention_layer_2_gpu_12 [color=blue style=dashed]
	stage1_ln1_layer_2 -> stage1_qkv_layer_2_gpu_13
	stage1_qkv_layer_2_gpu_13 -> stage1_attention_layer_2_gpu_13
	stage1_qkv_layer_2_gpu_8 -> stage1_attention_layer_2_gpu_13 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_9 -> stage1_attention_layer_2_gpu_13 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_10 -> stage1_attention_layer_2_gpu_13 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_11 -> stage1_attention_layer_2_gpu_13 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_12 -> stage1_attention_layer_2_gpu_13 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_14 -> stage1_attention_layer_2_gpu_13 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_15 -> stage1_attention_layer_2_gpu_13 [color=blue style=dashed]
	stage1_ln1_layer_2 -> stage1_qkv_layer_2_gpu_14
	stage1_qkv_layer_2_gpu_14 -> stage1_attention_layer_2_gpu_14
	stage1_qkv_layer_2_gpu_8 -> stage1_attention_layer_2_gpu_14 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_9 -> stage1_attention_layer_2_gpu_14 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_10 -> stage1_attention_layer_2_gpu_14 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_11 -> stage1_attention_layer_2_gpu_14 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_12 -> stage1_attention_layer_2_gpu_14 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_13 -> stage1_attention_layer_2_gpu_14 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_15 -> stage1_attention_layer_2_gpu_14 [color=blue style=dashed]
	stage1_ln1_layer_2 -> stage1_qkv_layer_2_gpu_15
	stage1_qkv_layer_2_gpu_15 -> stage1_attention_layer_2_gpu_15
	stage1_qkv_layer_2_gpu_8 -> stage1_attention_layer_2_gpu_15 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_9 -> stage1_attention_layer_2_gpu_15 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_10 -> stage1_attention_layer_2_gpu_15 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_11 -> stage1_attention_layer_2_gpu_15 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_12 -> stage1_attention_layer_2_gpu_15 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_13 -> stage1_attention_layer_2_gpu_15 [color=blue style=dashed]
	stage1_qkv_layer_2_gpu_14 -> stage1_attention_layer_2_gpu_15 [color=blue style=dashed]
	stage1_attention_layer_2_gpu_8 -> stage1_attention_reduce_layer_2
	stage1_attention_layer_2_gpu_9 -> stage1_attention_reduce_layer_2
	stage1_attention_layer_2_gpu_10 -> stage1_attention_reduce_layer_2
	stage1_attention_layer_2_gpu_11 -> stage1_attention_reduce_layer_2
	stage1_attention_layer_2_gpu_12 -> stage1_attention_reduce_layer_2
	stage1_attention_layer_2_gpu_13 -> stage1_attention_reduce_layer_2
	stage1_attention_layer_2_gpu_14 -> stage1_attention_reduce_layer_2
	stage1_attention_layer_2_gpu_15 -> stage1_attention_reduce_layer_2
	stage1_attention_reduce_layer_2 -> stage1_out_proj_layer_2
	stage1_out_proj_layer_2 -> stage1_residual1_layer_2
	stage1_ln1_layer_2 -> stage1_residual1_layer_2
	stage1_residual1_layer_2 -> stage1_ln2_layer_2
	stage1_ln2_layer_2 -> stage1_gate_layer_2 [style=dashed]
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_0_gpu_8 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_0_gpu_8
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_1_gpu_8 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_1_gpu_8
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_2_gpu_9 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_2_gpu_9
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_3_gpu_9 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_3_gpu_9
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_4_gpu_10 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_4_gpu_10
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_5_gpu_10 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_5_gpu_10
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_6_gpu_11 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_6_gpu_11
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_7_gpu_11 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_7_gpu_11
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_8_gpu_12 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_8_gpu_12
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_9_gpu_12 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_9_gpu_12
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_10_gpu_13 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_10_gpu_13
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_11_gpu_13 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_11_gpu_13
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_12_gpu_14 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_12_gpu_14
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_13_gpu_14 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_13_gpu_14
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_14_gpu_15 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_14_gpu_15
	stage1_gate_layer_2 -> stage1_expert_layer_2_expert_15_gpu_15 [color=red style=dashed]
	stage1_ln2_layer_2 -> stage1_expert_layer_2_expert_15_gpu_15
	stage1_expert_layer_2_expert_0_gpu_8 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_1_gpu_8 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_2_gpu_9 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_3_gpu_9 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_4_gpu_10 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_5_gpu_10 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_6_gpu_11 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_7_gpu_11 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_8_gpu_12 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_9_gpu_12 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_10_gpu_13 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_11_gpu_13 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_12_gpu_14 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_13_gpu_14 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_14_gpu_15 -> stage1_expert_agg_layer_2
	stage1_expert_layer_2_expert_15_gpu_15 -> stage1_expert_agg_layer_2
	stage1_expert_agg_layer_2 -> stage1_residual2_layer_2
	stage1_residual1_layer_2 -> stage1_residual2_layer_2
	stage1_residual2_layer_2 -> stage1_ln1_layer_3
	stage1_ln1_layer_3 -> stage1_qkv_layer_3_gpu_8
	stage1_qkv_layer_3_gpu_8 -> stage1_attention_layer_3_gpu_8
	stage1_qkv_layer_3_gpu_9 -> stage1_attention_layer_3_gpu_8 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_10 -> stage1_attention_layer_3_gpu_8 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_11 -> stage1_attention_layer_3_gpu_8 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_12 -> stage1_attention_layer_3_gpu_8 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_13 -> stage1_attention_layer_3_gpu_8 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_14 -> stage1_attention_layer_3_gpu_8 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_15 -> stage1_attention_layer_3_gpu_8 [color=blue style=dashed]
	stage1_ln1_layer_3 -> stage1_qkv_layer_3_gpu_9
	stage1_qkv_layer_3_gpu_9 -> stage1_attention_layer_3_gpu_9
	stage1_qkv_layer_3_gpu_8 -> stage1_attention_layer_3_gpu_9 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_10 -> stage1_attention_layer_3_gpu_9 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_11 -> stage1_attention_layer_3_gpu_9 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_12 -> stage1_attention_layer_3_gpu_9 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_13 -> stage1_attention_layer_3_gpu_9 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_14 -> stage1_attention_layer_3_gpu_9 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_15 -> stage1_attention_layer_3_gpu_9 [color=blue style=dashed]
	stage1_ln1_layer_3 -> stage1_qkv_layer_3_gpu_10
	stage1_qkv_layer_3_gpu_10 -> stage1_attention_layer_3_gpu_10
	stage1_qkv_layer_3_gpu_8 -> stage1_attention_layer_3_gpu_10 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_9 -> stage1_attention_layer_3_gpu_10 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_11 -> stage1_attention_layer_3_gpu_10 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_12 -> stage1_attention_layer_3_gpu_10 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_13 -> stage1_attention_layer_3_gpu_10 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_14 -> stage1_attention_layer_3_gpu_10 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_15 -> stage1_attention_layer_3_gpu_10 [color=blue style=dashed]
	stage1_ln1_layer_3 -> stage1_qkv_layer_3_gpu_11
	stage1_qkv_layer_3_gpu_11 -> stage1_attention_layer_3_gpu_11
	stage1_qkv_layer_3_gpu_8 -> stage1_attention_layer_3_gpu_11 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_9 -> stage1_attention_layer_3_gpu_11 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_10 -> stage1_attention_layer_3_gpu_11 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_12 -> stage1_attention_layer_3_gpu_11 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_13 -> stage1_attention_layer_3_gpu_11 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_14 -> stage1_attention_layer_3_gpu_11 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_15 -> stage1_attention_layer_3_gpu_11 [color=blue style=dashed]
	stage1_ln1_layer_3 -> stage1_qkv_layer_3_gpu_12
	stage1_qkv_layer_3_gpu_12 -> stage1_attention_layer_3_gpu_12
	stage1_qkv_layer_3_gpu_8 -> stage1_attention_layer_3_gpu_12 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_9 -> stage1_attention_layer_3_gpu_12 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_10 -> stage1_attention_layer_3_gpu_12 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_11 -> stage1_attention_layer_3_gpu_12 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_13 -> stage1_attention_layer_3_gpu_12 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_14 -> stage1_attention_layer_3_gpu_12 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_15 -> stage1_attention_layer_3_gpu_12 [color=blue style=dashed]
	stage1_ln1_layer_3 -> stage1_qkv_layer_3_gpu_13
	stage1_qkv_layer_3_gpu_13 -> stage1_attention_layer_3_gpu_13
	stage1_qkv_layer_3_gpu_8 -> stage1_attention_layer_3_gpu_13 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_9 -> stage1_attention_layer_3_gpu_13 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_10 -> stage1_attention_layer_3_gpu_13 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_11 -> stage1_attention_layer_3_gpu_13 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_12 -> stage1_attention_layer_3_gpu_13 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_14 -> stage1_attention_layer_3_gpu_13 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_15 -> stage1_attention_layer_3_gpu_13 [color=blue style=dashed]
	stage1_ln1_layer_3 -> stage1_qkv_layer_3_gpu_14
	stage1_qkv_layer_3_gpu_14 -> stage1_attention_layer_3_gpu_14
	stage1_qkv_layer_3_gpu_8 -> stage1_attention_layer_3_gpu_14 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_9 -> stage1_attention_layer_3_gpu_14 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_10 -> stage1_attention_layer_3_gpu_14 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_11 -> stage1_attention_layer_3_gpu_14 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_12 -> stage1_attention_layer_3_gpu_14 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_13 -> stage1_attention_layer_3_gpu_14 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_15 -> stage1_attention_layer_3_gpu_14 [color=blue style=dashed]
	stage1_ln1_layer_3 -> stage1_qkv_layer_3_gpu_15
	stage1_qkv_layer_3_gpu_15 -> stage1_attention_layer_3_gpu_15
	stage1_qkv_layer_3_gpu_8 -> stage1_attention_layer_3_gpu_15 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_9 -> stage1_attention_layer_3_gpu_15 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_10 -> stage1_attention_layer_3_gpu_15 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_11 -> stage1_attention_layer_3_gpu_15 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_12 -> stage1_attention_layer_3_gpu_15 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_13 -> stage1_attention_layer_3_gpu_15 [color=blue style=dashed]
	stage1_qkv_layer_3_gpu_14 -> stage1_attention_layer_3_gpu_15 [color=blue style=dashed]
	stage1_attention_layer_3_gpu_8 -> stage1_attention_reduce_layer_3
	stage1_attention_layer_3_gpu_9 -> stage1_attention_reduce_layer_3
	stage1_attention_layer_3_gpu_10 -> stage1_attention_reduce_layer_3
	stage1_attention_layer_3_gpu_11 -> stage1_attention_reduce_layer_3
	stage1_attention_layer_3_gpu_12 -> stage1_attention_reduce_layer_3
	stage1_attention_layer_3_gpu_13 -> stage1_attention_reduce_layer_3
	stage1_attention_layer_3_gpu_14 -> stage1_attention_reduce_layer_3
	stage1_attention_layer_3_gpu_15 -> stage1_attention_reduce_layer_3
	stage1_attention_reduce_layer_3 -> stage1_out_proj_layer_3
	stage1_out_proj_layer_3 -> stage1_residual1_layer_3
	stage1_residual2_layer_2 -> stage1_residual1_layer_3
	stage1_residual1_layer_3 -> stage1_ln2_layer_3
	stage1_ln2_layer_3 -> stage1_gate_layer_3 [style=dashed]
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_0_gpu_8 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_0_gpu_8
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_1_gpu_8 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_1_gpu_8
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_2_gpu_9 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_2_gpu_9
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_3_gpu_9 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_3_gpu_9
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_4_gpu_10 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_4_gpu_10
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_5_gpu_10 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_5_gpu_10
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_6_gpu_11 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_6_gpu_11
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_7_gpu_11 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_7_gpu_11
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_8_gpu_12 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_8_gpu_12
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_9_gpu_12 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_9_gpu_12
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_10_gpu_13 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_10_gpu_13
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_11_gpu_13 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_11_gpu_13
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_12_gpu_14 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_12_gpu_14
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_13_gpu_14 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_13_gpu_14
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_14_gpu_15 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_14_gpu_15
	stage1_gate_layer_3 -> stage1_expert_layer_3_expert_15_gpu_15 [color=red style=dashed]
	stage1_ln2_layer_3 -> stage1_expert_layer_3_expert_15_gpu_15
	stage1_expert_layer_3_expert_0_gpu_8 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_1_gpu_8 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_2_gpu_9 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_3_gpu_9 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_4_gpu_10 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_5_gpu_10 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_6_gpu_11 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_7_gpu_11 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_8_gpu_12 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_9_gpu_12 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_10_gpu_13 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_11_gpu_13 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_12_gpu_14 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_13_gpu_14 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_14_gpu_15 -> stage1_expert_agg_layer_3
	stage1_expert_layer_3_expert_15_gpu_15 -> stage1_expert_agg_layer_3
	stage1_expert_agg_layer_3 -> stage1_residual2_layer_3
	stage1_residual1_layer_3 -> stage1_residual2_layer_3
	stage1_residual2_layer_3 -> final_layernorm
	final_layernorm -> output
}
