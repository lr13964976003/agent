{
  "generated_dags": [
    "../outputs/2025-11-26-15-44-02/moe_comprehensive_dag.svg",
    "../outputs/2025-11-26-15-44-02/moe_comprehensive_dag.dot",
    "../outputs/2025-11-26-15-44-02/moe_simplified_dag.svg",
    "../outputs/2025-11-26-15-44-02/moe_simplified_dag.dot",
    "../outputs/2025-11-26-15-44-02/moe_parallel_strategies.svg",
    "../outputs/2025-11-26-15-44-02/moe_parallel_strategies.dot"
  ],
  "deployment_strategy": "Large-scale cross-node expert parallelism with single expert per GPU (EP>=16)",
  "model_configuration": {
    "total_layers": 61,
    "dense_layers": 3,
    "moe_layers": 58,
    "experts_per_moe_layer": 16,
    "hidden_dimension": 7168,
    "attention_heads": 128,
    "head_dimension": 128,
    "mlp_hidden_size": 2048
  },
  "parallel_strategies": {
    "primary": "Expert Parallelism (EP>=16)",
    "secondary": ["Data Parallelism (DP)", "Tensor Parallelism (TP)"],
    "hierarchy": "DP -> EP -> TP"
  },
  "hardware_requirements": {
    "gpu_type": "H100",
    "gpus_needed": 16,
    "memory_per_gpu": "64GB",
    "computing_power": "400TFlops per GPU",
    "network": "NVLink + InfiniBand"
  },
  "optimization_features": {
    "expert_placement": "One expert per GPU per layer",
    "routing": "Top-K gating with asynchronous token routing",
    "communication": "Compute-communication overlap with token batching",
    "load_balancing": "Dynamic probability adjustment",
    "memory_management": "TP fallback when exceeding single GPU memory"
  }
}