digraph optimized_mha_layer_0_pipelined {
	rankdir=TB size="25,35"
	node [fillcolor=lightblue shape=ellipse style=filled]
	
	input [label="Pipeline Input\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 0-7" fillcolor=lightgreen shape=parallelogram]
	
	// Optimized single-layer norm distributed
	ln [label="Distributed LayerNorm\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 0-7" fillcolor=lightyellow shape=rectangle]
	
	// 8-way tensor parallel QKV projections (instead of 16-way)
	q_proj_0 [label="Q Projection\nDevice 0\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 0" fillcolor=lightcoral shape=rectangle]
	k_proj_0 [label="K Projection\nDevice 0\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 0" fillcolor=lightcoral shape=rectangle]
	v_proj_0 [label="V Projection\nDevice 0\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 0" fillcolor=lightcoral shape=rectangle]
	
	q_proj_1 [label="Q Projection\nDevice 1\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 1" fillcolor=lightcoral shape=rectangle]
	k_proj_1 [label="K Projection\nDevice 1\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 1" fillcolor=lightcoral shape=rectangle]
	v_proj_1 [label="V Projection\nDevice 1\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 1" fillcolor=lightcoral shape=rectangle]
	
	q_proj_2 [label="Q Projection\nDevice 2\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 2" fillcolor=lightcoral shape=rectangle]
	k_proj_2 [label="K Projection\nDevice 2\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 2" fillcolor=lightcoral shape=rectangle]
	v_proj_2 [label="V Projection\nDevice 2\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 2" fillcolor=lightcoral shape=rectangle]
	
	q_proj_3 [label="Q Projection\nDevice 3\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 3" fillcolor=lightcoral shape=rectangle]
	k_proj_3 [label="K Projection\nDevice 3\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 3" fillcolor=lightcoral shape=rectangle]
	v_proj_3 [label="V Projection\nDevice 3\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 3" fillcolor=lightcoral shape=rectangle]
	
	q_proj_4 [label="Q Projection\nDevice 4\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 4" fillcolor=lightcoral shape=rectangle]
	k_proj_4 [label="K Projection\nDevice 4\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 4" fillcolor=lightcoral shape=rectangle]
	v_proj_4 [label="V Projection\nDevice 4\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 4" fillcolor=lightcoral shape=rectangle]
	
	q_proj_5 [label="Q Projection\nDevice 5\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 5" fillcolor=lightcoral shape=rectangle]
	k_proj_5 [label="K Projection\nDevice 5\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 5" fillcolor=lightcoral shape=rectangle]
	v_proj_5 [label="V Projection\nDevice 5\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 5" fillcolor=lightcoral shape=rectangle]
	
	q_proj_6 [label="Q Projection\nDevice 6\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 6" fillcolor=lightcoral shape=rectangle]
	k_proj_6 [label="K Projection\nDevice 6\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 6" fillcolor=lightcoral shape=rectangle]
	v_proj_6 [label="V Projection\nDevice 6\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 6" fillcolor=lightcoral shape=rectangle]
	
	q_proj_7 [label="Q Projection\nDevice 7\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 7" fillcolor=lightcoral shape=rectangle]
	k_proj_7 [label="K Projection\nDevice 7\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 7" fillcolor=lightcoral shape=rectangle]
	v_proj_7 [label="V Projection\nDevice 7\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 7" fillcolor=lightcoral shape=rectangle]
	
	// Optimized attention computation with fused operations
	attn_0 [label="Fused Attention\nDevice 0\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 0" fillcolor=lightpink shape=rectangle]
	
	attn_1 [label="Fused Attention\nDevice 1\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 1" fillcolor=lightpink shape=rectangle]
	
	attn_2 [label="Fused Attention\nDevice 2\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 2" fillcolor=lightpink shape=rectangle]
	
	attn_3 [label="Fused Attention\nDevice 3\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 3" fillcolor=lightpink shape=rectangle]
	
	attn_4 [label="Fused Attention\nDevice 4\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 4" fillcolor=lightpink shape=rectangle]
	
	attn_5 [label="Fused Attention\nDevice 5\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 5" fillcolor=lightpink shape=rectangle]
	
	attn_6 [label="Fused Attention\nDevice 6\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 6" fillcolor=lightpink shape=rectangle]
	
	attn_7 [label="Fused Attention\nDevice 7\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 7" fillcolor=lightpink shape=rectangle]
	
	// Single all-reduce operation instead of multi-level concatenation
	output_proj [label="Output Projection\nTensor Parallel w/ All-Reduce\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 0-7" fillcolor=lightcoral shape=rectangle]
	
	residual [label="Residual Add\nInput 1: [batch_size=1024, seq_len=10000, embed_dim=8192]\nInput 2: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 0-7" fillcolor=lightgray shape=rectangle]
	
	// Connections
	input -> ln
	
	// Broadcast input to all devices efficiently
	ln -> q_proj_0
	ln -> k_proj_0
	ln -> v_proj_0
	ln -> q_proj_1
	ln -> k_proj_1
	ln -> v_proj_1
	ln -> q_proj_2
	ln -> k_proj_2
	ln -> v_proj_2
	ln -> q_proj_3
	ln -> k_proj_3
	ln -> v_proj_3
	ln -> q_proj_4
	ln -> k_proj_4
	ln -> v_proj_4
	ln -> q_proj_5
	ln -> k_proj_5
	ln -> v_proj_5
	ln -> q_proj_6
	ln -> k_proj_6
	ln -> v_proj_6
	ln -> q_proj_7
	ln -> k_proj_7
	ln -> v_proj_7
	
	// Attention computations
	q_proj_0 -> attn_0
	k_proj_0 -> attn_0
	v_proj_0 -> attn_0
	
	q_proj_1 -> attn_1
	k_proj_1 -> attn_1
	v_proj_1 -> attn_1
	
	q_proj_2 -> attn_2
	k_proj_2 -> attn_2
	v_proj_2 -> attn_2
	
	q_proj_3 -> attn_3
	k_proj_3 -> attn_3
	v_proj_3 -> attn_3
	
	q_proj_4 -> attn_4
	k_proj_4 -> attn_4
	v_proj_4 -> attn_4
	
	q_proj_5 -> attn_5
	k_proj_5 -> attn_5
	v_proj_5 -> attn_5
	
	q_proj_6 -> attn_6
	k_proj_6 -> attn_6
	v_proj_6 -> attn_6
	
	q_proj_7 -> attn_7
	k_proj_7 -> attn_7
	v_proj_7 -> attn_7
	
	// All attention outputs to projection
	attn_0 -> output_proj
	attn_1 -> output_proj
	attn_2 -> output_proj
	attn_3 -> output_proj
	attn_4 -> output_proj
	attn_5 -> output_proj
	attn_6 -> output_proj
	attn_7 -> output_proj
	
	output_proj -> residual
	input -> residual
}