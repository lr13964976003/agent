{
  "generated_dags": {
    "optimized_complete_model": {
      "dot_path": "./outputs/2025-10-16-09-28-20/optimized_complete_helix_model.dot",
      "svg_path": "./outputs/2025-10-16-09-28-20/optimized_complete_helix_model.svg"
    },
    "optimized_mha_layer_0": {
      "dot_path": "./outputs/2025-10-16-09-28-20/optimized_mha_layer_0_pipelined.dot",
      "svg_path": "./outputs/2025-10-16-09-28-20/optimized_mha_layer_0_pipelined.svg"
    },
    "optimized_mha_layer_1": {
      "dot_path": "./outputs/2025-10-16-09-28-20/optimized_mha_layer_1_pipelined.dot",
      "svg_path": "./outputs/2025-10-16-09-28-20/optimized_mha_layer_1_pipelined.svg"
    },
    "optimized_mlp_layer_0": {
      "dot_path": "./outputs/2025-10-16-09-28-20/optimized_mlp_layer_0_tensor_parallel.dot",
      "svg_path": "./outputs/2025-10-16-09-28-20/optimized_mlp_layer_0_tensor_parallel.svg"
    },
    "optimized_mlp_layer_1": {
      "dot_path": "./outputs/2025-10-16-09-28-20/optimized_mlp_layer_1_tensor_parallel.dot",
      "svg_path": "./outputs/2025-10-16-09-28-20/optimized_mlp_layer_1_tensor_parallel.svg"
    },
    "optimized_communication_patterns": {
      "dot_path": "./outputs/2025-10-16-09-28-20/optimized_communication_patterns.dot",
      "svg_path": "./outputs/2025-10-16-09-28-20/optimized_communication_patterns.svg"
    }
  },
  "optimization_summary": {
    "strategy": "Pipeline parallelism with 8-way tensor parallel per stage",
    "improvements": [
      "Reduced communication overhead from 16-way to 8-way partitioning",
      "Introduced pipeline stages enabling micro-batch overlap",
      "Fused attention operations reducing kernel launch overhead",
      "Optimized all-reduce operations with ring topology",
      "Distributed LayerNorm reducing synchronization points",
      "Better GPU utilization and load balancing across stages"
    ],
    "expected_tps_improvement": "~2-3x improvement through reduced communication latency and improved overlap",
    "architecture_changes": [
      "Pipeline parallelism across 2 stages (8 GPUs each) instead of 16-way single stage",
      "Reduced concatenation steps from multi-level to single-level",
      "Fused operations to reduce kernel launches",
      "Micro-batch overlap between stages"
    ]
  }
}