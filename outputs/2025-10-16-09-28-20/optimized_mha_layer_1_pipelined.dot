digraph optimized_mha_layer_1_pipelined {
	rankdir=TB size="25,35"
	node [fillcolor=lightblue shape=ellipse style=filled]
	
	input [label="Pipeline Input\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 8-15" fillcolor=lightgreen shape=parallelogram]
	
	// Distributed LayerNorm
	ln [label="Distributed LayerNorm\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 8-15" fillcolor=lightyellow shape=rectangle]
	
	// 8-way tensor parallel QKV projections for second pipeline stage
	q_proj_8 [label="Q Projection\nDevice 8\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 8" fillcolor=lightcoral shape=rectangle]
	k_proj_8 [label="K Projection\nDevice 8\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 8" fillcolor=lightcoral shape=rectangle]
	v_proj_8 [label="V Projection\nDevice 8\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 8" fillcolor=lightcoral shape=rectangle]
	
	q_proj_9 [label="Q Projection\nDevice 9\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 9" fillcolor=lightcoral shape=rectangle]
	k_proj_9 [label="K Projection\nDevice 9\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 9" fillcolor=lightcoral shape=rectangle]
	v_proj_9 [label="V Projection\nDevice 9\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 9" fillcolor=lightcoral shape=rectangle]
	
	q_proj_10 [label="Q Projection\nDevice 10\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 10" fillcolor=lightcoral shape=rectangle]
	k_proj_10 [label="K Projection\nDevice 10\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 10" fillcolor=lightcoral shape=rectangle]
	v_proj_10 [label="V Projection\nDevice 10\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 10" fillcolor=lightcoral shape=rectangle]
	
	q_proj_11 [label="Q Projection\nDevice 11\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 11" fillcolor=lightcoral shape=rectangle]
	k_proj_11 [label="K Projection\nDevice 11\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 11" fillcolor=lightcoral shape=rectangle]
	v_proj_11 [label="V Projection\nDevice 11\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 11" fillcolor=lightcoral shape=rectangle]
	
	q_proj_12 [label="Q Projection\nDevice 12\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 12" fillcolor=lightcoral shape=rectangle]
	k_proj_12 [label="K Projection\nDevice 12\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 12" fillcolor=lightcoral shape=rectangle]
	v_proj_12 [label="V Projection\nDevice 12\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 12" fillcolor=lightcoral shape=rectangle]
	
	q_proj_13 [label="Q Projection\nDevice 13\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 13" fillcolor=lightcoral shape=rectangle]
	k_proj_13 [label="K Projection\nDevice 13\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 13" fillcolor=lightcoral shape=rectangle]
	v_proj_13 [label="V Projection\nDevice 13\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 13" fillcolor=lightcoral shape=rectangle]
	
	q_proj_14 [label="Q Projection\nDevice 14\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 14" fillcolor=lightcoral shape=rectangle]
	k_proj_14 [label="K Projection\nDevice 14\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 14" fillcolor=lightcoral shape=rectangle]
	v_proj_14 [label="V Projection\nDevice 14\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 14" fillcolor=lightcoral shape=rectangle]
	
	q_proj_15 [label="Q Projection\nDevice 15\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 15" fillcolor=lightcoral shape=rectangle]
	k_proj_15 [label="K Projection\nDevice 15\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nGPU: 15" fillcolor=lightcoral shape=rectangle]
	v_proj_15 [label="V Projection\nDevice 15\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 15" fillcolor=lightcoral shape=rectangle]
	
	// Fused attention computations for second stage
	attn_8 [label="Fused Attention\nDevice 8\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 8" fillcolor=lightpink shape=rectangle]
	
	attn_9 [label="Fused Attention\nDevice 9\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 9" fillcolor=lightpink shape=rectangle]
	
	attn_10 [label="Fused Attention\nDevice 10\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 10" fillcolor=lightpink shape=rectangle]
	
	attn_11 [label="Fused Attention\nDevice 11\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 11" fillcolor=lightpink shape=rectangle]
	
	attn_12 [label="Fused Attention\nDevice 12\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 12" fillcolor=lightpink shape=rectangle]
	
	attn_13 [label="Fused Attention\nDevice 13\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 13" fillcolor=lightpink shape=rectangle]
	
	attn_14 [label="Fused Attention\nDevice 14\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 14" fillcolor=lightpink shape=rectangle]
	
	attn_15 [label="Fused Attention\nDevice 15\nQ: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nK: [batch_size=1024, seq_len=10000, heads=8, d_k=128]\nV: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nOutput: [batch_size=1024, seq_len=10000, heads=8, d_v=128]\nGPU: 15" fillcolor=lightpink shape=rectangle]
	
	// Optimized output projection for second stage
	output_proj [label="Output Projection\nTensor Parallel w/ All-Reduce\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 8-15" fillcolor=lightcoral shape=rectangle]
	
	residual [label="Residual Add\nInput 1: [batch_size=1024, seq_len=10000, embed_dim=8192]\nInput 2: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: 8-15" fillcolor=lightgray shape=rectangle]
	
	// Connections for second stage
	input -> ln
	
	// Broadcast to all devices in second stage
	ln -> q_proj_8
	ln -> k_proj_8
	ln -> v_proj_8
	ln -> q_proj_9
	ln -> k_proj_9
	ln -> v_proj_9
	ln -> q_proj_10
	ln -> k_proj_10
	ln -> v_proj_10
	ln -> q_proj_11
	ln -> k_proj_11
	ln -> v_proj_11
	ln -> q_proj_12
	ln -> k_proj_12
	ln -> v_proj_12
	ln -> q_proj_13
	ln -> k_proj_13
	ln -> v_proj_13
	ln -> q_proj_14
	ln -> k_proj_14
	ln -> v_proj_14
	ln -> q_proj_15
	ln -> k_proj_15
	ln -> v_proj_15
	
	// Attention computations
	q_proj_8 -> attn_8
	k_proj_8 -> attn_8
	v_proj_8 -> attn_8
	
	q_proj_9 -> attn_9
	k_proj_9 -> attn_9
	v_proj_9 -> attn_9
	
	q_proj_10 -> attn_10
	k_proj_10 -> attn_10
	v_proj_10 -> attn_10
	
	q_proj_11 -> attn_11
	k_proj_11 -> attn_11
	v_proj_11 -> attn_11
	
	q_proj_12 -> attn_12
	k_proj_12 -> attn_12
	v_proj_12 -> attn_12
	
	q_proj_13 -> attn_13
	k_proj_13 -> attn_13
	v_proj_13 -> attn_13
	
	q_proj_14 -> attn_14
	k_proj_14 -> attn_14
	v_proj_14 -> attn_14
	
	q_proj_15 -> attn_15
	k_proj_15 -> attn_15
	v_proj_15 -> attn_15
	
	// All attention outputs to projection
	attn_8 -> output_proj
	attn_9 -> output_proj
	attn_10 -> output_proj
	attn_11 -> output_proj
	attn_12 -> output_proj
	attn_13 -> output_proj
	attn_14 -> output_proj
	attn_15 -> output_proj
	
	output_proj -> residual
	input -> residual
}