digraph proposed_two_level_partitioning {
	rankdir=TB size="20,30"
	node [fillcolor=lightblue shape=rectangle style=filled]
	node [fillcolor=lightgreen shape=ellipse]
	node [fillcolor=lightblue shape=rectangle]
	node [fillcolor=yellow shape=parallelogram]
	node [fillcolor=lightcoral shape=diamond]
	input [label="Model Input\nInput: [batch_size=1024, seq_len=10000, hidden_dim=8192]\nGPU: All GPUs" fillcolor=lightgreen shape=ellipse]
	label="Layer 0 - Multi-Head Attention (Two-Level Partitioning)"
	broadcast_0 [label="Broadcast Input\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 8192]\nGPU: All GPUs" fillcolor=yellow shape=parallelogram]
	q_proj_0_0 [label="Q Projection 0\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 0\nHeads: 0-3, Dims: 0-127" fillcolor=lightblue]
	k_proj_0_0 [label="K Projection 0\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 0\nHeads: 0-3, Dims: 0-127" fillcolor=lightblue]
	v_proj_0_0 [label="V Projection 0\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 0\nHeads: 0-3, Dims: 0-127" fillcolor=lightblue]
	attn_0_0 [label="Attention 0\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 0" fillcolor=lightblue]
	q_proj_0_1 [label="Q Projection 1\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 1\nHeads: 0-3, Dims: 128-255" fillcolor=lightblue]
	k_proj_0_1 [label="K Projection 1\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 1\nHeads: 0-3, Dims: 128-255" fillcolor=lightblue]
	v_proj_0_1 [label="V Projection 1\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 1\nHeads: 0-3, Dims: 128-255" fillcolor=lightblue]
	attn_0_1 [label="Attention 1\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 1" fillcolor=lightblue]
	q_proj_0_2 [label="Q Projection 2\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 2\nHeads: 0-3, Dims: 256-383" fillcolor=lightblue]
	k_proj_0_2 [label="K Projection 2\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 2\nHeads: 0-3, Dims: 256-383" fillcolor=lightblue]
	v_proj_0_2 [label="V Projection 2\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 2\nHeads: 0-3, Dims: 256-383" fillcolor=lightblue]
	attn_0_2 [label="Attention 2\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 2" fillcolor=lightblue]
	q_proj_0_3 [label="Q Projection 3\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 3\nHeads: 0-3, Dims: 384-511" fillcolor=lightblue]
	k_proj_0_3 [label="K Projection 3\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 3\nHeads: 0-3, Dims: 384-511" fillcolor=lightblue]
	v_proj_0_3 [label="V Projection 3\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 3\nHeads: 0-3, Dims: 384-511" fillcolor=lightblue]
	attn_0_3 [label="Attention 3\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 3" fillcolor=lightblue]
	q_proj_1_0 [label="Q Projection 4\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 4\nHeads: 4-7, Dims: 0-127" fillcolor=lightblue]
	k_proj_1_0 [label="K Projection 4\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 4\nHeads: 4-7, Dims: 0-127" fillcolor=lightblue]
	v_proj_1_0 [label="V Projection 4\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 4\nHeads: 4-7, Dims: 0-127" fillcolor=lightblue]
	attn_1_0 [label="Attention 4\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 4" fillcolor=lightblue]
	q_proj_1_1 [label="Q Projection 5\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 5\nHeads: 4-7, Dims: 128-255" fillcolor=lightblue]
	k_proj_1_1 [label="K Projection 5\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 5\nHeads: 4-7, Dims: 128-255" fillcolor=lightblue]
	v_proj_1_1 [label="V Projection 5\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 5\nHeads: 4-7, Dims: 128-255" fillcolor=lightblue]
	attn_1_1 [label="Attention 5\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 5" fillcolor=lightblue]
	q_proj_1_2 [label="Q Projection 6\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 6\nHeads: 4-7, Dims: 256-383" fillcolor=lightblue]
	k_proj_1_2 [label="K Projection 6\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 6\nHeads: 4-7, Dims: 256-383" fillcolor=lightblue]
	v_proj_1_2 [label="V Projection 6\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 6\nHeads: 4-7, Dims: 256-383" fillcolor=lightblue]
	attn_1_2 [label="Attention 6\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 6" fillcolor=lightblue]
	q_proj_1_3 [label="Q Projection 7\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 7\nHeads: 4-7, Dims: 384-511" fillcolor=lightblue]
	k_proj_1_3 [label="K Projection 7\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 7\nHeads: 4-7, Dims: 384-511" fillcolor=lightblue]
	v_proj_1_3 [label="V Projection 7\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 7\nHeads: 4-7, Dims: 384-511" fillcolor=lightblue]
	attn_1_3 [label="Attention 7\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 7" fillcolor=lightblue]
	q_proj_2_0 [label="Q Projection 8\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 8\nHeads: 8-11, Dims: 0-127" fillcolor=lightblue]
	k_proj_2_0 [label="K Projection 8\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 8\nHeads: 8-11, Dims: 0-127" fillcolor=lightblue]
	v_proj_2_0 [label="V Projection 8\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 8\nHeads: 8-11, Dims: 0-127" fillcolor=lightblue]
	attn_2_0 [label="Attention 8\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 8" fillcolor=lightblue]
	q_proj_2_1 [label="Q Projection 9\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 9\nHeads: 8-11, Dims: 128-255" fillcolor=lightblue]
	k_proj_2_1 [label="K Projection 9\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 9\nHeads: 8-11, Dims: 128-255" fillcolor=lightblue]
	v_proj_2_1 [label="V Projection 9\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 9\nHeads: 8-11, Dims: 128-255" fillcolor=lightblue]
	attn_2_1 [label="Attention 9\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 9" fillcolor=lightblue]
	q_proj_2_2 [label="Q Projection 10\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 10\nHeads: 8-11, Dims: 256-383" fillcolor=lightblue]
	k_proj_2_2 [label="K Projection 10\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 10\nHeads: 8-11, Dims: 256-383" fillcolor=lightblue]
	v_proj_2_2 [label="V Projection 10\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 10\nHeads: 8-11, Dims: 256-383" fillcolor=lightblue]
	attn_2_2 [label="Attention 10\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 10" fillcolor=lightblue]
	q_proj_2_3 [label="Q Projection 11\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 11\nHeads: 8-11, Dims: 384-511" fillcolor=lightblue]
	k_proj_2_3 [label="K Projection 11\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 11\nHeads: 8-11, Dims: 384-511" fillcolor=lightblue]
	v_proj_2_3 [label="V Projection 11\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 11\nHeads: 8-11, Dims: 384-511" fillcolor=lightblue]
	attn_2_3 [label="Attention 11\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 11" fillcolor=lightblue]
	q_proj_3_0 [label="Q Projection 12\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 12\nHeads: 12-15, Dims: 0-127" fillcolor=lightblue]
	k_proj_3_0 [label="K Projection 12\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 12\nHeads: 12-15, Dims: 0-127" fillcolor=lightblue]
	v_proj_3_0 [label="V Projection 12\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 12\nHeads: 12-15, Dims: 0-127" fillcolor=lightblue]
	attn_3_0 [label="Attention 12\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 12" fillcolor=lightblue]
	q_proj_3_1 [label="Q Projection 13\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 13\nHeads: 12-15, Dims: 128-255" fillcolor=lightblue]
	k_proj_3_1 [label="K Projection 13\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 13\nHeads: 12-15, Dims: 128-255" fillcolor=lightblue]
	v_proj_3_1 [label="V Projection 13\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 13\nHeads: 12-15, Dims: 128-255" fillcolor=lightblue]
	attn_3_1 [label="Attention 13\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 13" fillcolor=lightblue]
	q_proj_3_2 [label="Q Projection 14\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 14\nHeads: 12-15, Dims: 256-383" fillcolor=lightblue]
	k_proj_3_2 [label="K Projection 14\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 14\nHeads: 12-15, Dims: 256-383" fillcolor=lightblue]
	v_proj_3_2 [label="V Projection 14\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 14\nHeads: 12-15, Dims: 256-383" fillcolor=lightblue]
	attn_3_2 [label="Attention 14\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 14" fillcolor=lightblue]
	q_proj_3_3 [label="Q Projection 15\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 15\nHeads: 12-15, Dims: 384-511" fillcolor=lightblue]
	k_proj_3_3 [label="K Projection 15\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 15\nHeads: 12-15, Dims: 384-511" fillcolor=lightblue]
	v_proj_3_3 [label="V Projection 15\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 15\nHeads: 12-15, Dims: 384-511" fillcolor=lightblue]
	attn_3_3 [label="Attention 15\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 15" fillcolor=lightblue]
	broadcast_0 -> q_proj_0_0
	broadcast_0 -> k_proj_0_0
	broadcast_0 -> v_proj_0_0
	q_proj_0_0 -> attn_0_0
	k_proj_0_0 -> attn_0_0
	v_proj_0_0 -> attn_0_0
	broadcast_0 -> q_proj_0_1
	broadcast_0 -> k_proj_0_1
	broadcast_0 -> v_proj_0_1
	q_proj_0_1 -> attn_0_1
	k_proj_0_1 -> attn_0_1
	v_proj_0_1 -> attn_0_1
	broadcast_0 -> q_proj_0_2
	broadcast_0 -> k_proj_0_2
	broadcast_0 -> v_proj_0_2
	q_proj_0_2 -> attn_0_2
	k_proj_0_2 -> attn_0_2
	v_proj_0_2 -> attn_0_2
	broadcast_0 -> q_proj_0_3
	broadcast_0 -> k_proj_0_3
	broadcast_0 -> v_proj_0_3
	q_proj_0_3 -> attn_0_3
	k_proj_0_3 -> attn_0_3
	v_proj_0_3 -> attn_0_3
	broadcast_0 -> q_proj_1_0
	broadcast_0 -> k_proj_1_0
	broadcast_0 -> v_proj_1_0
	q_proj_1_0 -> attn_1_0
	k_proj_1_0 -> attn_1_0
	v_proj_1_0 -> attn_1_0
	broadcast_0 -> q_proj_1_1
	broadcast_0 -> k_proj_1_1
	broadcast_0 -> v_proj_1_1
	q_proj_1_1 -> attn_1_1
	k_proj_1_1 -> attn_1_1
	v_proj_1_1 -> attn_1_1
	broadcast_0 -> q_proj_1_2
	broadcast_0 -> k_proj_1_2
	broadcast_0 -> v_proj_1_2
	q_proj_1_2 -> attn_1_2
	k_proj_1_2 -> attn_1_2
	v_proj_1_2 -> attn_1_2
	broadcast_0 -> q_proj_1_3
	broadcast_0 -> k_proj_1_3
	broadcast_0 -> v_proj_1_3
	q_proj_1_3 -> attn_1_3
	k_proj_1_3 -> attn_1_3
	v_proj_1_3 -> attn_1_3
	broadcast_0 -> q_proj_2_0
	broadcast_0 -> k_proj_2_0
	broadcast_0 -> v_proj_2_0
	q_proj_2_0 -> attn_2_0
	k_proj_2_0 -> attn_2_0
	v_proj_2_0 -> attn_2_0
	broadcast_0 -> q_proj_2_1
	broadcast_0 -> k_proj_2_1
	broadcast_0 -> v_proj_2_1
	q_proj_2_1 -> attn_2_1
	k_proj_2_1 -> attn_2_1
	v_proj_2_1 -> attn_2_1
	broadcast_0 -> q_proj_2_2
	broadcast_0 -> k_proj_2_2
	broadcast_0 -> v_proj_2_2
	q_proj_2_2 -> attn_2_2
	k_proj_2_2 -> attn_2_2
	v_proj_2_2 -> attn_2_2
	broadcast_0 -> q_proj_2_3
	broadcast_0 -> k_proj_2_3
	broadcast_0 -> v_proj_2_3
	q_proj_2_3 -> attn_2_3
	k_proj_2_3 -> attn_2_3
	v_proj_2_3 -> attn_2_3
	broadcast_0 -> q_proj_3_0
	broadcast_0 -> k_proj_3_0
	broadcast_0 -> v_proj_3_0
	q_proj_3_0 -> attn_3_0
	k_proj_3_0 -> attn_3_0
	v_proj_3_0 -> attn_3_0
	broadcast_0 -> q_proj_3_1
	broadcast_0 -> k_proj_3_1
	broadcast_0 -> v_proj_3_1
	q_proj_3_1 -> attn_3_1
	k_proj_3_1 -> attn_3_1
	v_proj_3_1 -> attn_3_1
	broadcast_0 -> q_proj_3_2
	broadcast_0 -> k_proj_3_2
	broadcast_0 -> v_proj_3_2
	q_proj_3_2 -> attn_3_2
	k_proj_3_2 -> attn_3_2
	v_proj_3_2 -> attn_3_2
	broadcast_0 -> q_proj_3_3
	broadcast_0 -> k_proj_3_3
	broadcast_0 -> v_proj_3_3
	q_proj_3_3 -> attn_3_3
	k_proj_3_3 -> attn_3_3
	v_proj_3_3 -> attn_3_3
	dim_concat_0 [label="Dimension Concatenation Group 0\nInput: [4x [1024, 10000, 512]]\nOutput: [1024, 10000, 2048]\nGPU: 0-3" fillcolor=lightcoral shape=diamond]
	attn_0_0 -> dim_concat_0
	attn_0_1 -> dim_concat_0
	attn_0_2 -> dim_concat_0
	attn_0_3 -> dim_concat_0
	dim_concat_1 [label="Dimension Concatenation Group 1\nInput: [4x [1024, 10000, 512]]\nOutput: [1024, 10000, 2048]\nGPU: 4-7" fillcolor=lightcoral shape=diamond]
	attn_1_0 -> dim_concat_1
	attn_1_1 -> dim_concat_1
	attn_1_2 -> dim_concat_1
	attn_1_3 -> dim_concat_1
	dim_concat_2 [label="Dimension Concatenation Group 2\nInput: [4x [1024, 10000, 512]]\nOutput: [1024, 10000, 2048]\nGPU: 8-11" fillcolor=lightcoral shape=diamond]
	attn_2_0 -> dim_concat_2
	attn_2_1 -> dim_concat_2
	attn_2_2 -> dim_concat_2
	attn_2_3 -> dim_concat_2
	dim_concat_3 [label="Dimension Concatenation Group 3\nInput: [4x [1024, 10000, 512]]\nOutput: [1024, 10000, 2048]\nGPU: 12-15" fillcolor=lightcoral shape=diamond]
	attn_3_0 -> dim_concat_3
	attn_3_1 -> dim_concat_3
	attn_3_2 -> dim_concat_3
	attn_3_3 -> dim_concat_3
	head_concat_0 [label="Head Concatenation\nInput: [4x [1024, 10000, 2048]]\nOutput: [1024, 10000, 8192]\nGPU: All GPUs" fillcolor=lightcoral shape=diamond]
	dim_concat_0 -> head_concat_0
	dim_concat_1 -> head_concat_0
	dim_concat_2 -> head_concat_0
	dim_concat_3 -> head_concat_0
	residual_add_0 [label="Residual Add Layer 0\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024, 10000, 8192]\nGPU: All GPUs" fillcolor=lightcoral shape=diamond]
	input -> residual_add_0
	head_concat_0 -> residual_add_0
	label="Layer 0 - MLP (Tensor Parallel across 16 GPUs)"
	mlp_broadcast_0 [label="MLP Broadcast\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 8192]\nGPU: All GPUs" fillcolor=yellow shape=parallelogram]
	residual_add_0 -> mlp_broadcast_0
	mlp_linear1_0 [label="MLP First Linear 0\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 0" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_0
	mlp_linear1_1 [label="MLP First Linear 1\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 1" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_1
	mlp_linear1_2 [label="MLP First Linear 2\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 2" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_2
	mlp_linear1_3 [label="MLP First Linear 3\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 3" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_3
	mlp_linear1_4 [label="MLP First Linear 4\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 4" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_4
	mlp_linear1_5 [label="MLP First Linear 5\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 5" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_5
	mlp_linear1_6 [label="MLP First Linear 6\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 6" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_6
	mlp_linear1_7 [label="MLP First Linear 7\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 7" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_7
	mlp_linear1_8 [label="MLP First Linear 8\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 8" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_8
	mlp_linear1_9 [label="MLP First Linear 9\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 9" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_9
	mlp_linear1_10 [label="MLP First Linear 10\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 10" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_10
	mlp_linear1_11 [label="MLP First Linear 11\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 11" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_11
	mlp_linear1_12 [label="MLP First Linear 12\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 12" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_12
	mlp_linear1_13 [label="MLP First Linear 13\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 13" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_13
	mlp_linear1_14 [label="MLP First Linear 14\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 14" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_14
	mlp_linear1_15 [label="MLP First Linear 15\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 15" fillcolor=lightblue]
	mlp_broadcast_0 -> mlp_linear1_15
	mlp_gelu_0 [label="MLP GELU 0\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 0" fillcolor=lightblue]
	mlp_linear1_0 -> mlp_gelu_0
	mlp_gelu_1 [label="MLP GELU 1\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 1" fillcolor=lightblue]
	mlp_linear1_1 -> mlp_gelu_1
	mlp_gelu_2 [label="MLP GELU 2\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 2" fillcolor=lightblue]
	mlp_linear1_2 -> mlp_gelu_2
	mlp_gelu_3 [label="MLP GELU 3\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 3" fillcolor=lightblue]
	mlp_linear1_3 -> mlp_gelu_3
	mlp_gelu_4 [label="MLP GELU 4\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 4" fillcolor=lightblue]
	mlp_linear1_4 -> mlp_gelu_4
	mlp_gelu_5 [label="MLP GELU 5\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 5" fillcolor=lightblue]
	mlp_linear1_5 -> mlp_gelu_5
	mlp_gelu_6 [label="MLP GELU 6\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 6" fillcolor=lightblue]
	mlp_linear1_6 -> mlp_gelu_6
	mlp_gelu_7 [label="MLP GELU 7\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 7" fillcolor=lightblue]
	mlp_linear1_7 -> mlp_gelu_7
	mlp_gelu_8 [label="MLP GELU 8\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 8" fillcolor=lightblue]
	mlp_linear1_8 -> mlp_gelu_8
	mlp_gelu_9 [label="MLP GELU 9\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 9" fillcolor=lightblue]
	mlp_linear1_9 -> mlp_gelu_9
	mlp_gelu_10 [label="MLP GELU 10\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 10" fillcolor=lightblue]
	mlp_linear1_10 -> mlp_gelu_10
	mlp_gelu_11 [label="MLP GELU 11\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 11" fillcolor=lightblue]
	mlp_linear1_11 -> mlp_gelu_11
	mlp_gelu_12 [label="MLP GELU 12\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 12" fillcolor=lightblue]
	mlp_linear1_12 -> mlp_gelu_12
	mlp_gelu_13 [label="MLP GELU 13\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 13" fillcolor=lightblue]
	mlp_linear1_13 -> mlp_gelu_13
	mlp_gelu_14 [label="MLP GELU 14\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 14" fillcolor=lightblue]
	mlp_linear1_14 -> mlp_gelu_14
	mlp_gelu_15 [label="MLP GELU 15\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 15" fillcolor=lightblue]
	mlp_linear1_15 -> mlp_gelu_15
	mlp_linear2_0 [label="MLP Second Linear 0\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 0" fillcolor=lightblue]
	mlp_gelu_0 -> mlp_linear2_0
	mlp_linear2_1 [label="MLP Second Linear 1\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 1" fillcolor=lightblue]
	mlp_gelu_1 -> mlp_linear2_1
	mlp_linear2_2 [label="MLP Second Linear 2\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 2" fillcolor=lightblue]
	mlp_gelu_2 -> mlp_linear2_2
	mlp_linear2_3 [label="MLP Second Linear 3\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 3" fillcolor=lightblue]
	mlp_gelu_3 -> mlp_linear2_3
	mlp_linear2_4 [label="MLP Second Linear 4\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 4" fillcolor=lightblue]
	mlp_gelu_4 -> mlp_linear2_4
	mlp_linear2_5 [label="MLP Second Linear 5\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 5" fillcolor=lightblue]
	mlp_gelu_5 -> mlp_linear2_5
	mlp_linear2_6 [label="MLP Second Linear 6\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 6" fillcolor=lightblue]
	mlp_gelu_6 -> mlp_linear2_6
	mlp_linear2_7 [label="MLP Second Linear 7\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 7" fillcolor=lightblue]
	mlp_gelu_7 -> mlp_linear2_7
	mlp_linear2_8 [label="MLP Second Linear 8\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 8" fillcolor=lightblue]
	mlp_gelu_8 -> mlp_linear2_8
	mlp_linear2_9 [label="MLP Second Linear 9\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 9" fillcolor=lightblue]
	mlp_gelu_9 -> mlp_linear2_9
	mlp_linear2_10 [label="MLP Second Linear 10\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 10" fillcolor=lightblue]
	mlp_gelu_10 -> mlp_linear2_10
	mlp_linear2_11 [label="MLP Second Linear 11\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 11" fillcolor=lightblue]
	mlp_gelu_11 -> mlp_linear2_11
	mlp_linear2_12 [label="MLP Second Linear 12\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 12" fillcolor=lightblue]
	mlp_gelu_12 -> mlp_linear2_12
	mlp_linear2_13 [label="MLP Second Linear 13\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 13" fillcolor=lightblue]
	mlp_gelu_13 -> mlp_linear2_13
	mlp_linear2_14 [label="MLP Second Linear 14\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 14" fillcolor=lightblue]
	mlp_gelu_14 -> mlp_linear2_14
	mlp_linear2_15 [label="MLP Second Linear 15\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 15" fillcolor=lightblue]
	mlp_gelu_15 -> mlp_linear2_15
	mlp_allreduce_0 [label="MLP All-Reduce\nInput: [16x [1024, 10000, 512]]\nOutput: [1024, 10000, 8192]\nGPU: All GPUs" fillcolor=yellow shape=parallelogram]
	mlp_linear2_0 -> mlp_allreduce_0
	mlp_linear2_1 -> mlp_allreduce_0
	mlp_linear2_2 -> mlp_allreduce_0
	mlp_linear2_3 -> mlp_allreduce_0
	mlp_linear2_4 -> mlp_allreduce_0
	mlp_linear2_5 -> mlp_allreduce_0
	mlp_linear2_6 -> mlp_allreduce_0
	mlp_linear2_7 -> mlp_allreduce_0
	mlp_linear2_8 -> mlp_allreduce_0
	mlp_linear2_9 -> mlp_allreduce_0
	mlp_linear2_10 -> mlp_allreduce_0
	mlp_linear2_11 -> mlp_allreduce_0
	mlp_linear2_12 -> mlp_allreduce_0
	mlp_linear2_13 -> mlp_allreduce_0
	mlp_linear2_14 -> mlp_allreduce_0
	mlp_linear2_15 -> mlp_allreduce_0
	mlp_residual_0 [label="MLP Residual Add\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024, 10000, 8192]\nGPU: All GPUs" fillcolor=lightcoral shape=diamond]
	residual_add_0 -> mlp_residual_0
	mlp_allreduce_0 -> mlp_residual_0
	label="Layer 1 - Multi-Head Attention (Two-Level Partitioning)"
	broadcast_1 [label="Broadcast Input\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 8192]\nGPU: All GPUs" fillcolor=yellow shape=parallelogram]
	mlp_residual_0 -> broadcast_1
	q_proj_1_0_0 [label="Q Projection L1-0\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 0" fillcolor=lightblue]
	k_proj_1_0_0 [label="K Projection L1-0\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 0" fillcolor=lightblue]
	v_proj_1_0_0 [label="V Projection L1-0\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 0" fillcolor=lightblue]
	attn_1_0_0 [label="Attention L1-0\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 0" fillcolor=lightblue]
	q_proj_1_0_1 [label="Q Projection L1-1\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 1" fillcolor=lightblue]
	k_proj_1_0_1 [label="K Projection L1-1\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 1" fillcolor=lightblue]
	v_proj_1_0_1 [label="V Projection L1-1\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 1" fillcolor=lightblue]
	attn_1_0_1 [label="Attention L1-1\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 1" fillcolor=lightblue]
	q_proj_1_0_2 [label="Q Projection L1-2\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 2" fillcolor=lightblue]
	k_proj_1_0_2 [label="K Projection L1-2\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 2" fillcolor=lightblue]
	v_proj_1_0_2 [label="V Projection L1-2\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 2" fillcolor=lightblue]
	attn_1_0_2 [label="Attention L1-2\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 2" fillcolor=lightblue]
	q_proj_1_0_3 [label="Q Projection L1-3\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 3" fillcolor=lightblue]
	k_proj_1_0_3 [label="K Projection L1-3\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 3" fillcolor=lightblue]
	v_proj_1_0_3 [label="V Projection L1-3\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 3" fillcolor=lightblue]
	attn_1_0_3 [label="Attention L1-3\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 3" fillcolor=lightblue]
	q_proj_1_1_0 [label="Q Projection L1-4\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 4" fillcolor=lightblue]
	k_proj_1_1_0 [label="K Projection L1-4\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 4" fillcolor=lightblue]
	v_proj_1_1_0 [label="V Projection L1-4\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 4" fillcolor=lightblue]
	attn_1_1_0 [label="Attention L1-4\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 4" fillcolor=lightblue]
	q_proj_1_1_1 [label="Q Projection L1-5\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 5" fillcolor=lightblue]
	k_proj_1_1_1 [label="K Projection L1-5\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 5" fillcolor=lightblue]
	v_proj_1_1_1 [label="V Projection L1-5\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 5" fillcolor=lightblue]
	attn_1_1_1 [label="Attention L1-5\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 5" fillcolor=lightblue]
	q_proj_1_1_2 [label="Q Projection L1-6\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 6" fillcolor=lightblue]
	k_proj_1_1_2 [label="K Projection L1-6\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 6" fillcolor=lightblue]
	v_proj_1_1_2 [label="V Projection L1-6\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 6" fillcolor=lightblue]
	attn_1_1_2 [label="Attention L1-6\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 6" fillcolor=lightblue]
	q_proj_1_1_3 [label="Q Projection L1-7\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 7" fillcolor=lightblue]
	k_proj_1_1_3 [label="K Projection L1-7\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 7" fillcolor=lightblue]
	v_proj_1_1_3 [label="V Projection L1-7\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 7" fillcolor=lightblue]
	attn_1_1_3 [label="Attention L1-7\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 7" fillcolor=lightblue]
	q_proj_1_2_0 [label="Q Projection L1-8\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 8" fillcolor=lightblue]
	k_proj_1_2_0 [label="K Projection L1-8\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 8" fillcolor=lightblue]
	v_proj_1_2_0 [label="V Projection L1-8\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 8" fillcolor=lightblue]
	attn_1_2_0 [label="Attention L1-8\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 8" fillcolor=lightblue]
	q_proj_1_2_1 [label="Q Projection L1-9\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 9" fillcolor=lightblue]
	k_proj_1_2_1 [label="K Projection L1-9\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 9" fillcolor=lightblue]
	v_proj_1_2_1 [label="V Projection L1-9\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 9" fillcolor=lightblue]
	attn_1_2_1 [label="Attention L1-9\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 9" fillcolor=lightblue]
	q_proj_1_2_2 [label="Q Projection L1-10\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 10" fillcolor=lightblue]
	k_proj_1_2_2 [label="K Projection L1-10\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 10" fillcolor=lightblue]
	v_proj_1_2_2 [label="V Projection L1-10\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 10" fillcolor=lightblue]
	attn_1_2_2 [label="Attention L1-10\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 10" fillcolor=lightblue]
	q_proj_1_2_3 [label="Q Projection L1-11\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 11" fillcolor=lightblue]
	k_proj_1_2_3 [label="K Projection L1-11\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 11" fillcolor=lightblue]
	v_proj_1_2_3 [label="V Projection L1-11\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 11" fillcolor=lightblue]
	attn_1_2_3 [label="Attention L1-11\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 11" fillcolor=lightblue]
	q_proj_1_3_0 [label="Q Projection L1-12\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 12" fillcolor=lightblue]
	k_proj_1_3_0 [label="K Projection L1-12\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 12" fillcolor=lightblue]
	v_proj_1_3_0 [label="V Projection L1-12\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 12" fillcolor=lightblue]
	attn_1_3_0 [label="Attention L1-12\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 12" fillcolor=lightblue]
	q_proj_1_3_1 [label="Q Projection L1-13\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 13" fillcolor=lightblue]
	k_proj_1_3_1 [label="K Projection L1-13\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 13" fillcolor=lightblue]
	v_proj_1_3_1 [label="V Projection L1-13\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 13" fillcolor=lightblue]
	attn_1_3_1 [label="Attention L1-13\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 13" fillcolor=lightblue]
	q_proj_1_3_2 [label="Q Projection L1-14\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 14" fillcolor=lightblue]
	k_proj_1_3_2 [label="K Projection L1-14\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 14" fillcolor=lightblue]
	v_proj_1_3_2 [label="V Projection L1-14\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 14" fillcolor=lightblue]
	attn_1_3_2 [label="Attention L1-14\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 14" fillcolor=lightblue]
	q_proj_1_3_3 [label="Q Projection L1-15\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 15" fillcolor=lightblue]
	k_proj_1_3_3 [label="K Projection L1-15\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 15" fillcolor=lightblue]
	v_proj_1_3_3 [label="V Projection L1-15\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 512]\nGPU: 15" fillcolor=lightblue]
	attn_1_3_3 [label="Attention L1-15\nInput: Q:[1024,10000,512], K:[1024,10000,512], V:[1024,10000,512]\nOutput: [1024, 10000, 512]\nGPU: 15" fillcolor=lightblue]
	broadcast_1 -> q_proj_1_0_0
	broadcast_1 -> k_proj_1_0_0
	broadcast_1 -> v_proj_1_0_0
	q_proj_1_0_0 -> attn_1_0_0
	k_proj_1_0_0 -> attn_1_0_0
	v_proj_1_0_0 -> attn_1_0_0
	broadcast_1 -> q_proj_1_0_1
	broadcast_1 -> k_proj_1_0_1
	broadcast_1 -> v_proj_1_0_1
	q_proj_1_0_1 -> attn_1_0_1
	k_proj_1_0_1 -> attn_1_0_1
	v_proj_1_0_1 -> attn_1_0_1
	broadcast_1 -> q_proj_1_0_2
	broadcast_1 -> k_proj_1_0_2
	broadcast_1 -> v_proj_1_0_2
	q_proj_1_0_2 -> attn_1_0_2
	k_proj_1_0_2 -> attn_1_0_2
	v_proj_1_0_2 -> attn_1_0_2
	broadcast_1 -> q_proj_1_0_3
	broadcast_1 -> k_proj_1_0_3
	broadcast_1 -> v_proj_1_0_3
	q_proj_1_0_3 -> attn_1_0_3
	k_proj_1_0_3 -> attn_1_0_3
	v_proj_1_0_3 -> attn_1_0_3
	broadcast_1 -> q_proj_1_1_0
	broadcast_1 -> k_proj_1_1_0
	broadcast_1 -> v_proj_1_1_0
	q_proj_1_1_0 -> attn_1_1_0
	k_proj_1_1_0 -> attn_1_1_0
	v_proj_1_1_0 -> attn_1_1_0
	broadcast_1 -> q_proj_1_1_1
	broadcast_1 -> k_proj_1_1_1
	broadcast_1 -> v_proj_1_1_1
	q_proj_1_1_1 -> attn_1_1_1
	k_proj_1_1_1 -> attn_1_1_1
	v_proj_1_1_1 -> attn_1_1_1
	broadcast_1 -> q_proj_1_1_2
	broadcast_1 -> k_proj_1_1_2
	broadcast_1 -> v_proj_1_1_2
	q_proj_1_1_2 -> attn_1_1_2
	k_proj_1_1_2 -> attn_1_1_2
	v_proj_1_1_2 -> attn_1_1_2
	broadcast_1 -> q_proj_1_1_3
	broadcast_1 -> k_proj_1_1_3
	broadcast_1 -> v_proj_1_1_3
	q_proj_1_1_3 -> attn_1_1_3
	k_proj_1_1_3 -> attn_1_1_3
	v_proj_1_1_3 -> attn_1_1_3
	broadcast_1 -> q_proj_1_2_0
	broadcast_1 -> k_proj_1_2_0
	broadcast_1 -> v_proj_1_2_0
	q_proj_1_2_0 -> attn_1_2_0
	k_proj_1_2_0 -> attn_1_2_0
	v_proj_1_2_0 -> attn_1_2_0
	broadcast_1 -> q_proj_1_2_1
	broadcast_1 -> k_proj_1_2_1
	broadcast_1 -> v_proj_1_2_1
	q_proj_1_2_1 -> attn_1_2_1
	k_proj_1_2_1 -> attn_1_2_1
	v_proj_1_2_1 -> attn_1_2_1
	broadcast_1 -> q_proj_1_2_2
	broadcast_1 -> k_proj_1_2_2
	broadcast_1 -> v_proj_1_2_2
	q_proj_1_2_2 -> attn_1_2_2
	k_proj_1_2_2 -> attn_1_2_2
	v_proj_1_2_2 -> attn_1_2_2
	broadcast_1 -> q_proj_1_2_3
	broadcast_1 -> k_proj_1_2_3
	broadcast_1 -> v_proj_1_2_3
	q_proj_1_2_3 -> attn_1_2_3
	k_proj_1_2_3 -> attn_1_2_3
	v_proj_1_2_3 -> attn_1_2_3
	broadcast_1 -> q_proj_1_3_0
	broadcast_1 -> k_proj_1_3_0
	broadcast_1 -> v_proj_1_3_0
	q_proj_1_3_0 -> attn_1_3_0
	k_proj_1_3_0 -> attn_1_3_0
	v_proj_1_3_0 -> attn_1_3_0
	broadcast_1 -> q_proj_1_3_1
	broadcast_1 -> k_proj_1_3_1
	broadcast_1 -> v_proj_1_3_1
	q_proj_1_3_1 -> attn_1_3_1
	k_proj_1_3_1 -> attn_1_3_1
	v_proj_1_3_1 -> attn_1_3_1
	broadcast_1 -> q_proj_1_3_2
	broadcast_1 -> k_proj_1_3_2
	broadcast_1 -> v_proj_1_3_2
	q_proj_1_3_2 -> attn_1_3_2
	k_proj_1_3_2 -> attn_1_3_2
	v_proj_1_3_2 -> attn_1_3_2
	broadcast_1 -> q_proj_1_3_3
	broadcast_1 -> k_proj_1_3_3
	broadcast_1 -> v_proj_1_3_3
	q_proj_1_3_3 -> attn_1_3_3
	k_proj_1_3_3 -> attn_1_3_3
	v_proj_1_3_3 -> attn_1_3_3
	dim_concat_1_0 [label="Dimension Concatenation L1-Group 0\nInput: [4x [1024, 10000, 512]]\nOutput: [1024, 10000, 2048]\nGPU: 0-3" fillcolor=lightcoral shape=diamond]
	attn_1_0_0 -> dim_concat_1_0
	attn_1_0_1 -> dim_concat_1_0
	attn_1_0_2 -> dim_concat_1_0
	attn_1_0_3 -> dim_concat_1_0
	dim_concat_1_1 [label="Dimension Concatenation L1-Group 1\nInput: [4x [1024, 10000, 512]]\nOutput: [1024, 10000, 2048]\nGPU: 4-7" fillcolor=lightcoral shape=diamond]
	attn_1_1_0 -> dim_concat_1_1
	attn_1_1_1 -> dim_concat_1_1
	attn_1_1_2 -> dim_concat_1_1
	attn_1_1_3 -> dim_concat_1_1
	dim_concat_1_2 [label="Dimension Concatenation L1-Group 2\nInput: [4x [1024, 10000, 512]]\nOutput: [1024, 10000, 2048]\nGPU: 8-11" fillcolor=lightcoral shape=diamond]
	attn_1_2_0 -> dim_concat_1_2
	attn_1_2_1 -> dim_concat_1_2
	attn_1_2_2 -> dim_concat_1_2
	attn_1_2_3 -> dim_concat_1_2
	dim_concat_1_3 [label="Dimension Concatenation L1-Group 3\nInput: [4x [1024, 10000, 512]]\nOutput: [1024, 10000, 2048]\nGPU: 12-15" fillcolor=lightcoral shape=diamond]
	attn_1_3_0 -> dim_concat_1_3
	attn_1_3_1 -> dim_concat_1_3
	attn_1_3_2 -> dim_concat_1_3
	attn_1_3_3 -> dim_concat_1_3
	head_concat_1 [label="Head Concatenation L1\nInput: [4x [1024, 10000, 2048]]\nOutput: [1024, 10000, 8192]\nGPU: All GPUs" fillcolor=lightcoral shape=diamond]
	dim_concat_1_0 -> head_concat_1
	dim_concat_1_1 -> head_concat_1
	dim_concat_1_2 -> head_concat_1
	dim_concat_1_3 -> head_concat_1
	residual_add_1 [label="Residual Add Layer 1\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024, 10000, 8192]\nGPU: All GPUs" fillcolor=lightcoral shape=diamond]
	mlp_residual_0 -> residual_add_1
	head_concat_1 -> residual_add_1
	mlp_broadcast_1 [label="MLP Broadcast L1\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 8192]\nGPU: All GPUs" fillcolor=yellow shape=parallelogram]
	residual_add_1 -> mlp_broadcast_1
	mlp_linear1_1_0 [label="MLP First Linear L1-0\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 0" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_0
	mlp_linear1_1_1 [label="MLP First Linear L1-1\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 1" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_1
	mlp_linear1_1_2 [label="MLP First Linear L1-2\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 2" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_2
	mlp_linear1_1_3 [label="MLP First Linear L1-3\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 3" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_3
	mlp_linear1_1_4 [label="MLP First Linear L1-4\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 4" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_4
	mlp_linear1_1_5 [label="MLP First Linear L1-5\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 5" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_5
	mlp_linear1_1_6 [label="MLP First Linear L1-6\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 6" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_6
	mlp_linear1_1_7 [label="MLP First Linear L1-7\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 7" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_7
	mlp_linear1_1_8 [label="MLP First Linear L1-8\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 8" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_8
	mlp_linear1_1_9 [label="MLP First Linear L1-9\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 9" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_9
	mlp_linear1_1_10 [label="MLP First Linear L1-10\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 10" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_10
	mlp_linear1_1_11 [label="MLP First Linear L1-11\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 11" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_11
	mlp_linear1_1_12 [label="MLP First Linear L1-12\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 12" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_12
	mlp_linear1_1_13 [label="MLP First Linear L1-13\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 13" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_13
	mlp_linear1_1_14 [label="MLP First Linear L1-14\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 14" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_14
	mlp_linear1_1_15 [label="MLP First Linear L1-15\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 2048]\nGPU: 15" fillcolor=lightblue]
	mlp_broadcast_1 -> mlp_linear1_1_15
	mlp_gelu_1_0 [label="MLP GELU L1-0\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 0" fillcolor=lightblue]
	mlp_linear1_1_0 -> mlp_gelu_1_0
	mlp_gelu_1_1 [label="MLP GELU L1-1\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 1" fillcolor=lightblue]
	mlp_linear1_1_1 -> mlp_gelu_1_1
	mlp_gelu_1_2 [label="MLP GELU L1-2\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 2" fillcolor=lightblue]
	mlp_linear1_1_2 -> mlp_gelu_1_2
	mlp_gelu_1_3 [label="MLP GELU L1-3\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 3" fillcolor=lightblue]
	mlp_linear1_1_3 -> mlp_gelu_1_3
	mlp_gelu_1_4 [label="MLP GELU L1-4\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 4" fillcolor=lightblue]
	mlp_linear1_1_4 -> mlp_gelu_1_4
	mlp_gelu_1_5 [label="MLP GELU L1-5\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 5" fillcolor=lightblue]
	mlp_linear1_1_5 -> mlp_gelu_1_5
	mlp_gelu_1_6 [label="MLP GELU L1-6\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 6" fillcolor=lightblue]
	mlp_linear1_1_6 -> mlp_gelu_1_6
	mlp_gelu_1_7 [label="MLP GELU L1-7\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 7" fillcolor=lightblue]
	mlp_linear1_1_7 -> mlp_gelu_1_7
	mlp_gelu_1_8 [label="MLP GELU L1-8\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 8" fillcolor=lightblue]
	mlp_linear1_1_8 -> mlp_gelu_1_8
	mlp_gelu_1_9 [label="MLP GELU L1-9\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 9" fillcolor=lightblue]
	mlp_linear1_1_9 -> mlp_gelu_1_9
	mlp_gelu_1_10 [label="MLP GELU L1-10\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 10" fillcolor=lightblue]
	mlp_linear1_1_10 -> mlp_gelu_1_10
	mlp_gelu_1_11 [label="MLP GELU L1-11\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 11" fillcolor=lightblue]
	mlp_linear1_1_11 -> mlp_gelu_1_11
	mlp_gelu_1_12 [label="MLP GELU L1-12\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 12" fillcolor=lightblue]
	mlp_linear1_1_12 -> mlp_gelu_1_12
	mlp_gelu_1_13 [label="MLP GELU L1-13\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 13" fillcolor=lightblue]
	mlp_linear1_1_13 -> mlp_gelu_1_13
	mlp_gelu_1_14 [label="MLP GELU L1-14\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 14" fillcolor=lightblue]
	mlp_linear1_1_14 -> mlp_gelu_1_14
	mlp_gelu_1_15 [label="MLP GELU L1-15\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 2048]\nGPU: 15" fillcolor=lightblue]
	mlp_linear1_1_15 -> mlp_gelu_1_15
	mlp_linear2_1_0 [label="MLP Second Linear L1-0\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 0" fillcolor=lightblue]
	mlp_gelu_1_0 -> mlp_linear2_1_0
	mlp_linear2_1_1 [label="MLP Second Linear L1-1\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 1" fillcolor=lightblue]
	mlp_gelu_1_1 -> mlp_linear2_1_1
	mlp_linear2_1_2 [label="MLP Second Linear L1-2\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 2" fillcolor=lightblue]
	mlp_gelu_1_2 -> mlp_linear2_1_2
	mlp_linear2_1_3 [label="MLP Second Linear L1-3\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 3" fillcolor=lightblue]
	mlp_gelu_1_3 -> mlp_linear2_1_3
	mlp_linear2_1_4 [label="MLP Second Linear L1-4\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 4" fillcolor=lightblue]
	mlp_gelu_1_4 -> mlp_linear2_1_4
	mlp_linear2_1_5 [label="MLP Second Linear L1-5\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 5" fillcolor=lightblue]
	mlp_gelu_1_5 -> mlp_linear2_1_5
	mlp_linear2_1_6 [label="MLP Second Linear L1-6\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 6" fillcolor=lightblue]
	mlp_gelu_1_6 -> mlp_linear2_1_6
	mlp_linear2_1_7 [label="MLP Second Linear L1-7\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 7" fillcolor=lightblue]
	mlp_gelu_1_7 -> mlp_linear2_1_7
	mlp_linear2_1_8 [label="MLP Second Linear L1-8\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 8" fillcolor=lightblue]
	mlp_gelu_1_8 -> mlp_linear2_1_8
	mlp_linear2_1_9 [label="MLP Second Linear L1-9\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 9" fillcolor=lightblue]
	mlp_gelu_1_9 -> mlp_linear2_1_9
	mlp_linear2_1_10 [label="MLP Second Linear L1-10\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 10" fillcolor=lightblue]
	mlp_gelu_1_10 -> mlp_linear2_1_10
	mlp_linear2_1_11 [label="MLP Second Linear L1-11\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 11" fillcolor=lightblue]
	mlp_gelu_1_11 -> mlp_linear2_1_11
	mlp_linear2_1_12 [label="MLP Second Linear L1-12\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 12" fillcolor=lightblue]
	mlp_gelu_1_12 -> mlp_linear2_1_12
	mlp_linear2_1_13 [label="MLP Second Linear L1-13\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 13" fillcolor=lightblue]
	mlp_gelu_1_13 -> mlp_linear2_1_13
	mlp_linear2_1_14 [label="MLP Second Linear L1-14\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 14" fillcolor=lightblue]
	mlp_gelu_1_14 -> mlp_linear2_1_14
	mlp_linear2_1_15 [label="MLP Second Linear L1-15\nInput: [1024, 10000, 2048]\nOutput: [1024, 10000, 512]\nGPU: 15" fillcolor=lightblue]
	mlp_gelu_1_15 -> mlp_linear2_1_15
	mlp_allreduce_1 [label="MLP All-Reduce L1\nInput: [16x [1024, 10000, 512]]\nOutput: [1024, 10000, 8192]\nGPU: All GPUs" fillcolor=yellow shape=parallelogram]
	mlp_linear2_1_0 -> mlp_allreduce_1
	mlp_linear2_1_1 -> mlp_allreduce_1
	mlp_linear2_1_2 -> mlp_allreduce_1
	mlp_linear2_1_3 -> mlp_allreduce_1
	mlp_linear2_1_4 -> mlp_allreduce_1
	mlp_linear2_1_5 -> mlp_allreduce_1
	mlp_linear2_1_6 -> mlp_allreduce_1
	mlp_linear2_1_7 -> mlp_allreduce_1
	mlp_linear2_1_8 -> mlp_allreduce_1
	mlp_linear2_1_9 -> mlp_allreduce_1
	mlp_linear2_1_10 -> mlp_allreduce_1
	mlp_linear2_1_11 -> mlp_allreduce_1
	mlp_linear2_1_12 -> mlp_allreduce_1
	mlp_linear2_1_13 -> mlp_allreduce_1
	mlp_linear2_1_14 -> mlp_allreduce_1
	mlp_linear2_1_15 -> mlp_allreduce_1
	mlp_residual_1 [label="MLP Residual Add L1\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024, 10000, 8192]\nGPU: All GPUs" fillcolor=lightcoral shape=diamond]
	residual_add_1 -> mlp_residual_1
	mlp_allreduce_1 -> mlp_residual_1
	output [label="Model Output\nInput: [1024, 10000, 8192]\nOutput: [1024, 10000, 8192]\nGPU: All GPUs" fillcolor=lightgreen shape=ellipse]
	mlp_residual_1 -> output
}
