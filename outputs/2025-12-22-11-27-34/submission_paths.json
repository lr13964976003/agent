{
  "dag_files": [
    "../outputs/2025-12-22-11-27-34/llm_deployment_dag.dot",
    "../outputs/2025-12-22-11-27-34/llm_deployment_dag.svg"
  ],
  "strategy": "EP1-TP1-PP1-DP8",
  "description": "Complete LLM deployment DAG for 30B MoE model with EP1-TP1-PP1-DP8 parallel strategy",
  "validation": {
    "has_cycle": false,
    "total_edges": 1840,
    "total_nodes": 1840,
    "gpu_count": 8,
    "batch_size": 2048,
    "memory_utilization": "33%",
    "expert_parallelism": 1,
    "tensor_parallelism": 1,
    "pipeline_parallelism": 1,
    "data_parallelism": 8
  },
  "features": [
    "Complete operator-level granularity",
    "GPU-specific node labeling",
    "Communication nodes (ellipses)",
    "Computation nodes (rectangles)",
    "Routing/aggregation nodes (parallelograms)",
    "Dashed lines for gate selection",
    "Input/output dimension specifications",
    "No cycles verified",
    "Expert parallelism with 64 experts per layer",
    "Attention mechanism fully decomposed",
    "KV cache updates represented",
    "Data parallel split and aggregation"
  ]
}