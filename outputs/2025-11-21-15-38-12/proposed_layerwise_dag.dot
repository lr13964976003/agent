digraph proposed_layerwise_partitioning {
    rankdir=TB;
    node [shape=rectangle];
    
    // Input node
    input [shape=ellipse, label="Input\nInput: [batch_size=8, seq_len=10000, hidden=4096]\nOutput: [batch_size=8, seq_len=10000, hidden=4096]\nGPU: All GPUs"];
    
    // Layer 0 - GPU 0
    layer0_mha [label="Layer 0 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 0"];
    layer0_ffn [label="Layer 0 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 0"];
    
    // Communication 0→1
    comm0_1 [shape=ellipse, label="Communication\nGPU 0 → GPU 1\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 1 - GPU 1
    layer1_mha [label="Layer 1 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 1"];
    layer1_ffn [label="Layer 1 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 1"];
    
    // Communication 1→2
    comm1_2 [shape=ellipse, label="Communication\nGPU 1 → GPU 2\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 2 - GPU 2
    layer2_mha [label="Layer 2 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 2"];
    layer2_ffn [label="Layer 2 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 2"];
    
    // Communication 2→3
    comm2_3 [shape=ellipse, label="Communication\nGPU 2 → GPU 3\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 3 - GPU 3
    layer3_mha [label="Layer 3 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 3"];
    layer3_ffn [label="Layer 3 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 3"];
    
    // Communication 3→4
    comm3_4 [shape=ellipse, label="Communication\nGPU 3 → GPU 4\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 4 - GPU 4
    layer4_mha [label="Layer 4 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 4"];
    layer4_ffn [label="Layer 4 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 4"];
    
    // Communication 4→5
    comm4_5 [shape=ellipse, label="Communication\nGPU 4 → GPU 5\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 5 - GPU 5
    layer5_mha [label="Layer 5 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 5"];
    layer5_ffn [label="Layer 5 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 5"];
    
    // Communication 5→6
    comm5_6 [shape=ellipse, label="Communication\nGPU 5 → GPU 6\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 6 - GPU 6
    layer6_mha [label="Layer 6 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 6"];
    layer6_ffn [label="Layer 6 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 6"];
    
    // Communication 6→7
    comm6_7 [shape=ellipse, label="Communication\nGPU 6 → GPU 7\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 7 - GPU 7
    layer7_mha [label="Layer 7 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 7"];
    layer7_ffn [label="Layer 7 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 7"];
    
    // Communication 7→8
    comm7_8 [shape=ellipse, label="Communication\nGPU 7 → GPU 8\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 8 - GPU 8
    layer8_mha [label="Layer 8 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 8"];
    layer8_ffn [label="Layer 8 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 8"];
    
    // Communication 8→9
    comm8_9 [shape=ellipse, label="Communication\nGPU 8 → GPU 9\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 9 - GPU 9
    layer9_mha [label="Layer 9 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 9"];
    layer9_ffn [label="Layer 9 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 9"];
    
    // Communication 9→10
    comm9_10 [shape=ellipse, label="Communication\nGPU 9 → GPU 10\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 10 - GPU 10
    layer10_mha [label="Layer 10 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 10"];
    layer10_ffn [label="Layer 10 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 10"];
    
    // Communication 10→11
    comm10_11 [shape=ellipse, label="Communication\nGPU 10 → GPU 11\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 11 - GPU 11
    layer11_mha [label="Layer 11 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 11"];
    layer11_ffn [label="Layer 11 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 11"];
    
    // Communication 11→12
    comm11_12 [shape=ellipse, label="Communication\nGPU 11 → GPU 12\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 12 - GPU 12
    layer12_mha [label="Layer 12 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 12"];
    layer12_ffn [label="Layer 12 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 12"];
    
    // Communication 12→13
    comm12_13 [shape=ellipse, label="Communication\nGPU 12 → GPU 13\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 13 - GPU 13
    layer13_mha [label="Layer 13 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 13"];
    layer13_ffn [label="Layer 13 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 13"];
    
    // Communication 13→14
    comm13_14 [shape=ellipse, label="Communication\nGPU 13 → GPU 14\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 14 - GPU 14
    layer14_mha [label="Layer 14 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 14"];
    layer14_ffn [label="Layer 14 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 14"];
    
    // Communication 14→15
    comm14_15 [shape=ellipse, label="Communication\nGPU 14 → GPU 15\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]"];
    
    // Layer 15 - GPU 15
    layer15_mha [label="Layer 15 MHA\nInput: [batch=8, seq=10000, heads=32, d_k=128]\nOutput: [batch=8, seq=10000, heads=32, d_k=128]\nGPU: 15"];
    layer15_ffn [label="Layer 15 FFN\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 15"];
    
    // Output node
    output [shape=ellipse, label="Output\nInput: [batch=8, seq=10000, hidden=4096]\nOutput: [batch=8, seq=10000, hidden=4096]\nGPU: 15"];
    
    // Connections
    input -> layer0_mha;
    layer0_mha -> layer0_ffn;
    layer0_ffn -> comm0_1;
    comm0_1 -> layer1_mha;
    layer1_mha -> layer1_ffn;
    layer1_ffn -> comm1_2;
    comm1_2 -> layer2_mha;
    layer2_mha -> layer2_ffn;
    layer2_ffn -> comm2_3;
    comm2_3 -> layer3_mha;
    layer3_mha -> layer3_ffn;
    layer3_ffn -> comm3_4;
    comm3_4 -> layer4_mha;
    layer4_mha -> layer4_ffn;
    layer4_ffn -> comm4_5;
    comm4_5 -> layer5_mha;
    layer5_mha -> layer5_ffn;
    layer5_ffn -> comm5_6;
    comm5_6 -> layer6_mha;
    layer6_mha -> layer6_ffn;
    layer6_ffn -> comm6_7;
    comm6_7 -> layer7_mha;
    layer7_mha -> layer7_ffn;
    layer7_ffn -> comm7_8;
    comm7_8 -> layer8_mha;
    layer8_mha -> layer8_ffn;
    layer8_ffn -> comm8_9;
    comm8_9 -> layer9_mha;
    layer9_mha -> layer9_ffn;
    layer9_ffn -> comm9_10;
    comm9_10 -> layer10_mha;
    layer10_mha -> layer10_ffn;
    layer10_ffn -> comm10_11;
    comm10_11 -> layer11_mha;
    layer11_mha -> layer11_ffn;
    layer11_ffn -> comm11_12;
    comm11_12 -> layer12_mha;
    layer12_mha -> layer12_ffn;
    layer12_ffn -> comm12_13;
    comm12_13 -> layer13_mha;
    layer13_mha -> layer13_ffn;
    layer13_ffn -> comm13_14;
    comm13_14 -> layer14_mha;
    layer14_mha -> layer14_ffn;
    layer14_ffn -> comm14_15;
    comm14_15 -> layer15_mha;
    layer15_mha -> layer15_ffn;
    layer15_ffn -> output;
}