digraph baseline_tensor_pipeline {
    rankdir=TB;
    node [shape=rectangle];
    
    // Input node
    input [shape=ellipse, label="Input\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nGPU: All GPUs"];
    
    // Stage 0 - GPU 0-7 (TP=8)
    subgraph cluster_stage0 {
        label="Stage 0 - GPUs 0-7 (TP=8)";
        style=dashed;
        
        layer0_mha [label="Layer 0 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7"];
        layer0_ffn [label="Layer 0 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7"];
        
        layer1_mha [label="Layer 1 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7"];
        layer1_ffn [label="Layer 1 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7"];
        
        layer2_mha [label="Layer 2 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7"];
        layer2_ffn [label="Layer 2 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7"];
        
        layer3_mha [label="Layer 3 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7"];
        layer3_ffn [label="Layer 3 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7"];
        
        layer4_mha [label="Layer 4 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7"];
        layer4_ffn [label="Layer 4 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7"];
        
        layer5_mha [label="Layer 5 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7"];
        layer5_ffn [label="Layer 5 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7"];
        
        layer6_mha [label="Layer 6 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7"];
        layer6_ffn [label="Layer 6 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7"];
        
        layer7_mha [label="Layer 7 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7"];
        layer7_ffn [label="Layer 7 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7"];
    }
    
    // Communication between stages
    comm_stage0_to_stage1 [shape=ellipse, label="Communication\nStage 0 â†’ Stage 1\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 â†’ 8-15"];
    
    // Stage 1 - GPU 8-15 (TP=8)
    subgraph cluster_stage1 {
        label="Stage 1 - GPUs 8-15 (TP=8)";
        style=dashed;
        
        layer8_mha [label="Layer 8 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 8-15"];
        layer8_ffn [label="Layer 8 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 8-15"];
        
        layer9_mha [label="Layer 9 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 8-15"];
        layer9_ffn [label="Layer 9 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 8-15"];
        
        layer10_mha [label="Layer 10 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 8-15"];
        layer10_ffn [label="Layer 10 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 8-15"];
        
        layer11_mha [label="Layer 11 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 8-15"];
        layer11_ffn [label="Layer 11 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 8-15"];
        
        layer12_mha [label="Layer 12 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 8-15"];
        layer12_ffn [label="Layer 12 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 8-15"];
        
        layer13_mha [label="Layer 13 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 8-15"];
        layer13_ffn [label="Layer 13 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 8-15"];
        
        layer14_mha [label="Layer 14 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 8-15"];
        layer14_ffn [label="Layer 14 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 8-15"];
        
        layer15_mha [label="Layer 15 MHA\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 8-15"];
        layer15_ffn [label="Layer 15 FFN\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 8-15"];
    }
    
    // Output node
    output [shape=ellipse, label="Output\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 8-15"];
    
    // Connections
    input -> layer0_mha;
    layer0_mha -> layer0_ffn;
    layer0_ffn -> layer1_mha;
    layer1_mha -> layer1_ffn;
    layer1_ffn -> layer2_mha;
    layer2_mha -> layer2_ffn;
    layer2_ffn -> layer3_mha;
    layer3_mha -> layer3_ffn;
    layer3_ffn -> layer4_mha;
    layer4_mha -> layer4_ffn;
    layer4_ffn -> layer5_mha;
    layer5_mha -> layer5_ffn;
    layer5_ffn -> layer6_mha;
    layer6_mha -> layer6_ffn;
    layer6_ffn -> layer7_mha;
    layer7_mha -> layer7_ffn;
    layer7_ffn -> comm_stage0_to_stage1;
    
    comm_stage0_to_stage1 -> layer8_mha;
    layer8_mha -> layer8_ffn;
    layer8_ffn -> layer9_mha;
    layer9_mha -> layer9_ffn;
    layer9_ffn -> layer10_mha;
    layer10_mha -> layer10_ffn;
    layer10_ffn -> layer11_mha;
    layer11_mha -> layer11_ffn;
    layer11_ffn -> layer12_mha;
    layer12_mha -> layer12_ffn;
    layer12_ffn -> layer13_mha;
    layer13_mha -> layer13_ffn;
    layer13_ffn -> layer14_mha;
    layer14_mha -> layer14_ffn;
    layer14_ffn -> layer15_mha;
    layer15_mha -> layer15_ffn;
    layer15_ffn -> output;
}