{
  "generated_dags": [
    {
      "name": "baseline_tensor_pipeline_parallelism",
      "description": "Corrected DAG for baseline deployment using tensor parallelism (TP=8) and pipeline parallelism (PP=2) across 16 GPUs with explicit layer-by-layer representation",
      "dot_path": "../outputs/2025-11-21-15-38-12/baseline_tensor_pipeline_dag.dot",
      "svg_path": "../outputs/2025-11-21-15-38-12/baseline_tensor_pipeline_dag.svg",
      "deployment_strategy": {
        "type": "tensor_parallelism_plus_pipeline",
        "tensor_parallel_size": 8,
        "pipeline_parallel_size": 2,
        "total_gpus": 16,
        "layers_per_stage": 8,
        "stage0_gpus": "0-7",
        "stage1_gpus": "8-15"
      },
      "model_specifications": {
        "total_layers": 16,
        "hidden_size": 4096,
        "batch_size": 128,
        "sequence_length": 10000,
        "heads": 32,
        "head_dim": 128,
        "mlp_hidden": 16384
      },
      "validation": {
        "has_cycles": false,
        "input_node_present": true,
        "output_node_present": true,
        "gpu_specifications_complete": true,
        "explicit_layer_representations": true,
        "layer_count_matches_gpus": true
      }
    },
    {
      "name": "proposed_layer_wise_partitioning",
      "description": "Corrected DAG for proposed layer-wise partitioning deployment with 1 layer per GPU across 16 GPUs, optimized for cache-fitting with explicit individual layer representations",
      "dot_path": "../outputs/2025-11-21-15-38-12/proposed_layerwise_dag.dot",
      "svg_path": "../outputs/2025-11-21-15-38-12/proposed_layerwise_dag.svg",
      "deployment_strategy": {
        "type": "layerwise_partitioning",
        "layers_per_gpu": 1,
        "total_gpus": 16,
        "gpu_assignments": {
          "layer_0": "GPU 0",
          "layer_1": "GPU 1",
          "layer_2": "GPU 2",
          "layer_3": "GPU 3",
          "layer_4": "GPU 4",
          "layer_5": "GPU 5",
          "layer_6": "GPU 6",
          "layer_7": "GPU 7",
          "layer_8": "GPU 8",
          "layer_9": "GPU 9",
          "layer_10": "GPU 10",
          "layer_11": "GPU 11",
          "layer_12": "GPU 12",
          "layer_13": "GPU 13",
          "layer_14": "GPU 14",
          "layer_15": "GPU 15"
        },
        "cache_constraint_mb": 50,
        "optimization_strategies": ["gradient_checkpointing", "batch_size_reduction"]
      },
      "model_specifications": {
        "total_layers": 16,
        "hidden_size": 4096,
        "optimized_batch_size": 8,
        "sequence_length": 10000,
        "heads": 32,
        "head_dim": 128,
        "mlp_hidden": 16384,
        "memory_per_layer_gb": 9.5
      },
      "validation": {
        "has_cycles": false,
        "input_node_present": true,
        "output_node_present": true,
        "gpu_specifications_complete": true,
        "explicit_layer_representations": true,
        "layer_count_matches_gpus": true
      }
    }
  ]
}