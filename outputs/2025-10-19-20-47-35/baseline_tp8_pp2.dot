digraph baseline_tp8_pp2 {
    comment="Dense Transformer Baseline Deployment: TP=8, PP=2"
    rankdir=TB
    size="100,100"
    compound=true
    
    // Input node
    input [label="Input\n[batch_size=1024, seq_len=10000, d_model=8192]", shape=ellipse, style=filled, fillcolor=lightyellow]
    
    // Pipeline Stage 0: Devices 0-7 (Layers 0-1)
    subgraph cluster_stage0 {
        label="Pipeline Stage 0 (Devices 0-7)"
        style=dashed
        color=blue
        
        // Layer 0 - Attention
        layernorm_0_attn [label="LayerNorm L0-Attn\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
        q_proj_0 [label="Q Projection L0\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
        k_proj_0 [label="K Projection L0\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
        v_proj_0 [label="V Projection L0\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
        
        attention_0 [label="Multi-Head Attention L0\nDevices 0-7\nInput: QKV [1024,10000,512]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
        o_proj_0 [label="O Projection L0\nDevice 0\nInput: [1024,10000,512]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
        residual_0_attn [label="Residual Add L0-Attn\nDevice 0\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
        
        // Layer 0 - MLP
        layernorm_0_mlp [label="LayerNorm L0-MLP\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
        gate_proj_0 [label="Gate Projection L0\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightblue]
        up_proj_0 [label="Up Projection L0\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightblue]
        mlp_activation_0 [label="Swish Activation L0\nDevice 0\nInput: [1024,10000,16384]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightblue]
        down_proj_0 [label="Down Projection L0\nDevice 0\nInput: [1024,10000,16384]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
        residual_0_mlp [label="Residual Add L0-MLP\nDevice 0\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
        
        // Layer 1 - Attention
        layernorm_1_attn [label="LayerNorm L1-Attn\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
        q_proj_1 [label="Q Projection L1\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
        k_proj_1 [label="K Projection L1\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
        v_proj_1 [label="V Projection L1\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
        
        attention_1 [label="Multi-Head Attention L1\nDevices 0-7\nInput: QKV [1024,10000,512]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
        o_proj_1 [label="O Projection L1\nDevice 0\nInput: [1024,10000,512]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
        residual_1_attn [label="Residual Add L1-Attn\nDevice 0\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
        
        // Layer 1 - MLP
        layernorm_1_mlp [label="LayerNorm L1-MLP\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
        gate_proj_1 [label="Gate Projection L1\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightblue]
        up_proj_1 [label="Up Projection L1\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightblue]
        mlp_activation_1 [label="Swish Activation L1\nDevice 0\nInput: [1024,10000,16384]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightblue]
        down_proj_1 [label="Down Projection L1\nDevice 0\nInput: [1024,10000,16384]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
        residual_1_mlp [label="Residual Add L1-MLP\nDevice 0\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
    }
    
    // Pipeline Stage 1: Devices 8-15 (Layers 2-3)
    subgraph cluster_stage1 {
        label="Pipeline Stage 1 (Devices 8-15)"
        style=dashed
        color=green
        
        // Layer 2 - Attention
        layernorm_2_attn [label="LayerNorm L2-aten\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
        q_proj_2 [label="Q Projection L2\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
        k_proj_2 [label="K Projection L2\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
        v_proj_2 [label="V Projection L2\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        attention_2 [label="Multi-Head Attention L2\nDevices 8-15\nInput: QKV [1024,10000,512]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
        o_proj_2 [label="O Projection L2\nDevice 8\nInput: [1024,10000,512]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
        residual_2_attn [label="Residual Add L2-Attn\nDevice 8\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
        
        // Layer 2 - MLP
        layernorm_2_mlp [label="LayerNorm L2-MLP\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
        gate_proj_2 [label="Gate Projection L2\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightgreen]
        up_proj_2 [label="Up Projection L2\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightgreen]
        mlp_activation_2 [label="Swish Activation L2\nDevice 8\nInput: [1024,10000,16384]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightgreen]
        down_proj_2 [label="Down Projection L2\nDevice 8\nInput: [1024,10000,16384]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
        residual_2_mlp [label="Residual Add L2-MLP\nDevice 8\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
        
        // Layer 3 - Attention
        layernorm_3_attn [label="LayerNorm L3-Attn\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
        q_proj_3 [label="Q Projection L3\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
        k_proj_3 [label="K Projection L3\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
        v_proj_3 [label="V Projection L3\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        attention_3 [label="Multi-Head Attention L3\nDevices 8-15\nInput: QKV [1024,10000,512]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
        o_proj_3 [label="O Projection L3\nDevice 8\nInput: [1024,10000,512]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
        residual_3_attn [label="Residual Add L3-Attn\nDevice 8\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
        
        // Layer 3 - MLP
        layernorm_3_mlp [label="LayerNorm L3-MLP\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
        gate_proj_3 [label="Gate Projection L3\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightgreen]
        up_proj_3 [label="Up Projection L3\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightgreen]
        mlp_activation_3 [label="Swish Activation L3\nDevice 8\nInput: [1024,10000,16384]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightgreen]
        down_proj_3 [label="Down Projection L3\nDevice 8\nInput: [1024,10000,16384]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
        residual_3_mlp [label="Residual Add L3-MLP\nDevice 8\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
    }
    
    // Communication nodes
    all_reduce_0_attn [label="AllReduce TP\nDevices 0-7\n[1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=orange]
    all_reduce_0_mlp [label="AllReduce TP\nDevices 0-7\n[1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=orange]
    all_reduce_1_attn [label="AllReduce TP\nDevices 8-15\n[1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=orange]
    all_reduce_1_mlp [label="AllReduce TP\nDevices 8-15\n[1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=orange]
    pipeline_send [label="Pipeline Send\nStage 0 → Stage 1\n[1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=purple]
    
    // Output
    output [label="Output\n[batch_size=1024, seq_len=10000, d_model=8192]", shape=ellipse, style=filled, fillcolor=lightyellow]
    
    // Connections - Stage 0, Layer 0
    input -> layernorm_0_attn
    layernorm_0_attn -> q_proj_0
    layernorm_0_attn -> k_proj_0
    layernorm_0_attn -> v_proj_0
    q_proj_0 -> attention_0
    k_proj_0 -> attention_0
    v_proj_0 -> attention_0
    attention_0 -> o_proj_0
    o_proj_0 -> all_reduce_0_attn
    all_reduce_0_attn -> residual_0_attn
    input -> residual_0_attn
    
    residual_0_attn -> layernorm_0_mlp
    layernorm_0_mlp -> gate_proj_0
    layernorm_0_mlp -> up_proj_0
    gate_proj_0 -> mlp_activation_0
    up_proj_0 -> mlp_activation_0
    mlp_activation_0 -> down_proj_0
    down_proj_0 -> all_reduce_0_mlp
    all_reduce_0_mlp -> residual_0_mlp
    residual_0_attn -> residual_0_mlp
    
    // Connections - Stage 0, Layer 1
    residual_0_mlp -> layernorm_1_attn
    layernorm_1_attn -> q_proj_1
    layernorm_1_attn -> k_proj_1
    layernorm_1_attn -> v_proj_1
    q_proj_1 -> attention_1
    k_proj_1 -> attention_1
    v_proj_1 -> attention_1
    attention_1 -> o_proj_1
    o_proj_1 -> all_reduce_0_attn
    all_reduce_0_attn -> residual_1_attn
    residual_0_mlp -> residual_1_attn
    
    residual_1_attn -> layernorm_1_mlp
    layernorm_1_mlp -> gate_proj_1
    layernorm_1_mlp -> up_proj_1
    gate_proj_1 -> mlp_activation_1
    up_proj_1 -> mlp_activation_1
    mlp_activation_1 -> down_proj_1
    down_proj_1 -> all_reduce_0_mlp
    all_reduce_0_mlp -> residual_1_mlp
    residual_1_attn -> residual_1_mlp
    
    // Pipeline communication
    residual_1_mlp -> pipeline_send
    
    // Connections - Stage 1, Layer 2
    pipeline_send -> layernorm_2_attn
    layernorm_2_attn -> q_proj_2
    layernorm_2_attn -> k_proj_2
    layernorm_2_attn -> v_proj_2
    q_proj_2 -> attention_2
    k_proj_2 -> attention_2
    v_proj_2 -> attention_2
    attention_2 -> o_proj_2
    o_proj_2 -> all_reduce_1_attn
    all_reduce_1_attn -> residual_2_attn
    pipeline_send -> residual_2_attn
    
    residual_2_attn -> layernorm_2_mlp
    layernorm_2_mlp -> gate_proj_2
    layernorm_2_mlp -> up_proj_2
    gate_proj_2 -> mlp_activation_2
    up_proj_2 -> mlp_activation_2
    mlp_activation_2 -> down_proj_2
    down_proj_2 -> all_reduce_1_mlp
    all_reduce_1_mlp -> residual_2_mlp
    residual_2_attn -> residual_2_mlp
    
    // Connections - Stage 1, Layer 3
    residual_2_mlp -> layernorm_3_attn
    layernorm_3_attn -> q_proj_3
    layernorm_3_attn -> k_proj_3
    layernorm_3_attn -> v_proj_3
    q_proj_3 -> attention_3
    k_proj_3 -> attention_3
    v_proj_3 -> attention_3
    attention_3 -> o_proj_3
    o_proj_3 -> all_reduce_1_attn
    all_reduce_1_attn -> residual_3_attn
    residual_2_mlp -> residual_3_attn
    
    residual_3_attn -> layernorm_3_mlp
    layernorm_3_mlp -> gate_proj_3
    layernorm_3_mlp -> up_proj_3
    gate_proj_3 -> mlp_activation_3
    up_proj_3 -> mlp_activation_3
    mlp_activation_3 -> down_proj_3
    down_proj_3 -> all_reduce_1_mlp
    all_reduce_1_mlp -> residual_3_mlp
    residual_3_attn -> residual_3_mlp
    
    residual_3_mlp -> output
}