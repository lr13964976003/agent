digraph baseline_tp8_pp2_complete {
    comment="Dense Transformer Baseline: TP=8, PP=2 - Complete 4-Layer DAG"
    rankdir=TB
    size="150,120"
    compound=true
    
    // Input node
    input [label="Input\n[batch_size=1024, seq_len=10000, d_model=8192]", shape=ellipse, style=filled, fillcolor=lightyellow]
    
    // ========== PIPELINE STAGE 0 (Devices 0-7) - Layers 0-1 ==========
    subgraph cluster_stage0 {
        label="Pipeline Stage 0 (Devices 0-7)"
        style=dashed
        color=blue
        
        // ========== LAYER 0 ==========
        subgraph cluster_layer0 {
            label="Layer 0 (Devices 0-7)"
            style=dashed
            color=lightblue
            
            // LayerNorm for attention
            layernorm_0_attn [label="LayerNorm L0\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
            
            // QKV Projections - tensor parallel across devices 0-7
            q_proj_0 [label="Q Projection L0\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
            k_proj_0 [label="K Projection L0\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
            v_proj_0 [label="V Projection L0\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
            
            // Multi-head attention - tensor parallel across devices 0-7
            attention_0 [label="Multi-Head Attention L0\nDevices 0-7\nInput: Q[1024,10000,512], K[1024,10000,512], V[1024,10000,512]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
            
            // Output projection - tensor parallel
            o_proj_0 [label="O Projection L0\nDevices 0-7\nInput: [1024,10000,512]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
            
            // All-reduce for attention output
            all_reduce_0_attn [label="AllReduce TP\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=orange]
            
            // Residual connection for attention
            residual_0_attn [label="Residual Add L0-Attn\nDevice 0\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
            
            // MLP components
            layernorm_0_mlp [label="LayerNorm L0-MLP\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
            
            // MLP projections - tensor parallel
            gate_proj_0 [label="Gate Projection L0\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightblue]
            up_proj_0 [label="Up Projection L0\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightblue]
            
            // Activation
            mlp_activation_0 [label="Swish Activation L0\nDevices 0-7\nInput: [1024,10000,16384]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightblue]
            
            // Down projection
            down_proj_0 [label="Down Projection L0\nDevices 0-7\nInput: [1024,10000,16384]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
            
            // All-reduce for MLP output
            all_reduce_0_mlp [label="AllReduce TP\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=orange]
            
            // Residual connection for MLP
            residual_0_mlp [label="Residual Add L0-MLP\nDevice 0\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
        }
        
        // ========== LAYER 1 ==========
        subgraph cluster_layer1 {
            label="Layer 1 (Devices 0-7)"
            style=dashed
            color=lightgreen
            
            // LayerNorm for attention
            layernorm_1_attn [label="LayerNorm L1\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
            
            // QKV projections
            q_proj_1 [label="Q Projection L1\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
            k_proj_1 [label="K Projection L1\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
            v_proj_1 [label="V Projection L1\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
            
            // Attention
            attention_1 [label="Multi-Head Attention L1\nDevices 0-7\nInput: Q[1024,10000,512], K[1024,10000,512], V[1024,10000,512]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightblue]
            o_proj_1 [label="O Projection L1\nDevices 0-7\nInput: [1024,10000,512]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
            all_reduce_1_attn [label="AllReduce TP\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=orange]
            residual_1_attn [label="Residual Add L1-Attn\nDevice 0\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
            
            // MLP components
            layernorm_1_mlp [label="LayerNorm L1-MLP\nDevice 0\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
            gate_proj_1 [label="Gate Projection L1\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightblue]
            up_proj_1 [label="Up Projection L1\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightblue]
            mlp_activation_1 [label="Swish Activation L1\nDevices 0-7\nInput: [1024,10000,16384]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightblue]
            down_proj_1 [label="Down Projection L1\nDevices 0-7\nInput: [1024,10000,16384]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightblue]
            all_reduce_1_mlp [label="AllReduce TP\nDevices 0-7\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=orange]
            residual_1_mlp [label="Residual Add L1-MLP\nDevice 0\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
        }
    }
    
    // ========== PIPELINE STAGE 1 (Devices 8-15) - Layers 2-3 ==========
    subgraph cluster_stage1 {
        label="Pipeline Stage 1 (Devices 8-15)"
        style=dashed
        color=green
        
        // ========== LAYER 2 ==========
        subgraph cluster_layer2 {
            label="Layer 2 (Devices 8-15)"
            style=dashed
            color=lightgreen
            
            // LayerNorm for attention
            layernorm_2_attn [label="LayerNorm L2\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
            
            // QKV projections
            q_proj_2 [label="Q Projection L2\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
            k_proj_2 [label="K Projection L2\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
            v_proj_2 [label="V Projection L2\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
            
            // Attention
            attention_2 [label="Multi-Head Attention L2\nDevices 8-15\nInput: Q[1024,10000,512], K[1024,10000,512], V[1024,10000,512]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
            o_proj_2 [label="O Projection L2\nDevices 8-15\nInput: [1024,10000,512]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
            all_reduce_2_attn [label="AllReduce TP\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=orange]
            residual_2_attn [label="Residual Add L2-Attn\nDevice 8\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
            
            // MLP components
            layernorm_2_mlp [label="LayerNorm L2-MLP\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
            gate_proj_2 [label="Gate Projection L2\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightgreen]
            up_proj_2 [label="Up Projection L2\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightgreen]
            mlp_activation_2 [label="Swish Activation L2\nDevices 8-15\nInput: [1024,10000,16384]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightgreen]
            down_proj_2 [label="Down Projection L2\nDevices 8-15\nInput: [1024,10000,16384]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
            all_reduce_2_mlp [label="AllReduce TP\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=orange]
            residual_2_mlp [label="Residual Add L2-MLP\nDevice 8\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
        }
        
        // ========== LAYER 3 ==========
        subgraph cluster_layer3 {
            label="Layer 3 (Devices 8-15)"
            style=dashed
            color=darkgreen
            
            // LayerNorm for attention
            layernorm_3_attn [label="LayerNorm L3\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
            
            // QKV projections
            q_proj_3 [label="Q Projection L3\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
            k_proj_3 [label="K Projection L3\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
            v_proj_3 [label="V Projection L3\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
            
            // Attention
            attention_3 [label="Multi-Head Attention L3\nDevices 8-15\nInput: Q[1024,10000,512], K[1024,10000,512], V[1024,10000,512]\nOutput: [1024,10000,512]", shape=rectangle, style=filled, fillcolor=lightgreen]
            o_proj_3 [label="O Projection L3\nDevices 8-15\nInput: [1024,10000,512]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
            all_reduce_3_attn [label="AllReduce TP\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=orange]
            residual_3_attn [label="Residual Add L3-Attn\nDevice 8\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
            
            // MLP components
            layernorm_3_mlp [label="LayerNorm L3-MLP\nDevice 8\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
            gate_proj_3 [label="Gate Projection L3\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightgreen]
            up_proj_3 [label="Up Projection L3\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightgreen]
            mlp_activation_3 [label="Swish Activation L3\nDevices 8-15\nInput: [1024,10000,16384]\nOutput: [1024,10000,16384]", shape=rectangle, style=filled, fillcolor=lightgreen]
            down_proj_3 [label="Down Projection L3\nDevices 8-15\nInput: [1024,10000,16384]\nOutput: [1024,10000,8192]", shape=rectangle, style=filled, fillcolor=lightgreen]
            all_reduce_3_mlp [label="AllReduce TP\nDevices 8-15\nInput: [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=orange]
            residual_3_mlp [label="Residual Add L3-MLP\nDevice 8\nInput: [1024,10000,8192], [1024,10000,8192]\nOutput: [1024,10000,8192]", shape=diamond, style=filled, fillcolor=lightcoral]
        }
    }
    
    // Pipeline communication
    pipeline_send [label="Pipeline Send\nStage 0 â†’ Stage 1\n[1024,10000,8192]", shape=parallelogram, style=filled, fillcolor=purple]
    
    // Output
    output [label="Output\n[batch_size=1024, seq_len=10000, d_model=8192]", shape=ellipse, style=filled, fillcolor=lightyellow]
    
    // ========== CONNECTIONS ==========
    // Layer 0
    input -> layernorm_0_attn
    layernorm_0_attn -> q_proj_0
    layernorm_0_attn -> k_proj_0
    layernorm_0_attn -> v_proj_0
    q_proj_0 -> attention_0
    k_proj_0 -> attention_0
    v_proj_0 -> attention_0
    attention_0 -> o_proj_0
    o_proj_0 -> all_reduce_0_attn
    all_reduce_0_attn -> residual_0_attn
    input -> residual_0_attn
    
    residual_0_attn -> layernorm_0_mlp
    layernorm_0_mlp -> gate_proj_0
    layernorm_0_mlp -> up_proj_0
    gate_proj_0 -> mlp_activation_0
    up_proj_0 -> mlp_activation_0
    mlp_activation_0 -> down_proj_0
    down_proj_0 -> all_reduce_0_mlp
    all_reduce_0_mlp -> residual_0_mlp
    residual_0_attn -> residual_0_mlp
    
    // Layer 1
    residual_0_mlp -> layernorm_1_attn
    layernorm_1_attn -> q_proj_1
    layernorm_1_attn -> k_proj_1
    layernorm_1_attn -> v_proj_1
    q_proj_1 -> attention_1
    k_proj_1 -> attention_1
    v_proj_1 -> attention_1
    attention_1 -> o_proj_1
    o_proj_1 -> all_reduce_1_attn
    all_reduce_1_attn -> residual_1_attn
    residual_0_mlp -> residual_1_attn
    
    residual_1_attn -> layernorm_1_mlp
    layernorm_1_mlp -> gate_proj_1
    layernorm_1_mlp -> up_proj_1
    gate_proj_1 -> mlp_activation_1
    up_proj_1 -> mlp_activation_1
    mlp_activation_1 -> down_proj_1
    down_proj_1 -> all_reduce_1_mlp
    all_reduce_1_mlp -> residual_1_mlp
    residual_1_attn -> residual_1_mlp
    
    // Pipeline communication
    residual_1_mlp -> pipeline_send
    
    // Layer 2
    pipeline_send -> layernorm_2_attn
    layernorm_2_attn -> q_proj_2
    layernorm_2_attn -> k_proj_2
    layernorm_2_attn -> v_proj_2
    q_proj_2 -> attention_2
    k_proj_2 -> attention_2
    v_proj_2 -> attention_2
    attention_2 -> o_proj_2
    o_proj_2 -> all_reduce_2_attn
    all_reduce_2_attn -> residual_2_attn
    pipeline_send -> residual_2_attn
    
    residual_2_attn -> layernorm_2_mlp
    layernorm_2_mlp -> gate_proj_2
    layernorm_2_mlp -> up_proj_2
    gate_proj_2 -> mlp_activation_2
    up_proj_2 -> mlp_activation_2
    mlp_activation_2 -> down_proj_2
    down_proj_2 -> all_reduce_2_mlp
    all_reduce_2_mlp -> residual_2_mlp
    residual_2_attn -> residual_2_mlp
    
    // Layer 3
    residual_2_mlp -> layernorm_3_attn
    layernorm_3_attn -> q_proj_3
    layernorm_3_attn -> k_proj_3
    layernorm_3_attn -> v_proj_3
    q_proj_3 -> attention_3
    k_proj_3 -> attention_3
    v_proj_3 -> attention_3
    attention_3 -> o_proj_3
    o_proj_3 -> all_reduce_3_attn
    all_reduce_3_attn -> residual_3_attn
    residual_2_mlp -> residual_3_attn
    
    residual_3_attn -> layernorm_3_mlp
    layernorm_3_mlp -> gate_proj_3
    layernorm_3_mlp -> up_proj_3
    gate_proj_3 -> mlp_activation_3
    up_proj_3 -> mlp_activation_3
    mlp_activation_3 -> down_proj_3
    down_proj_3 -> all_reduce_3_mlp
    all_reduce_3_mlp -> residual_3_mlp
    residual_3_attn -> residual_3_mlp
    
    residual_3_mlp -> output
}