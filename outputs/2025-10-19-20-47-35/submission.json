{
  "baseline_tp8_pp2_fixed": {
    "path": "../outputs/2025-10-19-20-47-35/baseline_tp8_pp2.fixed.dot",
    "description": "Fixed baseline DAG with Tensor Parallelism (TP=8) and Pipeline Parallelism (PP=2) across 16 GPUs",
    "structure": "4-layer dense transformer with proper layer norm connections and no cycles"
  },
  "proposed_ra_sp": {
    "path": "../outputs/2025-10-19-20-47-35/proposed_ra_sp.dot", 
    "description": "Ring Attention + Sequence Parallelism DAG across 16 GPUs",
    "structure": "Sequence parallelism splitting 10000 tokens into 625-token chunks per device, with ring-based attention communication"
  },
  "baseline_original": {
    "path": "../outputs/2025-10-19-20-47-35/baseline_tp8_pp2.dot",
    "description": "Original baseline DAG with known issues documented for reference"
  }
}