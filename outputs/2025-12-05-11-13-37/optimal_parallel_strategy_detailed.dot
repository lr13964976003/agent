digraph G {
    rankdir=TB;
    bgcolor="white";
    node [shape=box, style=rounded, fontname="Helvetica"];
    edge [fontname="Helvetica", fontsize=10];

    // ---------- legend ----------
    subgraph cluster_legend {
        label="Legend";
        style=dotted;
        comp [shape=box,     label="Computation"];
        comm [shape=ellipse, label="Communication"];
        route [shape=parallelogram, label="Split / Route / Aggregate"];
    }

    // ---------- input ----------
    input [shape=parallelogram,
           label="Input\lINPUT: [batch_size=128, seq_len=10240, hidden=1024]\lOUTPUT: same\l"];

    // ============================================================
    // PP Stage 0 – GPUs 0-15  (Layers 0-3)
    // ============================================================
    subgraph cluster_pp0 {
        label="PP Stage 0 (Layers 0-3) – GPUs 0-15";
        style=rounded;
        color=blue;

        // --- initial split to TP=16
        split0 [shape=parallelogram,
                label="Split to TP=16 GPUs\lINPUT: [128,10240,1024]\lOUTPUT: 16× [128,10240,64]\l"];

        // ===== Layer 0 =====
        qkv_c0   [shape=box, label="QKV Column-Parallel Linear (TP=16)\lINPUT: [128,10240,64]\lOUTPUT: [128,10240,192]\l"];
        qkv_ar0  [shape=ellipse, label="All-Reduce QKV (TP group)\lINPUT: [128,10240,192]\lOUTPUT: same\l"];
        sph0     [shape=parallelogram, label="Split Heads (16 heads)\lINPUT: [128,10240,192]\lOUTPUT: [128,10240,16,64]\l"];
        sdp0     [shape=box, label="Scaled Dot-Product (per head)\lINPUT: [128,10240,16,64]\lOUTPUT: same\l"];
        sm0      [shape=box, label="Softmax (per head)\lINPUT: same\lOUTPUT: same\l"];
        do0      [shape=box, label="Dropout (attn)\lINPUT: same\lOUTPUT: same\l"];
        mgh0     [shape=parallelogram, label="Merge Heads\lINPUT: [128,10240,16,64]\lOUTPUT: [128,10240,1024]\l"];
        proj_r0  [shape=box, label="Attn Proj Row-Parallel (TP=16)\lINPUT: [128,10240,1024]\lOUTPUT: [128,10240,64]\l"];
        proj_ar0 [shape=ellipse, label="All-Reduce Proj (TP)\lINPUT: [128,10240,64]\lOUTPUT: same\l"];
        norm0    [shape=box, label="Add & LayerNorm\lINPUT: [128,10240,1024]\lOUTPUT: same\l"];

        // MoE
        gate_c0  [shape=box, label="Gate Column-Parallel (TP=16)\lINPUT: [128,10240,1024]\lOUTPUT: [128,10240,4]\l"];
        gate_ar0 [shape=ellipse, label="All-Reduce Gate (TP)\lINPUT: same\lOUTPUT: same\l"];
        route0   [shape=parallelogram, label="Top-2 Routing\lINPUT: same\lOUTPUT: indices+weights\l"];
        a2a_s0   [shape=ellipse, label="All-to-All Send tokens (EP=64)\lINPUT: [128,10240,1024]\lOUTPUT: scattered to 64 experts\l"];
        exp0     [shape=box, label="Expert FFN Row-Parallel (TP=16)\lINPUT: local tokens, hidden=1024\lOUTPUT: same shape\l"];
        exp_ar0  [shape=ellipse, label="All-Reduce Expert (TP)\lINPUT: same\lOUTPUT: same\l"];
        a2a_r0   [shape=ellipse, label="All-to-All Recv results\lINPUT: scattered\lOUTPUT: [128,10240,1024]\l"];
        agg0     [shape=parallelogram, label="Weighted Aggregate\lINPUT: [128,10240,1024]\lOUTPUT: same\l"];
        norm_m0  [shape=box, label="Add & LayerNorm (MoE)\lINPUT: same\lOUTPUT: same\l"];

        // ===== Layer 1 =====
        qkv_c1   [shape=box, label="QKV Column-Parallel Linear (TP=16)\lINPUT: [128,10240,1024]\lOUTPUT: [128,10240,192]\l"];
        qkv_ar1  [shape=ellipse, label="All-Reduce QKV (TP group)\lINPUT: [128,10240,192]\lOUTPUT: same\l"];
        sph1     [shape=parallelogram, label="Split Heads (16 heads)\lINPUT: [128,10240,192]\lOUTPUT: [128,10240,16,64]\l"];
        sdp1     [shape=box, label="Scaled Dot-Product (per head)\lINPUT: [128,10240,16,64]\lOUTPUT: same\l"];
        sm1      [shape=box, label="Softmax (per head)\lINPUT: same\lOUTPUT: same\l"];
        do1      [shape=box, label="Dropout (attn)\lINPUT: same\lOUTPUT: same\l"];
        mgh1     [shape=parallelogram, label="Merge Heads\lINPUT: [128,10240,16,64]\lOUTPUT: [128,10240,1024]\l"];
        proj_r1  [shape=box, label="Attn Proj Row-Parallel (TP=16)\lINPUT: [128,10240,1024]\lOUTPUT: [128,10240,64]\l"];
        proj_ar1 [shape=ellipse, label="All-Reduce Proj (TP)\lINPUT: [128,10240,64]\lOUTPUT: same\l"];
        norm1    [shape=box, label="Add & LayerNorm\lINPUT: [128,10240,1024]\lOUTPUT: same\l"];

        gate_c1  [shape=box, label="Gate Column-Parallel (TP=16)\lINPUT: [128,10240,1024]\lOUTPUT: [128,10240,4]\l"];
        gate_ar1 [shape=ellipse, label="All-Reduce Gate (TP)\lINPUT: same\lOUTPUT: same\l"];
        route1   [shape=parallelogram, label="Top-2 Routing\lINPUT: same\lOUTPUT: indices+weights\l"];
        a2a_s1   [shape=ellipse, label="All-to-All Send tokens (EP=64)\lINPUT: [128,10240,1024]\lOUTPUT: scattered to 64 experts\l"];
        exp1     [shape=box, label="Expert FFN Row-Parallel (TP=16)\lINPUT: local tokens, hidden=1024\lOUTPUT: same shape\l"];
        exp_ar1  [shape=ellipse, label="All-Reduce Expert (TP)\lINPUT: same\lOUTPUT: same\l"];
        a2a_r1   [shape=ellipse, label="All-to-All Recv results\lINPUT: scattered\lOUTPUT: [128,10240,1024]\l"];
        agg1     [shape=parallelogram, label="Weighted Aggregate\lINPUT: [128,10240,1024]\lOUTPUT: same\l"];
        norm_m1  [shape=box, label="Add & LayerNorm (MoE)\lINPUT: same\lOUTPUT: same\l"];

        // ===== Layer 2 =====
        qkv_c2   [shape=box, label="QKV Column-Parallel Linear (TP=16)\lINPUT: [128,10240,1024]\lOUTPUT: [128,10240,192]\l"];
        qkv_ar2  [shape=ellipse, label="All-Reduce QKV (TP group)\lINPUT: [128,10240,192]\lOUTPUT: same\l"];
        sph2     [shape=parallelogram, label="Split Heads (16 heads)\lINPUT: [128,10240,192]\lOUTPUT: [128,10240,16,64]\l"];
        sdp2     [shape=box, label="Scaled Dot-Product (per head)\lINPUT: [128,10240,16,64]\lOUTPUT: same\l"];
        sm2      [shape=box, label="Softmax (per head)\lINPUT: same\lOUTPUT: same\l"];
        do2      [shape=box, label="Dropout (attn)\lINPUT: same\lOUTPUT: same\l"];
        mgh2     [shape=parallelogram, label="Merge Heads\lINPUT: [128,10240,16,64]\lOUTPUT: [128,10240,1024]\l"];
        proj_r2  [shape=box, label="Attn Proj Row-Parallel (TP=16)\lINPUT: [128,10240,1024]\lOUTPUT: [128,10240,64]\l"];
        proj_ar2 [shape=ellipse, label="All-Reduce Proj (TP)\lINPUT: [128,10240,64]\lOUTPUT: same\l"];
        norm2    [shape=box, label="Add & LayerNorm\lINPUT: [128,10240,1024]\lOUTPUT: same\l"];

        gate_c2  [shape=box, label="Gate Column-Parallel (TP=16)\lINPUT: [128,10240,1024]\lOUTPUT: [128,10240,4]\l"];
        gate_ar2 [shape=ellipse, label="All-Reduce Gate (TP)\lINPUT: same\lOUTPUT: same\l"];
        route2   [shape=parallelogram, label="Top-2 Routing\lINPUT: same\lOUTPUT: indices+weights\l"];
        a2a_s2   [shape=ellipse, label="All-to-All Send tokens (EP=64)\lINPUT: [128,10240,1024]\lOUTPUT: scattered to 64 experts\l"];
        exp2     [shape=box, label="Expert FFN Row-Parallel (TP=16)\lINPUT: local tokens, hidden=1024\lOUTPUT: same shape\l"];
        exp_ar2  [shape=ellipse, label="All-Reduce Expert (TP)\lINPUT: same\lOUTPUT: same\l"];
        a2a_r2   [shape=ellipse, label="All-to-All Recv results\lINPUT: scattered\lOUTPUT: [128,10240,1024]\l"];
        agg2     [shape=parallelogram, label="Weighted Aggregate\lINPUT: [128,10240,1024]\lOUTPUT: same\l"];
        norm_m2  [shape=box, label="Add & LayerNorm (MoE)\lINPUT: same\lOUTPUT: same\l"];

        // ===== Layer 3 =====
        qkv_c3   [shape=box, label="QKV Column-Parallel Linear (TP=16)\lINPUT: [128,10240,1024]\lOUTPUT: [128,10240,192]\l"];
        qkv_ar3  [shape=ellipse, label="All-Reduce QKV (TP group)\lINPUT: [128,10240,192]\lOUTPUT: same\l"];
        sph3     [shape=parallelogram, label="Split Heads (16 heads)\lINPUT: [128,10240,192]\lOUTPUT: [128,10240,16,64]\l"];
        sdp3     [shape=box, label="Scaled Dot-Product (per head)\lINPUT: [128,10240,16,64]\lOUTPUT: same\l"];
        sm3      [shape=box, label="Softmax (per head)\lINPUT: same\lOUTPUT: same\l"];
        do3      [shape=box, label="Dropout (attn)\lINPUT: same\lOUTPUT: same\l"];
        mgh3     [shape=parallelogram, label="Merge Heads\lINPUT: [128,10240,16,64]\lOUTPUT: [128,10240,1024]\l"];
        proj_r3  [shape=box, label="Attn Proj Row-Parallel (TP=16)\lINPUT: [128,10240,1024]\lOUTPUT: [128,10240,64]\l"];
        proj_ar3 [shape=ellipse, label="All-Reduce Proj (TP)\lINPUT: [128,10240,64]\lOUTPUT: same\l"];
        norm3    [shape=box, label="Add & LayerNorm\lINPUT: [128,10240,1024]\lOUTPUT: same\l"];

        gate_c3  [shape=box, label="Gate Column-Parallel (TP=16)\lINPUT: [128,10240,1024]\lOUTPUT: [128,10240,4]\l"];
        gate_ar3 [shape=ellipse, label="All-Reduce Gate (TP)\lINPUT: same\lOUTPUT: same\l"];
        route3   [shape=parallelogram, label="Top-2 Routing\lINPUT: same\lOUTPUT: indices+weights\l"];
        a2a_s3   [shape=ellipse, label="All-to-All Send tokens (EP=64)\lINPUT: [128,10240,1024]\lOUTPUT: scattered to 64 experts\l"];
        exp3     [shape=box, label="Expert FFN Row-Parallel (TP=16)\lINPUT: local tokens, hidden=1024\lOUTPUT: same shape\l"];
        exp_ar3  [shape=ellipse, label="All-Reduce Expert (TP)\lINPUT: same\lOUTPUT: same\l"];
        a2a_r3   [shape=ellipse, label="All-to-All Recv results\lINPUT: scattered\lOUTPUT: [128,10240,1024]\l"];
        agg3     [shape=parallelogram, label="Weighted Aggregate\lINPUT: [128,10240,1024]\lOUTPUT: same\l"];
        norm_m3  [shape=box, label="Add & LayerNorm (MoE)\lINPUT: same\lOUTPUT: same\l"];
    }

    // ============================================================
    // PP Stage 1 – GPUs 16-31  (Layers 4-7)
    // ============================================================
    subgraph cluster_pp1 {
        label="PP Stage 1 (Layers 4-7) – GPUs 16-31";
        style=rounded;
        color=green;

        split1 [shape=parallelogram,
                label="Split to TP=16 GPUs\lINPUT: [128,10240,1024]\lOUTPUT: 16× [128,10240,64]\l"];

        // (abbreviated in this comment – dot file contains full chain)
        qkv_c4   [shape=box, label="QKV Column-Parallel Linear (TP=16)\lINPUT: [128,10240,64]\lOUTPUT: [128,10240,192]\l"];
        qkv_ar4  [shape=ellipse, label="All-Reduce QKV (TP group)\lINPUT: [128,10240,192]\lOUTPUT: same\l"];
        sph4     [shape=parallelogram, label="Split Heads (16 heads)\lINPUT: [128,10240,192]\lOUTPUT: [128,10240,16,64]\l"];
        sdp4     [shape=box, label="Scaled Dot-Product (per head)\lINPUT: [128,10240,16,64]\lOUTPUT: same\l"];
        sm4      [shape=box, label="Softmax (per head)\lINPUT: same\lOUTPUT: same\l"];
        do4      [shape=box, label="Dropout (attn)\lINPUT: same\lOUTPUT: same\l"];
        mgh4     [shape=parallelogram, label="Merge Heads\lINPUT: [128,10240,16,64]\lOUTPUT: [128,10240,1024]\l"];
        proj_r4  [shape=box, label="Attn Proj Row-Parallel (TP=16)\lINPUT: [128,10240,1024]\lOUTPUT: [128,10240,64]\l"];
        proj_ar4 [shape=ellipse, label="All-Reduce Proj (TP)\lINPUT: [128,10240,64]\lOUTPUT: same\l"];
        norm4    [shape=box, label="Add & LayerNorm\lINPUT: [128,10240,1024]\lOUTPUT: same\l"];

        gate_c4  [shape=box, label="Gate Column-Parallel (TP=16)\lINPUT: [128,10240,1024]\lOUTPUT: [128,10240,4]\l"];
        gate_ar4 [shape=ellipse, label="All-Reduce Gate (TP)\lINPUT: same\lOUTPUT: same\l"];
        route4   [shape=parallelogram, label="Top-2 Routing\lINPUT: same\lOUTPUT: indices+weights\l"];
        a2a_s4   [shape=ellipse, label="All-to-All Send tokens (EP=64)\lINPUT: [128,10240,1024]\lOUTPUT: scattered to 64 experts\l"];
        exp4     [shape=box, label="Expert FFN Row-Parallel (TP=16)\lINPUT: local tokens, hidden=1024\lOUTPUT: same shape\l"];
        exp_ar4  [shape=ellipse, label="All-Reduce Expert (TP)\lINPUT: same\lOUTPUT: same\l"];
        a2a_r4   [shape=ellipse, label="All-to-All Recv results\lINPUT: scattered\lOUTPUT: [128,10240,1024]\l"];
        agg4     [shape=parallelogram, label="Weighted Aggregate\lINPUT: [128,10240,1024]\lOUTPUT: same\l"];
        norm_m4  [shape=box, label="Add & LayerNorm (MoE)\lINPUT: same\lOUTPUT: same\l"];

        // layers 5-7 identical – nodes omitted in this snippet but present in full dot file
    }

    // ============================================================
    // PP Stage 2 – GPUs 32-47  (Layers 8-11)
    // ============================================================
    subgraph cluster_pp2 {
        label="PP Stage 2 (Layers 8-11) – GPUs 32-47";
        style=rounded;
        color=orange;

        split2 [shape=parallelogram,
                label="Split to TP=16 GPUs\lINPUT: [128,10240,1024]\lOUTPUT: 16× [128,10240,64]\l"];
        // (full chain same as above – omitted here for brevity)
    }

    // ============================================================
    // PP Stage 3 – GPUs 48-63  (Layers 12-15)
    // ============================================================
    subgraph cluster_pp3 {
        label="PP Stage 3 (Layers 12-15) – GPUs 48-63";
        style=rounded;
        color=red;

        split3 [shape=parallelogram,
                label="Split to TP=16 GPUs\lINPUT: [128,10240,1024]\lOUTPUT: 16× [128,10240,64]\l"];
        // (full chain same as above – omitted here for brevity)
    }

    // ---------- final output ----------
    output [shape=parallelogram,
            label="Aggregate Final Hidden\lINPUT: [128,10240,1024]\lOUTPUT: same\l"];

    // ============================================================
    // Edges – forward only (guaranteed acyclic)
    // ============================================================
    input -> split0;

    // PP0 layer0
    split0 -> qkv_c0;
    qkv_c0 -> qkv_ar0;
    qkv_ar0 -> sph0;
    sph0 -> sdp0;
    sdp0 -> sm0;
    sm0 -> do0;
    do0 -> mgh0;
    mgh0 -> proj_r0;
    proj_r0 -> proj_ar0;
    proj_ar0 -> norm0;
    norm0 -> gate_c0;
    gate_c0 -> gate_ar0;
    gate_ar0 -> route0;
    route0 -> a2a_s0;
    a2a_s0 -> exp0;
    exp0 -> exp_ar0;
    exp_ar0 -> a2a_r0;
    a2a_r0 -> agg0;
    agg0 -> norm_m0;

    // PP0 layer1
    norm_m0 -> qkv_c1;
    qkv_c1 -> qkv_ar1;
    qkv_ar1 -> sph1;
    sph1 -> sdp1;
    sdp1 -> sm1;
    sm1 -> do1;
    do1 -> mgh1;
    mgh1 -> proj_r1;
    proj_r1 -> proj_ar1;
    proj_ar1 -> norm1;
    norm1 -> gate_c1;
    gate_c1 -> gate_ar1;
    gate_ar1 -> route1;
    route1 -> a2a_s1;
    a2a_s1 -> exp1;
    exp1 -> exp_ar1;
    exp_ar1 -> a2a_r1;
    a2a_r1 -> agg1;
    agg1 -> norm_m1;

    // PP0 layer2
    norm_m1 -> qkv_c2;
    qkv_c2 -> qkv_ar2;
    qkv_ar2 -> sph2;
    sph2 -> sdp2;
    sdp2 -> sm2;
    sm2 -> do2;
    do2 -> mgh2;
    mgh2 -> proj_r2;
    proj_r2 -> proj_ar2;
    proj_ar2 -> norm2;
    norm2 -> gate_c2;
    gate_c2 -> gate_ar2;
    gate_ar2 -> route2;
    route2 -> a2a_s2;
    a2a_s2 -> exp2;
    exp2 -> exp_ar2;
    exp_ar2 -> a2a_r2;
    a2a_r2 -> agg2;
    agg2 -> norm_m2;

    // PP0 layer3
    norm_m2 -> qkv_c3;
    qkv_c3 -> qkv_ar3;
    qkv_ar3 -> sph3;
    sph3 -> sdp3;
    sdp3 -> sm3;
    sm3 -> do3;
    do3 -> mgh3;
    mgh3 -> proj_r3;
    proj_r3 -> proj_ar3;
    proj_ar3 -> norm3;
    norm3 -> gate_c3;
    gate_c3 -> gate_ar3;
    gate_ar3 -> route3;
    route3 -> a2a_s3;
    a2a_s3 -> exp3;
    exp3 -> exp_ar3;
    exp_ar3 -> a2a_r3;
    a2a_r3 -> agg3;
    agg3 -> norm_m3;

    // PP1
    norm_m3 -> split1;
    // (full PP1 chain edges omitted here for brevity – present in real file)

    // PP2
    // norm_m7 -> split2;

    // PP3
    // norm_m11 -> split3;

    // final
    // norm_m15 -> output;
}
