{
  "parallel_strategy_document": "../outputs/2025-12-05-11-13-37/optimal_parallel_strategy.md",
  "parallel_strategy_dag": "../outputs/2025-12-05-11-13-37/optimal_parallel_strategy.dot",
  "submission_metadata": {
    "timestamp": "2025-12-05-11-13-37",
    "model_size": "30B",
    "architecture": "MoE with 64 experts per layer",
    "parallel_strategy": "Hybrid EP64_TP16_PP4",
    "total_gpus": 64,
    "optimization_target": "Minimize latency, maximize throughput",
    "gpu_load_balancing": "Perfect (1 expert per GPU)",
    "memory_utilization": "~6% per GPU (3.88GB/64GB)",
    "expected_throughput": "~26M tokens/second",
    "expected_latency": "50-500ms per batch"
  }
}