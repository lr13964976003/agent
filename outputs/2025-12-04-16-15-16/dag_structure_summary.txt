LLM Deployment DAG Structure Summary
==================================================

Model Configuration:
- Total Layers: 16
- Hidden Size: 1024
- Attention Heads: 16
- Total Experts: 64

Parallel Configuration:
- Tensor Parallel (TP): 8 GPUs per group
- Pipeline Parallel (PP): 4 stages
- Expert Parallel (EP): 16 groups
- Data Parallel (DP): 4 replicas

Node Types:
- Rectangle: Computation operations
- Ellipse: Communication operations
- Parallelogram: Routing/Aggregation operations

Communication Types:
- Solid lines: Data flow within GPU
- Dashed lines: Inter-GPU communication
- Red dashed: Tensor Parallel All-Reduce
- Blue dashed: Expert Parallel Routing
- Green dashed: Data Parallel Gradient Sync
