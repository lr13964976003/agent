// LLM Layer Detail - All Parallel Dimensions
digraph {
	dpi=300 rankdir=LR size="15,10"
	node [fontsize=10 margin="0.1,0.05"]
	layer_input [label="Layer Input\n[batch=?, seq=?, hidden=1024]" fillcolor=lightblue shape=rectangle style=filled]
	tp_split_0 [label="TP Split 0\n[batch=?, seq=?, hidden=128]" fillcolor=yellow shape=parallelogram style=filled]
	attn_q_tp0 [label="Query\nTP0\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_k_tp0 [label="Key\nTP0\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_v_tp0 [label="Value\nTP0\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_out_tp0 [label="Attention Output\nTP0\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc1_tp0 [label="MLP FC1\nTP0\n[batch=?, seq=?, ffn=512]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc2_tp0 [label="MLP FC2\nTP0\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	tp_split_1 [label="TP Split 1\n[batch=?, seq=?, hidden=128]" fillcolor=yellow shape=parallelogram style=filled]
	attn_q_tp1 [label="Query\nTP1\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_k_tp1 [label="Key\nTP1\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_v_tp1 [label="Value\nTP1\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_out_tp1 [label="Attention Output\nTP1\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc1_tp1 [label="MLP FC1\nTP1\n[batch=?, seq=?, ffn=512]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc2_tp1 [label="MLP FC2\nTP1\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	tp_split_2 [label="TP Split 2\n[batch=?, seq=?, hidden=128]" fillcolor=yellow shape=parallelogram style=filled]
	attn_q_tp2 [label="Query\nTP2\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_k_tp2 [label="Key\nTP2\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_v_tp2 [label="Value\nTP2\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_out_tp2 [label="Attention Output\nTP2\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc1_tp2 [label="MLP FC1\nTP2\n[batch=?, seq=?, ffn=512]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc2_tp2 [label="MLP FC2\nTP2\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	tp_split_3 [label="TP Split 3\n[batch=?, seq=?, hidden=128]" fillcolor=yellow shape=parallelogram style=filled]
	attn_q_tp3 [label="Query\nTP3\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_k_tp3 [label="Key\nTP3\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_v_tp3 [label="Value\nTP3\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_out_tp3 [label="Attention Output\nTP3\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc1_tp3 [label="MLP FC1\nTP3\n[batch=?, seq=?, ffn=512]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc2_tp3 [label="MLP FC2\nTP3\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	tp_split_4 [label="TP Split 4\n[batch=?, seq=?, hidden=128]" fillcolor=yellow shape=parallelogram style=filled]
	attn_q_tp4 [label="Query\nTP4\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_k_tp4 [label="Key\nTP4\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_v_tp4 [label="Value\nTP4\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_out_tp4 [label="Attention Output\nTP4\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc1_tp4 [label="MLP FC1\nTP4\n[batch=?, seq=?, ffn=512]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc2_tp4 [label="MLP FC2\nTP4\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	tp_split_5 [label="TP Split 5\n[batch=?, seq=?, hidden=128]" fillcolor=yellow shape=parallelogram style=filled]
	attn_q_tp5 [label="Query\nTP5\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_k_tp5 [label="Key\nTP5\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_v_tp5 [label="Value\nTP5\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_out_tp5 [label="Attention Output\nTP5\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc1_tp5 [label="MLP FC1\nTP5\n[batch=?, seq=?, ffn=512]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc2_tp5 [label="MLP FC2\nTP5\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	tp_split_6 [label="TP Split 6\n[batch=?, seq=?, hidden=128]" fillcolor=yellow shape=parallelogram style=filled]
	attn_q_tp6 [label="Query\nTP6\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_k_tp6 [label="Key\nTP6\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_v_tp6 [label="Value\nTP6\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_out_tp6 [label="Attention Output\nTP6\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc1_tp6 [label="MLP FC1\nTP6\n[batch=?, seq=?, ffn=512]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc2_tp6 [label="MLP FC2\nTP6\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	tp_split_7 [label="TP Split 7\n[batch=?, seq=?, hidden=128]" fillcolor=yellow shape=parallelogram style=filled]
	attn_q_tp7 [label="Query\nTP7\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_k_tp7 [label="Key\nTP7\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_v_tp7 [label="Value\nTP7\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	attn_out_tp7 [label="Attention Output\nTP7\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc1_tp7 [label="MLP FC1\nTP7\n[batch=?, seq=?, ffn=512]" fillcolor=lightblue shape=rectangle style=filled]
	mlp_fc2_tp7 [label="MLP FC2\nTP7\n[batch=?, seq=?, hidden=128]" fillcolor=lightblue shape=rectangle style=filled]
	expert_gate [label="Expert Gate\n[batch=?, seq=?, experts=64]" fillcolor=yellow shape=parallelogram style=filled]
	allreduce_attn [label="All-Reduce\nAttention\n[batch=?, seq=?, hidden=1024]" fillcolor=lightgreen shape=ellipse style=filled]
	allreduce_mlp [label="All-Reduce\nMLP\n[batch=?, seq=?, hidden=1024]" fillcolor=lightgreen shape=ellipse style=filled]
	layer_output [label="Layer Output\n[batch=?, seq=?, hidden=1024]" fillcolor=lightblue shape=rectangle style=filled]
	layer_input -> tp_split_0
	tp_split_0 -> attn_q_tp0
	tp_split_0 -> attn_k_tp0
	tp_split_0 -> attn_v_tp0
	attn_q_tp0 -> expert_gate
	attn_k_tp0 -> expert_gate
	attn_v_tp0 -> expert_gate
	expert_gate -> attn_out_tp0
	attn_out_tp0 -> allreduce_attn
	allreduce_attn -> mlp_fc1_tp0
	mlp_fc1_tp0 -> mlp_fc2_tp0
	mlp_fc2_tp0 -> allreduce_mlp
	allreduce_mlp -> layer_output
	layer_input -> tp_split_1
	tp_split_1 -> attn_q_tp1
	tp_split_1 -> attn_k_tp1
	tp_split_1 -> attn_v_tp1
	attn_q_tp1 -> expert_gate
	attn_k_tp1 -> expert_gate
	attn_v_tp1 -> expert_gate
	expert_gate -> attn_out_tp1
	attn_out_tp1 -> allreduce_attn
	allreduce_attn -> mlp_fc1_tp1
	mlp_fc1_tp1 -> mlp_fc2_tp1
	mlp_fc2_tp1 -> allreduce_mlp
	allreduce_mlp -> layer_output
	layer_input -> tp_split_2
	tp_split_2 -> attn_q_tp2
	tp_split_2 -> attn_k_tp2
	tp_split_2 -> attn_v_tp2
	attn_q_tp2 -> expert_gate
	attn_k_tp2 -> expert_gate
	attn_v_tp2 -> expert_gate
	expert_gate -> attn_out_tp2
	attn_out_tp2 -> allreduce_attn
	allreduce_attn -> mlp_fc1_tp2
	mlp_fc1_tp2 -> mlp_fc2_tp2
	mlp_fc2_tp2 -> allreduce_mlp
	allreduce_mlp -> layer_output
	layer_input -> tp_split_3
	tp_split_3 -> attn_q_tp3
	tp_split_3 -> attn_k_tp3
	tp_split_3 -> attn_v_tp3
	attn_q_tp3 -> expert_gate
	attn_k_tp3 -> expert_gate
	attn_v_tp3 -> expert_gate
	expert_gate -> attn_out_tp3
	attn_out_tp3 -> allreduce_attn
	allreduce_attn -> mlp_fc1_tp3
	mlp_fc1_tp3 -> mlp_fc2_tp3
	mlp_fc2_tp3 -> allreduce_mlp
	allreduce_mlp -> layer_output
	layer_input -> tp_split_4
	tp_split_4 -> attn_q_tp4
	tp_split_4 -> attn_k_tp4
	tp_split_4 -> attn_v_tp4
	attn_q_tp4 -> expert_gate
	attn_k_tp4 -> expert_gate
	attn_v_tp4 -> expert_gate
	expert_gate -> attn_out_tp4
	attn_out_tp4 -> allreduce_attn
	allreduce_attn -> mlp_fc1_tp4
	mlp_fc1_tp4 -> mlp_fc2_tp4
	mlp_fc2_tp4 -> allreduce_mlp
	allreduce_mlp -> layer_output
	layer_input -> tp_split_5
	tp_split_5 -> attn_q_tp5
	tp_split_5 -> attn_k_tp5
	tp_split_5 -> attn_v_tp5
	attn_q_tp5 -> expert_gate
	attn_k_tp5 -> expert_gate
	attn_v_tp5 -> expert_gate
	expert_gate -> attn_out_tp5
	attn_out_tp5 -> allreduce_attn
	allreduce_attn -> mlp_fc1_tp5
	mlp_fc1_tp5 -> mlp_fc2_tp5
	mlp_fc2_tp5 -> allreduce_mlp
	allreduce_mlp -> layer_output
	layer_input -> tp_split_6
	tp_split_6 -> attn_q_tp6
	tp_split_6 -> attn_k_tp6
	tp_split_6 -> attn_v_tp6
	attn_q_tp6 -> expert_gate
	attn_k_tp6 -> expert_gate
	attn_v_tp6 -> expert_gate
	expert_gate -> attn_out_tp6
	attn_out_tp6 -> allreduce_attn
	allreduce_attn -> mlp_fc1_tp6
	mlp_fc1_tp6 -> mlp_fc2_tp6
	mlp_fc2_tp6 -> allreduce_mlp
	allreduce_mlp -> layer_output
	layer_input -> tp_split_7
	tp_split_7 -> attn_q_tp7
	tp_split_7 -> attn_k_tp7
	tp_split_7 -> attn_v_tp7
	attn_q_tp7 -> expert_gate
	attn_k_tp7 -> expert_gate
	attn_v_tp7 -> expert_gate
	expert_gate -> attn_out_tp7
	attn_out_tp7 -> allreduce_attn
	allreduce_attn -> mlp_fc1_tp7
	mlp_fc1_tp7 -> mlp_fc2_tp7
	mlp_fc2_tp7 -> allreduce_mlp
	allreduce_mlp -> layer_output
}
