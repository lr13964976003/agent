{
  "generation_timestamp": "2025-12-05-14-41-54",
  "deployment_strategy": "30B MoE Model - Optimized Hybrid Tensor-Expert-Pipeline Parallelism",
  "parallel_configuration": {
    "tensor_parallelism": 4,
    "expert_parallelism": 8,
    "pipeline_parallelism": 2,
    "data_parallelism": 1,
    "total_gpus": 16
  },
  "generated_files": {
    "dot_file": "../outputs/2025-12-05-14-41-54/moe_deployment_dag.dot",
    "svg_file": "../outputs/2025-12-05-14-41-54/moe_deployment_dag.svg",
    "python_generator": "../outputs/2025-12-05-14-41-54/generate_moe_dag.py"
  },
  "dag_validation": {
    "has_cycle": false,
    "total_nodes": 47,
    "total_edges": 79,
    "input_nodes": ["input"],
    "output_nodes": ["output"],
    "communication_nodes": 12,
    "computation_nodes": 31,
    "routing_nodes": 4
  },
  "key_features": [
    "Complete tensor parallelism implementation with 4-way split",
    "Expert parallelism with 8 experts per GPU distributed across 8 GPUs",
    "Pipeline parallelism with 2 stages (GPUs 0-7 and GPUs 8-15)",
    "Detailed attention mechanism with operator-level granularity",
    "MoE routing with dashed lines for expert selection",
    "All communication operations represented (All-Gather, All-Reduce, Pipeline)",
    "Input/output dimensions specified for each node",
    "GPU assignments clearly labeled for each operation",
    "Aggregation and split operations represented as parallelogrom nodes",
    "No cycles detected - valid DAG structure"
  ]
}