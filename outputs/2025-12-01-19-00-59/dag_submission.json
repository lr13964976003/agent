{
  "submission_info": {
    "task": "Complete MoE DAG Generation for Single GPU Deployment",
    "configuration": "EP1_TP1 (1-way Expert Parallelism, 1-way Tensor Parallelism)",
    "gpu_assignment": "All operations on GPU 0 (single GPU)",
    "model_parameters": {
      "layers": 8,
      "experts_per_layer": 4,
      "batch_size": 8,
      "sequence_length": 256,
      "token_dimension": 1024,
      "attention_heads": 8,
      "moe_hidden_dimension": 4096
    },
    "dag_features": {
      "card_boundary_division": "GPU 0 specified for all nodes",
      "multi_card_communication": "None (single GPU deployment)",
      "operator_level_detail": "Complete breakdown of attention and MoE operations",
      "node_shapes": {
        "computation": "rectangle",
        "communication": "ellipse", 
        "routing_aggregation": "parallelogram"
      },
      "gate_selection": "Dashed lines for expert selection",
      "residual_connections": "Multiple inputs shown for residual adds",
      "dimension_tracking": "Input/Output dimensions specified for all nodes"
    }
  },
  "generated_files": {
    "dot_file": {
      "path": "../outputs/2025-12-01-19-00-59/moe_single_gpu_deployment.dot",
      "description": "Complete Graphviz DOT code describing the MoE DAG",
      "size_bytes": 39254
    },
    "svg_file": {
      "path": "../outputs/2025-12-01-19-00-59/moe_single_gpu_deployment.svg", 
      "description": "Visual representation of the MoE DAG in SVG format",
      "size_bytes": 179637
    }
  },
  "validation_status": {
    "completeness": "All 8 layers with complete attention and MoE operations",
    "gpu_load_balancing": "Perfect (single GPU inherent)",
    "memory_constraints": "Optimized for 15.1GB Tesla T4 GPU",
    "engineering_validation": "Meets all specified requirements"
  }
}