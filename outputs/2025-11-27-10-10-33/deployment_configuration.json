{
  "deployment_configurations": {
    "baseline_conventional": {
      "name": "Conventional Expert Parallelism",
      "description": "Traditional approach with multiple experts per GPU",
      "model": {
        "type": "mixture_of_experts",
        "layers": 61,
        "moe_start_layer": 4,
        "moe_end_layer": 61,
        "total_moe_layers": 58,
        "expert_count": 16,
        "token_dimension": 7168,
        "mlp_hidden_dimension": 18432,
        "attention_heads": 128,
        "attention_head_dimension": 56,
        "precision": "bf16"
      },
      "parallel_strategy": {
        "expert_parallelism": {
          "degree": 4,
          "experts_per_gpu": 4,
          "placement": "intra_node_preferred"
        },
        "tensor_parallelism": {
          "enabled": false,
          "degree": 1
        },
        "data_parallelism": {
          "enabled": true,
          "degree": 1
        }
      },
      "device_mapping": {
        "total_gpus": 4,
        "gpu_memory_gb": 64,
        "device_assignments": [
          {
            "device_id": 0,
            "node_id": 0,
            "experts": [0, 1, 2, 3],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 1,
            "node_id": 0,
            "experts": [4, 5, 6, 7],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 2,
            "node_id": 0,
            "experts": [8, 9, 10, 11],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 3,
            "node_id": 0,
            "experts": [12, 13, 14, 15],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          }
        ]
      },
      "communication_strategy": {
        "type": "intra_node_preferred",
        "ring_all_reduce": false,
        "peer_to_peer": true,
        "nvlink_bandwidth": 900,
        "buffer_size_mb": 64
      }
    },
    
    "proposed_large_ep": {
      "name": "Large-Scale Cross-Node Expert Parallelism",
      "description": "Proposed method with one expert per GPU",
      "model": {
        "type": "mixture_of_experts",
        "layers": 61,
        "moe_start_layer": 4,
        "moe_end_layer": 61,
        "total_moe_layers": 58,
        "expert_count": 16,
        "token_dimension": 7168,
        "mlp_hidden_dimension": 18432,
        "attention_heads": 128,
        "attention_head_dimension": 56,
        "precision": "bf16"
      },
      "parallel_strategy": {
        "expert_parallelism": {
          "degree": 16,
          "experts_per_gpu": 1,
          "placement": "cross_node_optimized",
          "minimum_ep": 16
        },
        "tensor_parallelism": {
          "enabled": false,
          "degree": 1
        },
        "data_parallelism": {
          "enabled": true,
          "degree": 1
        }
      },
      "device_mapping": {
        "total_gpus": 16,
        "gpu_memory_gb": 64,
        "device_assignments": [
          {
            "device_id": 0,
            "node_id": 0,
            "experts": [0],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 1,
            "node_id": 0,
            "experts": [1],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 2,
            "node_id": 1,
            "experts": [2],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 3,
            "node_id": 1,
            "experts": [3],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 4,
            "node_id": 2,
            "experts": [4],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 5,
            "node_id": 2,
            "experts": [5],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 6,
            "node_id": 3,
            "experts": [6],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 7,
            "node_id": 3,
            "experts": [7],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 8,
            "node_id": 4,
            "experts": [8],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 9,
            "node_id": 4,
            "experts": [9],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 10,
            "node_id": 5,
            "experts": [10],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 11,
            "node_id": 5,
            "experts": [11],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 12,
            "node_id": 6,
            "experts": [12],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 13,
            "node_id": 6,
            "experts": [13],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 14,
            "node_id": 7,
            "experts": [14],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 15,
            "node_id": 7,
            "experts": [15],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          }
        ]
      },
      "communication_strategy": {
        "type": "cross_node_optimized",
        "topology_aware": true,
        "ring_all_reduce": false,
        "asynchronous_routing": true,
        "token_batching": true,
        "load_balancing": {
          "enabled": true,
          "alpha": 0.1,
          "rebalancing_interval_ms": 100
        },
        "network_parameters": {
          "infiniband_bandwidth_gbps": 200,
          "nvlink_bandwidth_gbps": 900,
          "latency_us": 10,
          "buffer_size_mb": 64
        }
      }
    },
    
    "proposed_large_ep_32": {
      "name": "Large-Scale Expert Parallelism (32 Experts)",
      "description": "Proposed method extended to 32 experts",
      "model": {
        "type": "mixture_of_experts",
        "layers": 61,
        "moe_start_layer": 4,
        "moe_end_layer": 61,
        "total_moe_layers": 58,
        "expert_count": 32,
        "token_dimension": 7168,
        "mlp_hidden_dimension": 18432,
        "attention_heads": 128,
        "attention_head_dimension": 56,
        "precision": "bf16"
      },
      "parallel_strategy": {
        "expert_parallelism": {
          "degree": 32,
          "experts_per_gpu": 1,
          "placement": "cross_node_optimized",
          "minimum_ep": 16
        },
        "tensor_parallelism": {
          "enabled": false,
          "degree": 1
        },
        "data_parallelism": {
          "enabled": true,
          "degree": 1
        }
      },
      "device_mapping": {
        "total_gpus": 32,
        "gpu_memory_gb": 64,
        "device_assignments": [
          {
            "device_id": 0,
            "node_id": 0,
            "experts": [0],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 1,
            "node_id": 0,
            "experts": [1],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 2,
            "node_id": 0,
            "experts": [2],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 3,
            "node_id": 0,
            "experts": [3],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 4,
            "node_id": 1,
            "experts": [4],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 5,
            "node_id": 1,
            "experts": [5],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 6,
            "node_id": 1,
            "experts": [6],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          },
          {
            "device_id": 7,
            "node_id": 1,
            "experts": [7],
            "layers": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
          }
        ]
      },
      "communication_strategy": {
        "type": "cross_node_optimized",
        "topology_aware": true,
        "ring_all_reduce": false,
        "asynchronous_routing": true,
        "token_batching": true,
        "load_balancing": {
          "enabled": true,
          "alpha": 0.1,
          "rebalancing_interval_ms": 100
        },
        "network_parameters": {
          "infiniband_bandwidth_gbps": 200,
          "nvlink_bandwidth_gbps": 900,
          "latency_us": 10,
          "buffer_size_mb": 64
        }
      }
    }
  },
  "shared_modules": {
    "dense_layers": {
      "layers": [1, 2, 3],
      "distribution": "replicated_across_nodes",
      "parameters": {
        "hidden_size": 7168,
        "mlp_hidden_size": 18432,
        "attention_heads": 128,
        "attention_head_dimension": 56
      }
    },
    "embedding_layers": {
      "type": "shared_across_all_devices",
      "vocab_size": 128000,
      "embedding_dimension": 7168,
      "precision": "bf16"
    }
  },
  "communication_overhead": {
    "token_routing_latency_us": 15,
    "expert_dispatch_bandwidth_requirement": "hidden_dimension * 2 bytes * tokens_per_batch",
    "all_reduce_for_dense_layers": {
      "enabled": true,
      "frequency": "per_microbatch",
      "algorithm": "tree"
    }
  }
}