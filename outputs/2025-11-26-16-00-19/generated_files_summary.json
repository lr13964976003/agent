{
  "generated_dag_files": {
    "complete_moe_dag_32_experts": {
      "dot_file": "../outputs/2025-11-26-16-00-19/complete_moe_dag_32_experts.dot",
      "svg_file": "../outputs/2025-11-26-16-00-19/complete_moe_dag_32_experts.svg", 
      "png_file": "../outputs/2025-11-26-16-00-19/complete_moe_dag_32_experts.png",
      "validation_status": "passed",
      "total_nodes": 105,
      "total_experts": 32,
      "expert_parallelism_degree": 32,
      "tensor_dimensions_verified": true,
      "no_cycles_detected": true
    }
  },
  "corrections_applied": [
    "Added all 32 experts (previously only had 16)",
    "Replaced all tensor shape placeholders with actual dimensions",
    "Calculated expert-specific batch sizes based on top-2 gating",
    "Assigned proper GPU IDs to each expert (0-31)",
    "Added missing nodes and edges for experts 16-31",
    "Validated DAG structure and connectivity",
    "Ensured proper input/output tensor flow",
    "Applied correct node shapes and colors per requirements"
  ],
  "deployment_specifications": {
    "model_architecture": "Large-scale cross-node expert parallelism",
    "parallel_strategy": "One expert per GPU",
    "total_experts": 32,
    "expert_parallelism_degree": 32,
    "batch_size": 32,
    "sequence_length": 2048,
    "hidden_dimension": 7168,
    "mlp_hidden_size": 2048,
    "top_k_routing": 2,
    "expert_batch_size": 4096
  }
}