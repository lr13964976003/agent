{
  "deployment_configurations": {
    "baseline": {
      "name": "Baseline_TP8_PP2",
      "description": "Traditional tensor and pipeline parallelism with colocated experts",
      "parallel_strategy": {
        "type": "hybrid",
        "tensor_parallelism": {
          "degree": 8,
          "method": "column_row_parallel"
        },
        "pipeline_parallelism": {
          "degree": 2,
          "method": "gpipe"
        },
        "expert_parallelism": {
          "degree": 1,
          "method": "colocated"
        }
      },
      "modules": {
        "layer_0": {
          "mha": {
            "type": "multi_head_attention",
            "dimensions": {
              "hidden_size": 8192,
              "num_heads": 16,
              "head_dim": 512
            },
            "parallel_strategy": "tensor_parallel_8",
            "parameters": {
              "qkv_weight": [8192, 8192],
              "qkv_bias": [8192],
              "out_weight": [8192, 8192],
              "out_bias": [8192]
            }
          },
          "experts": {
            "type": "mlp_experts",
            "count": 16,
            "dimensions": {
              "input_dim": 8192,
              "hidden_dim": 32768,
              "output_dim": 8192
            },
            "placement": "colocated_8_per_gpu",
            "parameters": {
              "expert_0": {
                "gate_weight": [32768, 8192],
                "gate_bias": [32768],
                "up_weight": [32768, 8192],
                "up_bias": [32768],
                "down_weight": [8192, 32768],
                "down_bias": [8192]
              },
              "expert_1_to_15": "same_structure_as_expert_0"
            }
          }
        },
        "layers_1_to_3": "same_as_layer_0"
      },
      "device_mapping": {
        "pipeline_stage_0": {
          "gpus": [0, 1, 2, 3, 4, 5, 6, 7],
          "layers": [0, 1],
          "experts_per_gpu": 8,
          "tensor_shard": "1/8_per_layer"
        },
        "pipeline_stage_1": {
          "gpus": [8, 9, 10, 11, 12, 13, 14, 15],
          "layers": [2, 3],
          "experts_per_gpu": 8,
          "tensor_shard": "1/8_per_layer"
        }
      }
    },
    "proposed": {
      "name": "Cross_Node_Expert_Parallelism_EP16",
      "description": "Large-scale expert parallelism with one expert per GPU",
      "parallel_strategy": {
        "type": "expert_parallelism",
        "expert_parallelism": {
          "degree": 16,
          "method": "one_expert_per_gpu",
          "large_ep": true
        },
        "tensor_parallelism": {
          "degree": 1,
          "method": "none"
        },
        "pipeline_parallelism": {
          "degree": 1,
          "method": "none"
        }
      },
      "modules": {
        "layer_0": {
          "mha_shared": {
            "type": "shared_multi_head_attention",
            "dimensions": {
              "hidden_size": 8192,
              "num_heads": 16,
              "head_dim": 512
            },
            "placement": "replicated_across_all_gpus",
            "parameters": {
              "qkv_weight": [8192, 8192],
              "qkv_bias": [8192],
              "out_weight": [8192, 8192],
              "out_bias": [8192]
            }
          },
          "experts": {
            "type": "distributed_mlp_experts",
            "count": 16,
            "dimensions": {
              "input_dim": 8192,
              "hidden_dim": 32768,
              "output_dim": 8192
            },
            "placement": "one_expert_per_gpu",
            "parameters": {
              "expert_0": {
                "gpu_id": 0,
                "gate_weight": [32768, 8192],
                "gate_bias": [32768],
                "up_weight": [32768, 8192],
                "up_bias": [32768],
                "down_weight": [8192, 32768],
                "down_bias": [8192]
              },
              "expert_1": {
                "gpu_id": 1,
                "parameters": "same_as_expert_0"
              },
              "expert_2": {
                "gpu_id": 2,
                "parameters": "same_as_expert_0"
              },
              "expert_3": {
                "gpu_id": 3,
                "parameters": "same_as_expert_0"
              },
              "expert_4": {
                "gpu_id": 4,
                "parameters": "same_as_expert_0"
              },
              "expert_5": {
                "gpu_id": 5,
                "parameters": "same_as_expert_0"
              },
              "expert_6": {
                "gpu_id": 6,
                "parameters": "same_as_expert_0"
              },
              "expert_7": {
                "gpu_id": 7,
                "parameters": "same_as_expert_0"
              },
              "expert_8": {
                "gpu_id": 8,
                "parameters": "same_as_expert_0"
              },
              "expert_9": {
                "gpu_id": 9,
                "parameters": "same_as_expert_0"
              },
              "expert_10": {
                "gpu_id": 10,
                "parameters": "same_as_expert_0"
              },
              "expert_11": {
                "gpu_id": 11,
                "parameters": "same_as_expert_0"
              },
              "expert_12": {
                "gpu_id": 12,
                "parameters": "same_as_expert_0"
              },
              "expert_13": {
                "gpu_id": 13,
                "parameters": "same_as_expert_0"
              },
              "expert_14": {
                "gpu_id": 14,
                "parameters": "same_as_expert_0"
              },
              "expert_15": {
                "gpu_id": 15,
                "parameters": "same_as_expert_0"
              }
            }
          }
        },
        "layers_1_to_3": "same_as_layer_0"
      },
      "device_mapping": {
        "gpu_0": {
          "device_id": 0,
          "node_id": 0,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_0",
            "expert_layer_1": "expert_0",
            "expert_layer_2": "expert_0",
            "expert_layer_3": "expert_0"
          }
        },
        "gpu_1": {
          "device_id": 1,
          "node_id": 0,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_1",
            "expert_layer_1": "expert_1",
            "expert_layer_2": "expert_1",
            "expert_layer_3": "expert_1"
          }
        },
        "gpu_2": {
          "device_id": 2,
          "node_id": 0,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_2",
            "expert_layer_1": "expert_2",
            "expert_layer_2": "expert_2",
            "expert_layer_3": "expert_2"
          }
        },
        "gpu_3": {
          "device_id": 3,
          "node_id": 0,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_3",
            "expert_layer_1": "expert_3",
            "expert_layer_2": "expert_3",
            "expert_layer_3": "expert_3"
          }
        },
        "gpu_4": {
          "device_id": 4,
          "node_id": 1,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_4",
            "expert_layer_1": "expert_4",
            "expert_layer_2": "expert_4",
            "expert_layer_3": "expert_4"
          }
        },
        "gpu_5": {
          "device_id": 5,
          "node_id": 1,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_5",
            "expert_layer_1": "expert_5",
            "expert_layer_2": "expert_5",
            "expert_layer_3": "expert_5"
          }
        },
        "gpu_6": {
          "device_id": 6,
          "node_id": 1,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_6",
            "expert_layer_1": "expert_6",
            "expert_layer_2": "expert_6",
            "expert_layer_3": "expert_6"
          }
        },
        "gpu_7": {
          "device_id": 7,
          "node_id": 1,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_7",
            "expert_layer_1": "expert_7",
            "expert_layer_2": "expert_7",
            "expert_layer_3": "expert_7"
          }
        },
        "gpu_8": {
          "device_id": 8,
          "node_id": 2,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_8",
            "expert_layer_1": "expert_8",
            "expert_layer_2": "expert_8",
            "expert_layer_3": "expert_8"
          }
        },
        "gpu_9": {
          "device_id": 9,
          "node_id": 2,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_9",
            "expert_layer_1": "expert_9",
            "expert_layer_2": "expert_9",
            "expert_layer_3": "expert_9"
          }
        },
        "gpu_10": {
          "device_id": 10,
          "node_id": 2,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_10",
            "expert_layer_1": "expert_10",
            "expert_layer_2": "expert_10",
            "expert_layer_3": "expert_10"
          }
        },
        "gpu_11": {
          "device_id": 11,
          "node_id": 2,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_11",
            "expert_layer_1": "expert_11",
            "expert_layer_2": "expert_11",
            "expert_layer_3": "expert_11"
          }
        },
        "gpu_12": {
          "device_id": 12,
          "node_id": 3,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_12",
            "expert_layer_1": "expert_12",
            "expert_layer_2": "expert_12",
            "expert_layer_3": "expert_12"
          }
        },
        "gpu_13": {
          "device_id": 13,
          "node_id": 3,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_13",
            "expert_layer_1": "expert_13",
            "expert_layer_2": "expert_13",
            "expert_layer_3": "expert_13"
          }
        },
        "gpu_14": {
          "device_id": 14,
          "node_id": 3,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_14",
            "expert_layer_1": "expert_14",
            "expert_layer_2": "expert_14",
            "expert_layer_3": "expert_14"
          }
        },
        "gpu_15": {
          "device_id": 15,
          "node_id": 3,
          "modules": {
            "shared_mha": "replicated",
            "expert_layer_0": "expert_15",
            "expert_layer_1": "expert_15",
            "expert_layer_2": "expert_15",
            "expert_layer_3": "expert_15"
          }
        }
      },
      "communication_config": {
        "backend": "NCCL",
        "async_operations": true,
        "cuda_streams": 4,
        "topology_aware_routing": true,
        "token_batching": {
          "enabled": true,
          "batch_size": 1024,
          "max_tokens_per_message": 10000
        },
        "load_balancing": {
          "enabled": true,
          "monitoring_interval_ms": 100,
          "gating_adjustment": true
        }
      }
    }
  }
}