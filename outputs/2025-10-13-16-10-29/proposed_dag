// Proposed MoE with EP16
digraph proposed_moe_dag {
	rankdir=TB size="25,25"
	input [label="Total Input\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightblue shape=ellipse style=filled]
	l0_mha_qkv_gpu0 [label="QKV Projection\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu0 [label="Attention\nGPU 0\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu0 [label="Output Projection\nGPU 0\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu0 [label="Residual Add\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_0_gpu0 [label="Expert 0\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu0 [label="Gate\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu0 [label="Expert Aggregation\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu0 [label="Residual Add\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu1 [label="QKV Projection\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu1 [label="Attention\nGPU 1\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu1 [label="Output Projection\nGPU 1\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu1 [label="Residual Add\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_1_gpu1 [label="Expert 1\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu1 [label="Gate\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu1 [label="Expert Aggregation\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu1 [label="Residual Add\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu2 [label="QKV Projection\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu2 [label="Attention\nGPU 2\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu2 [label="Output Projection\nGPU 2\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu2 [label="Residual Add\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_2_gpu2 [label="Expert 2\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu2 [label="Gate\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu2 [label="Expert Aggregation\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu2 [label="Residual Add\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu3 [label="QKV Projection\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu3 [label="Attention\nGPU 3\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu3 [label="Output Projection\nGPU 3\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu3 [label="Residual Add\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_3_gpu3 [label="Expert 3\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu3 [label="Gate\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu3 [label="Expert Aggregation\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu3 [label="Residual Add\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu4 [label="QKV Projection\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu4 [label="Attention\nGPU 4\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu4 [label="Output Projection\nGPU 4\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu4 [label="Residual Add\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_4_gpu4 [label="Expert 4\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu4 [label="Gate\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu4 [label="Expert Aggregation\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu4 [label="Residual Add\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu5 [label="QKV Projection\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu5 [label="Attention\nGPU 5\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu5 [label="Output Projection\nGPU 5\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu5 [label="Residual Add\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_5_gpu5 [label="Expert 5\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu5 [label="Gate\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu5 [label="Expert Aggregation\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu5 [label="Residual Add\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu6 [label="QKV Projection\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu6 [label="Attention\nGPU 6\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu6 [label="Output Projection\nGPU 6\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu6 [label="Residual Add\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_6_gpu6 [label="Expert 6\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu6 [label="Gate\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu6 [label="Expert Aggregation\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu6 [label="Residual Add\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu7 [label="QKV Projection\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu7 [label="Attention\nGPU 7\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu7 [label="Output Projection\nGPU 7\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu7 [label="Residual Add\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_7_gpu7 [label="Expert 7\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu7 [label="Gate\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu7 [label="Expert Aggregation\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu7 [label="Residual Add\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu8 [label="QKV Projection\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu8 [label="Attention\nGPU 8\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu8 [label="Output Projection\nGPU 8\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu8 [label="Residual Add\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_8_gpu8 [label="Expert 8\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu8 [label="Gate\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu8 [label="Expert Aggregation\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu8 [label="Residual Add\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu9 [label="QKV Projection\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu9 [label="Attention\nGPU 9\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu9 [label="Output Projection\nGPU 9\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu9 [label="Residual Add\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_9_gpu9 [label="Expert 9\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu9 [label="Gate\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu9 [label="Expert Aggregation\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu9 [label="Residual Add\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu10 [label="QKV Projection\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu10 [label="Attention\nGPU 10\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu10 [label="Output Projection\nGPU 10\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu10 [label="Residual Add\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_10_gpu10 [label="Expert 10\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu10 [label="Gate\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu10 [label="Expert Aggregation\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu10 [label="Residual Add\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu11 [label="QKV Projection\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu11 [label="Attention\nGPU 11\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu11 [label="Output Projection\nGPU 11\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu11 [label="Residual Add\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_11_gpu11 [label="Expert 11\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu11 [label="Gate\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu11 [label="Expert Aggregation\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu11 [label="Residual Add\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu12 [label="QKV Projection\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu12 [label="Attention\nGPU 12\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu12 [label="Output Projection\nGPU 12\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu12 [label="Residual Add\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_12_gpu12 [label="Expert 12\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu12 [label="Gate\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu12 [label="Expert Aggregation\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu12 [label="Residual Add\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu13 [label="QKV Projection\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu13 [label="Attention\nGPU 13\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu13 [label="Output Projection\nGPU 13\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu13 [label="Residual Add\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_13_gpu13 [label="Expert 13\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu13 [label="Gate\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu13 [label="Expert Aggregation\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu13 [label="Residual Add\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu14 [label="QKV Projection\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu14 [label="Attention\nGPU 14\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu14 [label="Output Projection\nGPU 14\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu14 [label="Residual Add\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_14_gpu14 [label="Expert 14\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu14 [label="Gate\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu14 [label="Expert Aggregation\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu14 [label="Residual Add\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_mha_qkv_gpu15 [label="QKV Projection\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn_gpu15 [label="Attention\nGPU 15\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_out_gpu15 [label="Output Projection\nGPU 15\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_res_add_gpu15 [label="Residual Add\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l0_expert_15_gpu15 [label="Expert 15\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l0_gate_gpu15 [label="Gate\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l0_expert_agg_gpu15 [label="Expert Aggregation\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l0_res2_gpu15 [label="Residual Add\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	subgraph cluster_layer_0 {
		label="Layer 0"
		subgraph cluster_gpu_0_layer_0 {
			label="GPU 0 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_1_layer_0 {
			label="GPU 1 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_2_layer_0 {
			label="GPU 2 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_3_layer_0 {
			label="GPU 3 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_4_layer_0 {
			label="GPU 4 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_5_layer_0 {
			label="GPU 5 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_6_layer_0 {
			label="GPU 6 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_7_layer_0 {
			label="GPU 7 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_8_layer_0 {
			label="GPU 8 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_9_layer_0 {
			label="GPU 9 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_10_layer_0 {
			label="GPU 10 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_11_layer_0 {
			label="GPU 11 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_12_layer_0 {
			label="GPU 12 (Node 3)" style=dashed
		}
		subgraph cluster_gpu_13_layer_0 {
			label="GPU 13 (Node 3)" style=dashed
		}
		subgraph cluster_gpu_14_layer_0 {
			label="GPU 14 (Node 3)" style=dashed
		}
		subgraph cluster_gpu_15_layer_0 {
			label="GPU 15 (Node 3)" style=dashed
		}
	}
	l1_mha_qkv_gpu0 [label="QKV Projection\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu0 [label="Attention\nGPU 0\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu0 [label="Output Projection\nGPU 0\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu0 [label="Residual Add\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_0_gpu0 [label="Expert 0\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu0 [label="Gate\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu0 [label="Expert Aggregation\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu0 [label="Residual Add\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu1 [label="QKV Projection\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu1 [label="Attention\nGPU 1\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu1 [label="Output Projection\nGPU 1\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu1 [label="Residual Add\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_1_gpu1 [label="Expert 1\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu1 [label="Gate\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu1 [label="Expert Aggregation\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu1 [label="Residual Add\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu2 [label="QKV Projection\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu2 [label="Attention\nGPU 2\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu2 [label="Output Projection\nGPU 2\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu2 [label="Residual Add\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_2_gpu2 [label="Expert 2\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu2 [label="Gate\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu2 [label="Expert Aggregation\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu2 [label="Residual Add\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu3 [label="QKV Projection\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu3 [label="Attention\nGPU 3\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu3 [label="Output Projection\nGPU 3\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu3 [label="Residual Add\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_3_gpu3 [label="Expert 3\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu3 [label="Gate\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu3 [label="Expert Aggregation\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu3 [label="Residual Add\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu4 [label="QKV Projection\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu4 [label="Attention\nGPU 4\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu4 [label="Output Projection\nGPU 4\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu4 [label="Residual Add\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_4_gpu4 [label="Expert 4\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu4 [label="Gate\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu4 [label="Expert Aggregation\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu4 [label="Residual Add\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu5 [label="QKV Projection\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu5 [label="Attention\nGPU 5\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu5 [label="Output Projection\nGPU 5\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu5 [label="Residual Add\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_5_gpu5 [label="Expert 5\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu5 [label="Gate\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu5 [label="Expert Aggregation\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu5 [label="Residual Add\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu6 [label="QKV Projection\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu6 [label="Attention\nGPU 6\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu6 [label="Output Projection\nGPU 6\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu6 [label="Residual Add\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_6_gpu6 [label="Expert 6\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu6 [label="Gate\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu6 [label="Expert Aggregation\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu6 [label="Residual Add\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu7 [label="QKV Projection\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu7 [label="Attention\nGPU 7\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu7 [label="Output Projection\nGPU 7\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu7 [label="Residual Add\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_7_gpu7 [label="Expert 7\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu7 [label="Gate\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu7 [label="Expert Aggregation\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu7 [label="Residual Add\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu8 [label="QKV Projection\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu8 [label="Attention\nGPU 8\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu8 [label="Output Projection\nGPU 8\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu8 [label="Residual Add\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_8_gpu8 [label="Expert 8\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu8 [label="Gate\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu8 [label="Expert Aggregation\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu8 [label="Residual Add\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu9 [label="QKV Projection\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu9 [label="Attention\nGPU 9\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu9 [label="Output Projection\nGPU 9\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu9 [label="Residual Add\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_9_gpu9 [label="Expert 9\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu9 [label="Gate\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu9 [label="Expert Aggregation\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu9 [label="Residual Add\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu10 [label="QKV Projection\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu10 [label="Attention\nGPU 10\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu10 [label="Output Projection\nGPU 10\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu10 [label="Residual Add\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_10_gpu10 [label="Expert 10\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu10 [label="Gate\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu10 [label="Expert Aggregation\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu10 [label="Residual Add\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu11 [label="QKV Projection\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu11 [label="Attention\nGPU 11\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu11 [label="Output Projection\nGPU 11\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu11 [label="Residual Add\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_11_gpu11 [label="Expert 11\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu11 [label="Gate\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu11 [label="Expert Aggregation\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu11 [label="Residual Add\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu12 [label="QKV Projection\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu12 [label="Attention\nGPU 12\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu12 [label="Output Projection\nGPU 12\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu12 [label="Residual Add\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_12_gpu12 [label="Expert 12\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu12 [label="Gate\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu12 [label="Expert Aggregation\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu12 [label="Residual Add\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu13 [label="QKV Projection\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu13 [label="Attention\nGPU 13\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu13 [label="Output Projection\nGPU 13\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu13 [label="Residual Add\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_13_gpu13 [label="Expert 13\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu13 [label="Gate\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu13 [label="Expert Aggregation\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu13 [label="Residual Add\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu14 [label="QKV Projection\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu14 [label="Attention\nGPU 14\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu14 [label="Output Projection\nGPU 14\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu14 [label="Residual Add\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_14_gpu14 [label="Expert 14\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu14 [label="Gate\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu14 [label="Expert Aggregation\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu14 [label="Residual Add\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_mha_qkv_gpu15 [label="QKV Projection\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn_gpu15 [label="Attention\nGPU 15\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_out_gpu15 [label="Output Projection\nGPU 15\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_res_add_gpu15 [label="Residual Add\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l1_expert_15_gpu15 [label="Expert 15\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l1_gate_gpu15 [label="Gate\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l1_expert_agg_gpu15 [label="Expert Aggregation\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l1_res2_gpu15 [label="Residual Add\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	subgraph cluster_layer_1 {
		label="Layer 1"
		subgraph cluster_gpu_0_layer_1 {
			label="GPU 0 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_1_layer_1 {
			label="GPU 1 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_2_layer_1 {
			label="GPU 2 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_3_layer_1 {
			label="GPU 3 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_4_layer_1 {
			label="GPU 4 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_5_layer_1 {
			label="GPU 5 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_6_layer_1 {
			label="GPU 6 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_7_layer_1 {
			label="GPU 7 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_8_layer_1 {
			label="GPU 8 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_9_layer_1 {
			label="GPU 9 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_10_layer_1 {
			label="GPU 10 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_11_layer_1 {
			label="GPU 11 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_12_layer_1 {
			label="GPU 12 (Node 3)" style=dashed
		}
		subgraph cluster_gpu_13_layer_1 {
			label="GPU 13 (Node 3)" style=dashed
		}
		subgraph cluster_gpu_14_layer_1 {
			label="GPU 14 (Node 3)" style=dashed
		}
		subgraph cluster_gpu_15_layer_1 {
			label="GPU 15 (Node 3)" style=dashed
		}
	}
	l2_mha_qkv_gpu0 [label="QKV Projection\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu0 [label="Attention\nGPU 0\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu0 [label="Output Projection\nGPU 0\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu0 [label="Residual Add\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_0_gpu0 [label="Expert 0\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu0 [label="Gate\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu0 [label="Expert Aggregation\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu0 [label="Residual Add\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu1 [label="QKV Projection\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu1 [label="Attention\nGPU 1\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu1 [label="Output Projection\nGPU 1\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu1 [label="Residual Add\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_1_gpu1 [label="Expert 1\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu1 [label="Gate\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu1 [label="Expert Aggregation\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu1 [label="Residual Add\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu2 [label="QKV Projection\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu2 [label="Attention\nGPU 2\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu2 [label="Output Projection\nGPU 2\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu2 [label="Residual Add\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_2_gpu2 [label="Expert 2\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu2 [label="Gate\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu2 [label="Expert Aggregation\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu2 [label="Residual Add\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu3 [label="QKV Projection\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu3 [label="Attention\nGPU 3\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu3 [label="Output Projection\nGPU 3\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu3 [label="Residual Add\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_3_gpu3 [label="Expert 3\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu3 [label="Gate\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu3 [label="Expert Aggregation\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu3 [label="Residual Add\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu4 [label="QKV Projection\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu4 [label="Attention\nGPU 4\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu4 [label="Output Projection\nGPU 4\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu4 [label="Residual Add\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_4_gpu4 [label="Expert 4\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu4 [label="Gate\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu4 [label="Expert Aggregation\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu4 [label="Residual Add\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu5 [label="QKV Projection\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu5 [label="Attention\nGPU 5\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu5 [label="Output Projection\nGPU 5\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu5 [label="Residual Add\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_5_gpu5 [label="Expert 5\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu5 [label="Gate\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu5 [label="Expert Aggregation\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu5 [label="Residual Add\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu6 [label="QKV Projection\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu6 [label="Attention\nGPU 6\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu6 [label="Output Projection\nGPU 6\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu6 [label="Residual Add\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_6_gpu6 [label="Expert 6\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu6 [label="Gate\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu6 [label="Expert Aggregation\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu6 [label="Residual Add\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu7 [label="QKV Projection\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu7 [label="Attention\nGPU 7\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu7 [label="Output Projection\nGPU 7\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu7 [label="Residual Add\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_7_gpu7 [label="Expert 7\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu7 [label="Gate\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu7 [label="Expert Aggregation\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu7 [label="Residual Add\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu8 [label="QKV Projection\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu8 [label="Attention\nGPU 8\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu8 [label="Output Projection\nGPU 8\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu8 [label="Residual Add\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_8_gpu8 [label="Expert 8\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu8 [label="Gate\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu8 [label="Expert Aggregation\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu8 [label="Residual Add\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu9 [label="QKV Projection\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu9 [label="Attention\nGPU 9\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu9 [label="Output Projection\nGPU 9\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu9 [label="Residual Add\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_9_gpu9 [label="Expert 9\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu9 [label="Gate\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu9 [label="Expert Aggregation\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu9 [label="Residual Add\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu10 [label="QKV Projection\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu10 [label="Attention\nGPU 10\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu10 [label="Output Projection\nGPU 10\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu10 [label="Residual Add\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_10_gpu10 [label="Expert 10\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu10 [label="Gate\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu10 [label="Expert Aggregation\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu10 [label="Residual Add\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu11 [label="QKV Projection\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu11 [label="Attention\nGPU 11\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu11 [label="Output Projection\nGPU 11\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu11 [label="Residual Add\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_11_gpu11 [label="Expert 11\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu11 [label="Gate\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu11 [label="Expert Aggregation\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu11 [label="Residual Add\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu12 [label="QKV Projection\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu12 [label="Attention\nGPU 12\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu12 [label="Output Projection\nGPU 12\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu12 [label="Residual Add\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_12_gpu12 [label="Expert 12\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu12 [label="Gate\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu12 [label="Expert Aggregation\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu12 [label="Residual Add\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu13 [label="QKV Projection\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu13 [label="Attention\nGPU 13\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu13 [label="Output Projection\nGPU 13\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu13 [label="Residual Add\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_13_gpu13 [label="Expert 13\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu13 [label="Gate\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu13 [label="Expert Aggregation\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu13 [label="Residual Add\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu14 [label="QKV Projection\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu14 [label="Attention\nGPU 14\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu14 [label="Output Projection\nGPU 14\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu14 [label="Residual Add\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_14_gpu14 [label="Expert 14\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu14 [label="Gate\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu14 [label="Expert Aggregation\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu14 [label="Residual Add\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_mha_qkv_gpu15 [label="QKV Projection\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn_gpu15 [label="Attention\nGPU 15\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_out_gpu15 [label="Output Projection\nGPU 15\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_res_add_gpu15 [label="Residual Add\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l2_expert_15_gpu15 [label="Expert 15\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l2_gate_gpu15 [label="Gate\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l2_expert_agg_gpu15 [label="Expert Aggregation\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l2_res2_gpu15 [label="Residual Add\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	subgraph cluster_layer_2 {
		label="Layer 2"
		subgraph cluster_gpu_0_layer_2 {
			label="GPU 0 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_1_layer_2 {
			label="GPU 1 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_2_layer_2 {
			label="GPU 2 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_3_layer_2 {
			label="GPU 3 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_4_layer_2 {
			label="GPU 4 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_5_layer_2 {
			label="GPU 5 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_6_layer_2 {
			label="GPU 6 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_7_layer_2 {
			label="GPU 7 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_8_layer_2 {
			label="GPU 8 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_9_layer_2 {
			label="GPU 9 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_10_layer_2 {
			label="GPU 10 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_11_layer_2 {
			label="GPU 11 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_12_layer_2 {
			label="GPU 12 (Node 3)" style=dashed
		}
		subgraph cluster_gpu_13_layer_2 {
			label="GPU 13 (Node 3)" style=dashed
		}
		subgraph cluster_gpu_14_layer_2 {
			label="GPU 14 (Node 3)" style=dashed
		}
		subgraph cluster_gpu_15_layer_2 {
			label="GPU 15 (Node 3)" style=dashed
		}
	}
	l3_mha_qkv_gpu0 [label="QKV Projection\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu0 [label="Attention\nGPU 0\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu0 [label="Output Projection\nGPU 0\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu0 [label="Residual Add\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_0_gpu0 [label="Expert 0\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu0 [label="Gate\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu0 [label="Expert Aggregation\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu0 [label="Residual Add\nGPU 0\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu1 [label="QKV Projection\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu1 [label="Attention\nGPU 1\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu1 [label="Output Projection\nGPU 1\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu1 [label="Residual Add\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_1_gpu1 [label="Expert 1\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu1 [label="Gate\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu1 [label="Expert Aggregation\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu1 [label="Residual Add\nGPU 1\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu2 [label="QKV Projection\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu2 [label="Attention\nGPU 2\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu2 [label="Output Projection\nGPU 2\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu2 [label="Residual Add\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_2_gpu2 [label="Expert 2\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu2 [label="Gate\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu2 [label="Expert Aggregation\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu2 [label="Residual Add\nGPU 2\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu3 [label="QKV Projection\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu3 [label="Attention\nGPU 3\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu3 [label="Output Projection\nGPU 3\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu3 [label="Residual Add\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_3_gpu3 [label="Expert 3\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu3 [label="Gate\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu3 [label="Expert Aggregation\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu3 [label="Residual Add\nGPU 3\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu4 [label="QKV Projection\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu4 [label="Attention\nGPU 4\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu4 [label="Output Projection\nGPU 4\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu4 [label="Residual Add\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_4_gpu4 [label="Expert 4\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu4 [label="Gate\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu4 [label="Expert Aggregation\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu4 [label="Residual Add\nGPU 4\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu5 [label="QKV Projection\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu5 [label="Attention\nGPU 5\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu5 [label="Output Projection\nGPU 5\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu5 [label="Residual Add\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_5_gpu5 [label="Expert 5\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu5 [label="Gate\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu5 [label="Expert Aggregation\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu5 [label="Residual Add\nGPU 5\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu6 [label="QKV Projection\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu6 [label="Attention\nGPU 6\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu6 [label="Output Projection\nGPU 6\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu6 [label="Residual Add\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_6_gpu6 [label="Expert 6\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu6 [label="Gate\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu6 [label="Expert Aggregation\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu6 [label="Residual Add\nGPU 6\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu7 [label="QKV Projection\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu7 [label="Attention\nGPU 7\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu7 [label="Output Projection\nGPU 7\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu7 [label="Residual Add\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_7_gpu7 [label="Expert 7\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu7 [label="Gate\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu7 [label="Expert Aggregation\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu7 [label="Residual Add\nGPU 7\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu8 [label="QKV Projection\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu8 [label="Attention\nGPU 8\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu8 [label="Output Projection\nGPU 8\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu8 [label="Residual Add\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_8_gpu8 [label="Expert 8\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu8 [label="Gate\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu8 [label="Expert Aggregation\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu8 [label="Residual Add\nGPU 8\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu9 [label="QKV Projection\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu9 [label="Attention\nGPU 9\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu9 [label="Output Projection\nGPU 9\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu9 [label="Residual Add\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_9_gpu9 [label="Expert 9\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu9 [label="Gate\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu9 [label="Expert Aggregation\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu9 [label="Residual Add\nGPU 9\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu10 [label="QKV Projection\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu10 [label="Attention\nGPU 10\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu10 [label="Output Projection\nGPU 10\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu10 [label="Residual Add\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_10_gpu10 [label="Expert 10\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu10 [label="Gate\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu10 [label="Expert Aggregation\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu10 [label="Residual Add\nGPU 10\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu11 [label="QKV Projection\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu11 [label="Attention\nGPU 11\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu11 [label="Output Projection\nGPU 11\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu11 [label="Residual Add\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_11_gpu11 [label="Expert 11\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu11 [label="Gate\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu11 [label="Expert Aggregation\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu11 [label="Residual Add\nGPU 11\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu12 [label="QKV Projection\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu12 [label="Attention\nGPU 12\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu12 [label="Output Projection\nGPU 12\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu12 [label="Residual Add\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_12_gpu12 [label="Expert 12\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu12 [label="Gate\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu12 [label="Expert Aggregation\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu12 [label="Residual Add\nGPU 12\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu13 [label="QKV Projection\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu13 [label="Attention\nGPU 13\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu13 [label="Output Projection\nGPU 13\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu13 [label="Residual Add\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_13_gpu13 [label="Expert 13\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu13 [label="Gate\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu13 [label="Expert Aggregation\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu13 [label="Residual Add\nGPU 13\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu14 [label="QKV Projection\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu14 [label="Attention\nGPU 14\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu14 [label="Output Projection\nGPU 14\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu14 [label="Residual Add\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_14_gpu14 [label="Expert 14\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu14 [label="Gate\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu14 [label="Expert Aggregation\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu14 [label="Residual Add\nGPU 14\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_mha_qkv_gpu15 [label="QKV Projection\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn_gpu15 [label="Attention\nGPU 15\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_out_gpu15 [label="Output Projection\nGPU 15\nInput: [batch_size=1024, seq_len=10000, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_res_add_gpu15 [label="Residual Add\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	l3_expert_15_gpu15 [label="Expert 15\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightpink shape=rectangle style=filled]
	l3_gate_gpu15 [label="Gate\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, num_experts=16]" fillcolor=lightcyan shape=parallelogram style=filled]
	l3_expert_agg_gpu15 [label="Expert Aggregation\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=yellow shape=parallelogram style=filled]
	l3_res2_gpu15 [label="Residual Add\nGPU 15\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightcoral shape=rectangle style=filled]
	subgraph cluster_layer_3 {
		label="Layer 3"
		subgraph cluster_gpu_0_layer_3 {
			label="GPU 0 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_1_layer_3 {
			label="GPU 1 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_2_layer_3 {
			label="GPU 2 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_3_layer_3 {
			label="GPU 3 (Node 0)" style=dashed
		}
		subgraph cluster_gpu_4_layer_3 {
			label="GPU 4 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_5_layer_3 {
			label="GPU 5 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_6_layer_3 {
			label="GPU 6 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_7_layer_3 {
			label="GPU 7 (Node 1)" style=dashed
		}
		subgraph cluster_gpu_8_layer_3 {
			label="GPU 8 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_9_layer_3 {
			label="GPU 9 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_10_layer_3 {
			label="GPU 10 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_11_layer_3 {
			label="GPU 11 (Node 2)" style=dashed
		}
		subgraph cluster_gpu_12_layer_3 {
			label="GPU 12 (Node 3)" style=dashed
		}
		subgraph cluster_gpu_13_layer_3 {
			label="GPU 13 (Node 3)" style=dashed
		}
		subgraph cluster_gpu_14_layer_3 {
			label="GPU 14 (Node 3)" style=dashed
		}
		subgraph cluster_gpu_15_layer_3 {
			label="GPU 15 (Node 3)" style=dashed
		}
	}
	output [label="Total Output\nInput: [batch_size=1024, seq_len=10000, hidden=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden=8192]" fillcolor=lightblue shape=ellipse style=filled]
	input -> l0_mha_qkv_gpu0
	l0_mha_qkv_gpu0 -> l0_mha_attn_gpu0
	l0_mha_attn_gpu0 -> l0_mha_out_gpu0
	l0_mha_out_gpu0 -> l0_mha_res_add_gpu0
	input -> l0_mha_res_add_gpu0
	l0_mha_res_add_gpu0 -> l0_gate_gpu0
	l0_gate_gpu0 -> l0_expert_0_gpu0 [style=dashed]
	l0_mha_res_add_gpu0 -> l0_expert_0_gpu0
	l0_expert_0_gpu0 -> l0_expert_agg_gpu0
	l0_expert_0_gpu0 -> l0_expert_agg_gpu1
	l0_expert_0_gpu0 -> l0_expert_agg_gpu2
	l0_expert_0_gpu0 -> l0_expert_agg_gpu3
	l0_expert_0_gpu0 -> l0_expert_agg_gpu4
	l0_expert_0_gpu0 -> l0_expert_agg_gpu5
	l0_expert_0_gpu0 -> l0_expert_agg_gpu6
	l0_expert_0_gpu0 -> l0_expert_agg_gpu7
	l0_expert_0_gpu0 -> l0_expert_agg_gpu8
	l0_expert_0_gpu0 -> l0_expert_agg_gpu9
	l0_expert_0_gpu0 -> l0_expert_agg_gpu10
	l0_expert_0_gpu0 -> l0_expert_agg_gpu11
	l0_expert_0_gpu0 -> l0_expert_agg_gpu12
	l0_expert_0_gpu0 -> l0_expert_agg_gpu13
	l0_expert_0_gpu0 -> l0_expert_agg_gpu14
	l0_expert_0_gpu0 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu0 -> l0_res2_gpu0
	l0_mha_res_add_gpu0 -> l0_res2_gpu0
	input -> l0_mha_qkv_gpu1
	l0_mha_qkv_gpu1 -> l0_mha_attn_gpu1
	l0_mha_attn_gpu1 -> l0_mha_out_gpu1
	l0_mha_out_gpu1 -> l0_mha_res_add_gpu1
	input -> l0_mha_res_add_gpu1
	l0_mha_res_add_gpu1 -> l0_gate_gpu1
	l0_gate_gpu1 -> l0_expert_1_gpu1 [style=dashed]
	l0_mha_res_add_gpu1 -> l0_expert_1_gpu1
	l0_expert_1_gpu1 -> l0_expert_agg_gpu0
	l0_expert_1_gpu1 -> l0_expert_agg_gpu1
	l0_expert_1_gpu1 -> l0_expert_agg_gpu2
	l0_expert_1_gpu1 -> l0_expert_agg_gpu3
	l0_expert_1_gpu1 -> l0_expert_agg_gpu4
	l0_expert_1_gpu1 -> l0_expert_agg_gpu5
	l0_expert_1_gpu1 -> l0_expert_agg_gpu6
	l0_expert_1_gpu1 -> l0_expert_agg_gpu7
	l0_expert_1_gpu1 -> l0_expert_agg_gpu8
	l0_expert_1_gpu1 -> l0_expert_agg_gpu9
	l0_expert_1_gpu1 -> l0_expert_agg_gpu10
	l0_expert_1_gpu1 -> l0_expert_agg_gpu11
	l0_expert_1_gpu1 -> l0_expert_agg_gpu12
	l0_expert_1_gpu1 -> l0_expert_agg_gpu13
	l0_expert_1_gpu1 -> l0_expert_agg_gpu14
	l0_expert_1_gpu1 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu1 -> l0_res2_gpu1
	l0_mha_res_add_gpu1 -> l0_res2_gpu1
	input -> l0_mha_qkv_gpu2
	l0_mha_qkv_gpu2 -> l0_mha_attn_gpu2
	l0_mha_attn_gpu2 -> l0_mha_out_gpu2
	l0_mha_out_gpu2 -> l0_mha_res_add_gpu2
	input -> l0_mha_res_add_gpu2
	l0_mha_res_add_gpu2 -> l0_gate_gpu2
	l0_gate_gpu2 -> l0_expert_2_gpu2 [style=dashed]
	l0_mha_res_add_gpu2 -> l0_expert_2_gpu2
	l0_expert_2_gpu2 -> l0_expert_agg_gpu0
	l0_expert_2_gpu2 -> l0_expert_agg_gpu1
	l0_expert_2_gpu2 -> l0_expert_agg_gpu2
	l0_expert_2_gpu2 -> l0_expert_agg_gpu3
	l0_expert_2_gpu2 -> l0_expert_agg_gpu4
	l0_expert_2_gpu2 -> l0_expert_agg_gpu5
	l0_expert_2_gpu2 -> l0_expert_agg_gpu6
	l0_expert_2_gpu2 -> l0_expert_agg_gpu7
	l0_expert_2_gpu2 -> l0_expert_agg_gpu8
	l0_expert_2_gpu2 -> l0_expert_agg_gpu9
	l0_expert_2_gpu2 -> l0_expert_agg_gpu10
	l0_expert_2_gpu2 -> l0_expert_agg_gpu11
	l0_expert_2_gpu2 -> l0_expert_agg_gpu12
	l0_expert_2_gpu2 -> l0_expert_agg_gpu13
	l0_expert_2_gpu2 -> l0_expert_agg_gpu14
	l0_expert_2_gpu2 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu2 -> l0_res2_gpu2
	l0_mha_res_add_gpu2 -> l0_res2_gpu2
	input -> l0_mha_qkv_gpu3
	l0_mha_qkv_gpu3 -> l0_mha_attn_gpu3
	l0_mha_attn_gpu3 -> l0_mha_out_gpu3
	l0_mha_out_gpu3 -> l0_mha_res_add_gpu3
	input -> l0_mha_res_add_gpu3
	l0_mha_res_add_gpu3 -> l0_gate_gpu3
	l0_gate_gpu3 -> l0_expert_3_gpu3 [style=dashed]
	l0_mha_res_add_gpu3 -> l0_expert_3_gpu3
	l0_expert_3_gpu3 -> l0_expert_agg_gpu0
	l0_expert_3_gpu3 -> l0_expert_agg_gpu1
	l0_expert_3_gpu3 -> l0_expert_agg_gpu2
	l0_expert_3_gpu3 -> l0_expert_agg_gpu3
	l0_expert_3_gpu3 -> l0_expert_agg_gpu4
	l0_expert_3_gpu3 -> l0_expert_agg_gpu5
	l0_expert_3_gpu3 -> l0_expert_agg_gpu6
	l0_expert_3_gpu3 -> l0_expert_agg_gpu7
	l0_expert_3_gpu3 -> l0_expert_agg_gpu8
	l0_expert_3_gpu3 -> l0_expert_agg_gpu9
	l0_expert_3_gpu3 -> l0_expert_agg_gpu10
	l0_expert_3_gpu3 -> l0_expert_agg_gpu11
	l0_expert_3_gpu3 -> l0_expert_agg_gpu12
	l0_expert_3_gpu3 -> l0_expert_agg_gpu13
	l0_expert_3_gpu3 -> l0_expert_agg_gpu14
	l0_expert_3_gpu3 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu3 -> l0_res2_gpu3
	l0_mha_res_add_gpu3 -> l0_res2_gpu3
	input -> l0_mha_qkv_gpu4
	l0_mha_qkv_gpu4 -> l0_mha_attn_gpu4
	l0_mha_attn_gpu4 -> l0_mha_out_gpu4
	l0_mha_out_gpu4 -> l0_mha_res_add_gpu4
	input -> l0_mha_res_add_gpu4
	l0_mha_res_add_gpu4 -> l0_gate_gpu4
	l0_gate_gpu4 -> l0_expert_4_gpu4 [style=dashed]
	l0_mha_res_add_gpu4 -> l0_expert_4_gpu4
	l0_expert_4_gpu4 -> l0_expert_agg_gpu0
	l0_expert_4_gpu4 -> l0_expert_agg_gpu1
	l0_expert_4_gpu4 -> l0_expert_agg_gpu2
	l0_expert_4_gpu4 -> l0_expert_agg_gpu3
	l0_expert_4_gpu4 -> l0_expert_agg_gpu4
	l0_expert_4_gpu4 -> l0_expert_agg_gpu5
	l0_expert_4_gpu4 -> l0_expert_agg_gpu6
	l0_expert_4_gpu4 -> l0_expert_agg_gpu7
	l0_expert_4_gpu4 -> l0_expert_agg_gpu8
	l0_expert_4_gpu4 -> l0_expert_agg_gpu9
	l0_expert_4_gpu4 -> l0_expert_agg_gpu10
	l0_expert_4_gpu4 -> l0_expert_agg_gpu11
	l0_expert_4_gpu4 -> l0_expert_agg_gpu12
	l0_expert_4_gpu4 -> l0_expert_agg_gpu13
	l0_expert_4_gpu4 -> l0_expert_agg_gpu14
	l0_expert_4_gpu4 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu4 -> l0_res2_gpu4
	l0_mha_res_add_gpu4 -> l0_res2_gpu4
	input -> l0_mha_qkv_gpu5
	l0_mha_qkv_gpu5 -> l0_mha_attn_gpu5
	l0_mha_attn_gpu5 -> l0_mha_out_gpu5
	l0_mha_out_gpu5 -> l0_mha_res_add_gpu5
	input -> l0_mha_res_add_gpu5
	l0_mha_res_add_gpu5 -> l0_gate_gpu5
	l0_gate_gpu5 -> l0_expert_5_gpu5 [style=dashed]
	l0_mha_res_add_gpu5 -> l0_expert_5_gpu5
	l0_expert_5_gpu5 -> l0_expert_agg_gpu0
	l0_expert_5_gpu5 -> l0_expert_agg_gpu1
	l0_expert_5_gpu5 -> l0_expert_agg_gpu2
	l0_expert_5_gpu5 -> l0_expert_agg_gpu3
	l0_expert_5_gpu5 -> l0_expert_agg_gpu4
	l0_expert_5_gpu5 -> l0_expert_agg_gpu5
	l0_expert_5_gpu5 -> l0_expert_agg_gpu6
	l0_expert_5_gpu5 -> l0_expert_agg_gpu7
	l0_expert_5_gpu5 -> l0_expert_agg_gpu8
	l0_expert_5_gpu5 -> l0_expert_agg_gpu9
	l0_expert_5_gpu5 -> l0_expert_agg_gpu10
	l0_expert_5_gpu5 -> l0_expert_agg_gpu11
	l0_expert_5_gpu5 -> l0_expert_agg_gpu12
	l0_expert_5_gpu5 -> l0_expert_agg_gpu13
	l0_expert_5_gpu5 -> l0_expert_agg_gpu14
	l0_expert_5_gpu5 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu5 -> l0_res2_gpu5
	l0_mha_res_add_gpu5 -> l0_res2_gpu5
	input -> l0_mha_qkv_gpu6
	l0_mha_qkv_gpu6 -> l0_mha_attn_gpu6
	l0_mha_attn_gpu6 -> l0_mha_out_gpu6
	l0_mha_out_gpu6 -> l0_mha_res_add_gpu6
	input -> l0_mha_res_add_gpu6
	l0_mha_res_add_gpu6 -> l0_gate_gpu6
	l0_gate_gpu6 -> l0_expert_6_gpu6 [style=dashed]
	l0_mha_res_add_gpu6 -> l0_expert_6_gpu6
	l0_expert_6_gpu6 -> l0_expert_agg_gpu0
	l0_expert_6_gpu6 -> l0_expert_agg_gpu1
	l0_expert_6_gpu6 -> l0_expert_agg_gpu2
	l0_expert_6_gpu6 -> l0_expert_agg_gpu3
	l0_expert_6_gpu6 -> l0_expert_agg_gpu4
	l0_expert_6_gpu6 -> l0_expert_agg_gpu5
	l0_expert_6_gpu6 -> l0_expert_agg_gpu6
	l0_expert_6_gpu6 -> l0_expert_agg_gpu7
	l0_expert_6_gpu6 -> l0_expert_agg_gpu8
	l0_expert_6_gpu6 -> l0_expert_agg_gpu9
	l0_expert_6_gpu6 -> l0_expert_agg_gpu10
	l0_expert_6_gpu6 -> l0_expert_agg_gpu11
	l0_expert_6_gpu6 -> l0_expert_agg_gpu12
	l0_expert_6_gpu6 -> l0_expert_agg_gpu13
	l0_expert_6_gpu6 -> l0_expert_agg_gpu14
	l0_expert_6_gpu6 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu6 -> l0_res2_gpu6
	l0_mha_res_add_gpu6 -> l0_res2_gpu6
	input -> l0_mha_qkv_gpu7
	l0_mha_qkv_gpu7 -> l0_mha_attn_gpu7
	l0_mha_attn_gpu7 -> l0_mha_out_gpu7
	l0_mha_out_gpu7 -> l0_mha_res_add_gpu7
	input -> l0_mha_res_add_gpu7
	l0_mha_res_add_gpu7 -> l0_gate_gpu7
	l0_gate_gpu7 -> l0_expert_7_gpu7 [style=dashed]
	l0_mha_res_add_gpu7 -> l0_expert_7_gpu7
	l0_expert_7_gpu7 -> l0_expert_agg_gpu0
	l0_expert_7_gpu7 -> l0_expert_agg_gpu1
	l0_expert_7_gpu7 -> l0_expert_agg_gpu2
	l0_expert_7_gpu7 -> l0_expert_agg_gpu3
	l0_expert_7_gpu7 -> l0_expert_agg_gpu4
	l0_expert_7_gpu7 -> l0_expert_agg_gpu5
	l0_expert_7_gpu7 -> l0_expert_agg_gpu6
	l0_expert_7_gpu7 -> l0_expert_agg_gpu7
	l0_expert_7_gpu7 -> l0_expert_agg_gpu8
	l0_expert_7_gpu7 -> l0_expert_agg_gpu9
	l0_expert_7_gpu7 -> l0_expert_agg_gpu10
	l0_expert_7_gpu7 -> l0_expert_agg_gpu11
	l0_expert_7_gpu7 -> l0_expert_agg_gpu12
	l0_expert_7_gpu7 -> l0_expert_agg_gpu13
	l0_expert_7_gpu7 -> l0_expert_agg_gpu14
	l0_expert_7_gpu7 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu7 -> l0_res2_gpu7
	l0_mha_res_add_gpu7 -> l0_res2_gpu7
	input -> l0_mha_qkv_gpu8
	l0_mha_qkv_gpu8 -> l0_mha_attn_gpu8
	l0_mha_attn_gpu8 -> l0_mha_out_gpu8
	l0_mha_out_gpu8 -> l0_mha_res_add_gpu8
	input -> l0_mha_res_add_gpu8
	l0_mha_res_add_gpu8 -> l0_gate_gpu8
	l0_gate_gpu8 -> l0_expert_8_gpu8 [style=dashed]
	l0_mha_res_add_gpu8 -> l0_expert_8_gpu8
	l0_expert_8_gpu8 -> l0_expert_agg_gpu0
	l0_expert_8_gpu8 -> l0_expert_agg_gpu1
	l0_expert_8_gpu8 -> l0_expert_agg_gpu2
	l0_expert_8_gpu8 -> l0_expert_agg_gpu3
	l0_expert_8_gpu8 -> l0_expert_agg_gpu4
	l0_expert_8_gpu8 -> l0_expert_agg_gpu5
	l0_expert_8_gpu8 -> l0_expert_agg_gpu6
	l0_expert_8_gpu8 -> l0_expert_agg_gpu7
	l0_expert_8_gpu8 -> l0_expert_agg_gpu8
	l0_expert_8_gpu8 -> l0_expert_agg_gpu9
	l0_expert_8_gpu8 -> l0_expert_agg_gpu10
	l0_expert_8_gpu8 -> l0_expert_agg_gpu11
	l0_expert_8_gpu8 -> l0_expert_agg_gpu12
	l0_expert_8_gpu8 -> l0_expert_agg_gpu13
	l0_expert_8_gpu8 -> l0_expert_agg_gpu14
	l0_expert_8_gpu8 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu8 -> l0_res2_gpu8
	l0_mha_res_add_gpu8 -> l0_res2_gpu8
	input -> l0_mha_qkv_gpu9
	l0_mha_qkv_gpu9 -> l0_mha_attn_gpu9
	l0_mha_attn_gpu9 -> l0_mha_out_gpu9
	l0_mha_out_gpu9 -> l0_mha_res_add_gpu9
	input -> l0_mha_res_add_gpu9
	l0_mha_res_add_gpu9 -> l0_gate_gpu9
	l0_gate_gpu9 -> l0_expert_9_gpu9 [style=dashed]
	l0_mha_res_add_gpu9 -> l0_expert_9_gpu9
	l0_expert_9_gpu9 -> l0_expert_agg_gpu0
	l0_expert_9_gpu9 -> l0_expert_agg_gpu1
	l0_expert_9_gpu9 -> l0_expert_agg_gpu2
	l0_expert_9_gpu9 -> l0_expert_agg_gpu3
	l0_expert_9_gpu9 -> l0_expert_agg_gpu4
	l0_expert_9_gpu9 -> l0_expert_agg_gpu5
	l0_expert_9_gpu9 -> l0_expert_agg_gpu6
	l0_expert_9_gpu9 -> l0_expert_agg_gpu7
	l0_expert_9_gpu9 -> l0_expert_agg_gpu8
	l0_expert_9_gpu9 -> l0_expert_agg_gpu9
	l0_expert_9_gpu9 -> l0_expert_agg_gpu10
	l0_expert_9_gpu9 -> l0_expert_agg_gpu11
	l0_expert_9_gpu9 -> l0_expert_agg_gpu12
	l0_expert_9_gpu9 -> l0_expert_agg_gpu13
	l0_expert_9_gpu9 -> l0_expert_agg_gpu14
	l0_expert_9_gpu9 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu9 -> l0_res2_gpu9
	l0_mha_res_add_gpu9 -> l0_res2_gpu9
	input -> l0_mha_qkv_gpu10
	l0_mha_qkv_gpu10 -> l0_mha_attn_gpu10
	l0_mha_attn_gpu10 -> l0_mha_out_gpu10
	l0_mha_out_gpu10 -> l0_mha_res_add_gpu10
	input -> l0_mha_res_add_gpu10
	l0_mha_res_add_gpu10 -> l0_gate_gpu10
	l0_gate_gpu10 -> l0_expert_10_gpu10 [style=dashed]
	l0_mha_res_add_gpu10 -> l0_expert_10_gpu10
	l0_expert_10_gpu10 -> l0_expert_agg_gpu0
	l0_expert_10_gpu10 -> l0_expert_agg_gpu1
	l0_expert_10_gpu10 -> l0_expert_agg_gpu2
	l0_expert_10_gpu10 -> l0_expert_agg_gpu3
	l0_expert_10_gpu10 -> l0_expert_agg_gpu4
	l0_expert_10_gpu10 -> l0_expert_agg_gpu5
	l0_expert_10_gpu10 -> l0_expert_agg_gpu6
	l0_expert_10_gpu10 -> l0_expert_agg_gpu7
	l0_expert_10_gpu10 -> l0_expert_agg_gpu8
	l0_expert_10_gpu10 -> l0_expert_agg_gpu9
	l0_expert_10_gpu10 -> l0_expert_agg_gpu10
	l0_expert_10_gpu10 -> l0_expert_agg_gpu11
	l0_expert_10_gpu10 -> l0_expert_agg_gpu12
	l0_expert_10_gpu10 -> l0_expert_agg_gpu13
	l0_expert_10_gpu10 -> l0_expert_agg_gpu14
	l0_expert_10_gpu10 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu10 -> l0_res2_gpu10
	l0_mha_res_add_gpu10 -> l0_res2_gpu10
	input -> l0_mha_qkv_gpu11
	l0_mha_qkv_gpu11 -> l0_mha_attn_gpu11
	l0_mha_attn_gpu11 -> l0_mha_out_gpu11
	l0_mha_out_gpu11 -> l0_mha_res_add_gpu11
	input -> l0_mha_res_add_gpu11
	l0_mha_res_add_gpu11 -> l0_gate_gpu11
	l0_gate_gpu11 -> l0_expert_11_gpu11 [style=dashed]
	l0_mha_res_add_gpu11 -> l0_expert_11_gpu11
	l0_expert_11_gpu11 -> l0_expert_agg_gpu0
	l0_expert_11_gpu11 -> l0_expert_agg_gpu1
	l0_expert_11_gpu11 -> l0_expert_agg_gpu2
	l0_expert_11_gpu11 -> l0_expert_agg_gpu3
	l0_expert_11_gpu11 -> l0_expert_agg_gpu4
	l0_expert_11_gpu11 -> l0_expert_agg_gpu5
	l0_expert_11_gpu11 -> l0_expert_agg_gpu6
	l0_expert_11_gpu11 -> l0_expert_agg_gpu7
	l0_expert_11_gpu11 -> l0_expert_agg_gpu8
	l0_expert_11_gpu11 -> l0_expert_agg_gpu9
	l0_expert_11_gpu11 -> l0_expert_agg_gpu10
	l0_expert_11_gpu11 -> l0_expert_agg_gpu11
	l0_expert_11_gpu11 -> l0_expert_agg_gpu12
	l0_expert_11_gpu11 -> l0_expert_agg_gpu13
	l0_expert_11_gpu11 -> l0_expert_agg_gpu14
	l0_expert_11_gpu11 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu11 -> l0_res2_gpu11
	l0_mha_res_add_gpu11 -> l0_res2_gpu11
	input -> l0_mha_qkv_gpu12
	l0_mha_qkv_gpu12 -> l0_mha_attn_gpu12
	l0_mha_attn_gpu12 -> l0_mha_out_gpu12
	l0_mha_out_gpu12 -> l0_mha_res_add_gpu12
	input -> l0_mha_res_add_gpu12
	l0_mha_res_add_gpu12 -> l0_gate_gpu12
	l0_gate_gpu12 -> l0_expert_12_gpu12 [style=dashed]
	l0_mha_res_add_gpu12 -> l0_expert_12_gpu12
	l0_expert_12_gpu12 -> l0_expert_agg_gpu0
	l0_expert_12_gpu12 -> l0_expert_agg_gpu1
	l0_expert_12_gpu12 -> l0_expert_agg_gpu2
	l0_expert_12_gpu12 -> l0_expert_agg_gpu3
	l0_expert_12_gpu12 -> l0_expert_agg_gpu4
	l0_expert_12_gpu12 -> l0_expert_agg_gpu5
	l0_expert_12_gpu12 -> l0_expert_agg_gpu6
	l0_expert_12_gpu12 -> l0_expert_agg_gpu7
	l0_expert_12_gpu12 -> l0_expert_agg_gpu8
	l0_expert_12_gpu12 -> l0_expert_agg_gpu9
	l0_expert_12_gpu12 -> l0_expert_agg_gpu10
	l0_expert_12_gpu12 -> l0_expert_agg_gpu11
	l0_expert_12_gpu12 -> l0_expert_agg_gpu12
	l0_expert_12_gpu12 -> l0_expert_agg_gpu13
	l0_expert_12_gpu12 -> l0_expert_agg_gpu14
	l0_expert_12_gpu12 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu12 -> l0_res2_gpu12
	l0_mha_res_add_gpu12 -> l0_res2_gpu12
	input -> l0_mha_qkv_gpu13
	l0_mha_qkv_gpu13 -> l0_mha_attn_gpu13
	l0_mha_attn_gpu13 -> l0_mha_out_gpu13
	l0_mha_out_gpu13 -> l0_mha_res_add_gpu13
	input -> l0_mha_res_add_gpu13
	l0_mha_res_add_gpu13 -> l0_gate_gpu13
	l0_gate_gpu13 -> l0_expert_13_gpu13 [style=dashed]
	l0_mha_res_add_gpu13 -> l0_expert_13_gpu13
	l0_expert_13_gpu13 -> l0_expert_agg_gpu0
	l0_expert_13_gpu13 -> l0_expert_agg_gpu1
	l0_expert_13_gpu13 -> l0_expert_agg_gpu2
	l0_expert_13_gpu13 -> l0_expert_agg_gpu3
	l0_expert_13_gpu13 -> l0_expert_agg_gpu4
	l0_expert_13_gpu13 -> l0_expert_agg_gpu5
	l0_expert_13_gpu13 -> l0_expert_agg_gpu6
	l0_expert_13_gpu13 -> l0_expert_agg_gpu7
	l0_expert_13_gpu13 -> l0_expert_agg_gpu8
	l0_expert_13_gpu13 -> l0_expert_agg_gpu9
	l0_expert_13_gpu13 -> l0_expert_agg_gpu10
	l0_expert_13_gpu13 -> l0_expert_agg_gpu11
	l0_expert_13_gpu13 -> l0_expert_agg_gpu12
	l0_expert_13_gpu13 -> l0_expert_agg_gpu13
	l0_expert_13_gpu13 -> l0_expert_agg_gpu14
	l0_expert_13_gpu13 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu13 -> l0_res2_gpu13
	l0_mha_res_add_gpu13 -> l0_res2_gpu13
	input -> l0_mha_qkv_gpu14
	l0_mha_qkv_gpu14 -> l0_mha_attn_gpu14
	l0_mha_attn_gpu14 -> l0_mha_out_gpu14
	l0_mha_out_gpu14 -> l0_mha_res_add_gpu14
	input -> l0_mha_res_add_gpu14
	l0_mha_res_add_gpu14 -> l0_gate_gpu14
	l0_gate_gpu14 -> l0_expert_14_gpu14 [style=dashed]
	l0_mha_res_add_gpu14 -> l0_expert_14_gpu14
	l0_expert_14_gpu14 -> l0_expert_agg_gpu0
	l0_expert_14_gpu14 -> l0_expert_agg_gpu1
	l0_expert_14_gpu14 -> l0_expert_agg_gpu2
	l0_expert_14_gpu14 -> l0_expert_agg_gpu3
	l0_expert_14_gpu14 -> l0_expert_agg_gpu4
	l0_expert_14_gpu14 -> l0_expert_agg_gpu5
	l0_expert_14_gpu14 -> l0_expert_agg_gpu6
	l0_expert_14_gpu14 -> l0_expert_agg_gpu7
	l0_expert_14_gpu14 -> l0_expert_agg_gpu8
	l0_expert_14_gpu14 -> l0_expert_agg_gpu9
	l0_expert_14_gpu14 -> l0_expert_agg_gpu10
	l0_expert_14_gpu14 -> l0_expert_agg_gpu11
	l0_expert_14_gpu14 -> l0_expert_agg_gpu12
	l0_expert_14_gpu14 -> l0_expert_agg_gpu13
	l0_expert_14_gpu14 -> l0_expert_agg_gpu14
	l0_expert_14_gpu14 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu14 -> l0_res2_gpu14
	l0_mha_res_add_gpu14 -> l0_res2_gpu14
	input -> l0_mha_qkv_gpu15
	l0_mha_qkv_gpu15 -> l0_mha_attn_gpu15
	l0_mha_attn_gpu15 -> l0_mha_out_gpu15
	l0_mha_out_gpu15 -> l0_mha_res_add_gpu15
	input -> l0_mha_res_add_gpu15
	l0_mha_res_add_gpu15 -> l0_gate_gpu15
	l0_gate_gpu15 -> l0_expert_15_gpu15 [style=dashed]
	l0_mha_res_add_gpu15 -> l0_expert_15_gpu15
	l0_expert_15_gpu15 -> l0_expert_agg_gpu0
	l0_expert_15_gpu15 -> l0_expert_agg_gpu1
	l0_expert_15_gpu15 -> l0_expert_agg_gpu2
	l0_expert_15_gpu15 -> l0_expert_agg_gpu3
	l0_expert_15_gpu15 -> l0_expert_agg_gpu4
	l0_expert_15_gpu15 -> l0_expert_agg_gpu5
	l0_expert_15_gpu15 -> l0_expert_agg_gpu6
	l0_expert_15_gpu15 -> l0_expert_agg_gpu7
	l0_expert_15_gpu15 -> l0_expert_agg_gpu8
	l0_expert_15_gpu15 -> l0_expert_agg_gpu9
	l0_expert_15_gpu15 -> l0_expert_agg_gpu10
	l0_expert_15_gpu15 -> l0_expert_agg_gpu11
	l0_expert_15_gpu15 -> l0_expert_agg_gpu12
	l0_expert_15_gpu15 -> l0_expert_agg_gpu13
	l0_expert_15_gpu15 -> l0_expert_agg_gpu14
	l0_expert_15_gpu15 -> l0_expert_agg_gpu15
	l0_expert_agg_gpu15 -> l0_res2_gpu15
	l0_mha_res_add_gpu15 -> l0_res2_gpu15
	l0_res2_gpu0 -> l1_mha_qkv_gpu0
	l1_mha_qkv_gpu0 -> l1_mha_attn_gpu0
	l1_mha_attn_gpu0 -> l1_mha_out_gpu0
	l1_mha_out_gpu0 -> l1_mha_res_add_gpu0
	l0_res2_gpu0 -> l1_mha_res_add_gpu0
	l1_mha_res_add_gpu0 -> l1_gate_gpu0
	l1_gate_gpu0 -> l1_expert_0_gpu0 [style=dashed]
	l1_mha_res_add_gpu0 -> l1_expert_0_gpu0
	l1_expert_0_gpu0 -> l1_expert_agg_gpu0
	l1_expert_0_gpu0 -> l1_expert_agg_gpu1
	l1_expert_0_gpu0 -> l1_expert_agg_gpu2
	l1_expert_0_gpu0 -> l1_expert_agg_gpu3
	l1_expert_0_gpu0 -> l1_expert_agg_gpu4
	l1_expert_0_gpu0 -> l1_expert_agg_gpu5
	l1_expert_0_gpu0 -> l1_expert_agg_gpu6
	l1_expert_0_gpu0 -> l1_expert_agg_gpu7
	l1_expert_0_gpu0 -> l1_expert_agg_gpu8
	l1_expert_0_gpu0 -> l1_expert_agg_gpu9
	l1_expert_0_gpu0 -> l1_expert_agg_gpu10
	l1_expert_0_gpu0 -> l1_expert_agg_gpu11
	l1_expert_0_gpu0 -> l1_expert_agg_gpu12
	l1_expert_0_gpu0 -> l1_expert_agg_gpu13
	l1_expert_0_gpu0 -> l1_expert_agg_gpu14
	l1_expert_0_gpu0 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu0 -> l1_res2_gpu0
	l1_mha_res_add_gpu0 -> l1_res2_gpu0
	l0_res2_gpu1 -> l1_mha_qkv_gpu1
	l1_mha_qkv_gpu1 -> l1_mha_attn_gpu1
	l1_mha_attn_gpu1 -> l1_mha_out_gpu1
	l1_mha_out_gpu1 -> l1_mha_res_add_gpu1
	l0_res2_gpu1 -> l1_mha_res_add_gpu1
	l1_mha_res_add_gpu1 -> l1_gate_gpu1
	l1_gate_gpu1 -> l1_expert_1_gpu1 [style=dashed]
	l1_mha_res_add_gpu1 -> l1_expert_1_gpu1
	l1_expert_1_gpu1 -> l1_expert_agg_gpu0
	l1_expert_1_gpu1 -> l1_expert_agg_gpu1
	l1_expert_1_gpu1 -> l1_expert_agg_gpu2
	l1_expert_1_gpu1 -> l1_expert_agg_gpu3
	l1_expert_1_gpu1 -> l1_expert_agg_gpu4
	l1_expert_1_gpu1 -> l1_expert_agg_gpu5
	l1_expert_1_gpu1 -> l1_expert_agg_gpu6
	l1_expert_1_gpu1 -> l1_expert_agg_gpu7
	l1_expert_1_gpu1 -> l1_expert_agg_gpu8
	l1_expert_1_gpu1 -> l1_expert_agg_gpu9
	l1_expert_1_gpu1 -> l1_expert_agg_gpu10
	l1_expert_1_gpu1 -> l1_expert_agg_gpu11
	l1_expert_1_gpu1 -> l1_expert_agg_gpu12
	l1_expert_1_gpu1 -> l1_expert_agg_gpu13
	l1_expert_1_gpu1 -> l1_expert_agg_gpu14
	l1_expert_1_gpu1 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu1 -> l1_res2_gpu1
	l1_mha_res_add_gpu1 -> l1_res2_gpu1
	l0_res2_gpu2 -> l1_mha_qkv_gpu2
	l1_mha_qkv_gpu2 -> l1_mha_attn_gpu2
	l1_mha_attn_gpu2 -> l1_mha_out_gpu2
	l1_mha_out_gpu2 -> l1_mha_res_add_gpu2
	l0_res2_gpu2 -> l1_mha_res_add_gpu2
	l1_mha_res_add_gpu2 -> l1_gate_gpu2
	l1_gate_gpu2 -> l1_expert_2_gpu2 [style=dashed]
	l1_mha_res_add_gpu2 -> l1_expert_2_gpu2
	l1_expert_2_gpu2 -> l1_expert_agg_gpu0
	l1_expert_2_gpu2 -> l1_expert_agg_gpu1
	l1_expert_2_gpu2 -> l1_expert_agg_gpu2
	l1_expert_2_gpu2 -> l1_expert_agg_gpu3
	l1_expert_2_gpu2 -> l1_expert_agg_gpu4
	l1_expert_2_gpu2 -> l1_expert_agg_gpu5
	l1_expert_2_gpu2 -> l1_expert_agg_gpu6
	l1_expert_2_gpu2 -> l1_expert_agg_gpu7
	l1_expert_2_gpu2 -> l1_expert_agg_gpu8
	l1_expert_2_gpu2 -> l1_expert_agg_gpu9
	l1_expert_2_gpu2 -> l1_expert_agg_gpu10
	l1_expert_2_gpu2 -> l1_expert_agg_gpu11
	l1_expert_2_gpu2 -> l1_expert_agg_gpu12
	l1_expert_2_gpu2 -> l1_expert_agg_gpu13
	l1_expert_2_gpu2 -> l1_expert_agg_gpu14
	l1_expert_2_gpu2 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu2 -> l1_res2_gpu2
	l1_mha_res_add_gpu2 -> l1_res2_gpu2
	l0_res2_gpu3 -> l1_mha_qkv_gpu3
	l1_mha_qkv_gpu3 -> l1_mha_attn_gpu3
	l1_mha_attn_gpu3 -> l1_mha_out_gpu3
	l1_mha_out_gpu3 -> l1_mha_res_add_gpu3
	l0_res2_gpu3 -> l1_mha_res_add_gpu3
	l1_mha_res_add_gpu3 -> l1_gate_gpu3
	l1_gate_gpu3 -> l1_expert_3_gpu3 [style=dashed]
	l1_mha_res_add_gpu3 -> l1_expert_3_gpu3
	l1_expert_3_gpu3 -> l1_expert_agg_gpu0
	l1_expert_3_gpu3 -> l1_expert_agg_gpu1
	l1_expert_3_gpu3 -> l1_expert_agg_gpu2
	l1_expert_3_gpu3 -> l1_expert_agg_gpu3
	l1_expert_3_gpu3 -> l1_expert_agg_gpu4
	l1_expert_3_gpu3 -> l1_expert_agg_gpu5
	l1_expert_3_gpu3 -> l1_expert_agg_gpu6
	l1_expert_3_gpu3 -> l1_expert_agg_gpu7
	l1_expert_3_gpu3 -> l1_expert_agg_gpu8
	l1_expert_3_gpu3 -> l1_expert_agg_gpu9
	l1_expert_3_gpu3 -> l1_expert_agg_gpu10
	l1_expert_3_gpu3 -> l1_expert_agg_gpu11
	l1_expert_3_gpu3 -> l1_expert_agg_gpu12
	l1_expert_3_gpu3 -> l1_expert_agg_gpu13
	l1_expert_3_gpu3 -> l1_expert_agg_gpu14
	l1_expert_3_gpu3 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu3 -> l1_res2_gpu3
	l1_mha_res_add_gpu3 -> l1_res2_gpu3
	l0_res2_gpu4 -> l1_mha_qkv_gpu4
	l1_mha_qkv_gpu4 -> l1_mha_attn_gpu4
	l1_mha_attn_gpu4 -> l1_mha_out_gpu4
	l1_mha_out_gpu4 -> l1_mha_res_add_gpu4
	l0_res2_gpu4 -> l1_mha_res_add_gpu4
	l1_mha_res_add_gpu4 -> l1_gate_gpu4
	l1_gate_gpu4 -> l1_expert_4_gpu4 [style=dashed]
	l1_mha_res_add_gpu4 -> l1_expert_4_gpu4
	l1_expert_4_gpu4 -> l1_expert_agg_gpu0
	l1_expert_4_gpu4 -> l1_expert_agg_gpu1
	l1_expert_4_gpu4 -> l1_expert_agg_gpu2
	l1_expert_4_gpu4 -> l1_expert_agg_gpu3
	l1_expert_4_gpu4 -> l1_expert_agg_gpu4
	l1_expert_4_gpu4 -> l1_expert_agg_gpu5
	l1_expert_4_gpu4 -> l1_expert_agg_gpu6
	l1_expert_4_gpu4 -> l1_expert_agg_gpu7
	l1_expert_4_gpu4 -> l1_expert_agg_gpu8
	l1_expert_4_gpu4 -> l1_expert_agg_gpu9
	l1_expert_4_gpu4 -> l1_expert_agg_gpu10
	l1_expert_4_gpu4 -> l1_expert_agg_gpu11
	l1_expert_4_gpu4 -> l1_expert_agg_gpu12
	l1_expert_4_gpu4 -> l1_expert_agg_gpu13
	l1_expert_4_gpu4 -> l1_expert_agg_gpu14
	l1_expert_4_gpu4 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu4 -> l1_res2_gpu4
	l1_mha_res_add_gpu4 -> l1_res2_gpu4
	l0_res2_gpu5 -> l1_mha_qkv_gpu5
	l1_mha_qkv_gpu5 -> l1_mha_attn_gpu5
	l1_mha_attn_gpu5 -> l1_mha_out_gpu5
	l1_mha_out_gpu5 -> l1_mha_res_add_gpu5
	l0_res2_gpu5 -> l1_mha_res_add_gpu5
	l1_mha_res_add_gpu5 -> l1_gate_gpu5
	l1_gate_gpu5 -> l1_expert_5_gpu5 [style=dashed]
	l1_mha_res_add_gpu5 -> l1_expert_5_gpu5
	l1_expert_5_gpu5 -> l1_expert_agg_gpu0
	l1_expert_5_gpu5 -> l1_expert_agg_gpu1
	l1_expert_5_gpu5 -> l1_expert_agg_gpu2
	l1_expert_5_gpu5 -> l1_expert_agg_gpu3
	l1_expert_5_gpu5 -> l1_expert_agg_gpu4
	l1_expert_5_gpu5 -> l1_expert_agg_gpu5
	l1_expert_5_gpu5 -> l1_expert_agg_gpu6
	l1_expert_5_gpu5 -> l1_expert_agg_gpu7
	l1_expert_5_gpu5 -> l1_expert_agg_gpu8
	l1_expert_5_gpu5 -> l1_expert_agg_gpu9
	l1_expert_5_gpu5 -> l1_expert_agg_gpu10
	l1_expert_5_gpu5 -> l1_expert_agg_gpu11
	l1_expert_5_gpu5 -> l1_expert_agg_gpu12
	l1_expert_5_gpu5 -> l1_expert_agg_gpu13
	l1_expert_5_gpu5 -> l1_expert_agg_gpu14
	l1_expert_5_gpu5 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu5 -> l1_res2_gpu5
	l1_mha_res_add_gpu5 -> l1_res2_gpu5
	l0_res2_gpu6 -> l1_mha_qkv_gpu6
	l1_mha_qkv_gpu6 -> l1_mha_attn_gpu6
	l1_mha_attn_gpu6 -> l1_mha_out_gpu6
	l1_mha_out_gpu6 -> l1_mha_res_add_gpu6
	l0_res2_gpu6 -> l1_mha_res_add_gpu6
	l1_mha_res_add_gpu6 -> l1_gate_gpu6
	l1_gate_gpu6 -> l1_expert_6_gpu6 [style=dashed]
	l1_mha_res_add_gpu6 -> l1_expert_6_gpu6
	l1_expert_6_gpu6 -> l1_expert_agg_gpu0
	l1_expert_6_gpu6 -> l1_expert_agg_gpu1
	l1_expert_6_gpu6 -> l1_expert_agg_gpu2
	l1_expert_6_gpu6 -> l1_expert_agg_gpu3
	l1_expert_6_gpu6 -> l1_expert_agg_gpu4
	l1_expert_6_gpu6 -> l1_expert_agg_gpu5
	l1_expert_6_gpu6 -> l1_expert_agg_gpu6
	l1_expert_6_gpu6 -> l1_expert_agg_gpu7
	l1_expert_6_gpu6 -> l1_expert_agg_gpu8
	l1_expert_6_gpu6 -> l1_expert_agg_gpu9
	l1_expert_6_gpu6 -> l1_expert_agg_gpu10
	l1_expert_6_gpu6 -> l1_expert_agg_gpu11
	l1_expert_6_gpu6 -> l1_expert_agg_gpu12
	l1_expert_6_gpu6 -> l1_expert_agg_gpu13
	l1_expert_6_gpu6 -> l1_expert_agg_gpu14
	l1_expert_6_gpu6 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu6 -> l1_res2_gpu6
	l1_mha_res_add_gpu6 -> l1_res2_gpu6
	l0_res2_gpu7 -> l1_mha_qkv_gpu7
	l1_mha_qkv_gpu7 -> l1_mha_attn_gpu7
	l1_mha_attn_gpu7 -> l1_mha_out_gpu7
	l1_mha_out_gpu7 -> l1_mha_res_add_gpu7
	l0_res2_gpu7 -> l1_mha_res_add_gpu7
	l1_mha_res_add_gpu7 -> l1_gate_gpu7
	l1_gate_gpu7 -> l1_expert_7_gpu7 [style=dashed]
	l1_mha_res_add_gpu7 -> l1_expert_7_gpu7
	l1_expert_7_gpu7 -> l1_expert_agg_gpu0
	l1_expert_7_gpu7 -> l1_expert_agg_gpu1
	l1_expert_7_gpu7 -> l1_expert_agg_gpu2
	l1_expert_7_gpu7 -> l1_expert_agg_gpu3
	l1_expert_7_gpu7 -> l1_expert_agg_gpu4
	l1_expert_7_gpu7 -> l1_expert_agg_gpu5
	l1_expert_7_gpu7 -> l1_expert_agg_gpu6
	l1_expert_7_gpu7 -> l1_expert_agg_gpu7
	l1_expert_7_gpu7 -> l1_expert_agg_gpu8
	l1_expert_7_gpu7 -> l1_expert_agg_gpu9
	l1_expert_7_gpu7 -> l1_expert_agg_gpu10
	l1_expert_7_gpu7 -> l1_expert_agg_gpu11
	l1_expert_7_gpu7 -> l1_expert_agg_gpu12
	l1_expert_7_gpu7 -> l1_expert_agg_gpu13
	l1_expert_7_gpu7 -> l1_expert_agg_gpu14
	l1_expert_7_gpu7 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu7 -> l1_res2_gpu7
	l1_mha_res_add_gpu7 -> l1_res2_gpu7
	l0_res2_gpu8 -> l1_mha_qkv_gpu8
	l1_mha_qkv_gpu8 -> l1_mha_attn_gpu8
	l1_mha_attn_gpu8 -> l1_mha_out_gpu8
	l1_mha_out_gpu8 -> l1_mha_res_add_gpu8
	l0_res2_gpu8 -> l1_mha_res_add_gpu8
	l1_mha_res_add_gpu8 -> l1_gate_gpu8
	l1_gate_gpu8 -> l1_expert_8_gpu8 [style=dashed]
	l1_mha_res_add_gpu8 -> l1_expert_8_gpu8
	l1_expert_8_gpu8 -> l1_expert_agg_gpu0
	l1_expert_8_gpu8 -> l1_expert_agg_gpu1
	l1_expert_8_gpu8 -> l1_expert_agg_gpu2
	l1_expert_8_gpu8 -> l1_expert_agg_gpu3
	l1_expert_8_gpu8 -> l1_expert_agg_gpu4
	l1_expert_8_gpu8 -> l1_expert_agg_gpu5
	l1_expert_8_gpu8 -> l1_expert_agg_gpu6
	l1_expert_8_gpu8 -> l1_expert_agg_gpu7
	l1_expert_8_gpu8 -> l1_expert_agg_gpu8
	l1_expert_8_gpu8 -> l1_expert_agg_gpu9
	l1_expert_8_gpu8 -> l1_expert_agg_gpu10
	l1_expert_8_gpu8 -> l1_expert_agg_gpu11
	l1_expert_8_gpu8 -> l1_expert_agg_gpu12
	l1_expert_8_gpu8 -> l1_expert_agg_gpu13
	l1_expert_8_gpu8 -> l1_expert_agg_gpu14
	l1_expert_8_gpu8 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu8 -> l1_res2_gpu8
	l1_mha_res_add_gpu8 -> l1_res2_gpu8
	l0_res2_gpu9 -> l1_mha_qkv_gpu9
	l1_mha_qkv_gpu9 -> l1_mha_attn_gpu9
	l1_mha_attn_gpu9 -> l1_mha_out_gpu9
	l1_mha_out_gpu9 -> l1_mha_res_add_gpu9
	l0_res2_gpu9 -> l1_mha_res_add_gpu9
	l1_mha_res_add_gpu9 -> l1_gate_gpu9
	l1_gate_gpu9 -> l1_expert_9_gpu9 [style=dashed]
	l1_mha_res_add_gpu9 -> l1_expert_9_gpu9
	l1_expert_9_gpu9 -> l1_expert_agg_gpu0
	l1_expert_9_gpu9 -> l1_expert_agg_gpu1
	l1_expert_9_gpu9 -> l1_expert_agg_gpu2
	l1_expert_9_gpu9 -> l1_expert_agg_gpu3
	l1_expert_9_gpu9 -> l1_expert_agg_gpu4
	l1_expert_9_gpu9 -> l1_expert_agg_gpu5
	l1_expert_9_gpu9 -> l1_expert_agg_gpu6
	l1_expert_9_gpu9 -> l1_expert_agg_gpu7
	l1_expert_9_gpu9 -> l1_expert_agg_gpu8
	l1_expert_9_gpu9 -> l1_expert_agg_gpu9
	l1_expert_9_gpu9 -> l1_expert_agg_gpu10
	l1_expert_9_gpu9 -> l1_expert_agg_gpu11
	l1_expert_9_gpu9 -> l1_expert_agg_gpu12
	l1_expert_9_gpu9 -> l1_expert_agg_gpu13
	l1_expert_9_gpu9 -> l1_expert_agg_gpu14
	l1_expert_9_gpu9 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu9 -> l1_res2_gpu9
	l1_mha_res_add_gpu9 -> l1_res2_gpu9
	l0_res2_gpu10 -> l1_mha_qkv_gpu10
	l1_mha_qkv_gpu10 -> l1_mha_attn_gpu10
	l1_mha_attn_gpu10 -> l1_mha_out_gpu10
	l1_mha_out_gpu10 -> l1_mha_res_add_gpu10
	l0_res2_gpu10 -> l1_mha_res_add_gpu10
	l1_mha_res_add_gpu10 -> l1_gate_gpu10
	l1_gate_gpu10 -> l1_expert_10_gpu10 [style=dashed]
	l1_mha_res_add_gpu10 -> l1_expert_10_gpu10
	l1_expert_10_gpu10 -> l1_expert_agg_gpu0
	l1_expert_10_gpu10 -> l1_expert_agg_gpu1
	l1_expert_10_gpu10 -> l1_expert_agg_gpu2
	l1_expert_10_gpu10 -> l1_expert_agg_gpu3
	l1_expert_10_gpu10 -> l1_expert_agg_gpu4
	l1_expert_10_gpu10 -> l1_expert_agg_gpu5
	l1_expert_10_gpu10 -> l1_expert_agg_gpu6
	l1_expert_10_gpu10 -> l1_expert_agg_gpu7
	l1_expert_10_gpu10 -> l1_expert_agg_gpu8
	l1_expert_10_gpu10 -> l1_expert_agg_gpu9
	l1_expert_10_gpu10 -> l1_expert_agg_gpu10
	l1_expert_10_gpu10 -> l1_expert_agg_gpu11
	l1_expert_10_gpu10 -> l1_expert_agg_gpu12
	l1_expert_10_gpu10 -> l1_expert_agg_gpu13
	l1_expert_10_gpu10 -> l1_expert_agg_gpu14
	l1_expert_10_gpu10 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu10 -> l1_res2_gpu10
	l1_mha_res_add_gpu10 -> l1_res2_gpu10
	l0_res2_gpu11 -> l1_mha_qkv_gpu11
	l1_mha_qkv_gpu11 -> l1_mha_attn_gpu11
	l1_mha_attn_gpu11 -> l1_mha_out_gpu11
	l1_mha_out_gpu11 -> l1_mha_res_add_gpu11
	l0_res2_gpu11 -> l1_mha_res_add_gpu11
	l1_mha_res_add_gpu11 -> l1_gate_gpu11
	l1_gate_gpu11 -> l1_expert_11_gpu11 [style=dashed]
	l1_mha_res_add_gpu11 -> l1_expert_11_gpu11
	l1_expert_11_gpu11 -> l1_expert_agg_gpu0
	l1_expert_11_gpu11 -> l1_expert_agg_gpu1
	l1_expert_11_gpu11 -> l1_expert_agg_gpu2
	l1_expert_11_gpu11 -> l1_expert_agg_gpu3
	l1_expert_11_gpu11 -> l1_expert_agg_gpu4
	l1_expert_11_gpu11 -> l1_expert_agg_gpu5
	l1_expert_11_gpu11 -> l1_expert_agg_gpu6
	l1_expert_11_gpu11 -> l1_expert_agg_gpu7
	l1_expert_11_gpu11 -> l1_expert_agg_gpu8
	l1_expert_11_gpu11 -> l1_expert_agg_gpu9
	l1_expert_11_gpu11 -> l1_expert_agg_gpu10
	l1_expert_11_gpu11 -> l1_expert_agg_gpu11
	l1_expert_11_gpu11 -> l1_expert_agg_gpu12
	l1_expert_11_gpu11 -> l1_expert_agg_gpu13
	l1_expert_11_gpu11 -> l1_expert_agg_gpu14
	l1_expert_11_gpu11 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu11 -> l1_res2_gpu11
	l1_mha_res_add_gpu11 -> l1_res2_gpu11
	l0_res2_gpu12 -> l1_mha_qkv_gpu12
	l1_mha_qkv_gpu12 -> l1_mha_attn_gpu12
	l1_mha_attn_gpu12 -> l1_mha_out_gpu12
	l1_mha_out_gpu12 -> l1_mha_res_add_gpu12
	l0_res2_gpu12 -> l1_mha_res_add_gpu12
	l1_mha_res_add_gpu12 -> l1_gate_gpu12
	l1_gate_gpu12 -> l1_expert_12_gpu12 [style=dashed]
	l1_mha_res_add_gpu12 -> l1_expert_12_gpu12
	l1_expert_12_gpu12 -> l1_expert_agg_gpu0
	l1_expert_12_gpu12 -> l1_expert_agg_gpu1
	l1_expert_12_gpu12 -> l1_expert_agg_gpu2
	l1_expert_12_gpu12 -> l1_expert_agg_gpu3
	l1_expert_12_gpu12 -> l1_expert_agg_gpu4
	l1_expert_12_gpu12 -> l1_expert_agg_gpu5
	l1_expert_12_gpu12 -> l1_expert_agg_gpu6
	l1_expert_12_gpu12 -> l1_expert_agg_gpu7
	l1_expert_12_gpu12 -> l1_expert_agg_gpu8
	l1_expert_12_gpu12 -> l1_expert_agg_gpu9
	l1_expert_12_gpu12 -> l1_expert_agg_gpu10
	l1_expert_12_gpu12 -> l1_expert_agg_gpu11
	l1_expert_12_gpu12 -> l1_expert_agg_gpu12
	l1_expert_12_gpu12 -> l1_expert_agg_gpu13
	l1_expert_12_gpu12 -> l1_expert_agg_gpu14
	l1_expert_12_gpu12 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu12 -> l1_res2_gpu12
	l1_mha_res_add_gpu12 -> l1_res2_gpu12
	l0_res2_gpu13 -> l1_mha_qkv_gpu13
	l1_mha_qkv_gpu13 -> l1_mha_attn_gpu13
	l1_mha_attn_gpu13 -> l1_mha_out_gpu13
	l1_mha_out_gpu13 -> l1_mha_res_add_gpu13
	l0_res2_gpu13 -> l1_mha_res_add_gpu13
	l1_mha_res_add_gpu13 -> l1_gate_gpu13
	l1_gate_gpu13 -> l1_expert_13_gpu13 [style=dashed]
	l1_mha_res_add_gpu13 -> l1_expert_13_gpu13
	l1_expert_13_gpu13 -> l1_expert_agg_gpu0
	l1_expert_13_gpu13 -> l1_expert_agg_gpu1
	l1_expert_13_gpu13 -> l1_expert_agg_gpu2
	l1_expert_13_gpu13 -> l1_expert_agg_gpu3
	l1_expert_13_gpu13 -> l1_expert_agg_gpu4
	l1_expert_13_gpu13 -> l1_expert_agg_gpu5
	l1_expert_13_gpu13 -> l1_expert_agg_gpu6
	l1_expert_13_gpu13 -> l1_expert_agg_gpu7
	l1_expert_13_gpu13 -> l1_expert_agg_gpu8
	l1_expert_13_gpu13 -> l1_expert_agg_gpu9
	l1_expert_13_gpu13 -> l1_expert_agg_gpu10
	l1_expert_13_gpu13 -> l1_expert_agg_gpu11
	l1_expert_13_gpu13 -> l1_expert_agg_gpu12
	l1_expert_13_gpu13 -> l1_expert_agg_gpu13
	l1_expert_13_gpu13 -> l1_expert_agg_gpu14
	l1_expert_13_gpu13 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu13 -> l1_res2_gpu13
	l1_mha_res_add_gpu13 -> l1_res2_gpu13
	l0_res2_gpu14 -> l1_mha_qkv_gpu14
	l1_mha_qkv_gpu14 -> l1_mha_attn_gpu14
	l1_mha_attn_gpu14 -> l1_mha_out_gpu14
	l1_mha_out_gpu14 -> l1_mha_res_add_gpu14
	l0_res2_gpu14 -> l1_mha_res_add_gpu14
	l1_mha_res_add_gpu14 -> l1_gate_gpu14
	l1_gate_gpu14 -> l1_expert_14_gpu14 [style=dashed]
	l1_mha_res_add_gpu14 -> l1_expert_14_gpu14
	l1_expert_14_gpu14 -> l1_expert_agg_gpu0
	l1_expert_14_gpu14 -> l1_expert_agg_gpu1
	l1_expert_14_gpu14 -> l1_expert_agg_gpu2
	l1_expert_14_gpu14 -> l1_expert_agg_gpu3
	l1_expert_14_gpu14 -> l1_expert_agg_gpu4
	l1_expert_14_gpu14 -> l1_expert_agg_gpu5
	l1_expert_14_gpu14 -> l1_expert_agg_gpu6
	l1_expert_14_gpu14 -> l1_expert_agg_gpu7
	l1_expert_14_gpu14 -> l1_expert_agg_gpu8
	l1_expert_14_gpu14 -> l1_expert_agg_gpu9
	l1_expert_14_gpu14 -> l1_expert_agg_gpu10
	l1_expert_14_gpu14 -> l1_expert_agg_gpu11
	l1_expert_14_gpu14 -> l1_expert_agg_gpu12
	l1_expert_14_gpu14 -> l1_expert_agg_gpu13
	l1_expert_14_gpu14 -> l1_expert_agg_gpu14
	l1_expert_14_gpu14 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu14 -> l1_res2_gpu14
	l1_mha_res_add_gpu14 -> l1_res2_gpu14
	l0_res2_gpu15 -> l1_mha_qkv_gpu15
	l1_mha_qkv_gpu15 -> l1_mha_attn_gpu15
	l1_mha_attn_gpu15 -> l1_mha_out_gpu15
	l1_mha_out_gpu15 -> l1_mha_res_add_gpu15
	l0_res2_gpu15 -> l1_mha_res_add_gpu15
	l1_mha_res_add_gpu15 -> l1_gate_gpu15
	l1_gate_gpu15 -> l1_expert_15_gpu15 [style=dashed]
	l1_mha_res_add_gpu15 -> l1_expert_15_gpu15
	l1_expert_15_gpu15 -> l1_expert_agg_gpu0
	l1_expert_15_gpu15 -> l1_expert_agg_gpu1
	l1_expert_15_gpu15 -> l1_expert_agg_gpu2
	l1_expert_15_gpu15 -> l1_expert_agg_gpu3
	l1_expert_15_gpu15 -> l1_expert_agg_gpu4
	l1_expert_15_gpu15 -> l1_expert_agg_gpu5
	l1_expert_15_gpu15 -> l1_expert_agg_gpu6
	l1_expert_15_gpu15 -> l1_expert_agg_gpu7
	l1_expert_15_gpu15 -> l1_expert_agg_gpu8
	l1_expert_15_gpu15 -> l1_expert_agg_gpu9
	l1_expert_15_gpu15 -> l1_expert_agg_gpu10
	l1_expert_15_gpu15 -> l1_expert_agg_gpu11
	l1_expert_15_gpu15 -> l1_expert_agg_gpu12
	l1_expert_15_gpu15 -> l1_expert_agg_gpu13
	l1_expert_15_gpu15 -> l1_expert_agg_gpu14
	l1_expert_15_gpu15 -> l1_expert_agg_gpu15
	l1_expert_agg_gpu15 -> l1_res2_gpu15
	l1_mha_res_add_gpu15 -> l1_res2_gpu15
	l1_res2_gpu0 -> l2_mha_qkv_gpu0
	l2_mha_qkv_gpu0 -> l2_mha_attn_gpu0
	l2_mha_attn_gpu0 -> l2_mha_out_gpu0
	l2_mha_out_gpu0 -> l2_mha_res_add_gpu0
	l1_res2_gpu0 -> l2_mha_res_add_gpu0
	l2_mha_res_add_gpu0 -> l2_gate_gpu0
	l2_gate_gpu0 -> l2_expert_0_gpu0 [style=dashed]
	l2_mha_res_add_gpu0 -> l2_expert_0_gpu0
	l2_expert_0_gpu0 -> l2_expert_agg_gpu0
	l2_expert_0_gpu0 -> l2_expert_agg_gpu1
	l2_expert_0_gpu0 -> l2_expert_agg_gpu2
	l2_expert_0_gpu0 -> l2_expert_agg_gpu3
	l2_expert_0_gpu0 -> l2_expert_agg_gpu4
	l2_expert_0_gpu0 -> l2_expert_agg_gpu5
	l2_expert_0_gpu0 -> l2_expert_agg_gpu6
	l2_expert_0_gpu0 -> l2_expert_agg_gpu7
	l2_expert_0_gpu0 -> l2_expert_agg_gpu8
	l2_expert_0_gpu0 -> l2_expert_agg_gpu9
	l2_expert_0_gpu0 -> l2_expert_agg_gpu10
	l2_expert_0_gpu0 -> l2_expert_agg_gpu11
	l2_expert_0_gpu0 -> l2_expert_agg_gpu12
	l2_expert_0_gpu0 -> l2_expert_agg_gpu13
	l2_expert_0_gpu0 -> l2_expert_agg_gpu14
	l2_expert_0_gpu0 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu0 -> l2_res2_gpu0
	l2_mha_res_add_gpu0 -> l2_res2_gpu0
	l1_res2_gpu1 -> l2_mha_qkv_gpu1
	l2_mha_qkv_gpu1 -> l2_mha_attn_gpu1
	l2_mha_attn_gpu1 -> l2_mha_out_gpu1
	l2_mha_out_gpu1 -> l2_mha_res_add_gpu1
	l1_res2_gpu1 -> l2_mha_res_add_gpu1
	l2_mha_res_add_gpu1 -> l2_gate_gpu1
	l2_gate_gpu1 -> l2_expert_1_gpu1 [style=dashed]
	l2_mha_res_add_gpu1 -> l2_expert_1_gpu1
	l2_expert_1_gpu1 -> l2_expert_agg_gpu0
	l2_expert_1_gpu1 -> l2_expert_agg_gpu1
	l2_expert_1_gpu1 -> l2_expert_agg_gpu2
	l2_expert_1_gpu1 -> l2_expert_agg_gpu3
	l2_expert_1_gpu1 -> l2_expert_agg_gpu4
	l2_expert_1_gpu1 -> l2_expert_agg_gpu5
	l2_expert_1_gpu1 -> l2_expert_agg_gpu6
	l2_expert_1_gpu1 -> l2_expert_agg_gpu7
	l2_expert_1_gpu1 -> l2_expert_agg_gpu8
	l2_expert_1_gpu1 -> l2_expert_agg_gpu9
	l2_expert_1_gpu1 -> l2_expert_agg_gpu10
	l2_expert_1_gpu1 -> l2_expert_agg_gpu11
	l2_expert_1_gpu1 -> l2_expert_agg_gpu12
	l2_expert_1_gpu1 -> l2_expert_agg_gpu13
	l2_expert_1_gpu1 -> l2_expert_agg_gpu14
	l2_expert_1_gpu1 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu1 -> l2_res2_gpu1
	l2_mha_res_add_gpu1 -> l2_res2_gpu1
	l1_res2_gpu2 -> l2_mha_qkv_gpu2
	l2_mha_qkv_gpu2 -> l2_mha_attn_gpu2
	l2_mha_attn_gpu2 -> l2_mha_out_gpu2
	l2_mha_out_gpu2 -> l2_mha_res_add_gpu2
	l1_res2_gpu2 -> l2_mha_res_add_gpu2
	l2_mha_res_add_gpu2 -> l2_gate_gpu2
	l2_gate_gpu2 -> l2_expert_2_gpu2 [style=dashed]
	l2_mha_res_add_gpu2 -> l2_expert_2_gpu2
	l2_expert_2_gpu2 -> l2_expert_agg_gpu0
	l2_expert_2_gpu2 -> l2_expert_agg_gpu1
	l2_expert_2_gpu2 -> l2_expert_agg_gpu2
	l2_expert_2_gpu2 -> l2_expert_agg_gpu3
	l2_expert_2_gpu2 -> l2_expert_agg_gpu4
	l2_expert_2_gpu2 -> l2_expert_agg_gpu5
	l2_expert_2_gpu2 -> l2_expert_agg_gpu6
	l2_expert_2_gpu2 -> l2_expert_agg_gpu7
	l2_expert_2_gpu2 -> l2_expert_agg_gpu8
	l2_expert_2_gpu2 -> l2_expert_agg_gpu9
	l2_expert_2_gpu2 -> l2_expert_agg_gpu10
	l2_expert_2_gpu2 -> l2_expert_agg_gpu11
	l2_expert_2_gpu2 -> l2_expert_agg_gpu12
	l2_expert_2_gpu2 -> l2_expert_agg_gpu13
	l2_expert_2_gpu2 -> l2_expert_agg_gpu14
	l2_expert_2_gpu2 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu2 -> l2_res2_gpu2
	l2_mha_res_add_gpu2 -> l2_res2_gpu2
	l1_res2_gpu3 -> l2_mha_qkv_gpu3
	l2_mha_qkv_gpu3 -> l2_mha_attn_gpu3
	l2_mha_attn_gpu3 -> l2_mha_out_gpu3
	l2_mha_out_gpu3 -> l2_mha_res_add_gpu3
	l1_res2_gpu3 -> l2_mha_res_add_gpu3
	l2_mha_res_add_gpu3 -> l2_gate_gpu3
	l2_gate_gpu3 -> l2_expert_3_gpu3 [style=dashed]
	l2_mha_res_add_gpu3 -> l2_expert_3_gpu3
	l2_expert_3_gpu3 -> l2_expert_agg_gpu0
	l2_expert_3_gpu3 -> l2_expert_agg_gpu1
	l2_expert_3_gpu3 -> l2_expert_agg_gpu2
	l2_expert_3_gpu3 -> l2_expert_agg_gpu3
	l2_expert_3_gpu3 -> l2_expert_agg_gpu4
	l2_expert_3_gpu3 -> l2_expert_agg_gpu5
	l2_expert_3_gpu3 -> l2_expert_agg_gpu6
	l2_expert_3_gpu3 -> l2_expert_agg_gpu7
	l2_expert_3_gpu3 -> l2_expert_agg_gpu8
	l2_expert_3_gpu3 -> l2_expert_agg_gpu9
	l2_expert_3_gpu3 -> l2_expert_agg_gpu10
	l2_expert_3_gpu3 -> l2_expert_agg_gpu11
	l2_expert_3_gpu3 -> l2_expert_agg_gpu12
	l2_expert_3_gpu3 -> l2_expert_agg_gpu13
	l2_expert_3_gpu3 -> l2_expert_agg_gpu14
	l2_expert_3_gpu3 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu3 -> l2_res2_gpu3
	l2_mha_res_add_gpu3 -> l2_res2_gpu3
	l1_res2_gpu4 -> l2_mha_qkv_gpu4
	l2_mha_qkv_gpu4 -> l2_mha_attn_gpu4
	l2_mha_attn_gpu4 -> l2_mha_out_gpu4
	l2_mha_out_gpu4 -> l2_mha_res_add_gpu4
	l1_res2_gpu4 -> l2_mha_res_add_gpu4
	l2_mha_res_add_gpu4 -> l2_gate_gpu4
	l2_gate_gpu4 -> l2_expert_4_gpu4 [style=dashed]
	l2_mha_res_add_gpu4 -> l2_expert_4_gpu4
	l2_expert_4_gpu4 -> l2_expert_agg_gpu0
	l2_expert_4_gpu4 -> l2_expert_agg_gpu1
	l2_expert_4_gpu4 -> l2_expert_agg_gpu2
	l2_expert_4_gpu4 -> l2_expert_agg_gpu3
	l2_expert_4_gpu4 -> l2_expert_agg_gpu4
	l2_expert_4_gpu4 -> l2_expert_agg_gpu5
	l2_expert_4_gpu4 -> l2_expert_agg_gpu6
	l2_expert_4_gpu4 -> l2_expert_agg_gpu7
	l2_expert_4_gpu4 -> l2_expert_agg_gpu8
	l2_expert_4_gpu4 -> l2_expert_agg_gpu9
	l2_expert_4_gpu4 -> l2_expert_agg_gpu10
	l2_expert_4_gpu4 -> l2_expert_agg_gpu11
	l2_expert_4_gpu4 -> l2_expert_agg_gpu12
	l2_expert_4_gpu4 -> l2_expert_agg_gpu13
	l2_expert_4_gpu4 -> l2_expert_agg_gpu14
	l2_expert_4_gpu4 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu4 -> l2_res2_gpu4
	l2_mha_res_add_gpu4 -> l2_res2_gpu4
	l1_res2_gpu5 -> l2_mha_qkv_gpu5
	l2_mha_qkv_gpu5 -> l2_mha_attn_gpu5
	l2_mha_attn_gpu5 -> l2_mha_out_gpu5
	l2_mha_out_gpu5 -> l2_mha_res_add_gpu5
	l1_res2_gpu5 -> l2_mha_res_add_gpu5
	l2_mha_res_add_gpu5 -> l2_gate_gpu5
	l2_gate_gpu5 -> l2_expert_5_gpu5 [style=dashed]
	l2_mha_res_add_gpu5 -> l2_expert_5_gpu5
	l2_expert_5_gpu5 -> l2_expert_agg_gpu0
	l2_expert_5_gpu5 -> l2_expert_agg_gpu1
	l2_expert_5_gpu5 -> l2_expert_agg_gpu2
	l2_expert_5_gpu5 -> l2_expert_agg_gpu3
	l2_expert_5_gpu5 -> l2_expert_agg_gpu4
	l2_expert_5_gpu5 -> l2_expert_agg_gpu5
	l2_expert_5_gpu5 -> l2_expert_agg_gpu6
	l2_expert_5_gpu5 -> l2_expert_agg_gpu7
	l2_expert_5_gpu5 -> l2_expert_agg_gpu8
	l2_expert_5_gpu5 -> l2_expert_agg_gpu9
	l2_expert_5_gpu5 -> l2_expert_agg_gpu10
	l2_expert_5_gpu5 -> l2_expert_agg_gpu11
	l2_expert_5_gpu5 -> l2_expert_agg_gpu12
	l2_expert_5_gpu5 -> l2_expert_agg_gpu13
	l2_expert_5_gpu5 -> l2_expert_agg_gpu14
	l2_expert_5_gpu5 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu5 -> l2_res2_gpu5
	l2_mha_res_add_gpu5 -> l2_res2_gpu5
	l1_res2_gpu6 -> l2_mha_qkv_gpu6
	l2_mha_qkv_gpu6 -> l2_mha_attn_gpu6
	l2_mha_attn_gpu6 -> l2_mha_out_gpu6
	l2_mha_out_gpu6 -> l2_mha_res_add_gpu6
	l1_res2_gpu6 -> l2_mha_res_add_gpu6
	l2_mha_res_add_gpu6 -> l2_gate_gpu6
	l2_gate_gpu6 -> l2_expert_6_gpu6 [style=dashed]
	l2_mha_res_add_gpu6 -> l2_expert_6_gpu6
	l2_expert_6_gpu6 -> l2_expert_agg_gpu0
	l2_expert_6_gpu6 -> l2_expert_agg_gpu1
	l2_expert_6_gpu6 -> l2_expert_agg_gpu2
	l2_expert_6_gpu6 -> l2_expert_agg_gpu3
	l2_expert_6_gpu6 -> l2_expert_agg_gpu4
	l2_expert_6_gpu6 -> l2_expert_agg_gpu5
	l2_expert_6_gpu6 -> l2_expert_agg_gpu6
	l2_expert_6_gpu6 -> l2_expert_agg_gpu7
	l2_expert_6_gpu6 -> l2_expert_agg_gpu8
	l2_expert_6_gpu6 -> l2_expert_agg_gpu9
	l2_expert_6_gpu6 -> l2_expert_agg_gpu10
	l2_expert_6_gpu6 -> l2_expert_agg_gpu11
	l2_expert_6_gpu6 -> l2_expert_agg_gpu12
	l2_expert_6_gpu6 -> l2_expert_agg_gpu13
	l2_expert_6_gpu6 -> l2_expert_agg_gpu14
	l2_expert_6_gpu6 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu6 -> l2_res2_gpu6
	l2_mha_res_add_gpu6 -> l2_res2_gpu6
	l1_res2_gpu7 -> l2_mha_qkv_gpu7
	l2_mha_qkv_gpu7 -> l2_mha_attn_gpu7
	l2_mha_attn_gpu7 -> l2_mha_out_gpu7
	l2_mha_out_gpu7 -> l2_mha_res_add_gpu7
	l1_res2_gpu7 -> l2_mha_res_add_gpu7
	l2_mha_res_add_gpu7 -> l2_gate_gpu7
	l2_gate_gpu7 -> l2_expert_7_gpu7 [style=dashed]
	l2_mha_res_add_gpu7 -> l2_expert_7_gpu7
	l2_expert_7_gpu7 -> l2_expert_agg_gpu0
	l2_expert_7_gpu7 -> l2_expert_agg_gpu1
	l2_expert_7_gpu7 -> l2_expert_agg_gpu2
	l2_expert_7_gpu7 -> l2_expert_agg_gpu3
	l2_expert_7_gpu7 -> l2_expert_agg_gpu4
	l2_expert_7_gpu7 -> l2_expert_agg_gpu5
	l2_expert_7_gpu7 -> l2_expert_agg_gpu6
	l2_expert_7_gpu7 -> l2_expert_agg_gpu7
	l2_expert_7_gpu7 -> l2_expert_agg_gpu8
	l2_expert_7_gpu7 -> l2_expert_agg_gpu9
	l2_expert_7_gpu7 -> l2_expert_agg_gpu10
	l2_expert_7_gpu7 -> l2_expert_agg_gpu11
	l2_expert_7_gpu7 -> l2_expert_agg_gpu12
	l2_expert_7_gpu7 -> l2_expert_agg_gpu13
	l2_expert_7_gpu7 -> l2_expert_agg_gpu14
	l2_expert_7_gpu7 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu7 -> l2_res2_gpu7
	l2_mha_res_add_gpu7 -> l2_res2_gpu7
	l1_res2_gpu8 -> l2_mha_qkv_gpu8
	l2_mha_qkv_gpu8 -> l2_mha_attn_gpu8
	l2_mha_attn_gpu8 -> l2_mha_out_gpu8
	l2_mha_out_gpu8 -> l2_mha_res_add_gpu8
	l1_res2_gpu8 -> l2_mha_res_add_gpu8
	l2_mha_res_add_gpu8 -> l2_gate_gpu8
	l2_gate_gpu8 -> l2_expert_8_gpu8 [style=dashed]
	l2_mha_res_add_gpu8 -> l2_expert_8_gpu8
	l2_expert_8_gpu8 -> l2_expert_agg_gpu0
	l2_expert_8_gpu8 -> l2_expert_agg_gpu1
	l2_expert_8_gpu8 -> l2_expert_agg_gpu2
	l2_expert_8_gpu8 -> l2_expert_agg_gpu3
	l2_expert_8_gpu8 -> l2_expert_agg_gpu4
	l2_expert_8_gpu8 -> l2_expert_agg_gpu5
	l2_expert_8_gpu8 -> l2_expert_agg_gpu6
	l2_expert_8_gpu8 -> l2_expert_agg_gpu7
	l2_expert_8_gpu8 -> l2_expert_agg_gpu8
	l2_expert_8_gpu8 -> l2_expert_agg_gpu9
	l2_expert_8_gpu8 -> l2_expert_agg_gpu10
	l2_expert_8_gpu8 -> l2_expert_agg_gpu11
	l2_expert_8_gpu8 -> l2_expert_agg_gpu12
	l2_expert_8_gpu8 -> l2_expert_agg_gpu13
	l2_expert_8_gpu8 -> l2_expert_agg_gpu14
	l2_expert_8_gpu8 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu8 -> l2_res2_gpu8
	l2_mha_res_add_gpu8 -> l2_res2_gpu8
	l1_res2_gpu9 -> l2_mha_qkv_gpu9
	l2_mha_qkv_gpu9 -> l2_mha_attn_gpu9
	l2_mha_attn_gpu9 -> l2_mha_out_gpu9
	l2_mha_out_gpu9 -> l2_mha_res_add_gpu9
	l1_res2_gpu9 -> l2_mha_res_add_gpu9
	l2_mha_res_add_gpu9 -> l2_gate_gpu9
	l2_gate_gpu9 -> l2_expert_9_gpu9 [style=dashed]
	l2_mha_res_add_gpu9 -> l2_expert_9_gpu9
	l2_expert_9_gpu9 -> l2_expert_agg_gpu0
	l2_expert_9_gpu9 -> l2_expert_agg_gpu1
	l2_expert_9_gpu9 -> l2_expert_agg_gpu2
	l2_expert_9_gpu9 -> l2_expert_agg_gpu3
	l2_expert_9_gpu9 -> l2_expert_agg_gpu4
	l2_expert_9_gpu9 -> l2_expert_agg_gpu5
	l2_expert_9_gpu9 -> l2_expert_agg_gpu6
	l2_expert_9_gpu9 -> l2_expert_agg_gpu7
	l2_expert_9_gpu9 -> l2_expert_agg_gpu8
	l2_expert_9_gpu9 -> l2_expert_agg_gpu9
	l2_expert_9_gpu9 -> l2_expert_agg_gpu10
	l2_expert_9_gpu9 -> l2_expert_agg_gpu11
	l2_expert_9_gpu9 -> l2_expert_agg_gpu12
	l2_expert_9_gpu9 -> l2_expert_agg_gpu13
	l2_expert_9_gpu9 -> l2_expert_agg_gpu14
	l2_expert_9_gpu9 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu9 -> l2_res2_gpu9
	l2_mha_res_add_gpu9 -> l2_res2_gpu9
	l1_res2_gpu10 -> l2_mha_qkv_gpu10
	l2_mha_qkv_gpu10 -> l2_mha_attn_gpu10
	l2_mha_attn_gpu10 -> l2_mha_out_gpu10
	l2_mha_out_gpu10 -> l2_mha_res_add_gpu10
	l1_res2_gpu10 -> l2_mha_res_add_gpu10
	l2_mha_res_add_gpu10 -> l2_gate_gpu10
	l2_gate_gpu10 -> l2_expert_10_gpu10 [style=dashed]
	l2_mha_res_add_gpu10 -> l2_expert_10_gpu10
	l2_expert_10_gpu10 -> l2_expert_agg_gpu0
	l2_expert_10_gpu10 -> l2_expert_agg_gpu1
	l2_expert_10_gpu10 -> l2_expert_agg_gpu2
	l2_expert_10_gpu10 -> l2_expert_agg_gpu3
	l2_expert_10_gpu10 -> l2_expert_agg_gpu4
	l2_expert_10_gpu10 -> l2_expert_agg_gpu5
	l2_expert_10_gpu10 -> l2_expert_agg_gpu6
	l2_expert_10_gpu10 -> l2_expert_agg_gpu7
	l2_expert_10_gpu10 -> l2_expert_agg_gpu8
	l2_expert_10_gpu10 -> l2_expert_agg_gpu9
	l2_expert_10_gpu10 -> l2_expert_agg_gpu10
	l2_expert_10_gpu10 -> l2_expert_agg_gpu11
	l2_expert_10_gpu10 -> l2_expert_agg_gpu12
	l2_expert_10_gpu10 -> l2_expert_agg_gpu13
	l2_expert_10_gpu10 -> l2_expert_agg_gpu14
	l2_expert_10_gpu10 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu10 -> l2_res2_gpu10
	l2_mha_res_add_gpu10 -> l2_res2_gpu10
	l1_res2_gpu11 -> l2_mha_qkv_gpu11
	l2_mha_qkv_gpu11 -> l2_mha_attn_gpu11
	l2_mha_attn_gpu11 -> l2_mha_out_gpu11
	l2_mha_out_gpu11 -> l2_mha_res_add_gpu11
	l1_res2_gpu11 -> l2_mha_res_add_gpu11
	l2_mha_res_add_gpu11 -> l2_gate_gpu11
	l2_gate_gpu11 -> l2_expert_11_gpu11 [style=dashed]
	l2_mha_res_add_gpu11 -> l2_expert_11_gpu11
	l2_expert_11_gpu11 -> l2_expert_agg_gpu0
	l2_expert_11_gpu11 -> l2_expert_agg_gpu1
	l2_expert_11_gpu11 -> l2_expert_agg_gpu2
	l2_expert_11_gpu11 -> l2_expert_agg_gpu3
	l2_expert_11_gpu11 -> l2_expert_agg_gpu4
	l2_expert_11_gpu11 -> l2_expert_agg_gpu5
	l2_expert_11_gpu11 -> l2_expert_agg_gpu6
	l2_expert_11_gpu11 -> l2_expert_agg_gpu7
	l2_expert_11_gpu11 -> l2_expert_agg_gpu8
	l2_expert_11_gpu11 -> l2_expert_agg_gpu9
	l2_expert_11_gpu11 -> l2_expert_agg_gpu10
	l2_expert_11_gpu11 -> l2_expert_agg_gpu11
	l2_expert_11_gpu11 -> l2_expert_agg_gpu12
	l2_expert_11_gpu11 -> l2_expert_agg_gpu13
	l2_expert_11_gpu11 -> l2_expert_agg_gpu14
	l2_expert_11_gpu11 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu11 -> l2_res2_gpu11
	l2_mha_res_add_gpu11 -> l2_res2_gpu11
	l1_res2_gpu12 -> l2_mha_qkv_gpu12
	l2_mha_qkv_gpu12 -> l2_mha_attn_gpu12
	l2_mha_attn_gpu12 -> l2_mha_out_gpu12
	l2_mha_out_gpu12 -> l2_mha_res_add_gpu12
	l1_res2_gpu12 -> l2_mha_res_add_gpu12
	l2_mha_res_add_gpu12 -> l2_gate_gpu12
	l2_gate_gpu12 -> l2_expert_12_gpu12 [style=dashed]
	l2_mha_res_add_gpu12 -> l2_expert_12_gpu12
	l2_expert_12_gpu12 -> l2_expert_agg_gpu0
	l2_expert_12_gpu12 -> l2_expert_agg_gpu1
	l2_expert_12_gpu12 -> l2_expert_agg_gpu2
	l2_expert_12_gpu12 -> l2_expert_agg_gpu3
	l2_expert_12_gpu12 -> l2_expert_agg_gpu4
	l2_expert_12_gpu12 -> l2_expert_agg_gpu5
	l2_expert_12_gpu12 -> l2_expert_agg_gpu6
	l2_expert_12_gpu12 -> l2_expert_agg_gpu7
	l2_expert_12_gpu12 -> l2_expert_agg_gpu8
	l2_expert_12_gpu12 -> l2_expert_agg_gpu9
	l2_expert_12_gpu12 -> l2_expert_agg_gpu10
	l2_expert_12_gpu12 -> l2_expert_agg_gpu11
	l2_expert_12_gpu12 -> l2_expert_agg_gpu12
	l2_expert_12_gpu12 -> l2_expert_agg_gpu13
	l2_expert_12_gpu12 -> l2_expert_agg_gpu14
	l2_expert_12_gpu12 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu12 -> l2_res2_gpu12
	l2_mha_res_add_gpu12 -> l2_res2_gpu12
	l1_res2_gpu13 -> l2_mha_qkv_gpu13
	l2_mha_qkv_gpu13 -> l2_mha_attn_gpu13
	l2_mha_attn_gpu13 -> l2_mha_out_gpu13
	l2_mha_out_gpu13 -> l2_mha_res_add_gpu13
	l1_res2_gpu13 -> l2_mha_res_add_gpu13
	l2_mha_res_add_gpu13 -> l2_gate_gpu13
	l2_gate_gpu13 -> l2_expert_13_gpu13 [style=dashed]
	l2_mha_res_add_gpu13 -> l2_expert_13_gpu13
	l2_expert_13_gpu13 -> l2_expert_agg_gpu0
	l2_expert_13_gpu13 -> l2_expert_agg_gpu1
	l2_expert_13_gpu13 -> l2_expert_agg_gpu2
	l2_expert_13_gpu13 -> l2_expert_agg_gpu3
	l2_expert_13_gpu13 -> l2_expert_agg_gpu4
	l2_expert_13_gpu13 -> l2_expert_agg_gpu5
	l2_expert_13_gpu13 -> l2_expert_agg_gpu6
	l2_expert_13_gpu13 -> l2_expert_agg_gpu7
	l2_expert_13_gpu13 -> l2_expert_agg_gpu8
	l2_expert_13_gpu13 -> l2_expert_agg_gpu9
	l2_expert_13_gpu13 -> l2_expert_agg_gpu10
	l2_expert_13_gpu13 -> l2_expert_agg_gpu11
	l2_expert_13_gpu13 -> l2_expert_agg_gpu12
	l2_expert_13_gpu13 -> l2_expert_agg_gpu13
	l2_expert_13_gpu13 -> l2_expert_agg_gpu14
	l2_expert_13_gpu13 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu13 -> l2_res2_gpu13
	l2_mha_res_add_gpu13 -> l2_res2_gpu13
	l1_res2_gpu14 -> l2_mha_qkv_gpu14
	l2_mha_qkv_gpu14 -> l2_mha_attn_gpu14
	l2_mha_attn_gpu14 -> l2_mha_out_gpu14
	l2_mha_out_gpu14 -> l2_mha_res_add_gpu14
	l1_res2_gpu14 -> l2_mha_res_add_gpu14
	l2_mha_res_add_gpu14 -> l2_gate_gpu14
	l2_gate_gpu14 -> l2_expert_14_gpu14 [style=dashed]
	l2_mha_res_add_gpu14 -> l2_expert_14_gpu14
	l2_expert_14_gpu14 -> l2_expert_agg_gpu0
	l2_expert_14_gpu14 -> l2_expert_agg_gpu1
	l2_expert_14_gpu14 -> l2_expert_agg_gpu2
	l2_expert_14_gpu14 -> l2_expert_agg_gpu3
	l2_expert_14_gpu14 -> l2_expert_agg_gpu4
	l2_expert_14_gpu14 -> l2_expert_agg_gpu5
	l2_expert_14_gpu14 -> l2_expert_agg_gpu6
	l2_expert_14_gpu14 -> l2_expert_agg_gpu7
	l2_expert_14_gpu14 -> l2_expert_agg_gpu8
	l2_expert_14_gpu14 -> l2_expert_agg_gpu9
	l2_expert_14_gpu14 -> l2_expert_agg_gpu10
	l2_expert_14_gpu14 -> l2_expert_agg_gpu11
	l2_expert_14_gpu14 -> l2_expert_agg_gpu12
	l2_expert_14_gpu14 -> l2_expert_agg_gpu13
	l2_expert_14_gpu14 -> l2_expert_agg_gpu14
	l2_expert_14_gpu14 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu14 -> l2_res2_gpu14
	l2_mha_res_add_gpu14 -> l2_res2_gpu14
	l1_res2_gpu15 -> l2_mha_qkv_gpu15
	l2_mha_qkv_gpu15 -> l2_mha_attn_gpu15
	l2_mha_attn_gpu15 -> l2_mha_out_gpu15
	l2_mha_out_gpu15 -> l2_mha_res_add_gpu15
	l1_res2_gpu15 -> l2_mha_res_add_gpu15
	l2_mha_res_add_gpu15 -> l2_gate_gpu15
	l2_gate_gpu15 -> l2_expert_15_gpu15 [style=dashed]
	l2_mha_res_add_gpu15 -> l2_expert_15_gpu15
	l2_expert_15_gpu15 -> l2_expert_agg_gpu0
	l2_expert_15_gpu15 -> l2_expert_agg_gpu1
	l2_expert_15_gpu15 -> l2_expert_agg_gpu2
	l2_expert_15_gpu15 -> l2_expert_agg_gpu3
	l2_expert_15_gpu15 -> l2_expert_agg_gpu4
	l2_expert_15_gpu15 -> l2_expert_agg_gpu5
	l2_expert_15_gpu15 -> l2_expert_agg_gpu6
	l2_expert_15_gpu15 -> l2_expert_agg_gpu7
	l2_expert_15_gpu15 -> l2_expert_agg_gpu8
	l2_expert_15_gpu15 -> l2_expert_agg_gpu9
	l2_expert_15_gpu15 -> l2_expert_agg_gpu10
	l2_expert_15_gpu15 -> l2_expert_agg_gpu11
	l2_expert_15_gpu15 -> l2_expert_agg_gpu12
	l2_expert_15_gpu15 -> l2_expert_agg_gpu13
	l2_expert_15_gpu15 -> l2_expert_agg_gpu14
	l2_expert_15_gpu15 -> l2_expert_agg_gpu15
	l2_expert_agg_gpu15 -> l2_res2_gpu15
	l2_mha_res_add_gpu15 -> l2_res2_gpu15
	l2_res2_gpu0 -> l3_mha_qkv_gpu0
	l3_mha_qkv_gpu0 -> l3_mha_attn_gpu0
	l3_mha_attn_gpu0 -> l3_mha_out_gpu0
	l3_mha_out_gpu0 -> l3_mha_res_add_gpu0
	l2_res2_gpu0 -> l3_mha_res_add_gpu0
	l3_mha_res_add_gpu0 -> l3_gate_gpu0
	l3_gate_gpu0 -> l3_expert_0_gpu0 [style=dashed]
	l3_mha_res_add_gpu0 -> l3_expert_0_gpu0
	l3_expert_0_gpu0 -> l3_expert_agg_gpu0
	l3_expert_0_gpu0 -> l3_expert_agg_gpu1
	l3_expert_0_gpu0 -> l3_expert_agg_gpu2
	l3_expert_0_gpu0 -> l3_expert_agg_gpu3
	l3_expert_0_gpu0 -> l3_expert_agg_gpu4
	l3_expert_0_gpu0 -> l3_expert_agg_gpu5
	l3_expert_0_gpu0 -> l3_expert_agg_gpu6
	l3_expert_0_gpu0 -> l3_expert_agg_gpu7
	l3_expert_0_gpu0 -> l3_expert_agg_gpu8
	l3_expert_0_gpu0 -> l3_expert_agg_gpu9
	l3_expert_0_gpu0 -> l3_expert_agg_gpu10
	l3_expert_0_gpu0 -> l3_expert_agg_gpu11
	l3_expert_0_gpu0 -> l3_expert_agg_gpu12
	l3_expert_0_gpu0 -> l3_expert_agg_gpu13
	l3_expert_0_gpu0 -> l3_expert_agg_gpu14
	l3_expert_0_gpu0 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu0 -> l3_res2_gpu0
	l3_mha_res_add_gpu0 -> l3_res2_gpu0
	l2_res2_gpu1 -> l3_mha_qkv_gpu1
	l3_mha_qkv_gpu1 -> l3_mha_attn_gpu1
	l3_mha_attn_gpu1 -> l3_mha_out_gpu1
	l3_mha_out_gpu1 -> l3_mha_res_add_gpu1
	l2_res2_gpu1 -> l3_mha_res_add_gpu1
	l3_mha_res_add_gpu1 -> l3_gate_gpu1
	l3_gate_gpu1 -> l3_expert_1_gpu1 [style=dashed]
	l3_mha_res_add_gpu1 -> l3_expert_1_gpu1
	l3_expert_1_gpu1 -> l3_expert_agg_gpu0
	l3_expert_1_gpu1 -> l3_expert_agg_gpu1
	l3_expert_1_gpu1 -> l3_expert_agg_gpu2
	l3_expert_1_gpu1 -> l3_expert_agg_gpu3
	l3_expert_1_gpu1 -> l3_expert_agg_gpu4
	l3_expert_1_gpu1 -> l3_expert_agg_gpu5
	l3_expert_1_gpu1 -> l3_expert_agg_gpu6
	l3_expert_1_gpu1 -> l3_expert_agg_gpu7
	l3_expert_1_gpu1 -> l3_expert_agg_gpu8
	l3_expert_1_gpu1 -> l3_expert_agg_gpu9
	l3_expert_1_gpu1 -> l3_expert_agg_gpu10
	l3_expert_1_gpu1 -> l3_expert_agg_gpu11
	l3_expert_1_gpu1 -> l3_expert_agg_gpu12
	l3_expert_1_gpu1 -> l3_expert_agg_gpu13
	l3_expert_1_gpu1 -> l3_expert_agg_gpu14
	l3_expert_1_gpu1 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu1 -> l3_res2_gpu1
	l3_mha_res_add_gpu1 -> l3_res2_gpu1
	l2_res2_gpu2 -> l3_mha_qkv_gpu2
	l3_mha_qkv_gpu2 -> l3_mha_attn_gpu2
	l3_mha_attn_gpu2 -> l3_mha_out_gpu2
	l3_mha_out_gpu2 -> l3_mha_res_add_gpu2
	l2_res2_gpu2 -> l3_mha_res_add_gpu2
	l3_mha_res_add_gpu2 -> l3_gate_gpu2
	l3_gate_gpu2 -> l3_expert_2_gpu2 [style=dashed]
	l3_mha_res_add_gpu2 -> l3_expert_2_gpu2
	l3_expert_2_gpu2 -> l3_expert_agg_gpu0
	l3_expert_2_gpu2 -> l3_expert_agg_gpu1
	l3_expert_2_gpu2 -> l3_expert_agg_gpu2
	l3_expert_2_gpu2 -> l3_expert_agg_gpu3
	l3_expert_2_gpu2 -> l3_expert_agg_gpu4
	l3_expert_2_gpu2 -> l3_expert_agg_gpu5
	l3_expert_2_gpu2 -> l3_expert_agg_gpu6
	l3_expert_2_gpu2 -> l3_expert_agg_gpu7
	l3_expert_2_gpu2 -> l3_expert_agg_gpu8
	l3_expert_2_gpu2 -> l3_expert_agg_gpu9
	l3_expert_2_gpu2 -> l3_expert_agg_gpu10
	l3_expert_2_gpu2 -> l3_expert_agg_gpu11
	l3_expert_2_gpu2 -> l3_expert_agg_gpu12
	l3_expert_2_gpu2 -> l3_expert_agg_gpu13
	l3_expert_2_gpu2 -> l3_expert_agg_gpu14
	l3_expert_2_gpu2 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu2 -> l3_res2_gpu2
	l3_mha_res_add_gpu2 -> l3_res2_gpu2
	l2_res2_gpu3 -> l3_mha_qkv_gpu3
	l3_mha_qkv_gpu3 -> l3_mha_attn_gpu3
	l3_mha_attn_gpu3 -> l3_mha_out_gpu3
	l3_mha_out_gpu3 -> l3_mha_res_add_gpu3
	l2_res2_gpu3 -> l3_mha_res_add_gpu3
	l3_mha_res_add_gpu3 -> l3_gate_gpu3
	l3_gate_gpu3 -> l3_expert_3_gpu3 [style=dashed]
	l3_mha_res_add_gpu3 -> l3_expert_3_gpu3
	l3_expert_3_gpu3 -> l3_expert_agg_gpu0
	l3_expert_3_gpu3 -> l3_expert_agg_gpu1
	l3_expert_3_gpu3 -> l3_expert_agg_gpu2
	l3_expert_3_gpu3 -> l3_expert_agg_gpu3
	l3_expert_3_gpu3 -> l3_expert_agg_gpu4
	l3_expert_3_gpu3 -> l3_expert_agg_gpu5
	l3_expert_3_gpu3 -> l3_expert_agg_gpu6
	l3_expert_3_gpu3 -> l3_expert_agg_gpu7
	l3_expert_3_gpu3 -> l3_expert_agg_gpu8
	l3_expert_3_gpu3 -> l3_expert_agg_gpu9
	l3_expert_3_gpu3 -> l3_expert_agg_gpu10
	l3_expert_3_gpu3 -> l3_expert_agg_gpu11
	l3_expert_3_gpu3 -> l3_expert_agg_gpu12
	l3_expert_3_gpu3 -> l3_expert_agg_gpu13
	l3_expert_3_gpu3 -> l3_expert_agg_gpu14
	l3_expert_3_gpu3 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu3 -> l3_res2_gpu3
	l3_mha_res_add_gpu3 -> l3_res2_gpu3
	l2_res2_gpu4 -> l3_mha_qkv_gpu4
	l3_mha_qkv_gpu4 -> l3_mha_attn_gpu4
	l3_mha_attn_gpu4 -> l3_mha_out_gpu4
	l3_mha_out_gpu4 -> l3_mha_res_add_gpu4
	l2_res2_gpu4 -> l3_mha_res_add_gpu4
	l3_mha_res_add_gpu4 -> l3_gate_gpu4
	l3_gate_gpu4 -> l3_expert_4_gpu4 [style=dashed]
	l3_mha_res_add_gpu4 -> l3_expert_4_gpu4
	l3_expert_4_gpu4 -> l3_expert_agg_gpu0
	l3_expert_4_gpu4 -> l3_expert_agg_gpu1
	l3_expert_4_gpu4 -> l3_expert_agg_gpu2
	l3_expert_4_gpu4 -> l3_expert_agg_gpu3
	l3_expert_4_gpu4 -> l3_expert_agg_gpu4
	l3_expert_4_gpu4 -> l3_expert_agg_gpu5
	l3_expert_4_gpu4 -> l3_expert_agg_gpu6
	l3_expert_4_gpu4 -> l3_expert_agg_gpu7
	l3_expert_4_gpu4 -> l3_expert_agg_gpu8
	l3_expert_4_gpu4 -> l3_expert_agg_gpu9
	l3_expert_4_gpu4 -> l3_expert_agg_gpu10
	l3_expert_4_gpu4 -> l3_expert_agg_gpu11
	l3_expert_4_gpu4 -> l3_expert_agg_gpu12
	l3_expert_4_gpu4 -> l3_expert_agg_gpu13
	l3_expert_4_gpu4 -> l3_expert_agg_gpu14
	l3_expert_4_gpu4 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu4 -> l3_res2_gpu4
	l3_mha_res_add_gpu4 -> l3_res2_gpu4
	l2_res2_gpu5 -> l3_mha_qkv_gpu5
	l3_mha_qkv_gpu5 -> l3_mha_attn_gpu5
	l3_mha_attn_gpu5 -> l3_mha_out_gpu5
	l3_mha_out_gpu5 -> l3_mha_res_add_gpu5
	l2_res2_gpu5 -> l3_mha_res_add_gpu5
	l3_mha_res_add_gpu5 -> l3_gate_gpu5
	l3_gate_gpu5 -> l3_expert_5_gpu5 [style=dashed]
	l3_mha_res_add_gpu5 -> l3_expert_5_gpu5
	l3_expert_5_gpu5 -> l3_expert_agg_gpu0
	l3_expert_5_gpu5 -> l3_expert_agg_gpu1
	l3_expert_5_gpu5 -> l3_expert_agg_gpu2
	l3_expert_5_gpu5 -> l3_expert_agg_gpu3
	l3_expert_5_gpu5 -> l3_expert_agg_gpu4
	l3_expert_5_gpu5 -> l3_expert_agg_gpu5
	l3_expert_5_gpu5 -> l3_expert_agg_gpu6
	l3_expert_5_gpu5 -> l3_expert_agg_gpu7
	l3_expert_5_gpu5 -> l3_expert_agg_gpu8
	l3_expert_5_gpu5 -> l3_expert_agg_gpu9
	l3_expert_5_gpu5 -> l3_expert_agg_gpu10
	l3_expert_5_gpu5 -> l3_expert_agg_gpu11
	l3_expert_5_gpu5 -> l3_expert_agg_gpu12
	l3_expert_5_gpu5 -> l3_expert_agg_gpu13
	l3_expert_5_gpu5 -> l3_expert_agg_gpu14
	l3_expert_5_gpu5 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu5 -> l3_res2_gpu5
	l3_mha_res_add_gpu5 -> l3_res2_gpu5
	l2_res2_gpu6 -> l3_mha_qkv_gpu6
	l3_mha_qkv_gpu6 -> l3_mha_attn_gpu6
	l3_mha_attn_gpu6 -> l3_mha_out_gpu6
	l3_mha_out_gpu6 -> l3_mha_res_add_gpu6
	l2_res2_gpu6 -> l3_mha_res_add_gpu6
	l3_mha_res_add_gpu6 -> l3_gate_gpu6
	l3_gate_gpu6 -> l3_expert_6_gpu6 [style=dashed]
	l3_mha_res_add_gpu6 -> l3_expert_6_gpu6
	l3_expert_6_gpu6 -> l3_expert_agg_gpu0
	l3_expert_6_gpu6 -> l3_expert_agg_gpu1
	l3_expert_6_gpu6 -> l3_expert_agg_gpu2
	l3_expert_6_gpu6 -> l3_expert_agg_gpu3
	l3_expert_6_gpu6 -> l3_expert_agg_gpu4
	l3_expert_6_gpu6 -> l3_expert_agg_gpu5
	l3_expert_6_gpu6 -> l3_expert_agg_gpu6
	l3_expert_6_gpu6 -> l3_expert_agg_gpu7
	l3_expert_6_gpu6 -> l3_expert_agg_gpu8
	l3_expert_6_gpu6 -> l3_expert_agg_gpu9
	l3_expert_6_gpu6 -> l3_expert_agg_gpu10
	l3_expert_6_gpu6 -> l3_expert_agg_gpu11
	l3_expert_6_gpu6 -> l3_expert_agg_gpu12
	l3_expert_6_gpu6 -> l3_expert_agg_gpu13
	l3_expert_6_gpu6 -> l3_expert_agg_gpu14
	l3_expert_6_gpu6 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu6 -> l3_res2_gpu6
	l3_mha_res_add_gpu6 -> l3_res2_gpu6
	l2_res2_gpu7 -> l3_mha_qkv_gpu7
	l3_mha_qkv_gpu7 -> l3_mha_attn_gpu7
	l3_mha_attn_gpu7 -> l3_mha_out_gpu7
	l3_mha_out_gpu7 -> l3_mha_res_add_gpu7
	l2_res2_gpu7 -> l3_mha_res_add_gpu7
	l3_mha_res_add_gpu7 -> l3_gate_gpu7
	l3_gate_gpu7 -> l3_expert_7_gpu7 [style=dashed]
	l3_mha_res_add_gpu7 -> l3_expert_7_gpu7
	l3_expert_7_gpu7 -> l3_expert_agg_gpu0
	l3_expert_7_gpu7 -> l3_expert_agg_gpu1
	l3_expert_7_gpu7 -> l3_expert_agg_gpu2
	l3_expert_7_gpu7 -> l3_expert_agg_gpu3
	l3_expert_7_gpu7 -> l3_expert_agg_gpu4
	l3_expert_7_gpu7 -> l3_expert_agg_gpu5
	l3_expert_7_gpu7 -> l3_expert_agg_gpu6
	l3_expert_7_gpu7 -> l3_expert_agg_gpu7
	l3_expert_7_gpu7 -> l3_expert_agg_gpu8
	l3_expert_7_gpu7 -> l3_expert_agg_gpu9
	l3_expert_7_gpu7 -> l3_expert_agg_gpu10
	l3_expert_7_gpu7 -> l3_expert_agg_gpu11
	l3_expert_7_gpu7 -> l3_expert_agg_gpu12
	l3_expert_7_gpu7 -> l3_expert_agg_gpu13
	l3_expert_7_gpu7 -> l3_expert_agg_gpu14
	l3_expert_7_gpu7 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu7 -> l3_res2_gpu7
	l3_mha_res_add_gpu7 -> l3_res2_gpu7
	l2_res2_gpu8 -> l3_mha_qkv_gpu8
	l3_mha_qkv_gpu8 -> l3_mha_attn_gpu8
	l3_mha_attn_gpu8 -> l3_mha_out_gpu8
	l3_mha_out_gpu8 -> l3_mha_res_add_gpu8
	l2_res2_gpu8 -> l3_mha_res_add_gpu8
	l3_mha_res_add_gpu8 -> l3_gate_gpu8
	l3_gate_gpu8 -> l3_expert_8_gpu8 [style=dashed]
	l3_mha_res_add_gpu8 -> l3_expert_8_gpu8
	l3_expert_8_gpu8 -> l3_expert_agg_gpu0
	l3_expert_8_gpu8 -> l3_expert_agg_gpu1
	l3_expert_8_gpu8 -> l3_expert_agg_gpu2
	l3_expert_8_gpu8 -> l3_expert_agg_gpu3
	l3_expert_8_gpu8 -> l3_expert_agg_gpu4
	l3_expert_8_gpu8 -> l3_expert_agg_gpu5
	l3_expert_8_gpu8 -> l3_expert_agg_gpu6
	l3_expert_8_gpu8 -> l3_expert_agg_gpu7
	l3_expert_8_gpu8 -> l3_expert_agg_gpu8
	l3_expert_8_gpu8 -> l3_expert_agg_gpu9
	l3_expert_8_gpu8 -> l3_expert_agg_gpu10
	l3_expert_8_gpu8 -> l3_expert_agg_gpu11
	l3_expert_8_gpu8 -> l3_expert_agg_gpu12
	l3_expert_8_gpu8 -> l3_expert_agg_gpu13
	l3_expert_8_gpu8 -> l3_expert_agg_gpu14
	l3_expert_8_gpu8 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu8 -> l3_res2_gpu8
	l3_mha_res_add_gpu8 -> l3_res2_gpu8
	l2_res2_gpu9 -> l3_mha_qkv_gpu9
	l3_mha_qkv_gpu9 -> l3_mha_attn_gpu9
	l3_mha_attn_gpu9 -> l3_mha_out_gpu9
	l3_mha_out_gpu9 -> l3_mha_res_add_gpu9
	l2_res2_gpu9 -> l3_mha_res_add_gpu9
	l3_mha_res_add_gpu9 -> l3_gate_gpu9
	l3_gate_gpu9 -> l3_expert_9_gpu9 [style=dashed]
	l3_mha_res_add_gpu9 -> l3_expert_9_gpu9
	l3_expert_9_gpu9 -> l3_expert_agg_gpu0
	l3_expert_9_gpu9 -> l3_expert_agg_gpu1
	l3_expert_9_gpu9 -> l3_expert_agg_gpu2
	l3_expert_9_gpu9 -> l3_expert_agg_gpu3
	l3_expert_9_gpu9 -> l3_expert_agg_gpu4
	l3_expert_9_gpu9 -> l3_expert_agg_gpu5
	l3_expert_9_gpu9 -> l3_expert_agg_gpu6
	l3_expert_9_gpu9 -> l3_expert_agg_gpu7
	l3_expert_9_gpu9 -> l3_expert_agg_gpu8
	l3_expert_9_gpu9 -> l3_expert_agg_gpu9
	l3_expert_9_gpu9 -> l3_expert_agg_gpu10
	l3_expert_9_gpu9 -> l3_expert_agg_gpu11
	l3_expert_9_gpu9 -> l3_expert_agg_gpu12
	l3_expert_9_gpu9 -> l3_expert_agg_gpu13
	l3_expert_9_gpu9 -> l3_expert_agg_gpu14
	l3_expert_9_gpu9 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu9 -> l3_res2_gpu9
	l3_mha_res_add_gpu9 -> l3_res2_gpu9
	l2_res2_gpu10 -> l3_mha_qkv_gpu10
	l3_mha_qkv_gpu10 -> l3_mha_attn_gpu10
	l3_mha_attn_gpu10 -> l3_mha_out_gpu10
	l3_mha_out_gpu10 -> l3_mha_res_add_gpu10
	l2_res2_gpu10 -> l3_mha_res_add_gpu10
	l3_mha_res_add_gpu10 -> l3_gate_gpu10
	l3_gate_gpu10 -> l3_expert_10_gpu10 [style=dashed]
	l3_mha_res_add_gpu10 -> l3_expert_10_gpu10
	l3_expert_10_gpu10 -> l3_expert_agg_gpu0
	l3_expert_10_gpu10 -> l3_expert_agg_gpu1
	l3_expert_10_gpu10 -> l3_expert_agg_gpu2
	l3_expert_10_gpu10 -> l3_expert_agg_gpu3
	l3_expert_10_gpu10 -> l3_expert_agg_gpu4
	l3_expert_10_gpu10 -> l3_expert_agg_gpu5
	l3_expert_10_gpu10 -> l3_expert_agg_gpu6
	l3_expert_10_gpu10 -> l3_expert_agg_gpu7
	l3_expert_10_gpu10 -> l3_expert_agg_gpu8
	l3_expert_10_gpu10 -> l3_expert_agg_gpu9
	l3_expert_10_gpu10 -> l3_expert_agg_gpu10
	l3_expert_10_gpu10 -> l3_expert_agg_gpu11
	l3_expert_10_gpu10 -> l3_expert_agg_gpu12
	l3_expert_10_gpu10 -> l3_expert_agg_gpu13
	l3_expert_10_gpu10 -> l3_expert_agg_gpu14
	l3_expert_10_gpu10 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu10 -> l3_res2_gpu10
	l3_mha_res_add_gpu10 -> l3_res2_gpu10
	l2_res2_gpu11 -> l3_mha_qkv_gpu11
	l3_mha_qkv_gpu11 -> l3_mha_attn_gpu11
	l3_mha_attn_gpu11 -> l3_mha_out_gpu11
	l3_mha_out_gpu11 -> l3_mha_res_add_gpu11
	l2_res2_gpu11 -> l3_mha_res_add_gpu11
	l3_mha_res_add_gpu11 -> l3_gate_gpu11
	l3_gate_gpu11 -> l3_expert_11_gpu11 [style=dashed]
	l3_mha_res_add_gpu11 -> l3_expert_11_gpu11
	l3_expert_11_gpu11 -> l3_expert_agg_gpu0
	l3_expert_11_gpu11 -> l3_expert_agg_gpu1
	l3_expert_11_gpu11 -> l3_expert_agg_gpu2
	l3_expert_11_gpu11 -> l3_expert_agg_gpu3
	l3_expert_11_gpu11 -> l3_expert_agg_gpu4
	l3_expert_11_gpu11 -> l3_expert_agg_gpu5
	l3_expert_11_gpu11 -> l3_expert_agg_gpu6
	l3_expert_11_gpu11 -> l3_expert_agg_gpu7
	l3_expert_11_gpu11 -> l3_expert_agg_gpu8
	l3_expert_11_gpu11 -> l3_expert_agg_gpu9
	l3_expert_11_gpu11 -> l3_expert_agg_gpu10
	l3_expert_11_gpu11 -> l3_expert_agg_gpu11
	l3_expert_11_gpu11 -> l3_expert_agg_gpu12
	l3_expert_11_gpu11 -> l3_expert_agg_gpu13
	l3_expert_11_gpu11 -> l3_expert_agg_gpu14
	l3_expert_11_gpu11 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu11 -> l3_res2_gpu11
	l3_mha_res_add_gpu11 -> l3_res2_gpu11
	l2_res2_gpu12 -> l3_mha_qkv_gpu12
	l3_mha_qkv_gpu12 -> l3_mha_attn_gpu12
	l3_mha_attn_gpu12 -> l3_mha_out_gpu12
	l3_mha_out_gpu12 -> l3_mha_res_add_gpu12
	l2_res2_gpu12 -> l3_mha_res_add_gpu12
	l3_mha_res_add_gpu12 -> l3_gate_gpu12
	l3_gate_gpu12 -> l3_expert_12_gpu12 [style=dashed]
	l3_mha_res_add_gpu12 -> l3_expert_12_gpu12
	l3_expert_12_gpu12 -> l3_expert_agg_gpu0
	l3_expert_12_gpu12 -> l3_expert_agg_gpu1
	l3_expert_12_gpu12 -> l3_expert_agg_gpu2
	l3_expert_12_gpu12 -> l3_expert_agg_gpu3
	l3_expert_12_gpu12 -> l3_expert_agg_gpu4
	l3_expert_12_gpu12 -> l3_expert_agg_gpu5
	l3_expert_12_gpu12 -> l3_expert_agg_gpu6
	l3_expert_12_gpu12 -> l3_expert_agg_gpu7
	l3_expert_12_gpu12 -> l3_expert_agg_gpu8
	l3_expert_12_gpu12 -> l3_expert_agg_gpu9
	l3_expert_12_gpu12 -> l3_expert_agg_gpu10
	l3_expert_12_gpu12 -> l3_expert_agg_gpu11
	l3_expert_12_gpu12 -> l3_expert_agg_gpu12
	l3_expert_12_gpu12 -> l3_expert_agg_gpu13
	l3_expert_12_gpu12 -> l3_expert_agg_gpu14
	l3_expert_12_gpu12 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu12 -> l3_res2_gpu12
	l3_mha_res_add_gpu12 -> l3_res2_gpu12
	l2_res2_gpu13 -> l3_mha_qkv_gpu13
	l3_mha_qkv_gpu13 -> l3_mha_attn_gpu13
	l3_mha_attn_gpu13 -> l3_mha_out_gpu13
	l3_mha_out_gpu13 -> l3_mha_res_add_gpu13
	l2_res2_gpu13 -> l3_mha_res_add_gpu13
	l3_mha_res_add_gpu13 -> l3_gate_gpu13
	l3_gate_gpu13 -> l3_expert_13_gpu13 [style=dashed]
	l3_mha_res_add_gpu13 -> l3_expert_13_gpu13
	l3_expert_13_gpu13 -> l3_expert_agg_gpu0
	l3_expert_13_gpu13 -> l3_expert_agg_gpu1
	l3_expert_13_gpu13 -> l3_expert_agg_gpu2
	l3_expert_13_gpu13 -> l3_expert_agg_gpu3
	l3_expert_13_gpu13 -> l3_expert_agg_gpu4
	l3_expert_13_gpu13 -> l3_expert_agg_gpu5
	l3_expert_13_gpu13 -> l3_expert_agg_gpu6
	l3_expert_13_gpu13 -> l3_expert_agg_gpu7
	l3_expert_13_gpu13 -> l3_expert_agg_gpu8
	l3_expert_13_gpu13 -> l3_expert_agg_gpu9
	l3_expert_13_gpu13 -> l3_expert_agg_gpu10
	l3_expert_13_gpu13 -> l3_expert_agg_gpu11
	l3_expert_13_gpu13 -> l3_expert_agg_gpu12
	l3_expert_13_gpu13 -> l3_expert_agg_gpu13
	l3_expert_13_gpu13 -> l3_expert_agg_gpu14
	l3_expert_13_gpu13 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu13 -> l3_res2_gpu13
	l3_mha_res_add_gpu13 -> l3_res2_gpu13
	l2_res2_gpu14 -> l3_mha_qkv_gpu14
	l3_mha_qkv_gpu14 -> l3_mha_attn_gpu14
	l3_mha_attn_gpu14 -> l3_mha_out_gpu14
	l3_mha_out_gpu14 -> l3_mha_res_add_gpu14
	l2_res2_gpu14 -> l3_mha_res_add_gpu14
	l3_mha_res_add_gpu14 -> l3_gate_gpu14
	l3_gate_gpu14 -> l3_expert_14_gpu14 [style=dashed]
	l3_mha_res_add_gpu14 -> l3_expert_14_gpu14
	l3_expert_14_gpu14 -> l3_expert_agg_gpu0
	l3_expert_14_gpu14 -> l3_expert_agg_gpu1
	l3_expert_14_gpu14 -> l3_expert_agg_gpu2
	l3_expert_14_gpu14 -> l3_expert_agg_gpu3
	l3_expert_14_gpu14 -> l3_expert_agg_gpu4
	l3_expert_14_gpu14 -> l3_expert_agg_gpu5
	l3_expert_14_gpu14 -> l3_expert_agg_gpu6
	l3_expert_14_gpu14 -> l3_expert_agg_gpu7
	l3_expert_14_gpu14 -> l3_expert_agg_gpu8
	l3_expert_14_gpu14 -> l3_expert_agg_gpu9
	l3_expert_14_gpu14 -> l3_expert_agg_gpu10
	l3_expert_14_gpu14 -> l3_expert_agg_gpu11
	l3_expert_14_gpu14 -> l3_expert_agg_gpu12
	l3_expert_14_gpu14 -> l3_expert_agg_gpu13
	l3_expert_14_gpu14 -> l3_expert_agg_gpu14
	l3_expert_14_gpu14 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu14 -> l3_res2_gpu14
	l3_mha_res_add_gpu14 -> l3_res2_gpu14
	l2_res2_gpu15 -> l3_mha_qkv_gpu15
	l3_mha_qkv_gpu15 -> l3_mha_attn_gpu15
	l3_mha_attn_gpu15 -> l3_mha_out_gpu15
	l3_mha_out_gpu15 -> l3_mha_res_add_gpu15
	l2_res2_gpu15 -> l3_mha_res_add_gpu15
	l3_mha_res_add_gpu15 -> l3_gate_gpu15
	l3_gate_gpu15 -> l3_expert_15_gpu15 [style=dashed]
	l3_mha_res_add_gpu15 -> l3_expert_15_gpu15
	l3_expert_15_gpu15 -> l3_expert_agg_gpu0
	l3_expert_15_gpu15 -> l3_expert_agg_gpu1
	l3_expert_15_gpu15 -> l3_expert_agg_gpu2
	l3_expert_15_gpu15 -> l3_expert_agg_gpu3
	l3_expert_15_gpu15 -> l3_expert_agg_gpu4
	l3_expert_15_gpu15 -> l3_expert_agg_gpu5
	l3_expert_15_gpu15 -> l3_expert_agg_gpu6
	l3_expert_15_gpu15 -> l3_expert_agg_gpu7
	l3_expert_15_gpu15 -> l3_expert_agg_gpu8
	l3_expert_15_gpu15 -> l3_expert_agg_gpu9
	l3_expert_15_gpu15 -> l3_expert_agg_gpu10
	l3_expert_15_gpu15 -> l3_expert_agg_gpu11
	l3_expert_15_gpu15 -> l3_expert_agg_gpu12
	l3_expert_15_gpu15 -> l3_expert_agg_gpu13
	l3_expert_15_gpu15 -> l3_expert_agg_gpu14
	l3_expert_15_gpu15 -> l3_expert_agg_gpu15
	l3_expert_agg_gpu15 -> l3_res2_gpu15
	l3_mha_res_add_gpu15 -> l3_res2_gpu15
	l3_res2_gpu0 -> output
	l3_res2_gpu1 -> output
	l3_res2_gpu2 -> output
	l3_res2_gpu3 -> output
	l3_res2_gpu4 -> output
	l3_res2_gpu5 -> output
	l3_res2_gpu6 -> output
	l3_res2_gpu7 -> output
	l3_res2_gpu8 -> output
	l3_res2_gpu9 -> output
	l3_res2_gpu10 -> output
	l3_res2_gpu11 -> output
	l3_res2_gpu12 -> output
	l3_res2_gpu13 -> output
	l3_res2_gpu14 -> output
	l3_res2_gpu15 -> output
}
