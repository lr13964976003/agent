digraph EnhancedLLMDeployment {
  rankdir=TB;
  node [shape=record, fontname="Helvetica"];
  edge [fontname="Helvetica"];
  
  node_1 [shape=box, label="Input\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=filled, fillcolor=lightblue];
  
  // ===== Layer 0 =====
  node_2 [shape=box, label="Layer0_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1 -> node_2;
  node_3 [shape=box, label="Layer0_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_4 [shape=box, label="Layer0_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_2 -> node_3;
  node_2 -> node_4;
  node_5 [shape=box, label="Layer0_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_6 [shape=box, label="Layer0_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_3 -> node_5;
  node_4 -> node_6;
  node_7 [shape=box, label="Layer0_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_8 [shape=box, label="Layer0_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_5 -> node_7;
  node_6 -> node_8;
  node_9 [shape=ellipse, label="Layer0_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_7 -> node_9 [style=dashed];
  node_8 -> node_9 [style=dashed];
  node_10 [shape=parallelogram, label="Layer0_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1 -> node_10;
  node_9 -> node_10;
  node_11 [shape=box, label="Layer0_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_10 -> node_11;
  node_12 [shape=parallelogram, label="Layer0_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_11 -> node_12 [style=dashed];
  node_13 [shape=ellipse, label="Layer0_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_12 -> node_13 [style=dashed];
  node_14 [shape=parallelogram, label="Layer0_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_15 [shape=box, label="Layer0_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_16 [shape=box, label="Layer0_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_17 [shape=box, label="Layer0_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_18 [shape=box, label="Layer0_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_19 [shape=box, label="Layer0_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_20 [shape=box, label="Layer0_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_21 [shape=ellipse, label="Layer0_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_14;
  node_14 -> node_15;
  node_14 -> node_16;
  node_15 -> node_17;
  node_16 -> node_18;
  node_17 -> node_19;
  node_18 -> node_20;
  node_19 -> node_21 [style=dashed];
  node_20 -> node_21 [style=dashed];
  node_22 [shape=parallelogram, label="Layer0_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_23 [shape=box, label="Layer0_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_24 [shape=box, label="Layer0_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_25 [shape=box, label="Layer0_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_26 [shape=box, label="Layer0_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_27 [shape=box, label="Layer0_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_28 [shape=box, label="Layer0_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_29 [shape=ellipse, label="Layer0_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_22;
  node_22 -> node_23;
  node_22 -> node_24;
  node_23 -> node_25;
  node_24 -> node_26;
  node_25 -> node_27;
  node_26 -> node_28;
  node_27 -> node_29 [style=dashed];
  node_28 -> node_29 [style=dashed];
  node_30 [shape=parallelogram, label="Layer0_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_31 [shape=box, label="Layer0_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_32 [shape=box, label="Layer0_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_33 [shape=box, label="Layer0_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_34 [shape=box, label="Layer0_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_35 [shape=box, label="Layer0_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_36 [shape=box, label="Layer0_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_37 [shape=ellipse, label="Layer0_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_30;
  node_30 -> node_31;
  node_30 -> node_32;
  node_31 -> node_33;
  node_32 -> node_34;
  node_33 -> node_35;
  node_34 -> node_36;
  node_35 -> node_37 [style=dashed];
  node_36 -> node_37 [style=dashed];
  node_38 [shape=parallelogram, label="Layer0_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_39 [shape=box, label="Layer0_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_40 [shape=box, label="Layer0_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_41 [shape=box, label="Layer0_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_42 [shape=box, label="Layer0_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_43 [shape=box, label="Layer0_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_44 [shape=box, label="Layer0_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_45 [shape=ellipse, label="Layer0_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_38;
  node_38 -> node_39;
  node_38 -> node_40;
  node_39 -> node_41;
  node_40 -> node_42;
  node_41 -> node_43;
  node_42 -> node_44;
  node_43 -> node_45 [style=dashed];
  node_44 -> node_45 [style=dashed];
  node_46 [shape=parallelogram, label="Layer0_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_47 [shape=box, label="Layer0_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_48 [shape=box, label="Layer0_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_49 [shape=box, label="Layer0_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_50 [shape=box, label="Layer0_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_51 [shape=box, label="Layer0_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_52 [shape=box, label="Layer0_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_53 [shape=ellipse, label="Layer0_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_46;
  node_46 -> node_47;
  node_46 -> node_48;
  node_47 -> node_49;
  node_48 -> node_50;
  node_49 -> node_51;
  node_50 -> node_52;
  node_51 -> node_53 [style=dashed];
  node_52 -> node_53 [style=dashed];
  node_54 [shape=parallelogram, label="Layer0_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_55 [shape=box, label="Layer0_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_56 [shape=box, label="Layer0_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_57 [shape=box, label="Layer0_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_58 [shape=box, label="Layer0_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_59 [shape=box, label="Layer0_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_60 [shape=box, label="Layer0_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_61 [shape=ellipse, label="Layer0_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_54;
  node_54 -> node_55;
  node_54 -> node_56;
  node_55 -> node_57;
  node_56 -> node_58;
  node_57 -> node_59;
  node_58 -> node_60;
  node_59 -> node_61 [style=dashed];
  node_60 -> node_61 [style=dashed];
  node_62 [shape=parallelogram, label="Layer0_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_63 [shape=box, label="Layer0_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_64 [shape=box, label="Layer0_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_65 [shape=box, label="Layer0_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_66 [shape=box, label="Layer0_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_67 [shape=box, label="Layer0_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_68 [shape=box, label="Layer0_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_69 [shape=ellipse, label="Layer0_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_62;
  node_62 -> node_63;
  node_62 -> node_64;
  node_63 -> node_65;
  node_64 -> node_66;
  node_65 -> node_67;
  node_66 -> node_68;
  node_67 -> node_69 [style=dashed];
  node_68 -> node_69 [style=dashed];
  node_70 [shape=parallelogram, label="Layer0_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_71 [shape=box, label="Layer0_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_72 [shape=box, label="Layer0_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_73 [shape=box, label="Layer0_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_74 [shape=box, label="Layer0_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_75 [shape=box, label="Layer0_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_76 [shape=box, label="Layer0_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_77 [shape=ellipse, label="Layer0_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_70;
  node_70 -> node_71;
  node_70 -> node_72;
  node_71 -> node_73;
  node_72 -> node_74;
  node_73 -> node_75;
  node_74 -> node_76;
  node_75 -> node_77 [style=dashed];
  node_76 -> node_77 [style=dashed];
  node_78 [shape=parallelogram, label="Layer0_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_79 [shape=box, label="Layer0_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_80 [shape=box, label="Layer0_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_81 [shape=box, label="Layer0_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_82 [shape=box, label="Layer0_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_83 [shape=box, label="Layer0_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_84 [shape=box, label="Layer0_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_85 [shape=ellipse, label="Layer0_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_78;
  node_78 -> node_79;
  node_78 -> node_80;
  node_79 -> node_81;
  node_80 -> node_82;
  node_81 -> node_83;
  node_82 -> node_84;
  node_83 -> node_85 [style=dashed];
  node_84 -> node_85 [style=dashed];
  node_86 [shape=parallelogram, label="Layer0_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_87 [shape=box, label="Layer0_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_88 [shape=box, label="Layer0_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_89 [shape=box, label="Layer0_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_90 [shape=box, label="Layer0_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_91 [shape=box, label="Layer0_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_92 [shape=box, label="Layer0_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_93 [shape=ellipse, label="Layer0_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_86;
  node_86 -> node_87;
  node_86 -> node_88;
  node_87 -> node_89;
  node_88 -> node_90;
  node_89 -> node_91;
  node_90 -> node_92;
  node_91 -> node_93 [style=dashed];
  node_92 -> node_93 [style=dashed];
  node_94 [shape=parallelogram, label="Layer0_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_95 [shape=box, label="Layer0_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_96 [shape=box, label="Layer0_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_97 [shape=box, label="Layer0_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_98 [shape=box, label="Layer0_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_99 [shape=box, label="Layer0_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_100 [shape=box, label="Layer0_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_101 [shape=ellipse, label="Layer0_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_94;
  node_94 -> node_95;
  node_94 -> node_96;
  node_95 -> node_97;
  node_96 -> node_98;
  node_97 -> node_99;
  node_98 -> node_100;
  node_99 -> node_101 [style=dashed];
  node_100 -> node_101 [style=dashed];
  node_102 [shape=parallelogram, label="Layer0_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_103 [shape=box, label="Layer0_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_104 [shape=box, label="Layer0_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_105 [shape=box, label="Layer0_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_106 [shape=box, label="Layer0_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_107 [shape=box, label="Layer0_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_108 [shape=box, label="Layer0_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_109 [shape=ellipse, label="Layer0_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_102;
  node_102 -> node_103;
  node_102 -> node_104;
  node_103 -> node_105;
  node_104 -> node_106;
  node_105 -> node_107;
  node_106 -> node_108;
  node_107 -> node_109 [style=dashed];
  node_108 -> node_109 [style=dashed];
  node_110 [shape=parallelogram, label="Layer0_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_111 [shape=box, label="Layer0_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_112 [shape=box, label="Layer0_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_113 [shape=box, label="Layer0_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_114 [shape=box, label="Layer0_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_115 [shape=box, label="Layer0_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_116 [shape=box, label="Layer0_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_117 [shape=ellipse, label="Layer0_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_110;
  node_110 -> node_111;
  node_110 -> node_112;
  node_111 -> node_113;
  node_112 -> node_114;
  node_113 -> node_115;
  node_114 -> node_116;
  node_115 -> node_117 [style=dashed];
  node_116 -> node_117 [style=dashed];
  node_118 [shape=parallelogram, label="Layer0_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_119 [shape=box, label="Layer0_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_120 [shape=box, label="Layer0_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_121 [shape=box, label="Layer0_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_122 [shape=box, label="Layer0_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_123 [shape=box, label="Layer0_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_124 [shape=box, label="Layer0_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_125 [shape=ellipse, label="Layer0_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_118;
  node_118 -> node_119;
  node_118 -> node_120;
  node_119 -> node_121;
  node_120 -> node_122;
  node_121 -> node_123;
  node_122 -> node_124;
  node_123 -> node_125 [style=dashed];
  node_124 -> node_125 [style=dashed];
  node_126 [shape=parallelogram, label="Layer0_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_127 [shape=box, label="Layer0_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_128 [shape=box, label="Layer0_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_129 [shape=box, label="Layer0_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_130 [shape=box, label="Layer0_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_131 [shape=box, label="Layer0_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_132 [shape=box, label="Layer0_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_133 [shape=ellipse, label="Layer0_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_126;
  node_126 -> node_127;
  node_126 -> node_128;
  node_127 -> node_129;
  node_128 -> node_130;
  node_129 -> node_131;
  node_130 -> node_132;
  node_131 -> node_133 [style=dashed];
  node_132 -> node_133 [style=dashed];
  node_134 [shape=parallelogram, label="Layer0_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_135 [shape=box, label="Layer0_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_136 [shape=box, label="Layer0_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_137 [shape=box, label="Layer0_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_138 [shape=box, label="Layer0_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_139 [shape=box, label="Layer0_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_140 [shape=box, label="Layer0_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_141 [shape=ellipse, label="Layer0_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_13 -> node_134;
  node_134 -> node_135;
  node_134 -> node_136;
  node_135 -> node_137;
  node_136 -> node_138;
  node_137 -> node_139;
  node_138 -> node_140;
  node_139 -> node_141 [style=dashed];
  node_140 -> node_141 [style=dashed];
  node_142 [shape=ellipse, label="Layer0_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_21 -> node_142 [style=dashed];
  node_29 -> node_142 [style=dashed];
  node_37 -> node_142 [style=dashed];
  node_45 -> node_142 [style=dashed];
  node_53 -> node_142 [style=dashed];
  node_61 -> node_142 [style=dashed];
  node_69 -> node_142 [style=dashed];
  node_77 -> node_142 [style=dashed];
  node_85 -> node_142 [style=dashed];
  node_93 -> node_142 [style=dashed];
  node_101 -> node_142 [style=dashed];
  node_109 -> node_142 [style=dashed];
  node_117 -> node_142 [style=dashed];
  node_125 -> node_142 [style=dashed];
  node_133 -> node_142 [style=dashed];
  node_141 -> node_142 [style=dashed];
  node_143 [shape=box, label="Layer0_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_142 -> node_143;
  node_144 [shape=parallelogram, label="Layer0_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_10 -> node_144;
  node_143 -> node_144;
  
  // ===== Layer 1 =====
  node_145 [shape=box, label="Layer1_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_144 -> node_145;
  node_146 [shape=box, label="Layer1_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_147 [shape=box, label="Layer1_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_145 -> node_146;
  node_145 -> node_147;
  node_148 [shape=box, label="Layer1_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_149 [shape=box, label="Layer1_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_146 -> node_148;
  node_147 -> node_149;
  node_150 [shape=box, label="Layer1_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_151 [shape=box, label="Layer1_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_148 -> node_150;
  node_149 -> node_151;
  node_152 [shape=ellipse, label="Layer1_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_150 -> node_152 [style=dashed];
  node_151 -> node_152 [style=dashed];
  node_153 [shape=parallelogram, label="Layer1_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_144 -> node_153;
  node_152 -> node_153;
  node_154 [shape=box, label="Layer1_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_153 -> node_154;
  node_155 [shape=parallelogram, label="Layer1_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_154 -> node_155 [style=dashed];
  node_156 [shape=ellipse, label="Layer1_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_155 -> node_156 [style=dashed];
  node_157 [shape=parallelogram, label="Layer1_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_158 [shape=box, label="Layer1_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_159 [shape=box, label="Layer1_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_160 [shape=box, label="Layer1_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_161 [shape=box, label="Layer1_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_162 [shape=box, label="Layer1_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_163 [shape=box, label="Layer1_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_164 [shape=ellipse, label="Layer1_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_157;
  node_157 -> node_158;
  node_157 -> node_159;
  node_158 -> node_160;
  node_159 -> node_161;
  node_160 -> node_162;
  node_161 -> node_163;
  node_162 -> node_164 [style=dashed];
  node_163 -> node_164 [style=dashed];
  node_165 [shape=parallelogram, label="Layer1_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_166 [shape=box, label="Layer1_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_167 [shape=box, label="Layer1_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_168 [shape=box, label="Layer1_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_169 [shape=box, label="Layer1_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_170 [shape=box, label="Layer1_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_171 [shape=box, label="Layer1_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_172 [shape=ellipse, label="Layer1_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_165;
  node_165 -> node_166;
  node_165 -> node_167;
  node_166 -> node_168;
  node_167 -> node_169;
  node_168 -> node_170;
  node_169 -> node_171;
  node_170 -> node_172 [style=dashed];
  node_171 -> node_172 [style=dashed];
  node_173 [shape=parallelogram, label="Layer1_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_174 [shape=box, label="Layer1_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_175 [shape=box, label="Layer1_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_176 [shape=box, label="Layer1_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_177 [shape=box, label="Layer1_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_178 [shape=box, label="Layer1_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_179 [shape=box, label="Layer1_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_180 [shape=ellipse, label="Layer1_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_173;
  node_173 -> node_174;
  node_173 -> node_175;
  node_174 -> node_176;
  node_175 -> node_177;
  node_176 -> node_178;
  node_177 -> node_179;
  node_178 -> node_180 [style=dashed];
  node_179 -> node_180 [style=dashed];
  node_181 [shape=parallelogram, label="Layer1_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_182 [shape=box, label="Layer1_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_183 [shape=box, label="Layer1_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_184 [shape=box, label="Layer1_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_185 [shape=box, label="Layer1_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_186 [shape=box, label="Layer1_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_187 [shape=box, label="Layer1_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_188 [shape=ellipse, label="Layer1_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_181;
  node_181 -> node_182;
  node_181 -> node_183;
  node_182 -> node_184;
  node_183 -> node_185;
  node_184 -> node_186;
  node_185 -> node_187;
  node_186 -> node_188 [style=dashed];
  node_187 -> node_188 [style=dashed];
  node_189 [shape=parallelogram, label="Layer1_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_190 [shape=box, label="Layer1_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_191 [shape=box, label="Layer1_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_192 [shape=box, label="Layer1_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_193 [shape=box, label="Layer1_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_194 [shape=box, label="Layer1_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_195 [shape=box, label="Layer1_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_196 [shape=ellipse, label="Layer1_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_189;
  node_189 -> node_190;
  node_189 -> node_191;
  node_190 -> node_192;
  node_191 -> node_193;
  node_192 -> node_194;
  node_193 -> node_195;
  node_194 -> node_196 [style=dashed];
  node_195 -> node_196 [style=dashed];
  node_197 [shape=parallelogram, label="Layer1_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_198 [shape=box, label="Layer1_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_199 [shape=box, label="Layer1_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_200 [shape=box, label="Layer1_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_201 [shape=box, label="Layer1_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_202 [shape=box, label="Layer1_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_203 [shape=box, label="Layer1_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_204 [shape=ellipse, label="Layer1_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_197;
  node_197 -> node_198;
  node_197 -> node_199;
  node_198 -> node_200;
  node_199 -> node_201;
  node_200 -> node_202;
  node_201 -> node_203;
  node_202 -> node_204 [style=dashed];
  node_203 -> node_204 [style=dashed];
  node_205 [shape=parallelogram, label="Layer1_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_206 [shape=box, label="Layer1_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_207 [shape=box, label="Layer1_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_208 [shape=box, label="Layer1_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_209 [shape=box, label="Layer1_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_210 [shape=box, label="Layer1_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_211 [shape=box, label="Layer1_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_212 [shape=ellipse, label="Layer1_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_205;
  node_205 -> node_206;
  node_205 -> node_207;
  node_206 -> node_208;
  node_207 -> node_209;
  node_208 -> node_210;
  node_209 -> node_211;
  node_210 -> node_212 [style=dashed];
  node_211 -> node_212 [style=dashed];
  node_213 [shape=parallelogram, label="Layer1_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_214 [shape=box, label="Layer1_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_215 [shape=box, label="Layer1_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_216 [shape=box, label="Layer1_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_217 [shape=box, label="Layer1_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_218 [shape=box, label="Layer1_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_219 [shape=box, label="Layer1_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_220 [shape=ellipse, label="Layer1_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_213;
  node_213 -> node_214;
  node_213 -> node_215;
  node_214 -> node_216;
  node_215 -> node_217;
  node_216 -> node_218;
  node_217 -> node_219;
  node_218 -> node_220 [style=dashed];
  node_219 -> node_220 [style=dashed];
  node_221 [shape=parallelogram, label="Layer1_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_222 [shape=box, label="Layer1_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_223 [shape=box, label="Layer1_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_224 [shape=box, label="Layer1_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_225 [shape=box, label="Layer1_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_226 [shape=box, label="Layer1_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_227 [shape=box, label="Layer1_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_228 [shape=ellipse, label="Layer1_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_221;
  node_221 -> node_222;
  node_221 -> node_223;
  node_222 -> node_224;
  node_223 -> node_225;
  node_224 -> node_226;
  node_225 -> node_227;
  node_226 -> node_228 [style=dashed];
  node_227 -> node_228 [style=dashed];
  node_229 [shape=parallelogram, label="Layer1_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_230 [shape=box, label="Layer1_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_231 [shape=box, label="Layer1_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_232 [shape=box, label="Layer1_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_233 [shape=box, label="Layer1_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_234 [shape=box, label="Layer1_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_235 [shape=box, label="Layer1_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_236 [shape=ellipse, label="Layer1_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_229;
  node_229 -> node_230;
  node_229 -> node_231;
  node_230 -> node_232;
  node_231 -> node_233;
  node_232 -> node_234;
  node_233 -> node_235;
  node_234 -> node_236 [style=dashed];
  node_235 -> node_236 [style=dashed];
  node_237 [shape=parallelogram, label="Layer1_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_238 [shape=box, label="Layer1_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_239 [shape=box, label="Layer1_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_240 [shape=box, label="Layer1_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_241 [shape=box, label="Layer1_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_242 [shape=box, label="Layer1_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_243 [shape=box, label="Layer1_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_244 [shape=ellipse, label="Layer1_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_237;
  node_237 -> node_238;
  node_237 -> node_239;
  node_238 -> node_240;
  node_239 -> node_241;
  node_240 -> node_242;
  node_241 -> node_243;
  node_242 -> node_244 [style=dashed];
  node_243 -> node_244 [style=dashed];
  node_245 [shape=parallelogram, label="Layer1_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_246 [shape=box, label="Layer1_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_247 [shape=box, label="Layer1_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_248 [shape=box, label="Layer1_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_249 [shape=box, label="Layer1_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_250 [shape=box, label="Layer1_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_251 [shape=box, label="Layer1_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_252 [shape=ellipse, label="Layer1_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_245;
  node_245 -> node_246;
  node_245 -> node_247;
  node_246 -> node_248;
  node_247 -> node_249;
  node_248 -> node_250;
  node_249 -> node_251;
  node_250 -> node_252 [style=dashed];
  node_251 -> node_252 [style=dashed];
  node_253 [shape=parallelogram, label="Layer1_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_254 [shape=box, label="Layer1_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_255 [shape=box, label="Layer1_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_256 [shape=box, label="Layer1_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_257 [shape=box, label="Layer1_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_258 [shape=box, label="Layer1_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_259 [shape=box, label="Layer1_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_260 [shape=ellipse, label="Layer1_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_253;
  node_253 -> node_254;
  node_253 -> node_255;
  node_254 -> node_256;
  node_255 -> node_257;
  node_256 -> node_258;
  node_257 -> node_259;
  node_258 -> node_260 [style=dashed];
  node_259 -> node_260 [style=dashed];
  node_261 [shape=parallelogram, label="Layer1_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_262 [shape=box, label="Layer1_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_263 [shape=box, label="Layer1_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_264 [shape=box, label="Layer1_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_265 [shape=box, label="Layer1_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_266 [shape=box, label="Layer1_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_267 [shape=box, label="Layer1_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_268 [shape=ellipse, label="Layer1_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_261;
  node_261 -> node_262;
  node_261 -> node_263;
  node_262 -> node_264;
  node_263 -> node_265;
  node_264 -> node_266;
  node_265 -> node_267;
  node_266 -> node_268 [style=dashed];
  node_267 -> node_268 [style=dashed];
  node_269 [shape=parallelogram, label="Layer1_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_270 [shape=box, label="Layer1_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_271 [shape=box, label="Layer1_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_272 [shape=box, label="Layer1_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_273 [shape=box, label="Layer1_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_274 [shape=box, label="Layer1_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_275 [shape=box, label="Layer1_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_276 [shape=ellipse, label="Layer1_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_269;
  node_269 -> node_270;
  node_269 -> node_271;
  node_270 -> node_272;
  node_271 -> node_273;
  node_272 -> node_274;
  node_273 -> node_275;
  node_274 -> node_276 [style=dashed];
  node_275 -> node_276 [style=dashed];
  node_277 [shape=parallelogram, label="Layer1_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_278 [shape=box, label="Layer1_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_279 [shape=box, label="Layer1_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_280 [shape=box, label="Layer1_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_281 [shape=box, label="Layer1_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_282 [shape=box, label="Layer1_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_283 [shape=box, label="Layer1_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_284 [shape=ellipse, label="Layer1_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_156 -> node_277;
  node_277 -> node_278;
  node_277 -> node_279;
  node_278 -> node_280;
  node_279 -> node_281;
  node_280 -> node_282;
  node_281 -> node_283;
  node_282 -> node_284 [style=dashed];
  node_283 -> node_284 [style=dashed];
  node_285 [shape=ellipse, label="Layer1_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_164 -> node_285 [style=dashed];
  node_172 -> node_285 [style=dashed];
  node_180 -> node_285 [style=dashed];
  node_188 -> node_285 [style=dashed];
  node_196 -> node_285 [style=dashed];
  node_204 -> node_285 [style=dashed];
  node_212 -> node_285 [style=dashed];
  node_220 -> node_285 [style=dashed];
  node_228 -> node_285 [style=dashed];
  node_236 -> node_285 [style=dashed];
  node_244 -> node_285 [style=dashed];
  node_252 -> node_285 [style=dashed];
  node_260 -> node_285 [style=dashed];
  node_268 -> node_285 [style=dashed];
  node_276 -> node_285 [style=dashed];
  node_284 -> node_285 [style=dashed];
  node_286 [shape=box, label="Layer1_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_285 -> node_286;
  node_287 [shape=parallelogram, label="Layer1_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_153 -> node_287;
  node_286 -> node_287;
  
  // ===== Layer 2 =====
  node_288 [shape=box, label="Layer2_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_287 -> node_288;
  node_289 [shape=box, label="Layer2_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_290 [shape=box, label="Layer2_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_288 -> node_289;
  node_288 -> node_290;
  node_291 [shape=box, label="Layer2_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_292 [shape=box, label="Layer2_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_289 -> node_291;
  node_290 -> node_292;
  node_293 [shape=box, label="Layer2_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_294 [shape=box, label="Layer2_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_291 -> node_293;
  node_292 -> node_294;
  node_295 [shape=ellipse, label="Layer2_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_293 -> node_295 [style=dashed];
  node_294 -> node_295 [style=dashed];
  node_296 [shape=parallelogram, label="Layer2_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_287 -> node_296;
  node_295 -> node_296;
  node_297 [shape=box, label="Layer2_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_296 -> node_297;
  node_298 [shape=parallelogram, label="Layer2_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_297 -> node_298 [style=dashed];
  node_299 [shape=ellipse, label="Layer2_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_298 -> node_299 [style=dashed];
  node_300 [shape=parallelogram, label="Layer2_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_301 [shape=box, label="Layer2_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_302 [shape=box, label="Layer2_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_303 [shape=box, label="Layer2_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_304 [shape=box, label="Layer2_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_305 [shape=box, label="Layer2_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_306 [shape=box, label="Layer2_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_307 [shape=ellipse, label="Layer2_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_300;
  node_300 -> node_301;
  node_300 -> node_302;
  node_301 -> node_303;
  node_302 -> node_304;
  node_303 -> node_305;
  node_304 -> node_306;
  node_305 -> node_307 [style=dashed];
  node_306 -> node_307 [style=dashed];
  node_308 [shape=parallelogram, label="Layer2_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_309 [shape=box, label="Layer2_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_310 [shape=box, label="Layer2_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_311 [shape=box, label="Layer2_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_312 [shape=box, label="Layer2_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_313 [shape=box, label="Layer2_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_314 [shape=box, label="Layer2_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_315 [shape=ellipse, label="Layer2_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_308;
  node_308 -> node_309;
  node_308 -> node_310;
  node_309 -> node_311;
  node_310 -> node_312;
  node_311 -> node_313;
  node_312 -> node_314;
  node_313 -> node_315 [style=dashed];
  node_314 -> node_315 [style=dashed];
  node_316 [shape=parallelogram, label="Layer2_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_317 [shape=box, label="Layer2_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_318 [shape=box, label="Layer2_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_319 [shape=box, label="Layer2_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_320 [shape=box, label="Layer2_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_321 [shape=box, label="Layer2_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_322 [shape=box, label="Layer2_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_323 [shape=ellipse, label="Layer2_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_316;
  node_316 -> node_317;
  node_316 -> node_318;
  node_317 -> node_319;
  node_318 -> node_320;
  node_319 -> node_321;
  node_320 -> node_322;
  node_321 -> node_323 [style=dashed];
  node_322 -> node_323 [style=dashed];
  node_324 [shape=parallelogram, label="Layer2_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_325 [shape=box, label="Layer2_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_326 [shape=box, label="Layer2_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_327 [shape=box, label="Layer2_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_328 [shape=box, label="Layer2_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_329 [shape=box, label="Layer2_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_330 [shape=box, label="Layer2_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_331 [shape=ellipse, label="Layer2_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_324;
  node_324 -> node_325;
  node_324 -> node_326;
  node_325 -> node_327;
  node_326 -> node_328;
  node_327 -> node_329;
  node_328 -> node_330;
  node_329 -> node_331 [style=dashed];
  node_330 -> node_331 [style=dashed];
  node_332 [shape=parallelogram, label="Layer2_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_333 [shape=box, label="Layer2_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_334 [shape=box, label="Layer2_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_335 [shape=box, label="Layer2_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_336 [shape=box, label="Layer2_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_337 [shape=box, label="Layer2_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_338 [shape=box, label="Layer2_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_339 [shape=ellipse, label="Layer2_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_332;
  node_332 -> node_333;
  node_332 -> node_334;
  node_333 -> node_335;
  node_334 -> node_336;
  node_335 -> node_337;
  node_336 -> node_338;
  node_337 -> node_339 [style=dashed];
  node_338 -> node_339 [style=dashed];
  node_340 [shape=parallelogram, label="Layer2_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_341 [shape=box, label="Layer2_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_342 [shape=box, label="Layer2_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_343 [shape=box, label="Layer2_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_344 [shape=box, label="Layer2_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_345 [shape=box, label="Layer2_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_346 [shape=box, label="Layer2_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_347 [shape=ellipse, label="Layer2_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_340;
  node_340 -> node_341;
  node_340 -> node_342;
  node_341 -> node_343;
  node_342 -> node_344;
  node_343 -> node_345;
  node_344 -> node_346;
  node_345 -> node_347 [style=dashed];
  node_346 -> node_347 [style=dashed];
  node_348 [shape=parallelogram, label="Layer2_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_349 [shape=box, label="Layer2_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_350 [shape=box, label="Layer2_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_351 [shape=box, label="Layer2_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_352 [shape=box, label="Layer2_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_353 [shape=box, label="Layer2_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_354 [shape=box, label="Layer2_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_355 [shape=ellipse, label="Layer2_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_348;
  node_348 -> node_349;
  node_348 -> node_350;
  node_349 -> node_351;
  node_350 -> node_352;
  node_351 -> node_353;
  node_352 -> node_354;
  node_353 -> node_355 [style=dashed];
  node_354 -> node_355 [style=dashed];
  node_356 [shape=parallelogram, label="Layer2_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_357 [shape=box, label="Layer2_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_358 [shape=box, label="Layer2_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_359 [shape=box, label="Layer2_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_360 [shape=box, label="Layer2_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_361 [shape=box, label="Layer2_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_362 [shape=box, label="Layer2_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_363 [shape=ellipse, label="Layer2_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_356;
  node_356 -> node_357;
  node_356 -> node_358;
  node_357 -> node_359;
  node_358 -> node_360;
  node_359 -> node_361;
  node_360 -> node_362;
  node_361 -> node_363 [style=dashed];
  node_362 -> node_363 [style=dashed];
  node_364 [shape=parallelogram, label="Layer2_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_365 [shape=box, label="Layer2_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_366 [shape=box, label="Layer2_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_367 [shape=box, label="Layer2_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_368 [shape=box, label="Layer2_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_369 [shape=box, label="Layer2_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_370 [shape=box, label="Layer2_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_371 [shape=ellipse, label="Layer2_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_364;
  node_364 -> node_365;
  node_364 -> node_366;
  node_365 -> node_367;
  node_366 -> node_368;
  node_367 -> node_369;
  node_368 -> node_370;
  node_369 -> node_371 [style=dashed];
  node_370 -> node_371 [style=dashed];
  node_372 [shape=parallelogram, label="Layer2_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_373 [shape=box, label="Layer2_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_374 [shape=box, label="Layer2_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_375 [shape=box, label="Layer2_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_376 [shape=box, label="Layer2_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_377 [shape=box, label="Layer2_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_378 [shape=box, label="Layer2_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_379 [shape=ellipse, label="Layer2_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_372;
  node_372 -> node_373;
  node_372 -> node_374;
  node_373 -> node_375;
  node_374 -> node_376;
  node_375 -> node_377;
  node_376 -> node_378;
  node_377 -> node_379 [style=dashed];
  node_378 -> node_379 [style=dashed];
  node_380 [shape=parallelogram, label="Layer2_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_381 [shape=box, label="Layer2_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_382 [shape=box, label="Layer2_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_383 [shape=box, label="Layer2_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_384 [shape=box, label="Layer2_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_385 [shape=box, label="Layer2_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_386 [shape=box, label="Layer2_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_387 [shape=ellipse, label="Layer2_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_380;
  node_380 -> node_381;
  node_380 -> node_382;
  node_381 -> node_383;
  node_382 -> node_384;
  node_383 -> node_385;
  node_384 -> node_386;
  node_385 -> node_387 [style=dashed];
  node_386 -> node_387 [style=dashed];
  node_388 [shape=parallelogram, label="Layer2_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_389 [shape=box, label="Layer2_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_390 [shape=box, label="Layer2_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_391 [shape=box, label="Layer2_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_392 [shape=box, label="Layer2_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_393 [shape=box, label="Layer2_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_394 [shape=box, label="Layer2_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_395 [shape=ellipse, label="Layer2_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_388;
  node_388 -> node_389;
  node_388 -> node_390;
  node_389 -> node_391;
  node_390 -> node_392;
  node_391 -> node_393;
  node_392 -> node_394;
  node_393 -> node_395 [style=dashed];
  node_394 -> node_395 [style=dashed];
  node_396 [shape=parallelogram, label="Layer2_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_397 [shape=box, label="Layer2_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_398 [shape=box, label="Layer2_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_399 [shape=box, label="Layer2_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_400 [shape=box, label="Layer2_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_401 [shape=box, label="Layer2_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_402 [shape=box, label="Layer2_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_403 [shape=ellipse, label="Layer2_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_396;
  node_396 -> node_397;
  node_396 -> node_398;
  node_397 -> node_399;
  node_398 -> node_400;
  node_399 -> node_401;
  node_400 -> node_402;
  node_401 -> node_403 [style=dashed];
  node_402 -> node_403 [style=dashed];
  node_404 [shape=parallelogram, label="Layer2_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_405 [shape=box, label="Layer2_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_406 [shape=box, label="Layer2_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_407 [shape=box, label="Layer2_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_408 [shape=box, label="Layer2_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_409 [shape=box, label="Layer2_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_410 [shape=box, label="Layer2_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_411 [shape=ellipse, label="Layer2_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_404;
  node_404 -> node_405;
  node_404 -> node_406;
  node_405 -> node_407;
  node_406 -> node_408;
  node_407 -> node_409;
  node_408 -> node_410;
  node_409 -> node_411 [style=dashed];
  node_410 -> node_411 [style=dashed];
  node_412 [shape=parallelogram, label="Layer2_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_413 [shape=box, label="Layer2_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_414 [shape=box, label="Layer2_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_415 [shape=box, label="Layer2_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_416 [shape=box, label="Layer2_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_417 [shape=box, label="Layer2_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_418 [shape=box, label="Layer2_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_419 [shape=ellipse, label="Layer2_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_412;
  node_412 -> node_413;
  node_412 -> node_414;
  node_413 -> node_415;
  node_414 -> node_416;
  node_415 -> node_417;
  node_416 -> node_418;
  node_417 -> node_419 [style=dashed];
  node_418 -> node_419 [style=dashed];
  node_420 [shape=parallelogram, label="Layer2_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_421 [shape=box, label="Layer2_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_422 [shape=box, label="Layer2_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_423 [shape=box, label="Layer2_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_424 [shape=box, label="Layer2_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_425 [shape=box, label="Layer2_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_426 [shape=box, label="Layer2_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_427 [shape=ellipse, label="Layer2_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_299 -> node_420;
  node_420 -> node_421;
  node_420 -> node_422;
  node_421 -> node_423;
  node_422 -> node_424;
  node_423 -> node_425;
  node_424 -> node_426;
  node_425 -> node_427 [style=dashed];
  node_426 -> node_427 [style=dashed];
  node_428 [shape=ellipse, label="Layer2_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_307 -> node_428 [style=dashed];
  node_315 -> node_428 [style=dashed];
  node_323 -> node_428 [style=dashed];
  node_331 -> node_428 [style=dashed];
  node_339 -> node_428 [style=dashed];
  node_347 -> node_428 [style=dashed];
  node_355 -> node_428 [style=dashed];
  node_363 -> node_428 [style=dashed];
  node_371 -> node_428 [style=dashed];
  node_379 -> node_428 [style=dashed];
  node_387 -> node_428 [style=dashed];
  node_395 -> node_428 [style=dashed];
  node_403 -> node_428 [style=dashed];
  node_411 -> node_428 [style=dashed];
  node_419 -> node_428 [style=dashed];
  node_427 -> node_428 [style=dashed];
  node_429 [shape=box, label="Layer2_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_428 -> node_429;
  node_430 [shape=parallelogram, label="Layer2_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_296 -> node_430;
  node_429 -> node_430;
  
  // ===== Layer 3 =====
  node_431 [shape=box, label="Layer3_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_430 -> node_431;
  node_432 [shape=box, label="Layer3_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_433 [shape=box, label="Layer3_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_431 -> node_432;
  node_431 -> node_433;
  node_434 [shape=box, label="Layer3_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_435 [shape=box, label="Layer3_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_432 -> node_434;
  node_433 -> node_435;
  node_436 [shape=box, label="Layer3_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_437 [shape=box, label="Layer3_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_434 -> node_436;
  node_435 -> node_437;
  node_438 [shape=ellipse, label="Layer3_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_436 -> node_438 [style=dashed];
  node_437 -> node_438 [style=dashed];
  node_439 [shape=parallelogram, label="Layer3_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_430 -> node_439;
  node_438 -> node_439;
  node_440 [shape=box, label="Layer3_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_439 -> node_440;
  node_441 [shape=parallelogram, label="Layer3_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_440 -> node_441 [style=dashed];
  node_442 [shape=ellipse, label="Layer3_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_441 -> node_442 [style=dashed];
  node_443 [shape=parallelogram, label="Layer3_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_444 [shape=box, label="Layer3_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_445 [shape=box, label="Layer3_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_446 [shape=box, label="Layer3_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_447 [shape=box, label="Layer3_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_448 [shape=box, label="Layer3_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_449 [shape=box, label="Layer3_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_450 [shape=ellipse, label="Layer3_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_443;
  node_443 -> node_444;
  node_443 -> node_445;
  node_444 -> node_446;
  node_445 -> node_447;
  node_446 -> node_448;
  node_447 -> node_449;
  node_448 -> node_450 [style=dashed];
  node_449 -> node_450 [style=dashed];
  node_451 [shape=parallelogram, label="Layer3_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_452 [shape=box, label="Layer3_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_453 [shape=box, label="Layer3_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_454 [shape=box, label="Layer3_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_455 [shape=box, label="Layer3_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_456 [shape=box, label="Layer3_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_457 [shape=box, label="Layer3_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_458 [shape=ellipse, label="Layer3_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_451;
  node_451 -> node_452;
  node_451 -> node_453;
  node_452 -> node_454;
  node_453 -> node_455;
  node_454 -> node_456;
  node_455 -> node_457;
  node_456 -> node_458 [style=dashed];
  node_457 -> node_458 [style=dashed];
  node_459 [shape=parallelogram, label="Layer3_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_460 [shape=box, label="Layer3_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_461 [shape=box, label="Layer3_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_462 [shape=box, label="Layer3_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_463 [shape=box, label="Layer3_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_464 [shape=box, label="Layer3_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_465 [shape=box, label="Layer3_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_466 [shape=ellipse, label="Layer3_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_459;
  node_459 -> node_460;
  node_459 -> node_461;
  node_460 -> node_462;
  node_461 -> node_463;
  node_462 -> node_464;
  node_463 -> node_465;
  node_464 -> node_466 [style=dashed];
  node_465 -> node_466 [style=dashed];
  node_467 [shape=parallelogram, label="Layer3_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_468 [shape=box, label="Layer3_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_469 [shape=box, label="Layer3_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_470 [shape=box, label="Layer3_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_471 [shape=box, label="Layer3_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_472 [shape=box, label="Layer3_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_473 [shape=box, label="Layer3_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_474 [shape=ellipse, label="Layer3_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_467;
  node_467 -> node_468;
  node_467 -> node_469;
  node_468 -> node_470;
  node_469 -> node_471;
  node_470 -> node_472;
  node_471 -> node_473;
  node_472 -> node_474 [style=dashed];
  node_473 -> node_474 [style=dashed];
  node_475 [shape=parallelogram, label="Layer3_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_476 [shape=box, label="Layer3_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_477 [shape=box, label="Layer3_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_478 [shape=box, label="Layer3_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_479 [shape=box, label="Layer3_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_480 [shape=box, label="Layer3_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_481 [shape=box, label="Layer3_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_482 [shape=ellipse, label="Layer3_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_475;
  node_475 -> node_476;
  node_475 -> node_477;
  node_476 -> node_478;
  node_477 -> node_479;
  node_478 -> node_480;
  node_479 -> node_481;
  node_480 -> node_482 [style=dashed];
  node_481 -> node_482 [style=dashed];
  node_483 [shape=parallelogram, label="Layer3_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_484 [shape=box, label="Layer3_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_485 [shape=box, label="Layer3_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_486 [shape=box, label="Layer3_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_487 [shape=box, label="Layer3_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_488 [shape=box, label="Layer3_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_489 [shape=box, label="Layer3_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_490 [shape=ellipse, label="Layer3_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_483;
  node_483 -> node_484;
  node_483 -> node_485;
  node_484 -> node_486;
  node_485 -> node_487;
  node_486 -> node_488;
  node_487 -> node_489;
  node_488 -> node_490 [style=dashed];
  node_489 -> node_490 [style=dashed];
  node_491 [shape=parallelogram, label="Layer3_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_492 [shape=box, label="Layer3_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_493 [shape=box, label="Layer3_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_494 [shape=box, label="Layer3_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_495 [shape=box, label="Layer3_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_496 [shape=box, label="Layer3_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_497 [shape=box, label="Layer3_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_498 [shape=ellipse, label="Layer3_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_491;
  node_491 -> node_492;
  node_491 -> node_493;
  node_492 -> node_494;
  node_493 -> node_495;
  node_494 -> node_496;
  node_495 -> node_497;
  node_496 -> node_498 [style=dashed];
  node_497 -> node_498 [style=dashed];
  node_499 [shape=parallelogram, label="Layer3_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_500 [shape=box, label="Layer3_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_501 [shape=box, label="Layer3_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_502 [shape=box, label="Layer3_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_503 [shape=box, label="Layer3_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_504 [shape=box, label="Layer3_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_505 [shape=box, label="Layer3_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_506 [shape=ellipse, label="Layer3_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_499;
  node_499 -> node_500;
  node_499 -> node_501;
  node_500 -> node_502;
  node_501 -> node_503;
  node_502 -> node_504;
  node_503 -> node_505;
  node_504 -> node_506 [style=dashed];
  node_505 -> node_506 [style=dashed];
  node_507 [shape=parallelogram, label="Layer3_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_508 [shape=box, label="Layer3_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_509 [shape=box, label="Layer3_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_510 [shape=box, label="Layer3_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_511 [shape=box, label="Layer3_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_512 [shape=box, label="Layer3_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_513 [shape=box, label="Layer3_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_514 [shape=ellipse, label="Layer3_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_507;
  node_507 -> node_508;
  node_507 -> node_509;
  node_508 -> node_510;
  node_509 -> node_511;
  node_510 -> node_512;
  node_511 -> node_513;
  node_512 -> node_514 [style=dashed];
  node_513 -> node_514 [style=dashed];
  node_515 [shape=parallelogram, label="Layer3_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_516 [shape=box, label="Layer3_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_517 [shape=box, label="Layer3_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_518 [shape=box, label="Layer3_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_519 [shape=box, label="Layer3_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_520 [shape=box, label="Layer3_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_521 [shape=box, label="Layer3_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_522 [shape=ellipse, label="Layer3_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_515;
  node_515 -> node_516;
  node_515 -> node_517;
  node_516 -> node_518;
  node_517 -> node_519;
  node_518 -> node_520;
  node_519 -> node_521;
  node_520 -> node_522 [style=dashed];
  node_521 -> node_522 [style=dashed];
  node_523 [shape=parallelogram, label="Layer3_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_524 [shape=box, label="Layer3_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_525 [shape=box, label="Layer3_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_526 [shape=box, label="Layer3_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_527 [shape=box, label="Layer3_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_528 [shape=box, label="Layer3_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_529 [shape=box, label="Layer3_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_530 [shape=ellipse, label="Layer3_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_523;
  node_523 -> node_524;
  node_523 -> node_525;
  node_524 -> node_526;
  node_525 -> node_527;
  node_526 -> node_528;
  node_527 -> node_529;
  node_528 -> node_530 [style=dashed];
  node_529 -> node_530 [style=dashed];
  node_531 [shape=parallelogram, label="Layer3_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_532 [shape=box, label="Layer3_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_533 [shape=box, label="Layer3_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_534 [shape=box, label="Layer3_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_535 [shape=box, label="Layer3_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_536 [shape=box, label="Layer3_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_537 [shape=box, label="Layer3_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_538 [shape=ellipse, label="Layer3_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_531;
  node_531 -> node_532;
  node_531 -> node_533;
  node_532 -> node_534;
  node_533 -> node_535;
  node_534 -> node_536;
  node_535 -> node_537;
  node_536 -> node_538 [style=dashed];
  node_537 -> node_538 [style=dashed];
  node_539 [shape=parallelogram, label="Layer3_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_540 [shape=box, label="Layer3_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_541 [shape=box, label="Layer3_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_542 [shape=box, label="Layer3_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_543 [shape=box, label="Layer3_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_544 [shape=box, label="Layer3_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_545 [shape=box, label="Layer3_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_546 [shape=ellipse, label="Layer3_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_539;
  node_539 -> node_540;
  node_539 -> node_541;
  node_540 -> node_542;
  node_541 -> node_543;
  node_542 -> node_544;
  node_543 -> node_545;
  node_544 -> node_546 [style=dashed];
  node_545 -> node_546 [style=dashed];
  node_547 [shape=parallelogram, label="Layer3_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_548 [shape=box, label="Layer3_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_549 [shape=box, label="Layer3_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_550 [shape=box, label="Layer3_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_551 [shape=box, label="Layer3_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_552 [shape=box, label="Layer3_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_553 [shape=box, label="Layer3_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_554 [shape=ellipse, label="Layer3_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_547;
  node_547 -> node_548;
  node_547 -> node_549;
  node_548 -> node_550;
  node_549 -> node_551;
  node_550 -> node_552;
  node_551 -> node_553;
  node_552 -> node_554 [style=dashed];
  node_553 -> node_554 [style=dashed];
  node_555 [shape=parallelogram, label="Layer3_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_556 [shape=box, label="Layer3_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_557 [shape=box, label="Layer3_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_558 [shape=box, label="Layer3_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_559 [shape=box, label="Layer3_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_560 [shape=box, label="Layer3_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_561 [shape=box, label="Layer3_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_562 [shape=ellipse, label="Layer3_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_555;
  node_555 -> node_556;
  node_555 -> node_557;
  node_556 -> node_558;
  node_557 -> node_559;
  node_558 -> node_560;
  node_559 -> node_561;
  node_560 -> node_562 [style=dashed];
  node_561 -> node_562 [style=dashed];
  node_563 [shape=parallelogram, label="Layer3_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_564 [shape=box, label="Layer3_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_565 [shape=box, label="Layer3_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_566 [shape=box, label="Layer3_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_567 [shape=box, label="Layer3_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_568 [shape=box, label="Layer3_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_569 [shape=box, label="Layer3_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_570 [shape=ellipse, label="Layer3_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_442 -> node_563;
  node_563 -> node_564;
  node_563 -> node_565;
  node_564 -> node_566;
  node_565 -> node_567;
  node_566 -> node_568;
  node_567 -> node_569;
  node_568 -> node_570 [style=dashed];
  node_569 -> node_570 [style=dashed];
  node_571 [shape=ellipse, label="Layer3_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_450 -> node_571 [style=dashed];
  node_458 -> node_571 [style=dashed];
  node_466 -> node_571 [style=dashed];
  node_474 -> node_571 [style=dashed];
  node_482 -> node_571 [style=dashed];
  node_490 -> node_571 [style=dashed];
  node_498 -> node_571 [style=dashed];
  node_506 -> node_571 [style=dashed];
  node_514 -> node_571 [style=dashed];
  node_522 -> node_571 [style=dashed];
  node_530 -> node_571 [style=dashed];
  node_538 -> node_571 [style=dashed];
  node_546 -> node_571 [style=dashed];
  node_554 -> node_571 [style=dashed];
  node_562 -> node_571 [style=dashed];
  node_570 -> node_571 [style=dashed];
  node_572 [shape=box, label="Layer3_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_571 -> node_572;
  node_573 [shape=parallelogram, label="Layer3_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_439 -> node_573;
  node_572 -> node_573;
  
  // ===== Layer 4 =====
  node_574 [shape=box, label="Layer4_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_573 -> node_574;
  node_575 [shape=box, label="Layer4_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_576 [shape=box, label="Layer4_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_574 -> node_575;
  node_574 -> node_576;
  node_577 [shape=box, label="Layer4_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_578 [shape=box, label="Layer4_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_575 -> node_577;
  node_576 -> node_578;
  node_579 [shape=box, label="Layer4_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_580 [shape=box, label="Layer4_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_577 -> node_579;
  node_578 -> node_580;
  node_581 [shape=ellipse, label="Layer4_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_579 -> node_581 [style=dashed];
  node_580 -> node_581 [style=dashed];
  node_582 [shape=parallelogram, label="Layer4_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_573 -> node_582;
  node_581 -> node_582;
  node_583 [shape=box, label="Layer4_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_582 -> node_583;
  node_584 [shape=parallelogram, label="Layer4_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_583 -> node_584 [style=dashed];
  node_585 [shape=ellipse, label="Layer4_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_584 -> node_585 [style=dashed];
  node_586 [shape=parallelogram, label="Layer4_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_587 [shape=box, label="Layer4_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_588 [shape=box, label="Layer4_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_589 [shape=box, label="Layer4_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_590 [shape=box, label="Layer4_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_591 [shape=box, label="Layer4_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_592 [shape=box, label="Layer4_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_593 [shape=ellipse, label="Layer4_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_586;
  node_586 -> node_587;
  node_586 -> node_588;
  node_587 -> node_589;
  node_588 -> node_590;
  node_589 -> node_591;
  node_590 -> node_592;
  node_591 -> node_593 [style=dashed];
  node_592 -> node_593 [style=dashed];
  node_594 [shape=parallelogram, label="Layer4_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_595 [shape=box, label="Layer4_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_596 [shape=box, label="Layer4_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_597 [shape=box, label="Layer4_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_598 [shape=box, label="Layer4_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_599 [shape=box, label="Layer4_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_600 [shape=box, label="Layer4_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_601 [shape=ellipse, label="Layer4_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_594;
  node_594 -> node_595;
  node_594 -> node_596;
  node_595 -> node_597;
  node_596 -> node_598;
  node_597 -> node_599;
  node_598 -> node_600;
  node_599 -> node_601 [style=dashed];
  node_600 -> node_601 [style=dashed];
  node_602 [shape=parallelogram, label="Layer4_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_603 [shape=box, label="Layer4_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_604 [shape=box, label="Layer4_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_605 [shape=box, label="Layer4_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_606 [shape=box, label="Layer4_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_607 [shape=box, label="Layer4_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_608 [shape=box, label="Layer4_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_609 [shape=ellipse, label="Layer4_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_602;
  node_602 -> node_603;
  node_602 -> node_604;
  node_603 -> node_605;
  node_604 -> node_606;
  node_605 -> node_607;
  node_606 -> node_608;
  node_607 -> node_609 [style=dashed];
  node_608 -> node_609 [style=dashed];
  node_610 [shape=parallelogram, label="Layer4_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_611 [shape=box, label="Layer4_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_612 [shape=box, label="Layer4_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_613 [shape=box, label="Layer4_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_614 [shape=box, label="Layer4_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_615 [shape=box, label="Layer4_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_616 [shape=box, label="Layer4_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_617 [shape=ellipse, label="Layer4_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_610;
  node_610 -> node_611;
  node_610 -> node_612;
  node_611 -> node_613;
  node_612 -> node_614;
  node_613 -> node_615;
  node_614 -> node_616;
  node_615 -> node_617 [style=dashed];
  node_616 -> node_617 [style=dashed];
  node_618 [shape=parallelogram, label="Layer4_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_619 [shape=box, label="Layer4_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_620 [shape=box, label="Layer4_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_621 [shape=box, label="Layer4_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_622 [shape=box, label="Layer4_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_623 [shape=box, label="Layer4_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_624 [shape=box, label="Layer4_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_625 [shape=ellipse, label="Layer4_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_618;
  node_618 -> node_619;
  node_618 -> node_620;
  node_619 -> node_621;
  node_620 -> node_622;
  node_621 -> node_623;
  node_622 -> node_624;
  node_623 -> node_625 [style=dashed];
  node_624 -> node_625 [style=dashed];
  node_626 [shape=parallelogram, label="Layer4_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_627 [shape=box, label="Layer4_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_628 [shape=box, label="Layer4_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_629 [shape=box, label="Layer4_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_630 [shape=box, label="Layer4_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_631 [shape=box, label="Layer4_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_632 [shape=box, label="Layer4_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_633 [shape=ellipse, label="Layer4_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_626;
  node_626 -> node_627;
  node_626 -> node_628;
  node_627 -> node_629;
  node_628 -> node_630;
  node_629 -> node_631;
  node_630 -> node_632;
  node_631 -> node_633 [style=dashed];
  node_632 -> node_633 [style=dashed];
  node_634 [shape=parallelogram, label="Layer4_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_635 [shape=box, label="Layer4_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_636 [shape=box, label="Layer4_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_637 [shape=box, label="Layer4_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_638 [shape=box, label="Layer4_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_639 [shape=box, label="Layer4_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_640 [shape=box, label="Layer4_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_641 [shape=ellipse, label="Layer4_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_634;
  node_634 -> node_635;
  node_634 -> node_636;
  node_635 -> node_637;
  node_636 -> node_638;
  node_637 -> node_639;
  node_638 -> node_640;
  node_639 -> node_641 [style=dashed];
  node_640 -> node_641 [style=dashed];
  node_642 [shape=parallelogram, label="Layer4_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_643 [shape=box, label="Layer4_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_644 [shape=box, label="Layer4_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_645 [shape=box, label="Layer4_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_646 [shape=box, label="Layer4_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_647 [shape=box, label="Layer4_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_648 [shape=box, label="Layer4_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_649 [shape=ellipse, label="Layer4_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_642;
  node_642 -> node_643;
  node_642 -> node_644;
  node_643 -> node_645;
  node_644 -> node_646;
  node_645 -> node_647;
  node_646 -> node_648;
  node_647 -> node_649 [style=dashed];
  node_648 -> node_649 [style=dashed];
  node_650 [shape=parallelogram, label="Layer4_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_651 [shape=box, label="Layer4_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_652 [shape=box, label="Layer4_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_653 [shape=box, label="Layer4_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_654 [shape=box, label="Layer4_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_655 [shape=box, label="Layer4_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_656 [shape=box, label="Layer4_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_657 [shape=ellipse, label="Layer4_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_650;
  node_650 -> node_651;
  node_650 -> node_652;
  node_651 -> node_653;
  node_652 -> node_654;
  node_653 -> node_655;
  node_654 -> node_656;
  node_655 -> node_657 [style=dashed];
  node_656 -> node_657 [style=dashed];
  node_658 [shape=parallelogram, label="Layer4_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_659 [shape=box, label="Layer4_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_660 [shape=box, label="Layer4_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_661 [shape=box, label="Layer4_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_662 [shape=box, label="Layer4_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_663 [shape=box, label="Layer4_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_664 [shape=box, label="Layer4_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_665 [shape=ellipse, label="Layer4_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_658;
  node_658 -> node_659;
  node_658 -> node_660;
  node_659 -> node_661;
  node_660 -> node_662;
  node_661 -> node_663;
  node_662 -> node_664;
  node_663 -> node_665 [style=dashed];
  node_664 -> node_665 [style=dashed];
  node_666 [shape=parallelogram, label="Layer4_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_667 [shape=box, label="Layer4_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_668 [shape=box, label="Layer4_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_669 [shape=box, label="Layer4_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_670 [shape=box, label="Layer4_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_671 [shape=box, label="Layer4_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_672 [shape=box, label="Layer4_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_673 [shape=ellipse, label="Layer4_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_666;
  node_666 -> node_667;
  node_666 -> node_668;
  node_667 -> node_669;
  node_668 -> node_670;
  node_669 -> node_671;
  node_670 -> node_672;
  node_671 -> node_673 [style=dashed];
  node_672 -> node_673 [style=dashed];
  node_674 [shape=parallelogram, label="Layer4_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_675 [shape=box, label="Layer4_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_676 [shape=box, label="Layer4_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_677 [shape=box, label="Layer4_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_678 [shape=box, label="Layer4_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_679 [shape=box, label="Layer4_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_680 [shape=box, label="Layer4_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_681 [shape=ellipse, label="Layer4_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_674;
  node_674 -> node_675;
  node_674 -> node_676;
  node_675 -> node_677;
  node_676 -> node_678;
  node_677 -> node_679;
  node_678 -> node_680;
  node_679 -> node_681 [style=dashed];
  node_680 -> node_681 [style=dashed];
  node_682 [shape=parallelogram, label="Layer4_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_683 [shape=box, label="Layer4_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_684 [shape=box, label="Layer4_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_685 [shape=box, label="Layer4_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_686 [shape=box, label="Layer4_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_687 [shape=box, label="Layer4_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_688 [shape=box, label="Layer4_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_689 [shape=ellipse, label="Layer4_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_682;
  node_682 -> node_683;
  node_682 -> node_684;
  node_683 -> node_685;
  node_684 -> node_686;
  node_685 -> node_687;
  node_686 -> node_688;
  node_687 -> node_689 [style=dashed];
  node_688 -> node_689 [style=dashed];
  node_690 [shape=parallelogram, label="Layer4_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_691 [shape=box, label="Layer4_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_692 [shape=box, label="Layer4_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_693 [shape=box, label="Layer4_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_694 [shape=box, label="Layer4_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_695 [shape=box, label="Layer4_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_696 [shape=box, label="Layer4_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_697 [shape=ellipse, label="Layer4_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_690;
  node_690 -> node_691;
  node_690 -> node_692;
  node_691 -> node_693;
  node_692 -> node_694;
  node_693 -> node_695;
  node_694 -> node_696;
  node_695 -> node_697 [style=dashed];
  node_696 -> node_697 [style=dashed];
  node_698 [shape=parallelogram, label="Layer4_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_699 [shape=box, label="Layer4_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_700 [shape=box, label="Layer4_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_701 [shape=box, label="Layer4_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_702 [shape=box, label="Layer4_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_703 [shape=box, label="Layer4_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_704 [shape=box, label="Layer4_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_705 [shape=ellipse, label="Layer4_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_698;
  node_698 -> node_699;
  node_698 -> node_700;
  node_699 -> node_701;
  node_700 -> node_702;
  node_701 -> node_703;
  node_702 -> node_704;
  node_703 -> node_705 [style=dashed];
  node_704 -> node_705 [style=dashed];
  node_706 [shape=parallelogram, label="Layer4_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_707 [shape=box, label="Layer4_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_708 [shape=box, label="Layer4_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_709 [shape=box, label="Layer4_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_710 [shape=box, label="Layer4_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_711 [shape=box, label="Layer4_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_712 [shape=box, label="Layer4_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_713 [shape=ellipse, label="Layer4_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_585 -> node_706;
  node_706 -> node_707;
  node_706 -> node_708;
  node_707 -> node_709;
  node_708 -> node_710;
  node_709 -> node_711;
  node_710 -> node_712;
  node_711 -> node_713 [style=dashed];
  node_712 -> node_713 [style=dashed];
  node_714 [shape=ellipse, label="Layer4_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_593 -> node_714 [style=dashed];
  node_601 -> node_714 [style=dashed];
  node_609 -> node_714 [style=dashed];
  node_617 -> node_714 [style=dashed];
  node_625 -> node_714 [style=dashed];
  node_633 -> node_714 [style=dashed];
  node_641 -> node_714 [style=dashed];
  node_649 -> node_714 [style=dashed];
  node_657 -> node_714 [style=dashed];
  node_665 -> node_714 [style=dashed];
  node_673 -> node_714 [style=dashed];
  node_681 -> node_714 [style=dashed];
  node_689 -> node_714 [style=dashed];
  node_697 -> node_714 [style=dashed];
  node_705 -> node_714 [style=dashed];
  node_713 -> node_714 [style=dashed];
  node_715 [shape=box, label="Layer4_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_714 -> node_715;
  node_716 [shape=parallelogram, label="Layer4_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_582 -> node_716;
  node_715 -> node_716;
  
  // ===== Layer 5 =====
  node_717 [shape=box, label="Layer5_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_716 -> node_717;
  node_718 [shape=box, label="Layer5_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_719 [shape=box, label="Layer5_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_717 -> node_718;
  node_717 -> node_719;
  node_720 [shape=box, label="Layer5_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_721 [shape=box, label="Layer5_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_718 -> node_720;
  node_719 -> node_721;
  node_722 [shape=box, label="Layer5_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_723 [shape=box, label="Layer5_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_720 -> node_722;
  node_721 -> node_723;
  node_724 [shape=ellipse, label="Layer5_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_722 -> node_724 [style=dashed];
  node_723 -> node_724 [style=dashed];
  node_725 [shape=parallelogram, label="Layer5_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_716 -> node_725;
  node_724 -> node_725;
  node_726 [shape=box, label="Layer5_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_725 -> node_726;
  node_727 [shape=parallelogram, label="Layer5_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_726 -> node_727 [style=dashed];
  node_728 [shape=ellipse, label="Layer5_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_727 -> node_728 [style=dashed];
  node_729 [shape=parallelogram, label="Layer5_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_730 [shape=box, label="Layer5_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_731 [shape=box, label="Layer5_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_732 [shape=box, label="Layer5_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_733 [shape=box, label="Layer5_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_734 [shape=box, label="Layer5_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_735 [shape=box, label="Layer5_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_736 [shape=ellipse, label="Layer5_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_729;
  node_729 -> node_730;
  node_729 -> node_731;
  node_730 -> node_732;
  node_731 -> node_733;
  node_732 -> node_734;
  node_733 -> node_735;
  node_734 -> node_736 [style=dashed];
  node_735 -> node_736 [style=dashed];
  node_737 [shape=parallelogram, label="Layer5_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_738 [shape=box, label="Layer5_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_739 [shape=box, label="Layer5_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_740 [shape=box, label="Layer5_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_741 [shape=box, label="Layer5_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_742 [shape=box, label="Layer5_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_743 [shape=box, label="Layer5_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_744 [shape=ellipse, label="Layer5_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_737;
  node_737 -> node_738;
  node_737 -> node_739;
  node_738 -> node_740;
  node_739 -> node_741;
  node_740 -> node_742;
  node_741 -> node_743;
  node_742 -> node_744 [style=dashed];
  node_743 -> node_744 [style=dashed];
  node_745 [shape=parallelogram, label="Layer5_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_746 [shape=box, label="Layer5_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_747 [shape=box, label="Layer5_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_748 [shape=box, label="Layer5_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_749 [shape=box, label="Layer5_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_750 [shape=box, label="Layer5_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_751 [shape=box, label="Layer5_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_752 [shape=ellipse, label="Layer5_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_745;
  node_745 -> node_746;
  node_745 -> node_747;
  node_746 -> node_748;
  node_747 -> node_749;
  node_748 -> node_750;
  node_749 -> node_751;
  node_750 -> node_752 [style=dashed];
  node_751 -> node_752 [style=dashed];
  node_753 [shape=parallelogram, label="Layer5_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_754 [shape=box, label="Layer5_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_755 [shape=box, label="Layer5_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_756 [shape=box, label="Layer5_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_757 [shape=box, label="Layer5_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_758 [shape=box, label="Layer5_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_759 [shape=box, label="Layer5_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_760 [shape=ellipse, label="Layer5_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_753;
  node_753 -> node_754;
  node_753 -> node_755;
  node_754 -> node_756;
  node_755 -> node_757;
  node_756 -> node_758;
  node_757 -> node_759;
  node_758 -> node_760 [style=dashed];
  node_759 -> node_760 [style=dashed];
  node_761 [shape=parallelogram, label="Layer5_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_762 [shape=box, label="Layer5_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_763 [shape=box, label="Layer5_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_764 [shape=box, label="Layer5_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_765 [shape=box, label="Layer5_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_766 [shape=box, label="Layer5_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_767 [shape=box, label="Layer5_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_768 [shape=ellipse, label="Layer5_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_761;
  node_761 -> node_762;
  node_761 -> node_763;
  node_762 -> node_764;
  node_763 -> node_765;
  node_764 -> node_766;
  node_765 -> node_767;
  node_766 -> node_768 [style=dashed];
  node_767 -> node_768 [style=dashed];
  node_769 [shape=parallelogram, label="Layer5_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_770 [shape=box, label="Layer5_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_771 [shape=box, label="Layer5_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_772 [shape=box, label="Layer5_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_773 [shape=box, label="Layer5_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_774 [shape=box, label="Layer5_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_775 [shape=box, label="Layer5_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_776 [shape=ellipse, label="Layer5_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_769;
  node_769 -> node_770;
  node_769 -> node_771;
  node_770 -> node_772;
  node_771 -> node_773;
  node_772 -> node_774;
  node_773 -> node_775;
  node_774 -> node_776 [style=dashed];
  node_775 -> node_776 [style=dashed];
  node_777 [shape=parallelogram, label="Layer5_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_778 [shape=box, label="Layer5_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_779 [shape=box, label="Layer5_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_780 [shape=box, label="Layer5_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_781 [shape=box, label="Layer5_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_782 [shape=box, label="Layer5_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_783 [shape=box, label="Layer5_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_784 [shape=ellipse, label="Layer5_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_777;
  node_777 -> node_778;
  node_777 -> node_779;
  node_778 -> node_780;
  node_779 -> node_781;
  node_780 -> node_782;
  node_781 -> node_783;
  node_782 -> node_784 [style=dashed];
  node_783 -> node_784 [style=dashed];
  node_785 [shape=parallelogram, label="Layer5_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_786 [shape=box, label="Layer5_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_787 [shape=box, label="Layer5_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_788 [shape=box, label="Layer5_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_789 [shape=box, label="Layer5_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_790 [shape=box, label="Layer5_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_791 [shape=box, label="Layer5_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_792 [shape=ellipse, label="Layer5_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_785;
  node_785 -> node_786;
  node_785 -> node_787;
  node_786 -> node_788;
  node_787 -> node_789;
  node_788 -> node_790;
  node_789 -> node_791;
  node_790 -> node_792 [style=dashed];
  node_791 -> node_792 [style=dashed];
  node_793 [shape=parallelogram, label="Layer5_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_794 [shape=box, label="Layer5_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_795 [shape=box, label="Layer5_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_796 [shape=box, label="Layer5_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_797 [shape=box, label="Layer5_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_798 [shape=box, label="Layer5_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_799 [shape=box, label="Layer5_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_800 [shape=ellipse, label="Layer5_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_793;
  node_793 -> node_794;
  node_793 -> node_795;
  node_794 -> node_796;
  node_795 -> node_797;
  node_796 -> node_798;
  node_797 -> node_799;
  node_798 -> node_800 [style=dashed];
  node_799 -> node_800 [style=dashed];
  node_801 [shape=parallelogram, label="Layer5_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_802 [shape=box, label="Layer5_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_803 [shape=box, label="Layer5_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_804 [shape=box, label="Layer5_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_805 [shape=box, label="Layer5_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_806 [shape=box, label="Layer5_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_807 [shape=box, label="Layer5_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_808 [shape=ellipse, label="Layer5_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_801;
  node_801 -> node_802;
  node_801 -> node_803;
  node_802 -> node_804;
  node_803 -> node_805;
  node_804 -> node_806;
  node_805 -> node_807;
  node_806 -> node_808 [style=dashed];
  node_807 -> node_808 [style=dashed];
  node_809 [shape=parallelogram, label="Layer5_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_810 [shape=box, label="Layer5_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_811 [shape=box, label="Layer5_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_812 [shape=box, label="Layer5_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_813 [shape=box, label="Layer5_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_814 [shape=box, label="Layer5_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_815 [shape=box, label="Layer5_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_816 [shape=ellipse, label="Layer5_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_809;
  node_809 -> node_810;
  node_809 -> node_811;
  node_810 -> node_812;
  node_811 -> node_813;
  node_812 -> node_814;
  node_813 -> node_815;
  node_814 -> node_816 [style=dashed];
  node_815 -> node_816 [style=dashed];
  node_817 [shape=parallelogram, label="Layer5_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_818 [shape=box, label="Layer5_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_819 [shape=box, label="Layer5_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_820 [shape=box, label="Layer5_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_821 [shape=box, label="Layer5_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_822 [shape=box, label="Layer5_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_823 [shape=box, label="Layer5_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_824 [shape=ellipse, label="Layer5_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_817;
  node_817 -> node_818;
  node_817 -> node_819;
  node_818 -> node_820;
  node_819 -> node_821;
  node_820 -> node_822;
  node_821 -> node_823;
  node_822 -> node_824 [style=dashed];
  node_823 -> node_824 [style=dashed];
  node_825 [shape=parallelogram, label="Layer5_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_826 [shape=box, label="Layer5_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_827 [shape=box, label="Layer5_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_828 [shape=box, label="Layer5_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_829 [shape=box, label="Layer5_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_830 [shape=box, label="Layer5_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_831 [shape=box, label="Layer5_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_832 [shape=ellipse, label="Layer5_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_825;
  node_825 -> node_826;
  node_825 -> node_827;
  node_826 -> node_828;
  node_827 -> node_829;
  node_828 -> node_830;
  node_829 -> node_831;
  node_830 -> node_832 [style=dashed];
  node_831 -> node_832 [style=dashed];
  node_833 [shape=parallelogram, label="Layer5_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_834 [shape=box, label="Layer5_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_835 [shape=box, label="Layer5_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_836 [shape=box, label="Layer5_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_837 [shape=box, label="Layer5_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_838 [shape=box, label="Layer5_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_839 [shape=box, label="Layer5_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_840 [shape=ellipse, label="Layer5_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_833;
  node_833 -> node_834;
  node_833 -> node_835;
  node_834 -> node_836;
  node_835 -> node_837;
  node_836 -> node_838;
  node_837 -> node_839;
  node_838 -> node_840 [style=dashed];
  node_839 -> node_840 [style=dashed];
  node_841 [shape=parallelogram, label="Layer5_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_842 [shape=box, label="Layer5_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_843 [shape=box, label="Layer5_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_844 [shape=box, label="Layer5_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_845 [shape=box, label="Layer5_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_846 [shape=box, label="Layer5_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_847 [shape=box, label="Layer5_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_848 [shape=ellipse, label="Layer5_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_841;
  node_841 -> node_842;
  node_841 -> node_843;
  node_842 -> node_844;
  node_843 -> node_845;
  node_844 -> node_846;
  node_845 -> node_847;
  node_846 -> node_848 [style=dashed];
  node_847 -> node_848 [style=dashed];
  node_849 [shape=parallelogram, label="Layer5_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_850 [shape=box, label="Layer5_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_851 [shape=box, label="Layer5_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_852 [shape=box, label="Layer5_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_853 [shape=box, label="Layer5_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_854 [shape=box, label="Layer5_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_855 [shape=box, label="Layer5_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_856 [shape=ellipse, label="Layer5_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_728 -> node_849;
  node_849 -> node_850;
  node_849 -> node_851;
  node_850 -> node_852;
  node_851 -> node_853;
  node_852 -> node_854;
  node_853 -> node_855;
  node_854 -> node_856 [style=dashed];
  node_855 -> node_856 [style=dashed];
  node_857 [shape=ellipse, label="Layer5_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_736 -> node_857 [style=dashed];
  node_744 -> node_857 [style=dashed];
  node_752 -> node_857 [style=dashed];
  node_760 -> node_857 [style=dashed];
  node_768 -> node_857 [style=dashed];
  node_776 -> node_857 [style=dashed];
  node_784 -> node_857 [style=dashed];
  node_792 -> node_857 [style=dashed];
  node_800 -> node_857 [style=dashed];
  node_808 -> node_857 [style=dashed];
  node_816 -> node_857 [style=dashed];
  node_824 -> node_857 [style=dashed];
  node_832 -> node_857 [style=dashed];
  node_840 -> node_857 [style=dashed];
  node_848 -> node_857 [style=dashed];
  node_856 -> node_857 [style=dashed];
  node_858 [shape=box, label="Layer5_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_857 -> node_858;
  node_859 [shape=parallelogram, label="Layer5_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_725 -> node_859;
  node_858 -> node_859;
  
  // ===== Layer 6 =====
  node_860 [shape=box, label="Layer6_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_859 -> node_860;
  node_861 [shape=box, label="Layer6_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_862 [shape=box, label="Layer6_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_860 -> node_861;
  node_860 -> node_862;
  node_863 [shape=box, label="Layer6_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_864 [shape=box, label="Layer6_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_861 -> node_863;
  node_862 -> node_864;
  node_865 [shape=box, label="Layer6_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_866 [shape=box, label="Layer6_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_863 -> node_865;
  node_864 -> node_866;
  node_867 [shape=ellipse, label="Layer6_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_865 -> node_867 [style=dashed];
  node_866 -> node_867 [style=dashed];
  node_868 [shape=parallelogram, label="Layer6_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_859 -> node_868;
  node_867 -> node_868;
  node_869 [shape=box, label="Layer6_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_868 -> node_869;
  node_870 [shape=parallelogram, label="Layer6_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_869 -> node_870 [style=dashed];
  node_871 [shape=ellipse, label="Layer6_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_870 -> node_871 [style=dashed];
  node_872 [shape=parallelogram, label="Layer6_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_873 [shape=box, label="Layer6_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_874 [shape=box, label="Layer6_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_875 [shape=box, label="Layer6_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_876 [shape=box, label="Layer6_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_877 [shape=box, label="Layer6_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_878 [shape=box, label="Layer6_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_879 [shape=ellipse, label="Layer6_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_872;
  node_872 -> node_873;
  node_872 -> node_874;
  node_873 -> node_875;
  node_874 -> node_876;
  node_875 -> node_877;
  node_876 -> node_878;
  node_877 -> node_879 [style=dashed];
  node_878 -> node_879 [style=dashed];
  node_880 [shape=parallelogram, label="Layer6_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_881 [shape=box, label="Layer6_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_882 [shape=box, label="Layer6_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_883 [shape=box, label="Layer6_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_884 [shape=box, label="Layer6_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_885 [shape=box, label="Layer6_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_886 [shape=box, label="Layer6_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_887 [shape=ellipse, label="Layer6_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_880;
  node_880 -> node_881;
  node_880 -> node_882;
  node_881 -> node_883;
  node_882 -> node_884;
  node_883 -> node_885;
  node_884 -> node_886;
  node_885 -> node_887 [style=dashed];
  node_886 -> node_887 [style=dashed];
  node_888 [shape=parallelogram, label="Layer6_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_889 [shape=box, label="Layer6_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_890 [shape=box, label="Layer6_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_891 [shape=box, label="Layer6_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_892 [shape=box, label="Layer6_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_893 [shape=box, label="Layer6_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_894 [shape=box, label="Layer6_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_895 [shape=ellipse, label="Layer6_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_888;
  node_888 -> node_889;
  node_888 -> node_890;
  node_889 -> node_891;
  node_890 -> node_892;
  node_891 -> node_893;
  node_892 -> node_894;
  node_893 -> node_895 [style=dashed];
  node_894 -> node_895 [style=dashed];
  node_896 [shape=parallelogram, label="Layer6_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_897 [shape=box, label="Layer6_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_898 [shape=box, label="Layer6_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_899 [shape=box, label="Layer6_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_900 [shape=box, label="Layer6_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_901 [shape=box, label="Layer6_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_902 [shape=box, label="Layer6_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_903 [shape=ellipse, label="Layer6_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_896;
  node_896 -> node_897;
  node_896 -> node_898;
  node_897 -> node_899;
  node_898 -> node_900;
  node_899 -> node_901;
  node_900 -> node_902;
  node_901 -> node_903 [style=dashed];
  node_902 -> node_903 [style=dashed];
  node_904 [shape=parallelogram, label="Layer6_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_905 [shape=box, label="Layer6_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_906 [shape=box, label="Layer6_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_907 [shape=box, label="Layer6_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_908 [shape=box, label="Layer6_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_909 [shape=box, label="Layer6_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_910 [shape=box, label="Layer6_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_911 [shape=ellipse, label="Layer6_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_904;
  node_904 -> node_905;
  node_904 -> node_906;
  node_905 -> node_907;
  node_906 -> node_908;
  node_907 -> node_909;
  node_908 -> node_910;
  node_909 -> node_911 [style=dashed];
  node_910 -> node_911 [style=dashed];
  node_912 [shape=parallelogram, label="Layer6_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_913 [shape=box, label="Layer6_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_914 [shape=box, label="Layer6_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_915 [shape=box, label="Layer6_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_916 [shape=box, label="Layer6_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_917 [shape=box, label="Layer6_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_918 [shape=box, label="Layer6_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_919 [shape=ellipse, label="Layer6_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_912;
  node_912 -> node_913;
  node_912 -> node_914;
  node_913 -> node_915;
  node_914 -> node_916;
  node_915 -> node_917;
  node_916 -> node_918;
  node_917 -> node_919 [style=dashed];
  node_918 -> node_919 [style=dashed];
  node_920 [shape=parallelogram, label="Layer6_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_921 [shape=box, label="Layer6_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_922 [shape=box, label="Layer6_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_923 [shape=box, label="Layer6_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_924 [shape=box, label="Layer6_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_925 [shape=box, label="Layer6_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_926 [shape=box, label="Layer6_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_927 [shape=ellipse, label="Layer6_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_920;
  node_920 -> node_921;
  node_920 -> node_922;
  node_921 -> node_923;
  node_922 -> node_924;
  node_923 -> node_925;
  node_924 -> node_926;
  node_925 -> node_927 [style=dashed];
  node_926 -> node_927 [style=dashed];
  node_928 [shape=parallelogram, label="Layer6_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_929 [shape=box, label="Layer6_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_930 [shape=box, label="Layer6_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_931 [shape=box, label="Layer6_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_932 [shape=box, label="Layer6_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_933 [shape=box, label="Layer6_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_934 [shape=box, label="Layer6_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_935 [shape=ellipse, label="Layer6_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_928;
  node_928 -> node_929;
  node_928 -> node_930;
  node_929 -> node_931;
  node_930 -> node_932;
  node_931 -> node_933;
  node_932 -> node_934;
  node_933 -> node_935 [style=dashed];
  node_934 -> node_935 [style=dashed];
  node_936 [shape=parallelogram, label="Layer6_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_937 [shape=box, label="Layer6_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_938 [shape=box, label="Layer6_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_939 [shape=box, label="Layer6_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_940 [shape=box, label="Layer6_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_941 [shape=box, label="Layer6_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_942 [shape=box, label="Layer6_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_943 [shape=ellipse, label="Layer6_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_936;
  node_936 -> node_937;
  node_936 -> node_938;
  node_937 -> node_939;
  node_938 -> node_940;
  node_939 -> node_941;
  node_940 -> node_942;
  node_941 -> node_943 [style=dashed];
  node_942 -> node_943 [style=dashed];
  node_944 [shape=parallelogram, label="Layer6_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_945 [shape=box, label="Layer6_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_946 [shape=box, label="Layer6_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_947 [shape=box, label="Layer6_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_948 [shape=box, label="Layer6_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_949 [shape=box, label="Layer6_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_950 [shape=box, label="Layer6_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_951 [shape=ellipse, label="Layer6_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_944;
  node_944 -> node_945;
  node_944 -> node_946;
  node_945 -> node_947;
  node_946 -> node_948;
  node_947 -> node_949;
  node_948 -> node_950;
  node_949 -> node_951 [style=dashed];
  node_950 -> node_951 [style=dashed];
  node_952 [shape=parallelogram, label="Layer6_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_953 [shape=box, label="Layer6_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_954 [shape=box, label="Layer6_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_955 [shape=box, label="Layer6_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_956 [shape=box, label="Layer6_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_957 [shape=box, label="Layer6_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_958 [shape=box, label="Layer6_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_959 [shape=ellipse, label="Layer6_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_952;
  node_952 -> node_953;
  node_952 -> node_954;
  node_953 -> node_955;
  node_954 -> node_956;
  node_955 -> node_957;
  node_956 -> node_958;
  node_957 -> node_959 [style=dashed];
  node_958 -> node_959 [style=dashed];
  node_960 [shape=parallelogram, label="Layer6_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_961 [shape=box, label="Layer6_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_962 [shape=box, label="Layer6_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_963 [shape=box, label="Layer6_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_964 [shape=box, label="Layer6_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_965 [shape=box, label="Layer6_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_966 [shape=box, label="Layer6_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_967 [shape=ellipse, label="Layer6_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_960;
  node_960 -> node_961;
  node_960 -> node_962;
  node_961 -> node_963;
  node_962 -> node_964;
  node_963 -> node_965;
  node_964 -> node_966;
  node_965 -> node_967 [style=dashed];
  node_966 -> node_967 [style=dashed];
  node_968 [shape=parallelogram, label="Layer6_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_969 [shape=box, label="Layer6_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_970 [shape=box, label="Layer6_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_971 [shape=box, label="Layer6_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_972 [shape=box, label="Layer6_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_973 [shape=box, label="Layer6_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_974 [shape=box, label="Layer6_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_975 [shape=ellipse, label="Layer6_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_968;
  node_968 -> node_969;
  node_968 -> node_970;
  node_969 -> node_971;
  node_970 -> node_972;
  node_971 -> node_973;
  node_972 -> node_974;
  node_973 -> node_975 [style=dashed];
  node_974 -> node_975 [style=dashed];
  node_976 [shape=parallelogram, label="Layer6_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_977 [shape=box, label="Layer6_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_978 [shape=box, label="Layer6_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_979 [shape=box, label="Layer6_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_980 [shape=box, label="Layer6_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_981 [shape=box, label="Layer6_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_982 [shape=box, label="Layer6_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_983 [shape=ellipse, label="Layer6_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_976;
  node_976 -> node_977;
  node_976 -> node_978;
  node_977 -> node_979;
  node_978 -> node_980;
  node_979 -> node_981;
  node_980 -> node_982;
  node_981 -> node_983 [style=dashed];
  node_982 -> node_983 [style=dashed];
  node_984 [shape=parallelogram, label="Layer6_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_985 [shape=box, label="Layer6_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_986 [shape=box, label="Layer6_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_987 [shape=box, label="Layer6_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_988 [shape=box, label="Layer6_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_989 [shape=box, label="Layer6_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_990 [shape=box, label="Layer6_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_991 [shape=ellipse, label="Layer6_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_984;
  node_984 -> node_985;
  node_984 -> node_986;
  node_985 -> node_987;
  node_986 -> node_988;
  node_987 -> node_989;
  node_988 -> node_990;
  node_989 -> node_991 [style=dashed];
  node_990 -> node_991 [style=dashed];
  node_992 [shape=parallelogram, label="Layer6_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_993 [shape=box, label="Layer6_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_994 [shape=box, label="Layer6_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_995 [shape=box, label="Layer6_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_996 [shape=box, label="Layer6_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_997 [shape=box, label="Layer6_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_998 [shape=box, label="Layer6_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_999 [shape=ellipse, label="Layer6_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_871 -> node_992;
  node_992 -> node_993;
  node_992 -> node_994;
  node_993 -> node_995;
  node_994 -> node_996;
  node_995 -> node_997;
  node_996 -> node_998;
  node_997 -> node_999 [style=dashed];
  node_998 -> node_999 [style=dashed];
  node_1000 [shape=ellipse, label="Layer6_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_879 -> node_1000 [style=dashed];
  node_887 -> node_1000 [style=dashed];
  node_895 -> node_1000 [style=dashed];
  node_903 -> node_1000 [style=dashed];
  node_911 -> node_1000 [style=dashed];
  node_919 -> node_1000 [style=dashed];
  node_927 -> node_1000 [style=dashed];
  node_935 -> node_1000 [style=dashed];
  node_943 -> node_1000 [style=dashed];
  node_951 -> node_1000 [style=dashed];
  node_959 -> node_1000 [style=dashed];
  node_967 -> node_1000 [style=dashed];
  node_975 -> node_1000 [style=dashed];
  node_983 -> node_1000 [style=dashed];
  node_991 -> node_1000 [style=dashed];
  node_999 -> node_1000 [style=dashed];
  node_1001 [shape=box, label="Layer6_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1000 -> node_1001;
  node_1002 [shape=parallelogram, label="Layer6_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_868 -> node_1002;
  node_1001 -> node_1002;
  
  // ===== Layer 7 =====
  node_1003 [shape=box, label="Layer7_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1002 -> node_1003;
  node_1004 [shape=box, label="Layer7_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1005 [shape=box, label="Layer7_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1003 -> node_1004;
  node_1003 -> node_1005;
  node_1006 [shape=box, label="Layer7_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1007 [shape=box, label="Layer7_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1004 -> node_1006;
  node_1005 -> node_1007;
  node_1008 [shape=box, label="Layer7_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1009 [shape=box, label="Layer7_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1006 -> node_1008;
  node_1007 -> node_1009;
  node_1010 [shape=ellipse, label="Layer7_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1008 -> node_1010 [style=dashed];
  node_1009 -> node_1010 [style=dashed];
  node_1011 [shape=parallelogram, label="Layer7_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1002 -> node_1011;
  node_1010 -> node_1011;
  node_1012 [shape=box, label="Layer7_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1011 -> node_1012;
  node_1013 [shape=parallelogram, label="Layer7_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_1012 -> node_1013 [style=dashed];
  node_1014 [shape=ellipse, label="Layer7_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1013 -> node_1014 [style=dashed];
  node_1015 [shape=parallelogram, label="Layer7_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1016 [shape=box, label="Layer7_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1017 [shape=box, label="Layer7_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1018 [shape=box, label="Layer7_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1019 [shape=box, label="Layer7_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1020 [shape=box, label="Layer7_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1021 [shape=box, label="Layer7_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1022 [shape=ellipse, label="Layer7_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1015;
  node_1015 -> node_1016;
  node_1015 -> node_1017;
  node_1016 -> node_1018;
  node_1017 -> node_1019;
  node_1018 -> node_1020;
  node_1019 -> node_1021;
  node_1020 -> node_1022 [style=dashed];
  node_1021 -> node_1022 [style=dashed];
  node_1023 [shape=parallelogram, label="Layer7_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1024 [shape=box, label="Layer7_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1025 [shape=box, label="Layer7_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1026 [shape=box, label="Layer7_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1027 [shape=box, label="Layer7_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1028 [shape=box, label="Layer7_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1029 [shape=box, label="Layer7_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1030 [shape=ellipse, label="Layer7_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1023;
  node_1023 -> node_1024;
  node_1023 -> node_1025;
  node_1024 -> node_1026;
  node_1025 -> node_1027;
  node_1026 -> node_1028;
  node_1027 -> node_1029;
  node_1028 -> node_1030 [style=dashed];
  node_1029 -> node_1030 [style=dashed];
  node_1031 [shape=parallelogram, label="Layer7_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1032 [shape=box, label="Layer7_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1033 [shape=box, label="Layer7_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1034 [shape=box, label="Layer7_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1035 [shape=box, label="Layer7_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1036 [shape=box, label="Layer7_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1037 [shape=box, label="Layer7_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1038 [shape=ellipse, label="Layer7_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1031;
  node_1031 -> node_1032;
  node_1031 -> node_1033;
  node_1032 -> node_1034;
  node_1033 -> node_1035;
  node_1034 -> node_1036;
  node_1035 -> node_1037;
  node_1036 -> node_1038 [style=dashed];
  node_1037 -> node_1038 [style=dashed];
  node_1039 [shape=parallelogram, label="Layer7_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1040 [shape=box, label="Layer7_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1041 [shape=box, label="Layer7_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1042 [shape=box, label="Layer7_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1043 [shape=box, label="Layer7_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1044 [shape=box, label="Layer7_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1045 [shape=box, label="Layer7_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1046 [shape=ellipse, label="Layer7_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1039;
  node_1039 -> node_1040;
  node_1039 -> node_1041;
  node_1040 -> node_1042;
  node_1041 -> node_1043;
  node_1042 -> node_1044;
  node_1043 -> node_1045;
  node_1044 -> node_1046 [style=dashed];
  node_1045 -> node_1046 [style=dashed];
  node_1047 [shape=parallelogram, label="Layer7_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1048 [shape=box, label="Layer7_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1049 [shape=box, label="Layer7_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1050 [shape=box, label="Layer7_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1051 [shape=box, label="Layer7_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1052 [shape=box, label="Layer7_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1053 [shape=box, label="Layer7_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1054 [shape=ellipse, label="Layer7_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1047;
  node_1047 -> node_1048;
  node_1047 -> node_1049;
  node_1048 -> node_1050;
  node_1049 -> node_1051;
  node_1050 -> node_1052;
  node_1051 -> node_1053;
  node_1052 -> node_1054 [style=dashed];
  node_1053 -> node_1054 [style=dashed];
  node_1055 [shape=parallelogram, label="Layer7_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1056 [shape=box, label="Layer7_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1057 [shape=box, label="Layer7_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1058 [shape=box, label="Layer7_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1059 [shape=box, label="Layer7_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1060 [shape=box, label="Layer7_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1061 [shape=box, label="Layer7_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1062 [shape=ellipse, label="Layer7_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1055;
  node_1055 -> node_1056;
  node_1055 -> node_1057;
  node_1056 -> node_1058;
  node_1057 -> node_1059;
  node_1058 -> node_1060;
  node_1059 -> node_1061;
  node_1060 -> node_1062 [style=dashed];
  node_1061 -> node_1062 [style=dashed];
  node_1063 [shape=parallelogram, label="Layer7_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1064 [shape=box, label="Layer7_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1065 [shape=box, label="Layer7_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1066 [shape=box, label="Layer7_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1067 [shape=box, label="Layer7_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1068 [shape=box, label="Layer7_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1069 [shape=box, label="Layer7_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1070 [shape=ellipse, label="Layer7_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1063;
  node_1063 -> node_1064;
  node_1063 -> node_1065;
  node_1064 -> node_1066;
  node_1065 -> node_1067;
  node_1066 -> node_1068;
  node_1067 -> node_1069;
  node_1068 -> node_1070 [style=dashed];
  node_1069 -> node_1070 [style=dashed];
  node_1071 [shape=parallelogram, label="Layer7_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1072 [shape=box, label="Layer7_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1073 [shape=box, label="Layer7_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1074 [shape=box, label="Layer7_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1075 [shape=box, label="Layer7_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1076 [shape=box, label="Layer7_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1077 [shape=box, label="Layer7_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1078 [shape=ellipse, label="Layer7_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1071;
  node_1071 -> node_1072;
  node_1071 -> node_1073;
  node_1072 -> node_1074;
  node_1073 -> node_1075;
  node_1074 -> node_1076;
  node_1075 -> node_1077;
  node_1076 -> node_1078 [style=dashed];
  node_1077 -> node_1078 [style=dashed];
  node_1079 [shape=parallelogram, label="Layer7_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1080 [shape=box, label="Layer7_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1081 [shape=box, label="Layer7_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1082 [shape=box, label="Layer7_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1083 [shape=box, label="Layer7_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1084 [shape=box, label="Layer7_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1085 [shape=box, label="Layer7_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1086 [shape=ellipse, label="Layer7_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1079;
  node_1079 -> node_1080;
  node_1079 -> node_1081;
  node_1080 -> node_1082;
  node_1081 -> node_1083;
  node_1082 -> node_1084;
  node_1083 -> node_1085;
  node_1084 -> node_1086 [style=dashed];
  node_1085 -> node_1086 [style=dashed];
  node_1087 [shape=parallelogram, label="Layer7_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1088 [shape=box, label="Layer7_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1089 [shape=box, label="Layer7_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1090 [shape=box, label="Layer7_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1091 [shape=box, label="Layer7_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1092 [shape=box, label="Layer7_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1093 [shape=box, label="Layer7_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1094 [shape=ellipse, label="Layer7_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1087;
  node_1087 -> node_1088;
  node_1087 -> node_1089;
  node_1088 -> node_1090;
  node_1089 -> node_1091;
  node_1090 -> node_1092;
  node_1091 -> node_1093;
  node_1092 -> node_1094 [style=dashed];
  node_1093 -> node_1094 [style=dashed];
  node_1095 [shape=parallelogram, label="Layer7_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1096 [shape=box, label="Layer7_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1097 [shape=box, label="Layer7_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1098 [shape=box, label="Layer7_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1099 [shape=box, label="Layer7_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1100 [shape=box, label="Layer7_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1101 [shape=box, label="Layer7_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1102 [shape=ellipse, label="Layer7_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1095;
  node_1095 -> node_1096;
  node_1095 -> node_1097;
  node_1096 -> node_1098;
  node_1097 -> node_1099;
  node_1098 -> node_1100;
  node_1099 -> node_1101;
  node_1100 -> node_1102 [style=dashed];
  node_1101 -> node_1102 [style=dashed];
  node_1103 [shape=parallelogram, label="Layer7_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1104 [shape=box, label="Layer7_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1105 [shape=box, label="Layer7_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1106 [shape=box, label="Layer7_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1107 [shape=box, label="Layer7_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1108 [shape=box, label="Layer7_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1109 [shape=box, label="Layer7_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1110 [shape=ellipse, label="Layer7_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1103;
  node_1103 -> node_1104;
  node_1103 -> node_1105;
  node_1104 -> node_1106;
  node_1105 -> node_1107;
  node_1106 -> node_1108;
  node_1107 -> node_1109;
  node_1108 -> node_1110 [style=dashed];
  node_1109 -> node_1110 [style=dashed];
  node_1111 [shape=parallelogram, label="Layer7_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1112 [shape=box, label="Layer7_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1113 [shape=box, label="Layer7_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1114 [shape=box, label="Layer7_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1115 [shape=box, label="Layer7_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1116 [shape=box, label="Layer7_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1117 [shape=box, label="Layer7_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1118 [shape=ellipse, label="Layer7_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1111;
  node_1111 -> node_1112;
  node_1111 -> node_1113;
  node_1112 -> node_1114;
  node_1113 -> node_1115;
  node_1114 -> node_1116;
  node_1115 -> node_1117;
  node_1116 -> node_1118 [style=dashed];
  node_1117 -> node_1118 [style=dashed];
  node_1119 [shape=parallelogram, label="Layer7_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1120 [shape=box, label="Layer7_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1121 [shape=box, label="Layer7_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1122 [shape=box, label="Layer7_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1123 [shape=box, label="Layer7_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1124 [shape=box, label="Layer7_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1125 [shape=box, label="Layer7_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1126 [shape=ellipse, label="Layer7_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1119;
  node_1119 -> node_1120;
  node_1119 -> node_1121;
  node_1120 -> node_1122;
  node_1121 -> node_1123;
  node_1122 -> node_1124;
  node_1123 -> node_1125;
  node_1124 -> node_1126 [style=dashed];
  node_1125 -> node_1126 [style=dashed];
  node_1127 [shape=parallelogram, label="Layer7_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1128 [shape=box, label="Layer7_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1129 [shape=box, label="Layer7_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1130 [shape=box, label="Layer7_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1131 [shape=box, label="Layer7_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1132 [shape=box, label="Layer7_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1133 [shape=box, label="Layer7_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1134 [shape=ellipse, label="Layer7_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1127;
  node_1127 -> node_1128;
  node_1127 -> node_1129;
  node_1128 -> node_1130;
  node_1129 -> node_1131;
  node_1130 -> node_1132;
  node_1131 -> node_1133;
  node_1132 -> node_1134 [style=dashed];
  node_1133 -> node_1134 [style=dashed];
  node_1135 [shape=parallelogram, label="Layer7_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1136 [shape=box, label="Layer7_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1137 [shape=box, label="Layer7_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1138 [shape=box, label="Layer7_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1139 [shape=box, label="Layer7_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1140 [shape=box, label="Layer7_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1141 [shape=box, label="Layer7_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1142 [shape=ellipse, label="Layer7_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1014 -> node_1135;
  node_1135 -> node_1136;
  node_1135 -> node_1137;
  node_1136 -> node_1138;
  node_1137 -> node_1139;
  node_1138 -> node_1140;
  node_1139 -> node_1141;
  node_1140 -> node_1142 [style=dashed];
  node_1141 -> node_1142 [style=dashed];
  node_1143 [shape=ellipse, label="Layer7_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1022 -> node_1143 [style=dashed];
  node_1030 -> node_1143 [style=dashed];
  node_1038 -> node_1143 [style=dashed];
  node_1046 -> node_1143 [style=dashed];
  node_1054 -> node_1143 [style=dashed];
  node_1062 -> node_1143 [style=dashed];
  node_1070 -> node_1143 [style=dashed];
  node_1078 -> node_1143 [style=dashed];
  node_1086 -> node_1143 [style=dashed];
  node_1094 -> node_1143 [style=dashed];
  node_1102 -> node_1143 [style=dashed];
  node_1110 -> node_1143 [style=dashed];
  node_1118 -> node_1143 [style=dashed];
  node_1126 -> node_1143 [style=dashed];
  node_1134 -> node_1143 [style=dashed];
  node_1142 -> node_1143 [style=dashed];
  node_1144 [shape=box, label="Layer7_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1143 -> node_1144;
  node_1145 [shape=parallelogram, label="Layer7_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1011 -> node_1145;
  node_1144 -> node_1145;
  
  // ===== Layer 8 =====
  node_1146 [shape=box, label="Layer8_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1145 -> node_1146;
  node_1147 [shape=box, label="Layer8_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1148 [shape=box, label="Layer8_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1146 -> node_1147;
  node_1146 -> node_1148;
  node_1149 [shape=box, label="Layer8_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1150 [shape=box, label="Layer8_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1147 -> node_1149;
  node_1148 -> node_1150;
  node_1151 [shape=box, label="Layer8_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1152 [shape=box, label="Layer8_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1149 -> node_1151;
  node_1150 -> node_1152;
  node_1153 [shape=ellipse, label="Layer8_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1151 -> node_1153 [style=dashed];
  node_1152 -> node_1153 [style=dashed];
  node_1154 [shape=parallelogram, label="Layer8_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1145 -> node_1154;
  node_1153 -> node_1154;
  node_1155 [shape=box, label="Layer8_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1154 -> node_1155;
  node_1156 [shape=parallelogram, label="Layer8_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_1155 -> node_1156 [style=dashed];
  node_1157 [shape=ellipse, label="Layer8_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1156 -> node_1157 [style=dashed];
  node_1158 [shape=parallelogram, label="Layer8_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1159 [shape=box, label="Layer8_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1160 [shape=box, label="Layer8_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1161 [shape=box, label="Layer8_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1162 [shape=box, label="Layer8_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1163 [shape=box, label="Layer8_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1164 [shape=box, label="Layer8_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1165 [shape=ellipse, label="Layer8_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1158;
  node_1158 -> node_1159;
  node_1158 -> node_1160;
  node_1159 -> node_1161;
  node_1160 -> node_1162;
  node_1161 -> node_1163;
  node_1162 -> node_1164;
  node_1163 -> node_1165 [style=dashed];
  node_1164 -> node_1165 [style=dashed];
  node_1166 [shape=parallelogram, label="Layer8_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1167 [shape=box, label="Layer8_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1168 [shape=box, label="Layer8_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1169 [shape=box, label="Layer8_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1170 [shape=box, label="Layer8_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1171 [shape=box, label="Layer8_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1172 [shape=box, label="Layer8_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1173 [shape=ellipse, label="Layer8_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1166;
  node_1166 -> node_1167;
  node_1166 -> node_1168;
  node_1167 -> node_1169;
  node_1168 -> node_1170;
  node_1169 -> node_1171;
  node_1170 -> node_1172;
  node_1171 -> node_1173 [style=dashed];
  node_1172 -> node_1173 [style=dashed];
  node_1174 [shape=parallelogram, label="Layer8_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1175 [shape=box, label="Layer8_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1176 [shape=box, label="Layer8_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1177 [shape=box, label="Layer8_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1178 [shape=box, label="Layer8_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1179 [shape=box, label="Layer8_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1180 [shape=box, label="Layer8_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1181 [shape=ellipse, label="Layer8_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1174;
  node_1174 -> node_1175;
  node_1174 -> node_1176;
  node_1175 -> node_1177;
  node_1176 -> node_1178;
  node_1177 -> node_1179;
  node_1178 -> node_1180;
  node_1179 -> node_1181 [style=dashed];
  node_1180 -> node_1181 [style=dashed];
  node_1182 [shape=parallelogram, label="Layer8_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1183 [shape=box, label="Layer8_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1184 [shape=box, label="Layer8_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1185 [shape=box, label="Layer8_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1186 [shape=box, label="Layer8_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1187 [shape=box, label="Layer8_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1188 [shape=box, label="Layer8_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1189 [shape=ellipse, label="Layer8_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1182;
  node_1182 -> node_1183;
  node_1182 -> node_1184;
  node_1183 -> node_1185;
  node_1184 -> node_1186;
  node_1185 -> node_1187;
  node_1186 -> node_1188;
  node_1187 -> node_1189 [style=dashed];
  node_1188 -> node_1189 [style=dashed];
  node_1190 [shape=parallelogram, label="Layer8_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1191 [shape=box, label="Layer8_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1192 [shape=box, label="Layer8_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1193 [shape=box, label="Layer8_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1194 [shape=box, label="Layer8_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1195 [shape=box, label="Layer8_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1196 [shape=box, label="Layer8_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1197 [shape=ellipse, label="Layer8_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1190;
  node_1190 -> node_1191;
  node_1190 -> node_1192;
  node_1191 -> node_1193;
  node_1192 -> node_1194;
  node_1193 -> node_1195;
  node_1194 -> node_1196;
  node_1195 -> node_1197 [style=dashed];
  node_1196 -> node_1197 [style=dashed];
  node_1198 [shape=parallelogram, label="Layer8_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1199 [shape=box, label="Layer8_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1200 [shape=box, label="Layer8_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1201 [shape=box, label="Layer8_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1202 [shape=box, label="Layer8_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1203 [shape=box, label="Layer8_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1204 [shape=box, label="Layer8_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1205 [shape=ellipse, label="Layer8_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1198;
  node_1198 -> node_1199;
  node_1198 -> node_1200;
  node_1199 -> node_1201;
  node_1200 -> node_1202;
  node_1201 -> node_1203;
  node_1202 -> node_1204;
  node_1203 -> node_1205 [style=dashed];
  node_1204 -> node_1205 [style=dashed];
  node_1206 [shape=parallelogram, label="Layer8_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1207 [shape=box, label="Layer8_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1208 [shape=box, label="Layer8_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1209 [shape=box, label="Layer8_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1210 [shape=box, label="Layer8_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1211 [shape=box, label="Layer8_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1212 [shape=box, label="Layer8_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1213 [shape=ellipse, label="Layer8_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1206;
  node_1206 -> node_1207;
  node_1206 -> node_1208;
  node_1207 -> node_1209;
  node_1208 -> node_1210;
  node_1209 -> node_1211;
  node_1210 -> node_1212;
  node_1211 -> node_1213 [style=dashed];
  node_1212 -> node_1213 [style=dashed];
  node_1214 [shape=parallelogram, label="Layer8_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1215 [shape=box, label="Layer8_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1216 [shape=box, label="Layer8_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1217 [shape=box, label="Layer8_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1218 [shape=box, label="Layer8_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1219 [shape=box, label="Layer8_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1220 [shape=box, label="Layer8_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1221 [shape=ellipse, label="Layer8_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1214;
  node_1214 -> node_1215;
  node_1214 -> node_1216;
  node_1215 -> node_1217;
  node_1216 -> node_1218;
  node_1217 -> node_1219;
  node_1218 -> node_1220;
  node_1219 -> node_1221 [style=dashed];
  node_1220 -> node_1221 [style=dashed];
  node_1222 [shape=parallelogram, label="Layer8_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1223 [shape=box, label="Layer8_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1224 [shape=box, label="Layer8_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1225 [shape=box, label="Layer8_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1226 [shape=box, label="Layer8_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1227 [shape=box, label="Layer8_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1228 [shape=box, label="Layer8_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1229 [shape=ellipse, label="Layer8_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1222;
  node_1222 -> node_1223;
  node_1222 -> node_1224;
  node_1223 -> node_1225;
  node_1224 -> node_1226;
  node_1225 -> node_1227;
  node_1226 -> node_1228;
  node_1227 -> node_1229 [style=dashed];
  node_1228 -> node_1229 [style=dashed];
  node_1230 [shape=parallelogram, label="Layer8_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1231 [shape=box, label="Layer8_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1232 [shape=box, label="Layer8_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1233 [shape=box, label="Layer8_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1234 [shape=box, label="Layer8_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1235 [shape=box, label="Layer8_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1236 [shape=box, label="Layer8_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1237 [shape=ellipse, label="Layer8_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1230;
  node_1230 -> node_1231;
  node_1230 -> node_1232;
  node_1231 -> node_1233;
  node_1232 -> node_1234;
  node_1233 -> node_1235;
  node_1234 -> node_1236;
  node_1235 -> node_1237 [style=dashed];
  node_1236 -> node_1237 [style=dashed];
  node_1238 [shape=parallelogram, label="Layer8_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1239 [shape=box, label="Layer8_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1240 [shape=box, label="Layer8_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1241 [shape=box, label="Layer8_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1242 [shape=box, label="Layer8_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1243 [shape=box, label="Layer8_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1244 [shape=box, label="Layer8_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1245 [shape=ellipse, label="Layer8_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1238;
  node_1238 -> node_1239;
  node_1238 -> node_1240;
  node_1239 -> node_1241;
  node_1240 -> node_1242;
  node_1241 -> node_1243;
  node_1242 -> node_1244;
  node_1243 -> node_1245 [style=dashed];
  node_1244 -> node_1245 [style=dashed];
  node_1246 [shape=parallelogram, label="Layer8_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1247 [shape=box, label="Layer8_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1248 [shape=box, label="Layer8_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1249 [shape=box, label="Layer8_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1250 [shape=box, label="Layer8_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1251 [shape=box, label="Layer8_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1252 [shape=box, label="Layer8_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1253 [shape=ellipse, label="Layer8_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1246;
  node_1246 -> node_1247;
  node_1246 -> node_1248;
  node_1247 -> node_1249;
  node_1248 -> node_1250;
  node_1249 -> node_1251;
  node_1250 -> node_1252;
  node_1251 -> node_1253 [style=dashed];
  node_1252 -> node_1253 [style=dashed];
  node_1254 [shape=parallelogram, label="Layer8_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1255 [shape=box, label="Layer8_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1256 [shape=box, label="Layer8_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1257 [shape=box, label="Layer8_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1258 [shape=box, label="Layer8_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1259 [shape=box, label="Layer8_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1260 [shape=box, label="Layer8_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1261 [shape=ellipse, label="Layer8_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1254;
  node_1254 -> node_1255;
  node_1254 -> node_1256;
  node_1255 -> node_1257;
  node_1256 -> node_1258;
  node_1257 -> node_1259;
  node_1258 -> node_1260;
  node_1259 -> node_1261 [style=dashed];
  node_1260 -> node_1261 [style=dashed];
  node_1262 [shape=parallelogram, label="Layer8_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1263 [shape=box, label="Layer8_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1264 [shape=box, label="Layer8_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1265 [shape=box, label="Layer8_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1266 [shape=box, label="Layer8_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1267 [shape=box, label="Layer8_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1268 [shape=box, label="Layer8_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1269 [shape=ellipse, label="Layer8_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1262;
  node_1262 -> node_1263;
  node_1262 -> node_1264;
  node_1263 -> node_1265;
  node_1264 -> node_1266;
  node_1265 -> node_1267;
  node_1266 -> node_1268;
  node_1267 -> node_1269 [style=dashed];
  node_1268 -> node_1269 [style=dashed];
  node_1270 [shape=parallelogram, label="Layer8_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1271 [shape=box, label="Layer8_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1272 [shape=box, label="Layer8_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1273 [shape=box, label="Layer8_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1274 [shape=box, label="Layer8_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1275 [shape=box, label="Layer8_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1276 [shape=box, label="Layer8_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1277 [shape=ellipse, label="Layer8_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1270;
  node_1270 -> node_1271;
  node_1270 -> node_1272;
  node_1271 -> node_1273;
  node_1272 -> node_1274;
  node_1273 -> node_1275;
  node_1274 -> node_1276;
  node_1275 -> node_1277 [style=dashed];
  node_1276 -> node_1277 [style=dashed];
  node_1278 [shape=parallelogram, label="Layer8_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1279 [shape=box, label="Layer8_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1280 [shape=box, label="Layer8_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1281 [shape=box, label="Layer8_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1282 [shape=box, label="Layer8_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1283 [shape=box, label="Layer8_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1284 [shape=box, label="Layer8_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1285 [shape=ellipse, label="Layer8_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1157 -> node_1278;
  node_1278 -> node_1279;
  node_1278 -> node_1280;
  node_1279 -> node_1281;
  node_1280 -> node_1282;
  node_1281 -> node_1283;
  node_1282 -> node_1284;
  node_1283 -> node_1285 [style=dashed];
  node_1284 -> node_1285 [style=dashed];
  node_1286 [shape=ellipse, label="Layer8_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1165 -> node_1286 [style=dashed];
  node_1173 -> node_1286 [style=dashed];
  node_1181 -> node_1286 [style=dashed];
  node_1189 -> node_1286 [style=dashed];
  node_1197 -> node_1286 [style=dashed];
  node_1205 -> node_1286 [style=dashed];
  node_1213 -> node_1286 [style=dashed];
  node_1221 -> node_1286 [style=dashed];
  node_1229 -> node_1286 [style=dashed];
  node_1237 -> node_1286 [style=dashed];
  node_1245 -> node_1286 [style=dashed];
  node_1253 -> node_1286 [style=dashed];
  node_1261 -> node_1286 [style=dashed];
  node_1269 -> node_1286 [style=dashed];
  node_1277 -> node_1286 [style=dashed];
  node_1285 -> node_1286 [style=dashed];
  node_1287 [shape=box, label="Layer8_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1286 -> node_1287;
  node_1288 [shape=parallelogram, label="Layer8_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1154 -> node_1288;
  node_1287 -> node_1288;
  
  // ===== Layer 9 =====
  node_1289 [shape=box, label="Layer9_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1288 -> node_1289;
  node_1290 [shape=box, label="Layer9_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1291 [shape=box, label="Layer9_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1289 -> node_1290;
  node_1289 -> node_1291;
  node_1292 [shape=box, label="Layer9_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1293 [shape=box, label="Layer9_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1290 -> node_1292;
  node_1291 -> node_1293;
  node_1294 [shape=box, label="Layer9_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1295 [shape=box, label="Layer9_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1292 -> node_1294;
  node_1293 -> node_1295;
  node_1296 [shape=ellipse, label="Layer9_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1294 -> node_1296 [style=dashed];
  node_1295 -> node_1296 [style=dashed];
  node_1297 [shape=parallelogram, label="Layer9_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1288 -> node_1297;
  node_1296 -> node_1297;
  node_1298 [shape=box, label="Layer9_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1297 -> node_1298;
  node_1299 [shape=parallelogram, label="Layer9_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_1298 -> node_1299 [style=dashed];
  node_1300 [shape=ellipse, label="Layer9_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1299 -> node_1300 [style=dashed];
  node_1301 [shape=parallelogram, label="Layer9_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1302 [shape=box, label="Layer9_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1303 [shape=box, label="Layer9_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1304 [shape=box, label="Layer9_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1305 [shape=box, label="Layer9_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1306 [shape=box, label="Layer9_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1307 [shape=box, label="Layer9_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1308 [shape=ellipse, label="Layer9_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1301;
  node_1301 -> node_1302;
  node_1301 -> node_1303;
  node_1302 -> node_1304;
  node_1303 -> node_1305;
  node_1304 -> node_1306;
  node_1305 -> node_1307;
  node_1306 -> node_1308 [style=dashed];
  node_1307 -> node_1308 [style=dashed];
  node_1309 [shape=parallelogram, label="Layer9_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1310 [shape=box, label="Layer9_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1311 [shape=box, label="Layer9_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1312 [shape=box, label="Layer9_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1313 [shape=box, label="Layer9_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1314 [shape=box, label="Layer9_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1315 [shape=box, label="Layer9_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1316 [shape=ellipse, label="Layer9_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1309;
  node_1309 -> node_1310;
  node_1309 -> node_1311;
  node_1310 -> node_1312;
  node_1311 -> node_1313;
  node_1312 -> node_1314;
  node_1313 -> node_1315;
  node_1314 -> node_1316 [style=dashed];
  node_1315 -> node_1316 [style=dashed];
  node_1317 [shape=parallelogram, label="Layer9_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1318 [shape=box, label="Layer9_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1319 [shape=box, label="Layer9_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1320 [shape=box, label="Layer9_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1321 [shape=box, label="Layer9_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1322 [shape=box, label="Layer9_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1323 [shape=box, label="Layer9_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1324 [shape=ellipse, label="Layer9_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1317;
  node_1317 -> node_1318;
  node_1317 -> node_1319;
  node_1318 -> node_1320;
  node_1319 -> node_1321;
  node_1320 -> node_1322;
  node_1321 -> node_1323;
  node_1322 -> node_1324 [style=dashed];
  node_1323 -> node_1324 [style=dashed];
  node_1325 [shape=parallelogram, label="Layer9_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1326 [shape=box, label="Layer9_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1327 [shape=box, label="Layer9_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1328 [shape=box, label="Layer9_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1329 [shape=box, label="Layer9_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1330 [shape=box, label="Layer9_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1331 [shape=box, label="Layer9_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1332 [shape=ellipse, label="Layer9_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1325;
  node_1325 -> node_1326;
  node_1325 -> node_1327;
  node_1326 -> node_1328;
  node_1327 -> node_1329;
  node_1328 -> node_1330;
  node_1329 -> node_1331;
  node_1330 -> node_1332 [style=dashed];
  node_1331 -> node_1332 [style=dashed];
  node_1333 [shape=parallelogram, label="Layer9_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1334 [shape=box, label="Layer9_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1335 [shape=box, label="Layer9_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1336 [shape=box, label="Layer9_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1337 [shape=box, label="Layer9_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1338 [shape=box, label="Layer9_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1339 [shape=box, label="Layer9_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1340 [shape=ellipse, label="Layer9_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1333;
  node_1333 -> node_1334;
  node_1333 -> node_1335;
  node_1334 -> node_1336;
  node_1335 -> node_1337;
  node_1336 -> node_1338;
  node_1337 -> node_1339;
  node_1338 -> node_1340 [style=dashed];
  node_1339 -> node_1340 [style=dashed];
  node_1341 [shape=parallelogram, label="Layer9_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1342 [shape=box, label="Layer9_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1343 [shape=box, label="Layer9_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1344 [shape=box, label="Layer9_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1345 [shape=box, label="Layer9_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1346 [shape=box, label="Layer9_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1347 [shape=box, label="Layer9_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1348 [shape=ellipse, label="Layer9_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1341;
  node_1341 -> node_1342;
  node_1341 -> node_1343;
  node_1342 -> node_1344;
  node_1343 -> node_1345;
  node_1344 -> node_1346;
  node_1345 -> node_1347;
  node_1346 -> node_1348 [style=dashed];
  node_1347 -> node_1348 [style=dashed];
  node_1349 [shape=parallelogram, label="Layer9_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1350 [shape=box, label="Layer9_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1351 [shape=box, label="Layer9_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1352 [shape=box, label="Layer9_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1353 [shape=box, label="Layer9_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1354 [shape=box, label="Layer9_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1355 [shape=box, label="Layer9_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1356 [shape=ellipse, label="Layer9_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1349;
  node_1349 -> node_1350;
  node_1349 -> node_1351;
  node_1350 -> node_1352;
  node_1351 -> node_1353;
  node_1352 -> node_1354;
  node_1353 -> node_1355;
  node_1354 -> node_1356 [style=dashed];
  node_1355 -> node_1356 [style=dashed];
  node_1357 [shape=parallelogram, label="Layer9_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1358 [shape=box, label="Layer9_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1359 [shape=box, label="Layer9_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1360 [shape=box, label="Layer9_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1361 [shape=box, label="Layer9_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1362 [shape=box, label="Layer9_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1363 [shape=box, label="Layer9_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1364 [shape=ellipse, label="Layer9_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1357;
  node_1357 -> node_1358;
  node_1357 -> node_1359;
  node_1358 -> node_1360;
  node_1359 -> node_1361;
  node_1360 -> node_1362;
  node_1361 -> node_1363;
  node_1362 -> node_1364 [style=dashed];
  node_1363 -> node_1364 [style=dashed];
  node_1365 [shape=parallelogram, label="Layer9_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1366 [shape=box, label="Layer9_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1367 [shape=box, label="Layer9_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1368 [shape=box, label="Layer9_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1369 [shape=box, label="Layer9_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1370 [shape=box, label="Layer9_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1371 [shape=box, label="Layer9_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1372 [shape=ellipse, label="Layer9_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1365;
  node_1365 -> node_1366;
  node_1365 -> node_1367;
  node_1366 -> node_1368;
  node_1367 -> node_1369;
  node_1368 -> node_1370;
  node_1369 -> node_1371;
  node_1370 -> node_1372 [style=dashed];
  node_1371 -> node_1372 [style=dashed];
  node_1373 [shape=parallelogram, label="Layer9_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1374 [shape=box, label="Layer9_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1375 [shape=box, label="Layer9_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1376 [shape=box, label="Layer9_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1377 [shape=box, label="Layer9_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1378 [shape=box, label="Layer9_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1379 [shape=box, label="Layer9_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1380 [shape=ellipse, label="Layer9_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1373;
  node_1373 -> node_1374;
  node_1373 -> node_1375;
  node_1374 -> node_1376;
  node_1375 -> node_1377;
  node_1376 -> node_1378;
  node_1377 -> node_1379;
  node_1378 -> node_1380 [style=dashed];
  node_1379 -> node_1380 [style=dashed];
  node_1381 [shape=parallelogram, label="Layer9_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1382 [shape=box, label="Layer9_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1383 [shape=box, label="Layer9_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1384 [shape=box, label="Layer9_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1385 [shape=box, label="Layer9_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1386 [shape=box, label="Layer9_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1387 [shape=box, label="Layer9_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1388 [shape=ellipse, label="Layer9_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1381;
  node_1381 -> node_1382;
  node_1381 -> node_1383;
  node_1382 -> node_1384;
  node_1383 -> node_1385;
  node_1384 -> node_1386;
  node_1385 -> node_1387;
  node_1386 -> node_1388 [style=dashed];
  node_1387 -> node_1388 [style=dashed];
  node_1389 [shape=parallelogram, label="Layer9_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1390 [shape=box, label="Layer9_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1391 [shape=box, label="Layer9_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1392 [shape=box, label="Layer9_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1393 [shape=box, label="Layer9_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1394 [shape=box, label="Layer9_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1395 [shape=box, label="Layer9_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1396 [shape=ellipse, label="Layer9_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1389;
  node_1389 -> node_1390;
  node_1389 -> node_1391;
  node_1390 -> node_1392;
  node_1391 -> node_1393;
  node_1392 -> node_1394;
  node_1393 -> node_1395;
  node_1394 -> node_1396 [style=dashed];
  node_1395 -> node_1396 [style=dashed];
  node_1397 [shape=parallelogram, label="Layer9_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1398 [shape=box, label="Layer9_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1399 [shape=box, label="Layer9_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1400 [shape=box, label="Layer9_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1401 [shape=box, label="Layer9_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1402 [shape=box, label="Layer9_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1403 [shape=box, label="Layer9_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1404 [shape=ellipse, label="Layer9_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1397;
  node_1397 -> node_1398;
  node_1397 -> node_1399;
  node_1398 -> node_1400;
  node_1399 -> node_1401;
  node_1400 -> node_1402;
  node_1401 -> node_1403;
  node_1402 -> node_1404 [style=dashed];
  node_1403 -> node_1404 [style=dashed];
  node_1405 [shape=parallelogram, label="Layer9_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1406 [shape=box, label="Layer9_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1407 [shape=box, label="Layer9_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1408 [shape=box, label="Layer9_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1409 [shape=box, label="Layer9_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1410 [shape=box, label="Layer9_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1411 [shape=box, label="Layer9_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1412 [shape=ellipse, label="Layer9_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1405;
  node_1405 -> node_1406;
  node_1405 -> node_1407;
  node_1406 -> node_1408;
  node_1407 -> node_1409;
  node_1408 -> node_1410;
  node_1409 -> node_1411;
  node_1410 -> node_1412 [style=dashed];
  node_1411 -> node_1412 [style=dashed];
  node_1413 [shape=parallelogram, label="Layer9_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1414 [shape=box, label="Layer9_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1415 [shape=box, label="Layer9_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1416 [shape=box, label="Layer9_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1417 [shape=box, label="Layer9_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1418 [shape=box, label="Layer9_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1419 [shape=box, label="Layer9_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1420 [shape=ellipse, label="Layer9_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1413;
  node_1413 -> node_1414;
  node_1413 -> node_1415;
  node_1414 -> node_1416;
  node_1415 -> node_1417;
  node_1416 -> node_1418;
  node_1417 -> node_1419;
  node_1418 -> node_1420 [style=dashed];
  node_1419 -> node_1420 [style=dashed];
  node_1421 [shape=parallelogram, label="Layer9_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1422 [shape=box, label="Layer9_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1423 [shape=box, label="Layer9_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1424 [shape=box, label="Layer9_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1425 [shape=box, label="Layer9_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1426 [shape=box, label="Layer9_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1427 [shape=box, label="Layer9_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1428 [shape=ellipse, label="Layer9_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1300 -> node_1421;
  node_1421 -> node_1422;
  node_1421 -> node_1423;
  node_1422 -> node_1424;
  node_1423 -> node_1425;
  node_1424 -> node_1426;
  node_1425 -> node_1427;
  node_1426 -> node_1428 [style=dashed];
  node_1427 -> node_1428 [style=dashed];
  node_1429 [shape=ellipse, label="Layer9_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1308 -> node_1429 [style=dashed];
  node_1316 -> node_1429 [style=dashed];
  node_1324 -> node_1429 [style=dashed];
  node_1332 -> node_1429 [style=dashed];
  node_1340 -> node_1429 [style=dashed];
  node_1348 -> node_1429 [style=dashed];
  node_1356 -> node_1429 [style=dashed];
  node_1364 -> node_1429 [style=dashed];
  node_1372 -> node_1429 [style=dashed];
  node_1380 -> node_1429 [style=dashed];
  node_1388 -> node_1429 [style=dashed];
  node_1396 -> node_1429 [style=dashed];
  node_1404 -> node_1429 [style=dashed];
  node_1412 -> node_1429 [style=dashed];
  node_1420 -> node_1429 [style=dashed];
  node_1428 -> node_1429 [style=dashed];
  node_1430 [shape=box, label="Layer9_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1429 -> node_1430;
  node_1431 [shape=parallelogram, label="Layer9_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1297 -> node_1431;
  node_1430 -> node_1431;
  
  // ===== Layer 10 =====
  node_1432 [shape=box, label="Layer10_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1431 -> node_1432;
  node_1433 [shape=box, label="Layer10_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1434 [shape=box, label="Layer10_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1432 -> node_1433;
  node_1432 -> node_1434;
  node_1435 [shape=box, label="Layer10_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1436 [shape=box, label="Layer10_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1433 -> node_1435;
  node_1434 -> node_1436;
  node_1437 [shape=box, label="Layer10_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1438 [shape=box, label="Layer10_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1435 -> node_1437;
  node_1436 -> node_1438;
  node_1439 [shape=ellipse, label="Layer10_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1437 -> node_1439 [style=dashed];
  node_1438 -> node_1439 [style=dashed];
  node_1440 [shape=parallelogram, label="Layer10_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1431 -> node_1440;
  node_1439 -> node_1440;
  node_1441 [shape=box, label="Layer10_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1440 -> node_1441;
  node_1442 [shape=parallelogram, label="Layer10_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_1441 -> node_1442 [style=dashed];
  node_1443 [shape=ellipse, label="Layer10_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1442 -> node_1443 [style=dashed];
  node_1444 [shape=parallelogram, label="Layer10_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1445 [shape=box, label="Layer10_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1446 [shape=box, label="Layer10_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1447 [shape=box, label="Layer10_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1448 [shape=box, label="Layer10_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1449 [shape=box, label="Layer10_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1450 [shape=box, label="Layer10_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1451 [shape=ellipse, label="Layer10_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1444;
  node_1444 -> node_1445;
  node_1444 -> node_1446;
  node_1445 -> node_1447;
  node_1446 -> node_1448;
  node_1447 -> node_1449;
  node_1448 -> node_1450;
  node_1449 -> node_1451 [style=dashed];
  node_1450 -> node_1451 [style=dashed];
  node_1452 [shape=parallelogram, label="Layer10_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1453 [shape=box, label="Layer10_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1454 [shape=box, label="Layer10_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1455 [shape=box, label="Layer10_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1456 [shape=box, label="Layer10_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1457 [shape=box, label="Layer10_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1458 [shape=box, label="Layer10_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1459 [shape=ellipse, label="Layer10_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1452;
  node_1452 -> node_1453;
  node_1452 -> node_1454;
  node_1453 -> node_1455;
  node_1454 -> node_1456;
  node_1455 -> node_1457;
  node_1456 -> node_1458;
  node_1457 -> node_1459 [style=dashed];
  node_1458 -> node_1459 [style=dashed];
  node_1460 [shape=parallelogram, label="Layer10_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1461 [shape=box, label="Layer10_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1462 [shape=box, label="Layer10_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1463 [shape=box, label="Layer10_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1464 [shape=box, label="Layer10_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1465 [shape=box, label="Layer10_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1466 [shape=box, label="Layer10_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1467 [shape=ellipse, label="Layer10_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1460;
  node_1460 -> node_1461;
  node_1460 -> node_1462;
  node_1461 -> node_1463;
  node_1462 -> node_1464;
  node_1463 -> node_1465;
  node_1464 -> node_1466;
  node_1465 -> node_1467 [style=dashed];
  node_1466 -> node_1467 [style=dashed];
  node_1468 [shape=parallelogram, label="Layer10_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1469 [shape=box, label="Layer10_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1470 [shape=box, label="Layer10_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1471 [shape=box, label="Layer10_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1472 [shape=box, label="Layer10_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1473 [shape=box, label="Layer10_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1474 [shape=box, label="Layer10_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1475 [shape=ellipse, label="Layer10_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1468;
  node_1468 -> node_1469;
  node_1468 -> node_1470;
  node_1469 -> node_1471;
  node_1470 -> node_1472;
  node_1471 -> node_1473;
  node_1472 -> node_1474;
  node_1473 -> node_1475 [style=dashed];
  node_1474 -> node_1475 [style=dashed];
  node_1476 [shape=parallelogram, label="Layer10_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1477 [shape=box, label="Layer10_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1478 [shape=box, label="Layer10_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1479 [shape=box, label="Layer10_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1480 [shape=box, label="Layer10_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1481 [shape=box, label="Layer10_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1482 [shape=box, label="Layer10_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1483 [shape=ellipse, label="Layer10_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1476;
  node_1476 -> node_1477;
  node_1476 -> node_1478;
  node_1477 -> node_1479;
  node_1478 -> node_1480;
  node_1479 -> node_1481;
  node_1480 -> node_1482;
  node_1481 -> node_1483 [style=dashed];
  node_1482 -> node_1483 [style=dashed];
  node_1484 [shape=parallelogram, label="Layer10_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1485 [shape=box, label="Layer10_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1486 [shape=box, label="Layer10_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1487 [shape=box, label="Layer10_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1488 [shape=box, label="Layer10_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1489 [shape=box, label="Layer10_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1490 [shape=box, label="Layer10_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1491 [shape=ellipse, label="Layer10_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1484;
  node_1484 -> node_1485;
  node_1484 -> node_1486;
  node_1485 -> node_1487;
  node_1486 -> node_1488;
  node_1487 -> node_1489;
  node_1488 -> node_1490;
  node_1489 -> node_1491 [style=dashed];
  node_1490 -> node_1491 [style=dashed];
  node_1492 [shape=parallelogram, label="Layer10_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1493 [shape=box, label="Layer10_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1494 [shape=box, label="Layer10_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1495 [shape=box, label="Layer10_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1496 [shape=box, label="Layer10_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1497 [shape=box, label="Layer10_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1498 [shape=box, label="Layer10_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1499 [shape=ellipse, label="Layer10_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1492;
  node_1492 -> node_1493;
  node_1492 -> node_1494;
  node_1493 -> node_1495;
  node_1494 -> node_1496;
  node_1495 -> node_1497;
  node_1496 -> node_1498;
  node_1497 -> node_1499 [style=dashed];
  node_1498 -> node_1499 [style=dashed];
  node_1500 [shape=parallelogram, label="Layer10_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1501 [shape=box, label="Layer10_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1502 [shape=box, label="Layer10_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1503 [shape=box, label="Layer10_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1504 [shape=box, label="Layer10_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1505 [shape=box, label="Layer10_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1506 [shape=box, label="Layer10_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1507 [shape=ellipse, label="Layer10_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1500;
  node_1500 -> node_1501;
  node_1500 -> node_1502;
  node_1501 -> node_1503;
  node_1502 -> node_1504;
  node_1503 -> node_1505;
  node_1504 -> node_1506;
  node_1505 -> node_1507 [style=dashed];
  node_1506 -> node_1507 [style=dashed];
  node_1508 [shape=parallelogram, label="Layer10_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1509 [shape=box, label="Layer10_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1510 [shape=box, label="Layer10_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1511 [shape=box, label="Layer10_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1512 [shape=box, label="Layer10_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1513 [shape=box, label="Layer10_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1514 [shape=box, label="Layer10_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1515 [shape=ellipse, label="Layer10_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1508;
  node_1508 -> node_1509;
  node_1508 -> node_1510;
  node_1509 -> node_1511;
  node_1510 -> node_1512;
  node_1511 -> node_1513;
  node_1512 -> node_1514;
  node_1513 -> node_1515 [style=dashed];
  node_1514 -> node_1515 [style=dashed];
  node_1516 [shape=parallelogram, label="Layer10_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1517 [shape=box, label="Layer10_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1518 [shape=box, label="Layer10_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1519 [shape=box, label="Layer10_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1520 [shape=box, label="Layer10_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1521 [shape=box, label="Layer10_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1522 [shape=box, label="Layer10_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1523 [shape=ellipse, label="Layer10_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1516;
  node_1516 -> node_1517;
  node_1516 -> node_1518;
  node_1517 -> node_1519;
  node_1518 -> node_1520;
  node_1519 -> node_1521;
  node_1520 -> node_1522;
  node_1521 -> node_1523 [style=dashed];
  node_1522 -> node_1523 [style=dashed];
  node_1524 [shape=parallelogram, label="Layer10_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1525 [shape=box, label="Layer10_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1526 [shape=box, label="Layer10_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1527 [shape=box, label="Layer10_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1528 [shape=box, label="Layer10_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1529 [shape=box, label="Layer10_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1530 [shape=box, label="Layer10_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1531 [shape=ellipse, label="Layer10_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1524;
  node_1524 -> node_1525;
  node_1524 -> node_1526;
  node_1525 -> node_1527;
  node_1526 -> node_1528;
  node_1527 -> node_1529;
  node_1528 -> node_1530;
  node_1529 -> node_1531 [style=dashed];
  node_1530 -> node_1531 [style=dashed];
  node_1532 [shape=parallelogram, label="Layer10_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1533 [shape=box, label="Layer10_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1534 [shape=box, label="Layer10_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1535 [shape=box, label="Layer10_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1536 [shape=box, label="Layer10_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1537 [shape=box, label="Layer10_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1538 [shape=box, label="Layer10_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1539 [shape=ellipse, label="Layer10_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1532;
  node_1532 -> node_1533;
  node_1532 -> node_1534;
  node_1533 -> node_1535;
  node_1534 -> node_1536;
  node_1535 -> node_1537;
  node_1536 -> node_1538;
  node_1537 -> node_1539 [style=dashed];
  node_1538 -> node_1539 [style=dashed];
  node_1540 [shape=parallelogram, label="Layer10_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1541 [shape=box, label="Layer10_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1542 [shape=box, label="Layer10_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1543 [shape=box, label="Layer10_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1544 [shape=box, label="Layer10_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1545 [shape=box, label="Layer10_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1546 [shape=box, label="Layer10_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1547 [shape=ellipse, label="Layer10_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1540;
  node_1540 -> node_1541;
  node_1540 -> node_1542;
  node_1541 -> node_1543;
  node_1542 -> node_1544;
  node_1543 -> node_1545;
  node_1544 -> node_1546;
  node_1545 -> node_1547 [style=dashed];
  node_1546 -> node_1547 [style=dashed];
  node_1548 [shape=parallelogram, label="Layer10_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1549 [shape=box, label="Layer10_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1550 [shape=box, label="Layer10_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1551 [shape=box, label="Layer10_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1552 [shape=box, label="Layer10_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1553 [shape=box, label="Layer10_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1554 [shape=box, label="Layer10_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1555 [shape=ellipse, label="Layer10_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1548;
  node_1548 -> node_1549;
  node_1548 -> node_1550;
  node_1549 -> node_1551;
  node_1550 -> node_1552;
  node_1551 -> node_1553;
  node_1552 -> node_1554;
  node_1553 -> node_1555 [style=dashed];
  node_1554 -> node_1555 [style=dashed];
  node_1556 [shape=parallelogram, label="Layer10_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1557 [shape=box, label="Layer10_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1558 [shape=box, label="Layer10_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1559 [shape=box, label="Layer10_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1560 [shape=box, label="Layer10_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1561 [shape=box, label="Layer10_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1562 [shape=box, label="Layer10_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1563 [shape=ellipse, label="Layer10_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1556;
  node_1556 -> node_1557;
  node_1556 -> node_1558;
  node_1557 -> node_1559;
  node_1558 -> node_1560;
  node_1559 -> node_1561;
  node_1560 -> node_1562;
  node_1561 -> node_1563 [style=dashed];
  node_1562 -> node_1563 [style=dashed];
  node_1564 [shape=parallelogram, label="Layer10_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1565 [shape=box, label="Layer10_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1566 [shape=box, label="Layer10_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1567 [shape=box, label="Layer10_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1568 [shape=box, label="Layer10_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1569 [shape=box, label="Layer10_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1570 [shape=box, label="Layer10_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1571 [shape=ellipse, label="Layer10_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1443 -> node_1564;
  node_1564 -> node_1565;
  node_1564 -> node_1566;
  node_1565 -> node_1567;
  node_1566 -> node_1568;
  node_1567 -> node_1569;
  node_1568 -> node_1570;
  node_1569 -> node_1571 [style=dashed];
  node_1570 -> node_1571 [style=dashed];
  node_1572 [shape=ellipse, label="Layer10_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1451 -> node_1572 [style=dashed];
  node_1459 -> node_1572 [style=dashed];
  node_1467 -> node_1572 [style=dashed];
  node_1475 -> node_1572 [style=dashed];
  node_1483 -> node_1572 [style=dashed];
  node_1491 -> node_1572 [style=dashed];
  node_1499 -> node_1572 [style=dashed];
  node_1507 -> node_1572 [style=dashed];
  node_1515 -> node_1572 [style=dashed];
  node_1523 -> node_1572 [style=dashed];
  node_1531 -> node_1572 [style=dashed];
  node_1539 -> node_1572 [style=dashed];
  node_1547 -> node_1572 [style=dashed];
  node_1555 -> node_1572 [style=dashed];
  node_1563 -> node_1572 [style=dashed];
  node_1571 -> node_1572 [style=dashed];
  node_1573 [shape=box, label="Layer10_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1572 -> node_1573;
  node_1574 [shape=parallelogram, label="Layer10_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1440 -> node_1574;
  node_1573 -> node_1574;
  
  // ===== Layer 11 =====
  node_1575 [shape=box, label="Layer11_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1574 -> node_1575;
  node_1576 [shape=box, label="Layer11_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1577 [shape=box, label="Layer11_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1575 -> node_1576;
  node_1575 -> node_1577;
  node_1578 [shape=box, label="Layer11_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1579 [shape=box, label="Layer11_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1576 -> node_1578;
  node_1577 -> node_1579;
  node_1580 [shape=box, label="Layer11_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1581 [shape=box, label="Layer11_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1578 -> node_1580;
  node_1579 -> node_1581;
  node_1582 [shape=ellipse, label="Layer11_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1580 -> node_1582 [style=dashed];
  node_1581 -> node_1582 [style=dashed];
  node_1583 [shape=parallelogram, label="Layer11_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1574 -> node_1583;
  node_1582 -> node_1583;
  node_1584 [shape=box, label="Layer11_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1583 -> node_1584;
  node_1585 [shape=parallelogram, label="Layer11_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_1584 -> node_1585 [style=dashed];
  node_1586 [shape=ellipse, label="Layer11_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1585 -> node_1586 [style=dashed];
  node_1587 [shape=parallelogram, label="Layer11_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1588 [shape=box, label="Layer11_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1589 [shape=box, label="Layer11_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1590 [shape=box, label="Layer11_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1591 [shape=box, label="Layer11_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1592 [shape=box, label="Layer11_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1593 [shape=box, label="Layer11_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1594 [shape=ellipse, label="Layer11_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1587;
  node_1587 -> node_1588;
  node_1587 -> node_1589;
  node_1588 -> node_1590;
  node_1589 -> node_1591;
  node_1590 -> node_1592;
  node_1591 -> node_1593;
  node_1592 -> node_1594 [style=dashed];
  node_1593 -> node_1594 [style=dashed];
  node_1595 [shape=parallelogram, label="Layer11_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1596 [shape=box, label="Layer11_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1597 [shape=box, label="Layer11_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1598 [shape=box, label="Layer11_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1599 [shape=box, label="Layer11_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1600 [shape=box, label="Layer11_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1601 [shape=box, label="Layer11_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1602 [shape=ellipse, label="Layer11_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1595;
  node_1595 -> node_1596;
  node_1595 -> node_1597;
  node_1596 -> node_1598;
  node_1597 -> node_1599;
  node_1598 -> node_1600;
  node_1599 -> node_1601;
  node_1600 -> node_1602 [style=dashed];
  node_1601 -> node_1602 [style=dashed];
  node_1603 [shape=parallelogram, label="Layer11_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1604 [shape=box, label="Layer11_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1605 [shape=box, label="Layer11_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1606 [shape=box, label="Layer11_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1607 [shape=box, label="Layer11_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1608 [shape=box, label="Layer11_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1609 [shape=box, label="Layer11_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1610 [shape=ellipse, label="Layer11_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1603;
  node_1603 -> node_1604;
  node_1603 -> node_1605;
  node_1604 -> node_1606;
  node_1605 -> node_1607;
  node_1606 -> node_1608;
  node_1607 -> node_1609;
  node_1608 -> node_1610 [style=dashed];
  node_1609 -> node_1610 [style=dashed];
  node_1611 [shape=parallelogram, label="Layer11_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1612 [shape=box, label="Layer11_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1613 [shape=box, label="Layer11_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1614 [shape=box, label="Layer11_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1615 [shape=box, label="Layer11_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1616 [shape=box, label="Layer11_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1617 [shape=box, label="Layer11_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1618 [shape=ellipse, label="Layer11_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1611;
  node_1611 -> node_1612;
  node_1611 -> node_1613;
  node_1612 -> node_1614;
  node_1613 -> node_1615;
  node_1614 -> node_1616;
  node_1615 -> node_1617;
  node_1616 -> node_1618 [style=dashed];
  node_1617 -> node_1618 [style=dashed];
  node_1619 [shape=parallelogram, label="Layer11_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1620 [shape=box, label="Layer11_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1621 [shape=box, label="Layer11_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1622 [shape=box, label="Layer11_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1623 [shape=box, label="Layer11_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1624 [shape=box, label="Layer11_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1625 [shape=box, label="Layer11_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1626 [shape=ellipse, label="Layer11_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1619;
  node_1619 -> node_1620;
  node_1619 -> node_1621;
  node_1620 -> node_1622;
  node_1621 -> node_1623;
  node_1622 -> node_1624;
  node_1623 -> node_1625;
  node_1624 -> node_1626 [style=dashed];
  node_1625 -> node_1626 [style=dashed];
  node_1627 [shape=parallelogram, label="Layer11_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1628 [shape=box, label="Layer11_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1629 [shape=box, label="Layer11_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1630 [shape=box, label="Layer11_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1631 [shape=box, label="Layer11_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1632 [shape=box, label="Layer11_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1633 [shape=box, label="Layer11_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1634 [shape=ellipse, label="Layer11_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1627;
  node_1627 -> node_1628;
  node_1627 -> node_1629;
  node_1628 -> node_1630;
  node_1629 -> node_1631;
  node_1630 -> node_1632;
  node_1631 -> node_1633;
  node_1632 -> node_1634 [style=dashed];
  node_1633 -> node_1634 [style=dashed];
  node_1635 [shape=parallelogram, label="Layer11_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1636 [shape=box, label="Layer11_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1637 [shape=box, label="Layer11_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1638 [shape=box, label="Layer11_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1639 [shape=box, label="Layer11_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1640 [shape=box, label="Layer11_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1641 [shape=box, label="Layer11_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1642 [shape=ellipse, label="Layer11_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1635;
  node_1635 -> node_1636;
  node_1635 -> node_1637;
  node_1636 -> node_1638;
  node_1637 -> node_1639;
  node_1638 -> node_1640;
  node_1639 -> node_1641;
  node_1640 -> node_1642 [style=dashed];
  node_1641 -> node_1642 [style=dashed];
  node_1643 [shape=parallelogram, label="Layer11_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1644 [shape=box, label="Layer11_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1645 [shape=box, label="Layer11_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1646 [shape=box, label="Layer11_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1647 [shape=box, label="Layer11_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1648 [shape=box, label="Layer11_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1649 [shape=box, label="Layer11_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1650 [shape=ellipse, label="Layer11_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1643;
  node_1643 -> node_1644;
  node_1643 -> node_1645;
  node_1644 -> node_1646;
  node_1645 -> node_1647;
  node_1646 -> node_1648;
  node_1647 -> node_1649;
  node_1648 -> node_1650 [style=dashed];
  node_1649 -> node_1650 [style=dashed];
  node_1651 [shape=parallelogram, label="Layer11_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1652 [shape=box, label="Layer11_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1653 [shape=box, label="Layer11_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1654 [shape=box, label="Layer11_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1655 [shape=box, label="Layer11_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1656 [shape=box, label="Layer11_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1657 [shape=box, label="Layer11_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1658 [shape=ellipse, label="Layer11_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1651;
  node_1651 -> node_1652;
  node_1651 -> node_1653;
  node_1652 -> node_1654;
  node_1653 -> node_1655;
  node_1654 -> node_1656;
  node_1655 -> node_1657;
  node_1656 -> node_1658 [style=dashed];
  node_1657 -> node_1658 [style=dashed];
  node_1659 [shape=parallelogram, label="Layer11_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1660 [shape=box, label="Layer11_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1661 [shape=box, label="Layer11_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1662 [shape=box, label="Layer11_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1663 [shape=box, label="Layer11_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1664 [shape=box, label="Layer11_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1665 [shape=box, label="Layer11_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1666 [shape=ellipse, label="Layer11_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1659;
  node_1659 -> node_1660;
  node_1659 -> node_1661;
  node_1660 -> node_1662;
  node_1661 -> node_1663;
  node_1662 -> node_1664;
  node_1663 -> node_1665;
  node_1664 -> node_1666 [style=dashed];
  node_1665 -> node_1666 [style=dashed];
  node_1667 [shape=parallelogram, label="Layer11_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1668 [shape=box, label="Layer11_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1669 [shape=box, label="Layer11_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1670 [shape=box, label="Layer11_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1671 [shape=box, label="Layer11_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1672 [shape=box, label="Layer11_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1673 [shape=box, label="Layer11_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1674 [shape=ellipse, label="Layer11_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1667;
  node_1667 -> node_1668;
  node_1667 -> node_1669;
  node_1668 -> node_1670;
  node_1669 -> node_1671;
  node_1670 -> node_1672;
  node_1671 -> node_1673;
  node_1672 -> node_1674 [style=dashed];
  node_1673 -> node_1674 [style=dashed];
  node_1675 [shape=parallelogram, label="Layer11_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1676 [shape=box, label="Layer11_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1677 [shape=box, label="Layer11_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1678 [shape=box, label="Layer11_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1679 [shape=box, label="Layer11_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1680 [shape=box, label="Layer11_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1681 [shape=box, label="Layer11_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1682 [shape=ellipse, label="Layer11_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1675;
  node_1675 -> node_1676;
  node_1675 -> node_1677;
  node_1676 -> node_1678;
  node_1677 -> node_1679;
  node_1678 -> node_1680;
  node_1679 -> node_1681;
  node_1680 -> node_1682 [style=dashed];
  node_1681 -> node_1682 [style=dashed];
  node_1683 [shape=parallelogram, label="Layer11_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1684 [shape=box, label="Layer11_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1685 [shape=box, label="Layer11_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1686 [shape=box, label="Layer11_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1687 [shape=box, label="Layer11_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1688 [shape=box, label="Layer11_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1689 [shape=box, label="Layer11_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1690 [shape=ellipse, label="Layer11_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1683;
  node_1683 -> node_1684;
  node_1683 -> node_1685;
  node_1684 -> node_1686;
  node_1685 -> node_1687;
  node_1686 -> node_1688;
  node_1687 -> node_1689;
  node_1688 -> node_1690 [style=dashed];
  node_1689 -> node_1690 [style=dashed];
  node_1691 [shape=parallelogram, label="Layer11_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1692 [shape=box, label="Layer11_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1693 [shape=box, label="Layer11_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1694 [shape=box, label="Layer11_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1695 [shape=box, label="Layer11_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1696 [shape=box, label="Layer11_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1697 [shape=box, label="Layer11_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1698 [shape=ellipse, label="Layer11_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1691;
  node_1691 -> node_1692;
  node_1691 -> node_1693;
  node_1692 -> node_1694;
  node_1693 -> node_1695;
  node_1694 -> node_1696;
  node_1695 -> node_1697;
  node_1696 -> node_1698 [style=dashed];
  node_1697 -> node_1698 [style=dashed];
  node_1699 [shape=parallelogram, label="Layer11_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1700 [shape=box, label="Layer11_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1701 [shape=box, label="Layer11_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1702 [shape=box, label="Layer11_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1703 [shape=box, label="Layer11_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1704 [shape=box, label="Layer11_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1705 [shape=box, label="Layer11_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1706 [shape=ellipse, label="Layer11_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1699;
  node_1699 -> node_1700;
  node_1699 -> node_1701;
  node_1700 -> node_1702;
  node_1701 -> node_1703;
  node_1702 -> node_1704;
  node_1703 -> node_1705;
  node_1704 -> node_1706 [style=dashed];
  node_1705 -> node_1706 [style=dashed];
  node_1707 [shape=parallelogram, label="Layer11_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1708 [shape=box, label="Layer11_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1709 [shape=box, label="Layer11_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1710 [shape=box, label="Layer11_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1711 [shape=box, label="Layer11_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1712 [shape=box, label="Layer11_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1713 [shape=box, label="Layer11_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1714 [shape=ellipse, label="Layer11_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1586 -> node_1707;
  node_1707 -> node_1708;
  node_1707 -> node_1709;
  node_1708 -> node_1710;
  node_1709 -> node_1711;
  node_1710 -> node_1712;
  node_1711 -> node_1713;
  node_1712 -> node_1714 [style=dashed];
  node_1713 -> node_1714 [style=dashed];
  node_1715 [shape=ellipse, label="Layer11_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1594 -> node_1715 [style=dashed];
  node_1602 -> node_1715 [style=dashed];
  node_1610 -> node_1715 [style=dashed];
  node_1618 -> node_1715 [style=dashed];
  node_1626 -> node_1715 [style=dashed];
  node_1634 -> node_1715 [style=dashed];
  node_1642 -> node_1715 [style=dashed];
  node_1650 -> node_1715 [style=dashed];
  node_1658 -> node_1715 [style=dashed];
  node_1666 -> node_1715 [style=dashed];
  node_1674 -> node_1715 [style=dashed];
  node_1682 -> node_1715 [style=dashed];
  node_1690 -> node_1715 [style=dashed];
  node_1698 -> node_1715 [style=dashed];
  node_1706 -> node_1715 [style=dashed];
  node_1714 -> node_1715 [style=dashed];
  node_1716 [shape=box, label="Layer11_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1715 -> node_1716;
  node_1717 [shape=parallelogram, label="Layer11_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1583 -> node_1717;
  node_1716 -> node_1717;
  
  // ===== Layer 12 =====
  node_1718 [shape=box, label="Layer12_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1717 -> node_1718;
  node_1719 [shape=box, label="Layer12_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1720 [shape=box, label="Layer12_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1718 -> node_1719;
  node_1718 -> node_1720;
  node_1721 [shape=box, label="Layer12_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1722 [shape=box, label="Layer12_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1719 -> node_1721;
  node_1720 -> node_1722;
  node_1723 [shape=box, label="Layer12_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1724 [shape=box, label="Layer12_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1721 -> node_1723;
  node_1722 -> node_1724;
  node_1725 [shape=ellipse, label="Layer12_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1723 -> node_1725 [style=dashed];
  node_1724 -> node_1725 [style=dashed];
  node_1726 [shape=parallelogram, label="Layer12_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1717 -> node_1726;
  node_1725 -> node_1726;
  node_1727 [shape=box, label="Layer12_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1726 -> node_1727;
  node_1728 [shape=parallelogram, label="Layer12_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_1727 -> node_1728 [style=dashed];
  node_1729 [shape=ellipse, label="Layer12_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1728 -> node_1729 [style=dashed];
  node_1730 [shape=parallelogram, label="Layer12_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1731 [shape=box, label="Layer12_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1732 [shape=box, label="Layer12_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1733 [shape=box, label="Layer12_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1734 [shape=box, label="Layer12_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1735 [shape=box, label="Layer12_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1736 [shape=box, label="Layer12_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1737 [shape=ellipse, label="Layer12_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1730;
  node_1730 -> node_1731;
  node_1730 -> node_1732;
  node_1731 -> node_1733;
  node_1732 -> node_1734;
  node_1733 -> node_1735;
  node_1734 -> node_1736;
  node_1735 -> node_1737 [style=dashed];
  node_1736 -> node_1737 [style=dashed];
  node_1738 [shape=parallelogram, label="Layer12_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1739 [shape=box, label="Layer12_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1740 [shape=box, label="Layer12_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1741 [shape=box, label="Layer12_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1742 [shape=box, label="Layer12_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1743 [shape=box, label="Layer12_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1744 [shape=box, label="Layer12_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1745 [shape=ellipse, label="Layer12_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1738;
  node_1738 -> node_1739;
  node_1738 -> node_1740;
  node_1739 -> node_1741;
  node_1740 -> node_1742;
  node_1741 -> node_1743;
  node_1742 -> node_1744;
  node_1743 -> node_1745 [style=dashed];
  node_1744 -> node_1745 [style=dashed];
  node_1746 [shape=parallelogram, label="Layer12_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1747 [shape=box, label="Layer12_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1748 [shape=box, label="Layer12_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1749 [shape=box, label="Layer12_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1750 [shape=box, label="Layer12_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1751 [shape=box, label="Layer12_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1752 [shape=box, label="Layer12_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1753 [shape=ellipse, label="Layer12_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1746;
  node_1746 -> node_1747;
  node_1746 -> node_1748;
  node_1747 -> node_1749;
  node_1748 -> node_1750;
  node_1749 -> node_1751;
  node_1750 -> node_1752;
  node_1751 -> node_1753 [style=dashed];
  node_1752 -> node_1753 [style=dashed];
  node_1754 [shape=parallelogram, label="Layer12_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1755 [shape=box, label="Layer12_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1756 [shape=box, label="Layer12_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1757 [shape=box, label="Layer12_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1758 [shape=box, label="Layer12_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1759 [shape=box, label="Layer12_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1760 [shape=box, label="Layer12_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1761 [shape=ellipse, label="Layer12_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1754;
  node_1754 -> node_1755;
  node_1754 -> node_1756;
  node_1755 -> node_1757;
  node_1756 -> node_1758;
  node_1757 -> node_1759;
  node_1758 -> node_1760;
  node_1759 -> node_1761 [style=dashed];
  node_1760 -> node_1761 [style=dashed];
  node_1762 [shape=parallelogram, label="Layer12_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1763 [shape=box, label="Layer12_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1764 [shape=box, label="Layer12_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1765 [shape=box, label="Layer12_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1766 [shape=box, label="Layer12_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1767 [shape=box, label="Layer12_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1768 [shape=box, label="Layer12_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1769 [shape=ellipse, label="Layer12_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1762;
  node_1762 -> node_1763;
  node_1762 -> node_1764;
  node_1763 -> node_1765;
  node_1764 -> node_1766;
  node_1765 -> node_1767;
  node_1766 -> node_1768;
  node_1767 -> node_1769 [style=dashed];
  node_1768 -> node_1769 [style=dashed];
  node_1770 [shape=parallelogram, label="Layer12_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1771 [shape=box, label="Layer12_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1772 [shape=box, label="Layer12_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1773 [shape=box, label="Layer12_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1774 [shape=box, label="Layer12_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1775 [shape=box, label="Layer12_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1776 [shape=box, label="Layer12_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1777 [shape=ellipse, label="Layer12_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1770;
  node_1770 -> node_1771;
  node_1770 -> node_1772;
  node_1771 -> node_1773;
  node_1772 -> node_1774;
  node_1773 -> node_1775;
  node_1774 -> node_1776;
  node_1775 -> node_1777 [style=dashed];
  node_1776 -> node_1777 [style=dashed];
  node_1778 [shape=parallelogram, label="Layer12_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1779 [shape=box, label="Layer12_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1780 [shape=box, label="Layer12_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1781 [shape=box, label="Layer12_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1782 [shape=box, label="Layer12_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1783 [shape=box, label="Layer12_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1784 [shape=box, label="Layer12_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1785 [shape=ellipse, label="Layer12_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1778;
  node_1778 -> node_1779;
  node_1778 -> node_1780;
  node_1779 -> node_1781;
  node_1780 -> node_1782;
  node_1781 -> node_1783;
  node_1782 -> node_1784;
  node_1783 -> node_1785 [style=dashed];
  node_1784 -> node_1785 [style=dashed];
  node_1786 [shape=parallelogram, label="Layer12_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1787 [shape=box, label="Layer12_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1788 [shape=box, label="Layer12_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1789 [shape=box, label="Layer12_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1790 [shape=box, label="Layer12_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1791 [shape=box, label="Layer12_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1792 [shape=box, label="Layer12_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1793 [shape=ellipse, label="Layer12_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1786;
  node_1786 -> node_1787;
  node_1786 -> node_1788;
  node_1787 -> node_1789;
  node_1788 -> node_1790;
  node_1789 -> node_1791;
  node_1790 -> node_1792;
  node_1791 -> node_1793 [style=dashed];
  node_1792 -> node_1793 [style=dashed];
  node_1794 [shape=parallelogram, label="Layer12_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1795 [shape=box, label="Layer12_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1796 [shape=box, label="Layer12_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1797 [shape=box, label="Layer12_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1798 [shape=box, label="Layer12_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1799 [shape=box, label="Layer12_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1800 [shape=box, label="Layer12_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1801 [shape=ellipse, label="Layer12_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1794;
  node_1794 -> node_1795;
  node_1794 -> node_1796;
  node_1795 -> node_1797;
  node_1796 -> node_1798;
  node_1797 -> node_1799;
  node_1798 -> node_1800;
  node_1799 -> node_1801 [style=dashed];
  node_1800 -> node_1801 [style=dashed];
  node_1802 [shape=parallelogram, label="Layer12_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1803 [shape=box, label="Layer12_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1804 [shape=box, label="Layer12_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1805 [shape=box, label="Layer12_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1806 [shape=box, label="Layer12_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1807 [shape=box, label="Layer12_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1808 [shape=box, label="Layer12_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1809 [shape=ellipse, label="Layer12_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1802;
  node_1802 -> node_1803;
  node_1802 -> node_1804;
  node_1803 -> node_1805;
  node_1804 -> node_1806;
  node_1805 -> node_1807;
  node_1806 -> node_1808;
  node_1807 -> node_1809 [style=dashed];
  node_1808 -> node_1809 [style=dashed];
  node_1810 [shape=parallelogram, label="Layer12_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1811 [shape=box, label="Layer12_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1812 [shape=box, label="Layer12_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1813 [shape=box, label="Layer12_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1814 [shape=box, label="Layer12_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1815 [shape=box, label="Layer12_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1816 [shape=box, label="Layer12_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1817 [shape=ellipse, label="Layer12_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1810;
  node_1810 -> node_1811;
  node_1810 -> node_1812;
  node_1811 -> node_1813;
  node_1812 -> node_1814;
  node_1813 -> node_1815;
  node_1814 -> node_1816;
  node_1815 -> node_1817 [style=dashed];
  node_1816 -> node_1817 [style=dashed];
  node_1818 [shape=parallelogram, label="Layer12_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1819 [shape=box, label="Layer12_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1820 [shape=box, label="Layer12_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1821 [shape=box, label="Layer12_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1822 [shape=box, label="Layer12_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1823 [shape=box, label="Layer12_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1824 [shape=box, label="Layer12_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1825 [shape=ellipse, label="Layer12_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1818;
  node_1818 -> node_1819;
  node_1818 -> node_1820;
  node_1819 -> node_1821;
  node_1820 -> node_1822;
  node_1821 -> node_1823;
  node_1822 -> node_1824;
  node_1823 -> node_1825 [style=dashed];
  node_1824 -> node_1825 [style=dashed];
  node_1826 [shape=parallelogram, label="Layer12_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1827 [shape=box, label="Layer12_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1828 [shape=box, label="Layer12_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1829 [shape=box, label="Layer12_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1830 [shape=box, label="Layer12_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1831 [shape=box, label="Layer12_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1832 [shape=box, label="Layer12_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1833 [shape=ellipse, label="Layer12_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1826;
  node_1826 -> node_1827;
  node_1826 -> node_1828;
  node_1827 -> node_1829;
  node_1828 -> node_1830;
  node_1829 -> node_1831;
  node_1830 -> node_1832;
  node_1831 -> node_1833 [style=dashed];
  node_1832 -> node_1833 [style=dashed];
  node_1834 [shape=parallelogram, label="Layer12_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1835 [shape=box, label="Layer12_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1836 [shape=box, label="Layer12_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1837 [shape=box, label="Layer12_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1838 [shape=box, label="Layer12_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1839 [shape=box, label="Layer12_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1840 [shape=box, label="Layer12_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1841 [shape=ellipse, label="Layer12_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1834;
  node_1834 -> node_1835;
  node_1834 -> node_1836;
  node_1835 -> node_1837;
  node_1836 -> node_1838;
  node_1837 -> node_1839;
  node_1838 -> node_1840;
  node_1839 -> node_1841 [style=dashed];
  node_1840 -> node_1841 [style=dashed];
  node_1842 [shape=parallelogram, label="Layer12_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1843 [shape=box, label="Layer12_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1844 [shape=box, label="Layer12_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1845 [shape=box, label="Layer12_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1846 [shape=box, label="Layer12_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1847 [shape=box, label="Layer12_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1848 [shape=box, label="Layer12_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1849 [shape=ellipse, label="Layer12_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1842;
  node_1842 -> node_1843;
  node_1842 -> node_1844;
  node_1843 -> node_1845;
  node_1844 -> node_1846;
  node_1845 -> node_1847;
  node_1846 -> node_1848;
  node_1847 -> node_1849 [style=dashed];
  node_1848 -> node_1849 [style=dashed];
  node_1850 [shape=parallelogram, label="Layer12_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1851 [shape=box, label="Layer12_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1852 [shape=box, label="Layer12_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1853 [shape=box, label="Layer12_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1854 [shape=box, label="Layer12_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1855 [shape=box, label="Layer12_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1856 [shape=box, label="Layer12_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1857 [shape=ellipse, label="Layer12_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1729 -> node_1850;
  node_1850 -> node_1851;
  node_1850 -> node_1852;
  node_1851 -> node_1853;
  node_1852 -> node_1854;
  node_1853 -> node_1855;
  node_1854 -> node_1856;
  node_1855 -> node_1857 [style=dashed];
  node_1856 -> node_1857 [style=dashed];
  node_1858 [shape=ellipse, label="Layer12_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1737 -> node_1858 [style=dashed];
  node_1745 -> node_1858 [style=dashed];
  node_1753 -> node_1858 [style=dashed];
  node_1761 -> node_1858 [style=dashed];
  node_1769 -> node_1858 [style=dashed];
  node_1777 -> node_1858 [style=dashed];
  node_1785 -> node_1858 [style=dashed];
  node_1793 -> node_1858 [style=dashed];
  node_1801 -> node_1858 [style=dashed];
  node_1809 -> node_1858 [style=dashed];
  node_1817 -> node_1858 [style=dashed];
  node_1825 -> node_1858 [style=dashed];
  node_1833 -> node_1858 [style=dashed];
  node_1841 -> node_1858 [style=dashed];
  node_1849 -> node_1858 [style=dashed];
  node_1857 -> node_1858 [style=dashed];
  node_1859 [shape=box, label="Layer12_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1858 -> node_1859;
  node_1860 [shape=parallelogram, label="Layer12_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1726 -> node_1860;
  node_1859 -> node_1860;
  
  // ===== Layer 13 =====
  node_1861 [shape=box, label="Layer13_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1860 -> node_1861;
  node_1862 [shape=box, label="Layer13_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1863 [shape=box, label="Layer13_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1861 -> node_1862;
  node_1861 -> node_1863;
  node_1864 [shape=box, label="Layer13_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1865 [shape=box, label="Layer13_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_1862 -> node_1864;
  node_1863 -> node_1865;
  node_1866 [shape=box, label="Layer13_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1867 [shape=box, label="Layer13_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_1864 -> node_1866;
  node_1865 -> node_1867;
  node_1868 [shape=ellipse, label="Layer13_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1866 -> node_1868 [style=dashed];
  node_1867 -> node_1868 [style=dashed];
  node_1869 [shape=parallelogram, label="Layer13_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1860 -> node_1869;
  node_1868 -> node_1869;
  node_1870 [shape=box, label="Layer13_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1869 -> node_1870;
  node_1871 [shape=parallelogram, label="Layer13_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_1870 -> node_1871 [style=dashed];
  node_1872 [shape=ellipse, label="Layer13_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1871 -> node_1872 [style=dashed];
  node_1873 [shape=parallelogram, label="Layer13_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1874 [shape=box, label="Layer13_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1875 [shape=box, label="Layer13_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1876 [shape=box, label="Layer13_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1877 [shape=box, label="Layer13_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1878 [shape=box, label="Layer13_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1879 [shape=box, label="Layer13_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1880 [shape=ellipse, label="Layer13_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1873;
  node_1873 -> node_1874;
  node_1873 -> node_1875;
  node_1874 -> node_1876;
  node_1875 -> node_1877;
  node_1876 -> node_1878;
  node_1877 -> node_1879;
  node_1878 -> node_1880 [style=dashed];
  node_1879 -> node_1880 [style=dashed];
  node_1881 [shape=parallelogram, label="Layer13_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1882 [shape=box, label="Layer13_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1883 [shape=box, label="Layer13_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1884 [shape=box, label="Layer13_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1885 [shape=box, label="Layer13_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1886 [shape=box, label="Layer13_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1887 [shape=box, label="Layer13_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1888 [shape=ellipse, label="Layer13_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1881;
  node_1881 -> node_1882;
  node_1881 -> node_1883;
  node_1882 -> node_1884;
  node_1883 -> node_1885;
  node_1884 -> node_1886;
  node_1885 -> node_1887;
  node_1886 -> node_1888 [style=dashed];
  node_1887 -> node_1888 [style=dashed];
  node_1889 [shape=parallelogram, label="Layer13_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1890 [shape=box, label="Layer13_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1891 [shape=box, label="Layer13_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1892 [shape=box, label="Layer13_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1893 [shape=box, label="Layer13_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1894 [shape=box, label="Layer13_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1895 [shape=box, label="Layer13_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1896 [shape=ellipse, label="Layer13_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1889;
  node_1889 -> node_1890;
  node_1889 -> node_1891;
  node_1890 -> node_1892;
  node_1891 -> node_1893;
  node_1892 -> node_1894;
  node_1893 -> node_1895;
  node_1894 -> node_1896 [style=dashed];
  node_1895 -> node_1896 [style=dashed];
  node_1897 [shape=parallelogram, label="Layer13_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1898 [shape=box, label="Layer13_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1899 [shape=box, label="Layer13_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1900 [shape=box, label="Layer13_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1901 [shape=box, label="Layer13_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1902 [shape=box, label="Layer13_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1903 [shape=box, label="Layer13_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1904 [shape=ellipse, label="Layer13_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1897;
  node_1897 -> node_1898;
  node_1897 -> node_1899;
  node_1898 -> node_1900;
  node_1899 -> node_1901;
  node_1900 -> node_1902;
  node_1901 -> node_1903;
  node_1902 -> node_1904 [style=dashed];
  node_1903 -> node_1904 [style=dashed];
  node_1905 [shape=parallelogram, label="Layer13_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1906 [shape=box, label="Layer13_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1907 [shape=box, label="Layer13_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1908 [shape=box, label="Layer13_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1909 [shape=box, label="Layer13_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1910 [shape=box, label="Layer13_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1911 [shape=box, label="Layer13_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1912 [shape=ellipse, label="Layer13_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1905;
  node_1905 -> node_1906;
  node_1905 -> node_1907;
  node_1906 -> node_1908;
  node_1907 -> node_1909;
  node_1908 -> node_1910;
  node_1909 -> node_1911;
  node_1910 -> node_1912 [style=dashed];
  node_1911 -> node_1912 [style=dashed];
  node_1913 [shape=parallelogram, label="Layer13_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1914 [shape=box, label="Layer13_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1915 [shape=box, label="Layer13_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1916 [shape=box, label="Layer13_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1917 [shape=box, label="Layer13_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1918 [shape=box, label="Layer13_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1919 [shape=box, label="Layer13_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1920 [shape=ellipse, label="Layer13_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1913;
  node_1913 -> node_1914;
  node_1913 -> node_1915;
  node_1914 -> node_1916;
  node_1915 -> node_1917;
  node_1916 -> node_1918;
  node_1917 -> node_1919;
  node_1918 -> node_1920 [style=dashed];
  node_1919 -> node_1920 [style=dashed];
  node_1921 [shape=parallelogram, label="Layer13_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1922 [shape=box, label="Layer13_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1923 [shape=box, label="Layer13_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1924 [shape=box, label="Layer13_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1925 [shape=box, label="Layer13_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1926 [shape=box, label="Layer13_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1927 [shape=box, label="Layer13_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1928 [shape=ellipse, label="Layer13_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1921;
  node_1921 -> node_1922;
  node_1921 -> node_1923;
  node_1922 -> node_1924;
  node_1923 -> node_1925;
  node_1924 -> node_1926;
  node_1925 -> node_1927;
  node_1926 -> node_1928 [style=dashed];
  node_1927 -> node_1928 [style=dashed];
  node_1929 [shape=parallelogram, label="Layer13_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1930 [shape=box, label="Layer13_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1931 [shape=box, label="Layer13_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1932 [shape=box, label="Layer13_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1933 [shape=box, label="Layer13_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1934 [shape=box, label="Layer13_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1935 [shape=box, label="Layer13_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1936 [shape=ellipse, label="Layer13_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1929;
  node_1929 -> node_1930;
  node_1929 -> node_1931;
  node_1930 -> node_1932;
  node_1931 -> node_1933;
  node_1932 -> node_1934;
  node_1933 -> node_1935;
  node_1934 -> node_1936 [style=dashed];
  node_1935 -> node_1936 [style=dashed];
  node_1937 [shape=parallelogram, label="Layer13_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1938 [shape=box, label="Layer13_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1939 [shape=box, label="Layer13_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1940 [shape=box, label="Layer13_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1941 [shape=box, label="Layer13_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1942 [shape=box, label="Layer13_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1943 [shape=box, label="Layer13_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1944 [shape=ellipse, label="Layer13_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1937;
  node_1937 -> node_1938;
  node_1937 -> node_1939;
  node_1938 -> node_1940;
  node_1939 -> node_1941;
  node_1940 -> node_1942;
  node_1941 -> node_1943;
  node_1942 -> node_1944 [style=dashed];
  node_1943 -> node_1944 [style=dashed];
  node_1945 [shape=parallelogram, label="Layer13_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1946 [shape=box, label="Layer13_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1947 [shape=box, label="Layer13_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1948 [shape=box, label="Layer13_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1949 [shape=box, label="Layer13_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1950 [shape=box, label="Layer13_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1951 [shape=box, label="Layer13_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1952 [shape=ellipse, label="Layer13_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1945;
  node_1945 -> node_1946;
  node_1945 -> node_1947;
  node_1946 -> node_1948;
  node_1947 -> node_1949;
  node_1948 -> node_1950;
  node_1949 -> node_1951;
  node_1950 -> node_1952 [style=dashed];
  node_1951 -> node_1952 [style=dashed];
  node_1953 [shape=parallelogram, label="Layer13_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1954 [shape=box, label="Layer13_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1955 [shape=box, label="Layer13_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1956 [shape=box, label="Layer13_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1957 [shape=box, label="Layer13_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1958 [shape=box, label="Layer13_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1959 [shape=box, label="Layer13_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1960 [shape=ellipse, label="Layer13_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1953;
  node_1953 -> node_1954;
  node_1953 -> node_1955;
  node_1954 -> node_1956;
  node_1955 -> node_1957;
  node_1956 -> node_1958;
  node_1957 -> node_1959;
  node_1958 -> node_1960 [style=dashed];
  node_1959 -> node_1960 [style=dashed];
  node_1961 [shape=parallelogram, label="Layer13_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1962 [shape=box, label="Layer13_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1963 [shape=box, label="Layer13_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1964 [shape=box, label="Layer13_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1965 [shape=box, label="Layer13_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1966 [shape=box, label="Layer13_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1967 [shape=box, label="Layer13_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1968 [shape=ellipse, label="Layer13_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1961;
  node_1961 -> node_1962;
  node_1961 -> node_1963;
  node_1962 -> node_1964;
  node_1963 -> node_1965;
  node_1964 -> node_1966;
  node_1965 -> node_1967;
  node_1966 -> node_1968 [style=dashed];
  node_1967 -> node_1968 [style=dashed];
  node_1969 [shape=parallelogram, label="Layer13_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1970 [shape=box, label="Layer13_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1971 [shape=box, label="Layer13_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1972 [shape=box, label="Layer13_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1973 [shape=box, label="Layer13_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1974 [shape=box, label="Layer13_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1975 [shape=box, label="Layer13_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1976 [shape=ellipse, label="Layer13_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1969;
  node_1969 -> node_1970;
  node_1969 -> node_1971;
  node_1970 -> node_1972;
  node_1971 -> node_1973;
  node_1972 -> node_1974;
  node_1973 -> node_1975;
  node_1974 -> node_1976 [style=dashed];
  node_1975 -> node_1976 [style=dashed];
  node_1977 [shape=parallelogram, label="Layer13_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1978 [shape=box, label="Layer13_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1979 [shape=box, label="Layer13_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1980 [shape=box, label="Layer13_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1981 [shape=box, label="Layer13_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1982 [shape=box, label="Layer13_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1983 [shape=box, label="Layer13_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1984 [shape=ellipse, label="Layer13_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1977;
  node_1977 -> node_1978;
  node_1977 -> node_1979;
  node_1978 -> node_1980;
  node_1979 -> node_1981;
  node_1980 -> node_1982;
  node_1981 -> node_1983;
  node_1982 -> node_1984 [style=dashed];
  node_1983 -> node_1984 [style=dashed];
  node_1985 [shape=parallelogram, label="Layer13_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1986 [shape=box, label="Layer13_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1987 [shape=box, label="Layer13_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1988 [shape=box, label="Layer13_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1989 [shape=box, label="Layer13_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1990 [shape=box, label="Layer13_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1991 [shape=box, label="Layer13_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1992 [shape=ellipse, label="Layer13_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1985;
  node_1985 -> node_1986;
  node_1985 -> node_1987;
  node_1986 -> node_1988;
  node_1987 -> node_1989;
  node_1988 -> node_1990;
  node_1989 -> node_1991;
  node_1990 -> node_1992 [style=dashed];
  node_1991 -> node_1992 [style=dashed];
  node_1993 [shape=parallelogram, label="Layer13_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_1994 [shape=box, label="Layer13_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1995 [shape=box, label="Layer13_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1996 [shape=box, label="Layer13_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1997 [shape=box, label="Layer13_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_1998 [shape=box, label="Layer13_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_1999 [shape=box, label="Layer13_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2000 [shape=ellipse, label="Layer13_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_1872 -> node_1993;
  node_1993 -> node_1994;
  node_1993 -> node_1995;
  node_1994 -> node_1996;
  node_1995 -> node_1997;
  node_1996 -> node_1998;
  node_1997 -> node_1999;
  node_1998 -> node_2000 [style=dashed];
  node_1999 -> node_2000 [style=dashed];
  node_2001 [shape=ellipse, label="Layer13_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_1880 -> node_2001 [style=dashed];
  node_1888 -> node_2001 [style=dashed];
  node_1896 -> node_2001 [style=dashed];
  node_1904 -> node_2001 [style=dashed];
  node_1912 -> node_2001 [style=dashed];
  node_1920 -> node_2001 [style=dashed];
  node_1928 -> node_2001 [style=dashed];
  node_1936 -> node_2001 [style=dashed];
  node_1944 -> node_2001 [style=dashed];
  node_1952 -> node_2001 [style=dashed];
  node_1960 -> node_2001 [style=dashed];
  node_1968 -> node_2001 [style=dashed];
  node_1976 -> node_2001 [style=dashed];
  node_1984 -> node_2001 [style=dashed];
  node_1992 -> node_2001 [style=dashed];
  node_2000 -> node_2001 [style=dashed];
  node_2002 [shape=box, label="Layer13_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_2001 -> node_2002;
  node_2003 [shape=parallelogram, label="Layer13_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_1869 -> node_2003;
  node_2002 -> node_2003;
  
  // ===== Layer 14 =====
  node_2004 [shape=box, label="Layer14_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_2003 -> node_2004;
  node_2005 [shape=box, label="Layer14_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_2006 [shape=box, label="Layer14_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_2004 -> node_2005;
  node_2004 -> node_2006;
  node_2007 [shape=box, label="Layer14_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_2008 [shape=box, label="Layer14_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_2005 -> node_2007;
  node_2006 -> node_2008;
  node_2009 [shape=box, label="Layer14_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_2010 [shape=box, label="Layer14_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_2007 -> node_2009;
  node_2008 -> node_2010;
  node_2011 [shape=ellipse, label="Layer14_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_2009 -> node_2011 [style=dashed];
  node_2010 -> node_2011 [style=dashed];
  node_2012 [shape=parallelogram, label="Layer14_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_2003 -> node_2012;
  node_2011 -> node_2012;
  node_2013 [shape=box, label="Layer14_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_2012 -> node_2013;
  node_2014 [shape=parallelogram, label="Layer14_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_2013 -> node_2014 [style=dashed];
  node_2015 [shape=ellipse, label="Layer14_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_2014 -> node_2015 [style=dashed];
  node_2016 [shape=parallelogram, label="Layer14_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2017 [shape=box, label="Layer14_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2018 [shape=box, label="Layer14_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2019 [shape=box, label="Layer14_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2020 [shape=box, label="Layer14_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2021 [shape=box, label="Layer14_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2022 [shape=box, label="Layer14_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2023 [shape=ellipse, label="Layer14_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2016;
  node_2016 -> node_2017;
  node_2016 -> node_2018;
  node_2017 -> node_2019;
  node_2018 -> node_2020;
  node_2019 -> node_2021;
  node_2020 -> node_2022;
  node_2021 -> node_2023 [style=dashed];
  node_2022 -> node_2023 [style=dashed];
  node_2024 [shape=parallelogram, label="Layer14_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2025 [shape=box, label="Layer14_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2026 [shape=box, label="Layer14_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2027 [shape=box, label="Layer14_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2028 [shape=box, label="Layer14_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2029 [shape=box, label="Layer14_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2030 [shape=box, label="Layer14_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2031 [shape=ellipse, label="Layer14_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2024;
  node_2024 -> node_2025;
  node_2024 -> node_2026;
  node_2025 -> node_2027;
  node_2026 -> node_2028;
  node_2027 -> node_2029;
  node_2028 -> node_2030;
  node_2029 -> node_2031 [style=dashed];
  node_2030 -> node_2031 [style=dashed];
  node_2032 [shape=parallelogram, label="Layer14_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2033 [shape=box, label="Layer14_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2034 [shape=box, label="Layer14_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2035 [shape=box, label="Layer14_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2036 [shape=box, label="Layer14_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2037 [shape=box, label="Layer14_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2038 [shape=box, label="Layer14_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2039 [shape=ellipse, label="Layer14_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2032;
  node_2032 -> node_2033;
  node_2032 -> node_2034;
  node_2033 -> node_2035;
  node_2034 -> node_2036;
  node_2035 -> node_2037;
  node_2036 -> node_2038;
  node_2037 -> node_2039 [style=dashed];
  node_2038 -> node_2039 [style=dashed];
  node_2040 [shape=parallelogram, label="Layer14_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2041 [shape=box, label="Layer14_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2042 [shape=box, label="Layer14_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2043 [shape=box, label="Layer14_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2044 [shape=box, label="Layer14_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2045 [shape=box, label="Layer14_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2046 [shape=box, label="Layer14_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2047 [shape=ellipse, label="Layer14_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2040;
  node_2040 -> node_2041;
  node_2040 -> node_2042;
  node_2041 -> node_2043;
  node_2042 -> node_2044;
  node_2043 -> node_2045;
  node_2044 -> node_2046;
  node_2045 -> node_2047 [style=dashed];
  node_2046 -> node_2047 [style=dashed];
  node_2048 [shape=parallelogram, label="Layer14_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2049 [shape=box, label="Layer14_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2050 [shape=box, label="Layer14_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2051 [shape=box, label="Layer14_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2052 [shape=box, label="Layer14_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2053 [shape=box, label="Layer14_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2054 [shape=box, label="Layer14_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2055 [shape=ellipse, label="Layer14_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2048;
  node_2048 -> node_2049;
  node_2048 -> node_2050;
  node_2049 -> node_2051;
  node_2050 -> node_2052;
  node_2051 -> node_2053;
  node_2052 -> node_2054;
  node_2053 -> node_2055 [style=dashed];
  node_2054 -> node_2055 [style=dashed];
  node_2056 [shape=parallelogram, label="Layer14_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2057 [shape=box, label="Layer14_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2058 [shape=box, label="Layer14_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2059 [shape=box, label="Layer14_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2060 [shape=box, label="Layer14_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2061 [shape=box, label="Layer14_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2062 [shape=box, label="Layer14_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2063 [shape=ellipse, label="Layer14_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2056;
  node_2056 -> node_2057;
  node_2056 -> node_2058;
  node_2057 -> node_2059;
  node_2058 -> node_2060;
  node_2059 -> node_2061;
  node_2060 -> node_2062;
  node_2061 -> node_2063 [style=dashed];
  node_2062 -> node_2063 [style=dashed];
  node_2064 [shape=parallelogram, label="Layer14_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2065 [shape=box, label="Layer14_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2066 [shape=box, label="Layer14_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2067 [shape=box, label="Layer14_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2068 [shape=box, label="Layer14_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2069 [shape=box, label="Layer14_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2070 [shape=box, label="Layer14_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2071 [shape=ellipse, label="Layer14_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2064;
  node_2064 -> node_2065;
  node_2064 -> node_2066;
  node_2065 -> node_2067;
  node_2066 -> node_2068;
  node_2067 -> node_2069;
  node_2068 -> node_2070;
  node_2069 -> node_2071 [style=dashed];
  node_2070 -> node_2071 [style=dashed];
  node_2072 [shape=parallelogram, label="Layer14_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2073 [shape=box, label="Layer14_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2074 [shape=box, label="Layer14_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2075 [shape=box, label="Layer14_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2076 [shape=box, label="Layer14_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2077 [shape=box, label="Layer14_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2078 [shape=box, label="Layer14_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2079 [shape=ellipse, label="Layer14_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2072;
  node_2072 -> node_2073;
  node_2072 -> node_2074;
  node_2073 -> node_2075;
  node_2074 -> node_2076;
  node_2075 -> node_2077;
  node_2076 -> node_2078;
  node_2077 -> node_2079 [style=dashed];
  node_2078 -> node_2079 [style=dashed];
  node_2080 [shape=parallelogram, label="Layer14_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2081 [shape=box, label="Layer14_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2082 [shape=box, label="Layer14_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2083 [shape=box, label="Layer14_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2084 [shape=box, label="Layer14_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2085 [shape=box, label="Layer14_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2086 [shape=box, label="Layer14_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2087 [shape=ellipse, label="Layer14_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2080;
  node_2080 -> node_2081;
  node_2080 -> node_2082;
  node_2081 -> node_2083;
  node_2082 -> node_2084;
  node_2083 -> node_2085;
  node_2084 -> node_2086;
  node_2085 -> node_2087 [style=dashed];
  node_2086 -> node_2087 [style=dashed];
  node_2088 [shape=parallelogram, label="Layer14_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2089 [shape=box, label="Layer14_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2090 [shape=box, label="Layer14_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2091 [shape=box, label="Layer14_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2092 [shape=box, label="Layer14_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2093 [shape=box, label="Layer14_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2094 [shape=box, label="Layer14_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2095 [shape=ellipse, label="Layer14_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2088;
  node_2088 -> node_2089;
  node_2088 -> node_2090;
  node_2089 -> node_2091;
  node_2090 -> node_2092;
  node_2091 -> node_2093;
  node_2092 -> node_2094;
  node_2093 -> node_2095 [style=dashed];
  node_2094 -> node_2095 [style=dashed];
  node_2096 [shape=parallelogram, label="Layer14_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2097 [shape=box, label="Layer14_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2098 [shape=box, label="Layer14_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2099 [shape=box, label="Layer14_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2100 [shape=box, label="Layer14_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2101 [shape=box, label="Layer14_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2102 [shape=box, label="Layer14_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2103 [shape=ellipse, label="Layer14_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2096;
  node_2096 -> node_2097;
  node_2096 -> node_2098;
  node_2097 -> node_2099;
  node_2098 -> node_2100;
  node_2099 -> node_2101;
  node_2100 -> node_2102;
  node_2101 -> node_2103 [style=dashed];
  node_2102 -> node_2103 [style=dashed];
  node_2104 [shape=parallelogram, label="Layer14_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2105 [shape=box, label="Layer14_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2106 [shape=box, label="Layer14_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2107 [shape=box, label="Layer14_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2108 [shape=box, label="Layer14_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2109 [shape=box, label="Layer14_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2110 [shape=box, label="Layer14_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2111 [shape=ellipse, label="Layer14_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2104;
  node_2104 -> node_2105;
  node_2104 -> node_2106;
  node_2105 -> node_2107;
  node_2106 -> node_2108;
  node_2107 -> node_2109;
  node_2108 -> node_2110;
  node_2109 -> node_2111 [style=dashed];
  node_2110 -> node_2111 [style=dashed];
  node_2112 [shape=parallelogram, label="Layer14_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2113 [shape=box, label="Layer14_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2114 [shape=box, label="Layer14_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2115 [shape=box, label="Layer14_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2116 [shape=box, label="Layer14_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2117 [shape=box, label="Layer14_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2118 [shape=box, label="Layer14_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2119 [shape=ellipse, label="Layer14_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2112;
  node_2112 -> node_2113;
  node_2112 -> node_2114;
  node_2113 -> node_2115;
  node_2114 -> node_2116;
  node_2115 -> node_2117;
  node_2116 -> node_2118;
  node_2117 -> node_2119 [style=dashed];
  node_2118 -> node_2119 [style=dashed];
  node_2120 [shape=parallelogram, label="Layer14_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2121 [shape=box, label="Layer14_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2122 [shape=box, label="Layer14_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2123 [shape=box, label="Layer14_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2124 [shape=box, label="Layer14_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2125 [shape=box, label="Layer14_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2126 [shape=box, label="Layer14_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2127 [shape=ellipse, label="Layer14_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2120;
  node_2120 -> node_2121;
  node_2120 -> node_2122;
  node_2121 -> node_2123;
  node_2122 -> node_2124;
  node_2123 -> node_2125;
  node_2124 -> node_2126;
  node_2125 -> node_2127 [style=dashed];
  node_2126 -> node_2127 [style=dashed];
  node_2128 [shape=parallelogram, label="Layer14_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2129 [shape=box, label="Layer14_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2130 [shape=box, label="Layer14_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2131 [shape=box, label="Layer14_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2132 [shape=box, label="Layer14_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2133 [shape=box, label="Layer14_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2134 [shape=box, label="Layer14_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2135 [shape=ellipse, label="Layer14_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2128;
  node_2128 -> node_2129;
  node_2128 -> node_2130;
  node_2129 -> node_2131;
  node_2130 -> node_2132;
  node_2131 -> node_2133;
  node_2132 -> node_2134;
  node_2133 -> node_2135 [style=dashed];
  node_2134 -> node_2135 [style=dashed];
  node_2136 [shape=parallelogram, label="Layer14_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2137 [shape=box, label="Layer14_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2138 [shape=box, label="Layer14_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2139 [shape=box, label="Layer14_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2140 [shape=box, label="Layer14_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2141 [shape=box, label="Layer14_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2142 [shape=box, label="Layer14_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2143 [shape=ellipse, label="Layer14_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2015 -> node_2136;
  node_2136 -> node_2137;
  node_2136 -> node_2138;
  node_2137 -> node_2139;
  node_2138 -> node_2140;
  node_2139 -> node_2141;
  node_2140 -> node_2142;
  node_2141 -> node_2143 [style=dashed];
  node_2142 -> node_2143 [style=dashed];
  node_2144 [shape=ellipse, label="Layer14_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_2023 -> node_2144 [style=dashed];
  node_2031 -> node_2144 [style=dashed];
  node_2039 -> node_2144 [style=dashed];
  node_2047 -> node_2144 [style=dashed];
  node_2055 -> node_2144 [style=dashed];
  node_2063 -> node_2144 [style=dashed];
  node_2071 -> node_2144 [style=dashed];
  node_2079 -> node_2144 [style=dashed];
  node_2087 -> node_2144 [style=dashed];
  node_2095 -> node_2144 [style=dashed];
  node_2103 -> node_2144 [style=dashed];
  node_2111 -> node_2144 [style=dashed];
  node_2119 -> node_2144 [style=dashed];
  node_2127 -> node_2144 [style=dashed];
  node_2135 -> node_2144 [style=dashed];
  node_2143 -> node_2144 [style=dashed];
  node_2145 [shape=box, label="Layer14_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_2144 -> node_2145;
  node_2146 [shape=parallelogram, label="Layer14_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_2012 -> node_2146;
  node_2145 -> node_2146;
  
  // ===== Layer 15 =====
  node_2147 [shape=box, label="Layer15_RMSNorm_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_2146 -> node_2147;
  node_2148 [shape=box, label="Layer15_QKV_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_2149 [shape=box, label="Layer15_QKV_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_2147 -> node_2148;
  node_2147 -> node_2149;
  node_2150 [shape=box, label="Layer15_FlashAttn_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_2151 [shape=box, label="Layer15_FlashAttn_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, heads=32, dim=64]"];
  node_2148 -> node_2150;
  node_2149 -> node_2151;
  node_2152 [shape=box, label="Layer15_AttnOut_TP0\nGPU: 0-63\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_2153 [shape=box, label="Layer15_AttnOut_TP1\nGPU: 64-127\nINPUT: [batch=128, seq=10000, heads=32, dim=64]\nOUTPUT: [batch=128, seq=10000, hidden=2048]"];
  node_2150 -> node_2152;
  node_2151 -> node_2153;
  node_2154 [shape=ellipse, label="Layer15_AttnAllReduce\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=2048]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_2152 -> node_2154 [style=dashed];
  node_2153 -> node_2154 [style=dashed];
  node_2155 [shape=parallelogram, label="Layer15_Residual_Attn\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_2146 -> node_2155;
  node_2154 -> node_2155;
  node_2156 [shape=box, label="Layer15_RMSNorm_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_2155 -> node_2156;
  node_2157 [shape=parallelogram, label="Layer15_Router\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, experts=16]", style=dashed];
  node_2156 -> node_2157 [style=dashed];
  node_2158 [shape=ellipse, label="Layer15_All2All_Dispatch\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_2157 -> node_2158 [style=dashed];
  node_2159 [shape=parallelogram, label="Layer15_Expert0_Base\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2160 [shape=box, label="Layer15_Expert0_MLP_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2161 [shape=box, label="Layer15_Expert0_MLP_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2162 [shape=box, label="Layer15_Expert0_GELU_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2163 [shape=box, label="Layer15_Expert0_GELU_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2164 [shape=box, label="Layer15_Expert0_Out_TP0\nGPU: 0\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2165 [shape=box, label="Layer15_Expert0_Out_TP1\nGPU: 1\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2166 [shape=ellipse, label="Layer15_Expert0_AllReduce\nGPU: 0-1\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2159;
  node_2159 -> node_2160;
  node_2159 -> node_2161;
  node_2160 -> node_2162;
  node_2161 -> node_2163;
  node_2162 -> node_2164;
  node_2163 -> node_2165;
  node_2164 -> node_2166 [style=dashed];
  node_2165 -> node_2166 [style=dashed];
  node_2167 [shape=parallelogram, label="Layer15_Expert1_Base\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2168 [shape=box, label="Layer15_Expert1_MLP_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2169 [shape=box, label="Layer15_Expert1_MLP_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2170 [shape=box, label="Layer15_Expert1_GELU_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2171 [shape=box, label="Layer15_Expert1_GELU_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2172 [shape=box, label="Layer15_Expert1_Out_TP0\nGPU: 2\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2173 [shape=box, label="Layer15_Expert1_Out_TP1\nGPU: 3\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2174 [shape=ellipse, label="Layer15_Expert1_AllReduce\nGPU: 2-3\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2167;
  node_2167 -> node_2168;
  node_2167 -> node_2169;
  node_2168 -> node_2170;
  node_2169 -> node_2171;
  node_2170 -> node_2172;
  node_2171 -> node_2173;
  node_2172 -> node_2174 [style=dashed];
  node_2173 -> node_2174 [style=dashed];
  node_2175 [shape=parallelogram, label="Layer15_Expert2_Base\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2176 [shape=box, label="Layer15_Expert2_MLP_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2177 [shape=box, label="Layer15_Expert2_MLP_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2178 [shape=box, label="Layer15_Expert2_GELU_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2179 [shape=box, label="Layer15_Expert2_GELU_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2180 [shape=box, label="Layer15_Expert2_Out_TP0\nGPU: 4\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2181 [shape=box, label="Layer15_Expert2_Out_TP1\nGPU: 5\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2182 [shape=ellipse, label="Layer15_Expert2_AllReduce\nGPU: 4-5\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2175;
  node_2175 -> node_2176;
  node_2175 -> node_2177;
  node_2176 -> node_2178;
  node_2177 -> node_2179;
  node_2178 -> node_2180;
  node_2179 -> node_2181;
  node_2180 -> node_2182 [style=dashed];
  node_2181 -> node_2182 [style=dashed];
  node_2183 [shape=parallelogram, label="Layer15_Expert3_Base\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2184 [shape=box, label="Layer15_Expert3_MLP_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2185 [shape=box, label="Layer15_Expert3_MLP_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2186 [shape=box, label="Layer15_Expert3_GELU_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2187 [shape=box, label="Layer15_Expert3_GELU_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2188 [shape=box, label="Layer15_Expert3_Out_TP0\nGPU: 6\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2189 [shape=box, label="Layer15_Expert3_Out_TP1\nGPU: 7\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2190 [shape=ellipse, label="Layer15_Expert3_AllReduce\nGPU: 6-7\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2183;
  node_2183 -> node_2184;
  node_2183 -> node_2185;
  node_2184 -> node_2186;
  node_2185 -> node_2187;
  node_2186 -> node_2188;
  node_2187 -> node_2189;
  node_2188 -> node_2190 [style=dashed];
  node_2189 -> node_2190 [style=dashed];
  node_2191 [shape=parallelogram, label="Layer15_Expert4_Base\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2192 [shape=box, label="Layer15_Expert4_MLP_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2193 [shape=box, label="Layer15_Expert4_MLP_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2194 [shape=box, label="Layer15_Expert4_GELU_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2195 [shape=box, label="Layer15_Expert4_GELU_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2196 [shape=box, label="Layer15_Expert4_Out_TP0\nGPU: 8\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2197 [shape=box, label="Layer15_Expert4_Out_TP1\nGPU: 9\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2198 [shape=ellipse, label="Layer15_Expert4_AllReduce\nGPU: 8-9\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2191;
  node_2191 -> node_2192;
  node_2191 -> node_2193;
  node_2192 -> node_2194;
  node_2193 -> node_2195;
  node_2194 -> node_2196;
  node_2195 -> node_2197;
  node_2196 -> node_2198 [style=dashed];
  node_2197 -> node_2198 [style=dashed];
  node_2199 [shape=parallelogram, label="Layer15_Expert5_Base\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2200 [shape=box, label="Layer15_Expert5_MLP_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2201 [shape=box, label="Layer15_Expert5_MLP_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2202 [shape=box, label="Layer15_Expert5_GELU_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2203 [shape=box, label="Layer15_Expert5_GELU_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2204 [shape=box, label="Layer15_Expert5_Out_TP0\nGPU: 10\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2205 [shape=box, label="Layer15_Expert5_Out_TP1\nGPU: 11\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2206 [shape=ellipse, label="Layer15_Expert5_AllReduce\nGPU: 10-11\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2199;
  node_2199 -> node_2200;
  node_2199 -> node_2201;
  node_2200 -> node_2202;
  node_2201 -> node_2203;
  node_2202 -> node_2204;
  node_2203 -> node_2205;
  node_2204 -> node_2206 [style=dashed];
  node_2205 -> node_2206 [style=dashed];
  node_2207 [shape=parallelogram, label="Layer15_Expert6_Base\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2208 [shape=box, label="Layer15_Expert6_MLP_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2209 [shape=box, label="Layer15_Expert6_MLP_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2210 [shape=box, label="Layer15_Expert6_GELU_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2211 [shape=box, label="Layer15_Expert6_GELU_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2212 [shape=box, label="Layer15_Expert6_Out_TP0\nGPU: 12\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2213 [shape=box, label="Layer15_Expert6_Out_TP1\nGPU: 13\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2214 [shape=ellipse, label="Layer15_Expert6_AllReduce\nGPU: 12-13\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2207;
  node_2207 -> node_2208;
  node_2207 -> node_2209;
  node_2208 -> node_2210;
  node_2209 -> node_2211;
  node_2210 -> node_2212;
  node_2211 -> node_2213;
  node_2212 -> node_2214 [style=dashed];
  node_2213 -> node_2214 [style=dashed];
  node_2215 [shape=parallelogram, label="Layer15_Expert7_Base\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2216 [shape=box, label="Layer15_Expert7_MLP_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2217 [shape=box, label="Layer15_Expert7_MLP_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2218 [shape=box, label="Layer15_Expert7_GELU_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2219 [shape=box, label="Layer15_Expert7_GELU_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2220 [shape=box, label="Layer15_Expert7_Out_TP0\nGPU: 14\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2221 [shape=box, label="Layer15_Expert7_Out_TP1\nGPU: 15\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2222 [shape=ellipse, label="Layer15_Expert7_AllReduce\nGPU: 14-15\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2215;
  node_2215 -> node_2216;
  node_2215 -> node_2217;
  node_2216 -> node_2218;
  node_2217 -> node_2219;
  node_2218 -> node_2220;
  node_2219 -> node_2221;
  node_2220 -> node_2222 [style=dashed];
  node_2221 -> node_2222 [style=dashed];
  node_2223 [shape=parallelogram, label="Layer15_Expert8_Base\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2224 [shape=box, label="Layer15_Expert8_MLP_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2225 [shape=box, label="Layer15_Expert8_MLP_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2226 [shape=box, label="Layer15_Expert8_GELU_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2227 [shape=box, label="Layer15_Expert8_GELU_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2228 [shape=box, label="Layer15_Expert8_Out_TP0\nGPU: 16\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2229 [shape=box, label="Layer15_Expert8_Out_TP1\nGPU: 17\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2230 [shape=ellipse, label="Layer15_Expert8_AllReduce\nGPU: 16-17\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2223;
  node_2223 -> node_2224;
  node_2223 -> node_2225;
  node_2224 -> node_2226;
  node_2225 -> node_2227;
  node_2226 -> node_2228;
  node_2227 -> node_2229;
  node_2228 -> node_2230 [style=dashed];
  node_2229 -> node_2230 [style=dashed];
  node_2231 [shape=parallelogram, label="Layer15_Expert9_Base\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2232 [shape=box, label="Layer15_Expert9_MLP_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2233 [shape=box, label="Layer15_Expert9_MLP_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2234 [shape=box, label="Layer15_Expert9_GELU_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2235 [shape=box, label="Layer15_Expert9_GELU_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2236 [shape=box, label="Layer15_Expert9_Out_TP0\nGPU: 18\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2237 [shape=box, label="Layer15_Expert9_Out_TP1\nGPU: 19\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2238 [shape=ellipse, label="Layer15_Expert9_AllReduce\nGPU: 18-19\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2231;
  node_2231 -> node_2232;
  node_2231 -> node_2233;
  node_2232 -> node_2234;
  node_2233 -> node_2235;
  node_2234 -> node_2236;
  node_2235 -> node_2237;
  node_2236 -> node_2238 [style=dashed];
  node_2237 -> node_2238 [style=dashed];
  node_2239 [shape=parallelogram, label="Layer15_Expert10_Base\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2240 [shape=box, label="Layer15_Expert10_MLP_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2241 [shape=box, label="Layer15_Expert10_MLP_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2242 [shape=box, label="Layer15_Expert10_GELU_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2243 [shape=box, label="Layer15_Expert10_GELU_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2244 [shape=box, label="Layer15_Expert10_Out_TP0\nGPU: 20\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2245 [shape=box, label="Layer15_Expert10_Out_TP1\nGPU: 21\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2246 [shape=ellipse, label="Layer15_Expert10_AllReduce\nGPU: 20-21\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2239;
  node_2239 -> node_2240;
  node_2239 -> node_2241;
  node_2240 -> node_2242;
  node_2241 -> node_2243;
  node_2242 -> node_2244;
  node_2243 -> node_2245;
  node_2244 -> node_2246 [style=dashed];
  node_2245 -> node_2246 [style=dashed];
  node_2247 [shape=parallelogram, label="Layer15_Expert11_Base\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2248 [shape=box, label="Layer15_Expert11_MLP_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2249 [shape=box, label="Layer15_Expert11_MLP_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2250 [shape=box, label="Layer15_Expert11_GELU_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2251 [shape=box, label="Layer15_Expert11_GELU_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2252 [shape=box, label="Layer15_Expert11_Out_TP0\nGPU: 22\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2253 [shape=box, label="Layer15_Expert11_Out_TP1\nGPU: 23\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2254 [shape=ellipse, label="Layer15_Expert11_AllReduce\nGPU: 22-23\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2247;
  node_2247 -> node_2248;
  node_2247 -> node_2249;
  node_2248 -> node_2250;
  node_2249 -> node_2251;
  node_2250 -> node_2252;
  node_2251 -> node_2253;
  node_2252 -> node_2254 [style=dashed];
  node_2253 -> node_2254 [style=dashed];
  node_2255 [shape=parallelogram, label="Layer15_Expert12_Base\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2256 [shape=box, label="Layer15_Expert12_MLP_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2257 [shape=box, label="Layer15_Expert12_MLP_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2258 [shape=box, label="Layer15_Expert12_GELU_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2259 [shape=box, label="Layer15_Expert12_GELU_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2260 [shape=box, label="Layer15_Expert12_Out_TP0\nGPU: 24\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2261 [shape=box, label="Layer15_Expert12_Out_TP1\nGPU: 25\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2262 [shape=ellipse, label="Layer15_Expert12_AllReduce\nGPU: 24-25\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2255;
  node_2255 -> node_2256;
  node_2255 -> node_2257;
  node_2256 -> node_2258;
  node_2257 -> node_2259;
  node_2258 -> node_2260;
  node_2259 -> node_2261;
  node_2260 -> node_2262 [style=dashed];
  node_2261 -> node_2262 [style=dashed];
  node_2263 [shape=parallelogram, label="Layer15_Expert13_Base\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2264 [shape=box, label="Layer15_Expert13_MLP_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2265 [shape=box, label="Layer15_Expert13_MLP_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2266 [shape=box, label="Layer15_Expert13_GELU_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2267 [shape=box, label="Layer15_Expert13_GELU_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2268 [shape=box, label="Layer15_Expert13_Out_TP0\nGPU: 26\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2269 [shape=box, label="Layer15_Expert13_Out_TP1\nGPU: 27\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2270 [shape=ellipse, label="Layer15_Expert13_AllReduce\nGPU: 26-27\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2263;
  node_2263 -> node_2264;
  node_2263 -> node_2265;
  node_2264 -> node_2266;
  node_2265 -> node_2267;
  node_2266 -> node_2268;
  node_2267 -> node_2269;
  node_2268 -> node_2270 [style=dashed];
  node_2269 -> node_2270 [style=dashed];
  node_2271 [shape=parallelogram, label="Layer15_Expert14_Base\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2272 [shape=box, label="Layer15_Expert14_MLP_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2273 [shape=box, label="Layer15_Expert14_MLP_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2274 [shape=box, label="Layer15_Expert14_GELU_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2275 [shape=box, label="Layer15_Expert14_GELU_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2276 [shape=box, label="Layer15_Expert14_Out_TP0\nGPU: 28\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2277 [shape=box, label="Layer15_Expert14_Out_TP1\nGPU: 29\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2278 [shape=ellipse, label="Layer15_Expert14_AllReduce\nGPU: 28-29\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2271;
  node_2271 -> node_2272;
  node_2271 -> node_2273;
  node_2272 -> node_2274;
  node_2273 -> node_2275;
  node_2274 -> node_2276;
  node_2275 -> node_2277;
  node_2276 -> node_2278 [style=dashed];
  node_2277 -> node_2278 [style=dashed];
  node_2279 [shape=parallelogram, label="Layer15_Expert15_Base\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, hidden=4096]"];
  node_2280 [shape=box, label="Layer15_Expert15_MLP_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2281 [shape=box, label="Layer15_Expert15_MLP_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2282 [shape=box, label="Layer15_Expert15_GELU_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2283 [shape=box, label="Layer15_Expert15_GELU_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, ffn=8192]"];
  node_2284 [shape=box, label="Layer15_Expert15_Out_TP0\nGPU: 30\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2285 [shape=box, label="Layer15_Expert15_Out_TP1\nGPU: 31\nINPUT: [batch=8, seq=10000, ffn=8192]\nOUTPUT: [batch=8, seq=10000, hidden=2048]"];
  node_2286 [shape=ellipse, label="Layer15_Expert15_AllReduce\nGPU: 30-31\nINPUT: [batch=8, seq=10000, hidden=2048]\nOUTPUT: [batch=8, seq=10000, hidden=4096]", style=dashed];
  node_2158 -> node_2279;
  node_2279 -> node_2280;
  node_2279 -> node_2281;
  node_2280 -> node_2282;
  node_2281 -> node_2283;
  node_2282 -> node_2284;
  node_2283 -> node_2285;
  node_2284 -> node_2286 [style=dashed];
  node_2285 -> node_2286 [style=dashed];
  node_2287 [shape=ellipse, label="Layer15_All2All_Combine\nGPU: ALL\nINPUT: [batch=8, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=dashed];
  node_2166 -> node_2287 [style=dashed];
  node_2174 -> node_2287 [style=dashed];
  node_2182 -> node_2287 [style=dashed];
  node_2190 -> node_2287 [style=dashed];
  node_2198 -> node_2287 [style=dashed];
  node_2206 -> node_2287 [style=dashed];
  node_2214 -> node_2287 [style=dashed];
  node_2222 -> node_2287 [style=dashed];
  node_2230 -> node_2287 [style=dashed];
  node_2238 -> node_2287 [style=dashed];
  node_2246 -> node_2287 [style=dashed];
  node_2254 -> node_2287 [style=dashed];
  node_2262 -> node_2287 [style=dashed];
  node_2270 -> node_2287 [style=dashed];
  node_2278 -> node_2287 [style=dashed];
  node_2286 -> node_2287 [style=dashed];
  node_2288 [shape=box, label="Layer15_MLP_Out\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_2287 -> node_2288;
  node_2289 [shape=parallelogram, label="Layer15_Residual_MLP\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_2155 -> node_2289;
  node_2288 -> node_2289;
  node_2290 [shape=box, label="Final_RMSNorm\nGPU: ALL\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]"];
  node_2289 -> node_2290;
  node_2291 [shape=box, label="Output\nINPUT: [batch=128, seq=10000, hidden=4096]\nOUTPUT: [batch=128, seq=10000, hidden=4096]", style=filled, fillcolor=lightgreen];
  node_2290 -> node_2291;
}
