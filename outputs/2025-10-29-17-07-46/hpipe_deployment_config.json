{
  "deployment_configuration": {
    "framework": "HPipe",
    "parallel_strategy": {
      "type": "token_level_pipeline_parallelism",
      "description": "Pipeline parallelism on token dimension with heterogeneous device support",
      "key_parameters": {
        "partition_granularity": "layer_level",
        "sequence_slicing": "dynamic_programming",
        "communication_pattern": "point_to_point",
        "synchronization_frequency": "per_subsequence"
      }
    },
    "models": {
      "LLaMA_7B": {
        "total_layers": 32,
        "hidden_size": 4096,
        "ffn_hidden_size": 11008,
        "attention_heads": 32,
        "vocab_size": 32000,
        "max_sequence_length": 2048,
        "batch_size": 6,
        "parameters": {
          "total_params": 7000000000,
          "memory_required": "~26GB"
        }
      },
      "GPT3_2B": {
        "total_layers": 24,
        "hidden_size": 2048,
        "ffn_hidden_size": 8192,
        "attention_heads": 16,
        "vocab_size": 50257,
        "max_sequence_length": 2048,
        "batch_size": 12,
        "parameters": {
          "total_params": 2200000000,
          "memory_required": "~8.5GB"
        }
      }
    },
    "device_specifications": {
      "hosts": [
        {
          "host_id": "host_1",
          "devices": [
            {
              "device_id": "P100_0",
              "type": "Pascal_P100",
              "memory": 12288,
              "compute_capability": 6.0,
              "bandwidth_to_host": 32,
              "device_memory_bandwidth": 732
            },
            {
              "device_id": "P100_1",
              "type": "Pascal_P100",
              "memory": 12288,
              "compute_capability": 6.0,
              "bandwidth_to_host": 32,
              "device_memory_bandwidth": 732
            },
            {
              "device_id": "P100_2",
              "type": "Pascal_P100",
              "memory": 12288,
              "compute_capability": 6.0,
              "bandwidth_to_host": 32,
              "device_memory_bandwidth": 732
            },
            {
              "device_id": "P100_3",
              "type": "Pascal_P100",
              "memory": 12288,
              "compute_capability": 6.0,
              "bandwidth_to_host": 32,
              "device_memory_bandwidth": 732
            }
          ],
          "network_bandwidth": 1000
        },
        {
          "host_id": "host_2",
          "devices": [
            {
              "device_id": "RTX3090_0",
              "type": "RTX_3090",
              "memory": 24576,
              "compute_capability": 8.6,
              "bandwidth_to_host": 64,
              "device_memory_bandwidth": 936
            },
            {
              "device_id": "RTX3090_1",
              "type": "RTX_3090",
              "memory": 24576,
              "compute_capability": 8.6,
              "bandwidth_to_host": 64,
              "device_memory_bandwidth": 936
            }
          ],
          "network_bandwidth": 1000
        }
      ]
    },
    "model_partitioning": {
      "LLaMA_7B": {
        "layer_distribution": [
          {
            "device_id": "P100_0",
            "layers": [1, 2, 3, 4, 5],
            "layer_count": 5,
            "memory_allocation": "1800MB"
          },
          {
            "device_id": "P100_1",
            "layers": [6, 7, 8, 9, 10, 11],
            "layer_count": 6,
            "memory_allocation": "2100MB"
          },
          {
            "device_id": "P100_2",
            "layers": [12, 13, 14, 15, 16, 17, 18],
            "layer_count": 7,
            "memory_allocation": "2400MB"
          },
          {
            "device_id": "P100_3",
            "layers": [19, 20, 21, 22, 23],
            "layer_count": 5,
            "memory_allocation": "1800MB"
          },
          {
            "device_id": "RTX3090_0",
            "layers": [24, 25, 26, 27, 28, 29],
            "layer_count": 6,
            "memory_allocation": "8700MB"
          },
          {
            "device_id": "RTX3090_1",
            "layers": [30, 31, 32],
            "layer_count": 3,
            "memory_allocation": "10000MB"
          }
        ],
        "communication_overhead": {
          "intra_host": "PCIe",
          "inter_host": "1000Mbps_Ethernet",
          "activation_size": "16MB_per_layer"
        }
      },
      "GPT3_2B": {
        "layer_distribution": [
          {
            "device_id": "P100_0",
            "layers": [1, 2, 3, 4],
            "layer_count": 4,
            "memory_allocation": "4600MB"
          },
          {
            "device_id": "P100_1",
            "layers": [5, 6, 7, 8],
            "layer_count": 4,
            "memory_allocation": "4600MB"
          },
          {
            "device_id": "P100_2",
            "layers": [9, 10, 11, 12],
            "layer_count": 4,
            "memory_allocation": "3100MB"
          },
          {
            "device_id": "RTX3090_0",
            "layers": [13, 14, 15, 16, 17, 18],
            "layer_count": 6,
            "memory_allocation": "3900MB"
          },
          {
            "device_id": "RTX3090_1",
            "layers": [19, 20, 21, 22, 23, 24],
            "layer_count": 6,
            "memory_allocation": "7900MB"
          }
        ],
        "communication_overhead": {
          "intra_host": "PCIe",
          "inter_host": "1000Mbps_Ethernet",
          "activation_size": "8MB_per_layer"
        }
      }
    },
    "sequence_slicing": {
      "algorithm": "dynamic_programming",
      "parameters": {
        "max_slice_time": "adaptive_based_on_slowest_device",
        "slice_granularity": "token_level",
        "cache_kv": true,
        "slice_pattern": "decreasing_length"
      },
      "slice_examples": {
        "2048_tokens": {
          "optimal_slices": [512, 384, 320, 256, 224, 192, 160],
          "total_slices": 7,
          "rationale": "Longer slices at beginning, shorter toward end"
        }
      }
    },
    "deployment_workflow": {
      "prepare_phase": {
        "steps": [
          "1. Analyze device specifications and network conditions",
          "2. Run dynamic programming for optimal layer distribution",
          "3. Pre-compute sequence slicing schemes for all lengths",
          "4. Establish device communication topology"
        ],
        "duration": "~30 seconds for LLaMA-7B"
      },
      "runtime_phase": {
        "steps": [
          "1. Receive input sequence",
          "2. Apply pre-computed slicing scheme",
          "3. Execute token-level pipeline across devices",
          "4. Cache K,V values for subsequent slices",
          "5. Aggregate final results"
        ],
        "latency_optimization": {
          "overlap_computation": true,
          "minimize_communication": true,
          "balance_stages": true
        }
      }
    },
    "performance_metrics": {
      "expected_speedup": {
        "LLaMA_7B": {
          "latency_reduction": "9.06x",
          "throughput_improvement": "8.98x",
          "energy_reduction": "68.2%"
        },
        "GPT3_2B": {
          "latency_reduction": "2.28x",
          "throughput_improvement": "2.28x",
          "energy_reduction": "~60%"
        }
      },
      "memory_efficiency": {
        "layer_balancing": "reduces_per_device_memory",
        "sequence_slicing": "reduces_peak_memory",
        "activation_caching": "reuses_computed_values"
      }
    },
    "baseline_configurations": {
      "Base": {
        "strategy": "uniform_distribution_sequential",
        "mapping": "all_layers_all_devices",
        "memory_usage": "replicated"
      },
      "GPipe": {
        "strategy": "pipeline_micro_batch",
        "mapping": "uniform_layer_distribution",
        "batch_dimension": true
      },
      "MegatronLM": {
        "strategy": "tensor_plus_pipeline",
        "tensor_parallelism": {
          "row_parallel": "attention_weights",
          "column_parallel": "feedforward_weights"
        },
        "communication_overhead": "high_synchronization"
      }
    }
  }
}