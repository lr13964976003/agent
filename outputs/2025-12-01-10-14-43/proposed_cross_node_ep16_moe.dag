// Proposed Cross-Node EP=16 MoE DAG
digraph {
	rankdir=TB size="30,40"
	node [shape=rectangle]
	edge [arrowhead=normal]
	node [fillcolor=lightblue shape=ellipse style=filled]
	node [fillcolor=lightgreen shape=rectangle style=filled]
	node [fillcolor=lightyellow shape=parallelogram style=filled]
	input [label="Input\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightblue shape=ellipse]
	subgraph cluster_layer0 {
		label="Layer 0"
		fillcolor=lightgray style="rounded,filled"
		layer0_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer0_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer0_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer0_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer0_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer0_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer0_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer0_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer0_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer0_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer0_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer0_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer0_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer0_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer0_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer0_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer0_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer0_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer0_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer0_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer0_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer0_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer0_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer0_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer0_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer0_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer0_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer0_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer0_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer0_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer0_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer0_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer0_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer0_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer0_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer0_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer0_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer0_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer0_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer0_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer0_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer0_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer0_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer0_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer0_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer0_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer0_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer0_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer0_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer0_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer0_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer0_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer0_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer0_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer0_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer0_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer0_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer0_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer0_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer0_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer0_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer0_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer0_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer0_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer0_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer0_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer0_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer0_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer0_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer0_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer0_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer0_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer0_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer0_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer0_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer0_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer0_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer0_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer0_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer0_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer0_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer0_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer0_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer0_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer0_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer0_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer0_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer0_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer0_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer0_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer1 {
		label="Layer 1"
		fillcolor=lightgray style="rounded,filled"
		layer1_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer1_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer1_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer1_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer1_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer1_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer1_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer1_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer1_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer1_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer1_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer1_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer1_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer1_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer1_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer1_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer1_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer1_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer1_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer1_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer1_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer1_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer1_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer1_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer1_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer1_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer1_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer1_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer1_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer1_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer1_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer1_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer1_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer1_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer1_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer1_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer1_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer1_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer1_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer1_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer1_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer1_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer1_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer1_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer1_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer1_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer1_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer1_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer1_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer1_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer1_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer1_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer1_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer1_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer1_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer1_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer1_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer1_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer1_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer1_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer1_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer1_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer1_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer1_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer1_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer1_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer1_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer1_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer1_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer1_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer1_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer1_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer1_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer1_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer1_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer1_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer1_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer1_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer1_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer1_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer1_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer1_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer1_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer1_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer1_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer1_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer1_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer1_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer1_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer1_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer2 {
		label="Layer 2"
		fillcolor=lightgray style="rounded,filled"
		layer2_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer2_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer2_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer2_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer2_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer2_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer2_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer2_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer2_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer2_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer2_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer2_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer2_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer2_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer2_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer2_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer2_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer2_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer2_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer2_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer2_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer2_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer2_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer2_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer2_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer2_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer2_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer2_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer2_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer2_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer2_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer2_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer2_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer2_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer2_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer2_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer2_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer2_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer2_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer2_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer2_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer2_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer2_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer2_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer2_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer2_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer2_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer2_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer2_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer2_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer2_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer2_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer2_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer2_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer2_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer2_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer2_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer2_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer2_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer2_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer2_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer2_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer2_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer2_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer2_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer2_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer2_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer2_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer2_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer2_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer2_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer2_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer2_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer2_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer2_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer2_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer2_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer2_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer2_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer2_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer2_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer2_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer2_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer2_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer2_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer2_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer2_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer2_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer2_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer2_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer3 {
		label="Layer 3"
		fillcolor=lightgray style="rounded,filled"
		layer3_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer3_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer3_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer3_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer3_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer3_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer3_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer3_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer3_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer3_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer3_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer3_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer3_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer3_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer3_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer3_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer3_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer3_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer3_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer3_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer3_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer3_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer3_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer3_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer3_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer3_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer3_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer3_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer3_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer3_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer3_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer3_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer3_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer3_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer3_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer3_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer3_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer3_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer3_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer3_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer3_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer3_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer3_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer3_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer3_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer3_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer3_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer3_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer3_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer3_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer3_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer3_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer3_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer3_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer3_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer3_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer3_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer3_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer3_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer3_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer3_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer3_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer3_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer3_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer3_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer3_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer3_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer3_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer3_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer3_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer3_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer3_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer3_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer3_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer3_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer3_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer3_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer3_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer3_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer3_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer3_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer3_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer3_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer3_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer3_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer3_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer3_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer3_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer3_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer3_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer4 {
		label="Layer 4"
		fillcolor=lightgray style="rounded,filled"
		layer4_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer4_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer4_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer4_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer4_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer4_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer4_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer4_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer4_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer4_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer4_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer4_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer4_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer4_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer4_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer4_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer4_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer4_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer4_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer4_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer4_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer4_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer4_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer4_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer4_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer4_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer4_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer4_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer4_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer4_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer4_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer4_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer4_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer4_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer4_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer4_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer4_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer4_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer4_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer4_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer4_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer4_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer4_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer4_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer4_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer4_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer4_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer4_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer4_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer4_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer4_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer4_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer4_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer4_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer4_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer4_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer4_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer4_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer4_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer4_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer4_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer4_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer4_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer4_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer4_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer4_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer4_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer4_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer4_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer4_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer4_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer4_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer4_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer4_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer4_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer4_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer4_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer4_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer4_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer4_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer4_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer4_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer4_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer4_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer4_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer4_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer4_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer4_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer4_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer4_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer5 {
		label="Layer 5"
		fillcolor=lightgray style="rounded,filled"
		layer5_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer5_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer5_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer5_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer5_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer5_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer5_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer5_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer5_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer5_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer5_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer5_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer5_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer5_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer5_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer5_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer5_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer5_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer5_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer5_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer5_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer5_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer5_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer5_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer5_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer5_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer5_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer5_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer5_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer5_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer5_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer5_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer5_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer5_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer5_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer5_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer5_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer5_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer5_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer5_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer5_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer5_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer5_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer5_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer5_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer5_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer5_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer5_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer5_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer5_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer5_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer5_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer5_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer5_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer5_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer5_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer5_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer5_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer5_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer5_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer5_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer5_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer5_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer5_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer5_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer5_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer5_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer5_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer5_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer5_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer5_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer5_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer5_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer5_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer5_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer5_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer5_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer5_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer5_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer5_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer5_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer5_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer5_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer5_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer5_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer5_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer5_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer5_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer5_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer5_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer6 {
		label="Layer 6"
		fillcolor=lightgray style="rounded,filled"
		layer6_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer6_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer6_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer6_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer6_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer6_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer6_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer6_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer6_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer6_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer6_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer6_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer6_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer6_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer6_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer6_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer6_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer6_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer6_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer6_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer6_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer6_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer6_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer6_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer6_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer6_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer6_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer6_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer6_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer6_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer6_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer6_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer6_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer6_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer6_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer6_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer6_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer6_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer6_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer6_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer6_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer6_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer6_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer6_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer6_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer6_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer6_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer6_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer6_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer6_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer6_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer6_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer6_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer6_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer6_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer6_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer6_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer6_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer6_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer6_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer6_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer6_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer6_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer6_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer6_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer6_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer6_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer6_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer6_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer6_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer6_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer6_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer6_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer6_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer6_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer6_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer6_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer6_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer6_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer6_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer6_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer6_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer6_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer6_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer6_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer6_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer6_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer6_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer6_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer6_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer7 {
		label="Layer 7"
		fillcolor=lightgray style="rounded,filled"
		layer7_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer7_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer7_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer7_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer7_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer7_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer7_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer7_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer7_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer7_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer7_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer7_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer7_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer7_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer7_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer7_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer7_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer7_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer7_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer7_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer7_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer7_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer7_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer7_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer7_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer7_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer7_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer7_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer7_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer7_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer7_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer7_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer7_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer7_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer7_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer7_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer7_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer7_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer7_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer7_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer7_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer7_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer7_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer7_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer7_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer7_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer7_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer7_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer7_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer7_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer7_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer7_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer7_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer7_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer7_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer7_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer7_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer7_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer7_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer7_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer7_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer7_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer7_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer7_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer7_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer7_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer7_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer7_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer7_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer7_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer7_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer7_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer7_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer7_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer7_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer7_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer7_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer7_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer7_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer7_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer7_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer7_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer7_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer7_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer7_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer7_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer7_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer7_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer7_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer7_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer8 {
		label="Layer 8"
		fillcolor=lightgray style="rounded,filled"
		layer8_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer8_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer8_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer8_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer8_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer8_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer8_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer8_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer8_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer8_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer8_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer8_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer8_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer8_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer8_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer8_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer8_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer8_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer8_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer8_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer8_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer8_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer8_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer8_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer8_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer8_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer8_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer8_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer8_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer8_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer8_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer8_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer8_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer8_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer8_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer8_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer8_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer8_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer8_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer8_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer8_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer8_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer8_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer8_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer8_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer8_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer8_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer8_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer8_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer8_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer8_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer8_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer8_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer8_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer8_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer8_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer8_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer8_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer8_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer8_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer8_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer8_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer8_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer8_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer8_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer8_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer8_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer8_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer8_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer8_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer8_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer8_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer8_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer8_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer8_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer8_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer8_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer8_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer8_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer8_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer8_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer8_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer8_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer8_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer8_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer8_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer8_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer8_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer8_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer8_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer9 {
		label="Layer 9"
		fillcolor=lightgray style="rounded,filled"
		layer9_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer9_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer9_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer9_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer9_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer9_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer9_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer9_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer9_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer9_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer9_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer9_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer9_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer9_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer9_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer9_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer9_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer9_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer9_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer9_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer9_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer9_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer9_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer9_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer9_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer9_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer9_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer9_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer9_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer9_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer9_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer9_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer9_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer9_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer9_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer9_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer9_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer9_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer9_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer9_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer9_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer9_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer9_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer9_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer9_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer9_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer9_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer9_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer9_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer9_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer9_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer9_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer9_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer9_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer9_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer9_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer9_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer9_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer9_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer9_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer9_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer9_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer9_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer9_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer9_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer9_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer9_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer9_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer9_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer9_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer9_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer9_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer9_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer9_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer9_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer9_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer9_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer9_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer9_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer9_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer9_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer9_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer9_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer9_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer9_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer9_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer9_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer9_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer9_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer9_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer10 {
		label="Layer 10"
		fillcolor=lightgray style="rounded,filled"
		layer10_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer10_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer10_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer10_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer10_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer10_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer10_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer10_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer10_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer10_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer10_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer10_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer10_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer10_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer10_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer10_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer10_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer10_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer10_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer10_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer10_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer10_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer10_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer10_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer10_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer10_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer10_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer10_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer10_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer10_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer10_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer10_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer10_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer10_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer10_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer10_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer10_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer10_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer10_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer10_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer10_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer10_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer10_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer10_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer10_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer10_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer10_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer10_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer10_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer10_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer10_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer10_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer10_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer10_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer10_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer10_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer10_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer10_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer10_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer10_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer10_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer10_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer10_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer10_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer10_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer10_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer10_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer10_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer10_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer10_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer10_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer10_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer10_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer10_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer10_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer10_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer10_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer10_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer10_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer10_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer10_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer10_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer10_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer10_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer10_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer10_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer10_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer10_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer10_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer10_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer11 {
		label="Layer 11"
		fillcolor=lightgray style="rounded,filled"
		layer11_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer11_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer11_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer11_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer11_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer11_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer11_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer11_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer11_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer11_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer11_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer11_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer11_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer11_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer11_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer11_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer11_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer11_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer11_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer11_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer11_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer11_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer11_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer11_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer11_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer11_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer11_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer11_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer11_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer11_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer11_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer11_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer11_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer11_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer11_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer11_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer11_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer11_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer11_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer11_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer11_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer11_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer11_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer11_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer11_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer11_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer11_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer11_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer11_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer11_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer11_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer11_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer11_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer11_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer11_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer11_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer11_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer11_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer11_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer11_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer11_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer11_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer11_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer11_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer11_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer11_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer11_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer11_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer11_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer11_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer11_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer11_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer11_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer11_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer11_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer11_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer11_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer11_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer11_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer11_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer11_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer11_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer11_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer11_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer11_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer11_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer11_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer11_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer11_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer11_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer12 {
		label="Layer 12"
		fillcolor=lightgray style="rounded,filled"
		layer12_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer12_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer12_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer12_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer12_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer12_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer12_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer12_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer12_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer12_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer12_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer12_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer12_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer12_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer12_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer12_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer12_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer12_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer12_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer12_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer12_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer12_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer12_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer12_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer12_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer12_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer12_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer12_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer12_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer12_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer12_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer12_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer12_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer12_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer12_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer12_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer12_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer12_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer12_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer12_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer12_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer12_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer12_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer12_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer12_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer12_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer12_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer12_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer12_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer12_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer12_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer12_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer12_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer12_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer12_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer12_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer12_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer12_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer12_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer12_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer12_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer12_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer12_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer12_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer12_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer12_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer12_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer12_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer12_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer12_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer12_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer12_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer12_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer12_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer12_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer12_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer12_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer12_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer12_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer12_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer12_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer12_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer12_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer12_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer12_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer12_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer12_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer12_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer12_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer12_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer13 {
		label="Layer 13"
		fillcolor=lightgray style="rounded,filled"
		layer13_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer13_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer13_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer13_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer13_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer13_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer13_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer13_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer13_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer13_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer13_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer13_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer13_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer13_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer13_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer13_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer13_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer13_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer13_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer13_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer13_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer13_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer13_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer13_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer13_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer13_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer13_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer13_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer13_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer13_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer13_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer13_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer13_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer13_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer13_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer13_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer13_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer13_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer13_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer13_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer13_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer13_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer13_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer13_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer13_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer13_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer13_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer13_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer13_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer13_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer13_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer13_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer13_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer13_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer13_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer13_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer13_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer13_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer13_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer13_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer13_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer13_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer13_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer13_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer13_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer13_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer13_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer13_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer13_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer13_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer13_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer13_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer13_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer13_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer13_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer13_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer13_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer13_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer13_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer13_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer13_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer13_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer13_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer13_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer13_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer13_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer13_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer13_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer13_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer13_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer14 {
		label="Layer 14"
		fillcolor=lightgray style="rounded,filled"
		layer14_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer14_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer14_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer14_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer14_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer14_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer14_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer14_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer14_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer14_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer14_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer14_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer14_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer14_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer14_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer14_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer14_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer14_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer14_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer14_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer14_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer14_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer14_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer14_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer14_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer14_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer14_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer14_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer14_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer14_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer14_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer14_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer14_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer14_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer14_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer14_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer14_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer14_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer14_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer14_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer14_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer14_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer14_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer14_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer14_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer14_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer14_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer14_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer14_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer14_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer14_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer14_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer14_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer14_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer14_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer14_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer14_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer14_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer14_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer14_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer14_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer14_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer14_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer14_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer14_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer14_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer14_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer14_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer14_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer14_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer14_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer14_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer14_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer14_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer14_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer14_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer14_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer14_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer14_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer14_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer14_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer14_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer14_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer14_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer14_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer14_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer14_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer14_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer14_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer14_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	subgraph cluster_layer15 {
		label="Layer 15"
		fillcolor=lightgray style="rounded,filled"
		layer15_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer15_qkv_proj [label="QKV Projection\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer15_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64, 3x]\nOutput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer15_attn_out [label="Attention Output\nInput: [batch_size=128, seq_len=128, heads=16, head_dim=64]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer15_residual1 [label="Residual Add 1\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer15_layernorm2 [label="LayerNorm 2\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer15_global_gate [label="Global Gate\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, experts=64]\nGPU: All" fillcolor=lightgreen shape=rectangle]
		layer15_routing [label="Token Routing\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer15_gpu0_token_recv [label="Token Receive GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer15_gpu0_expert_mlp1 [label="Expert 0 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer15_gpu0_expert_act [label="Expert 0 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer15_gpu0_expert_mlp2 [label="Expert 0 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 0" fillcolor=lightgreen shape=rectangle]
		layer15_gpu0_token_send [label="Token Send GPU0\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 0" fillcolor=lightblue shape=ellipse]
		layer15_gpu1_token_recv [label="Token Receive GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer15_gpu1_expert_mlp1 [label="Expert 1 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer15_gpu1_expert_act [label="Expert 1 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer15_gpu1_expert_mlp2 [label="Expert 1 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 1" fillcolor=lightgreen shape=rectangle]
		layer15_gpu1_token_send [label="Token Send GPU1\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 1" fillcolor=lightblue shape=ellipse]
		layer15_gpu2_token_recv [label="Token Receive GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer15_gpu2_expert_mlp1 [label="Expert 2 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer15_gpu2_expert_act [label="Expert 2 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer15_gpu2_expert_mlp2 [label="Expert 2 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 2" fillcolor=lightgreen shape=rectangle]
		layer15_gpu2_token_send [label="Token Send GPU2\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 2" fillcolor=lightblue shape=ellipse]
		layer15_gpu3_token_recv [label="Token Receive GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer15_gpu3_expert_mlp1 [label="Expert 3 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer15_gpu3_expert_act [label="Expert 3 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer15_gpu3_expert_mlp2 [label="Expert 3 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 3" fillcolor=lightgreen shape=rectangle]
		layer15_gpu3_token_send [label="Token Send GPU3\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 3" fillcolor=lightblue shape=ellipse]
		layer15_gpu4_token_recv [label="Token Receive GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer15_gpu4_expert_mlp1 [label="Expert 4 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer15_gpu4_expert_act [label="Expert 4 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer15_gpu4_expert_mlp2 [label="Expert 4 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 4" fillcolor=lightgreen shape=rectangle]
		layer15_gpu4_token_send [label="Token Send GPU4\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 4" fillcolor=lightblue shape=ellipse]
		layer15_gpu5_token_recv [label="Token Receive GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer15_gpu5_expert_mlp1 [label="Expert 5 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer15_gpu5_expert_act [label="Expert 5 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer15_gpu5_expert_mlp2 [label="Expert 5 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 5" fillcolor=lightgreen shape=rectangle]
		layer15_gpu5_token_send [label="Token Send GPU5\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 5" fillcolor=lightblue shape=ellipse]
		layer15_gpu6_token_recv [label="Token Receive GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer15_gpu6_expert_mlp1 [label="Expert 6 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer15_gpu6_expert_act [label="Expert 6 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer15_gpu6_expert_mlp2 [label="Expert 6 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 6" fillcolor=lightgreen shape=rectangle]
		layer15_gpu6_token_send [label="Token Send GPU6\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 6" fillcolor=lightblue shape=ellipse]
		layer15_gpu7_token_recv [label="Token Receive GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer15_gpu7_expert_mlp1 [label="Expert 7 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer15_gpu7_expert_act [label="Expert 7 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer15_gpu7_expert_mlp2 [label="Expert 7 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 7" fillcolor=lightgreen shape=rectangle]
		layer15_gpu7_token_send [label="Token Send GPU7\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 7" fillcolor=lightblue shape=ellipse]
		layer15_gpu8_token_recv [label="Token Receive GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer15_gpu8_expert_mlp1 [label="Expert 8 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer15_gpu8_expert_act [label="Expert 8 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer15_gpu8_expert_mlp2 [label="Expert 8 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 8" fillcolor=lightgreen shape=rectangle]
		layer15_gpu8_token_send [label="Token Send GPU8\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 8" fillcolor=lightblue shape=ellipse]
		layer15_gpu9_token_recv [label="Token Receive GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer15_gpu9_expert_mlp1 [label="Expert 9 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer15_gpu9_expert_act [label="Expert 9 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer15_gpu9_expert_mlp2 [label="Expert 9 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 9" fillcolor=lightgreen shape=rectangle]
		layer15_gpu9_token_send [label="Token Send GPU9\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 9" fillcolor=lightblue shape=ellipse]
		layer15_gpu10_token_recv [label="Token Receive GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer15_gpu10_expert_mlp1 [label="Expert 10 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer15_gpu10_expert_act [label="Expert 10 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer15_gpu10_expert_mlp2 [label="Expert 10 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 10" fillcolor=lightgreen shape=rectangle]
		layer15_gpu10_token_send [label="Token Send GPU10\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 10" fillcolor=lightblue shape=ellipse]
		layer15_gpu11_token_recv [label="Token Receive GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer15_gpu11_expert_mlp1 [label="Expert 11 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer15_gpu11_expert_act [label="Expert 11 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer15_gpu11_expert_mlp2 [label="Expert 11 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 11" fillcolor=lightgreen shape=rectangle]
		layer15_gpu11_token_send [label="Token Send GPU11\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 11" fillcolor=lightblue shape=ellipse]
		layer15_gpu12_token_recv [label="Token Receive GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer15_gpu12_expert_mlp1 [label="Expert 12 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer15_gpu12_expert_act [label="Expert 12 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer15_gpu12_expert_mlp2 [label="Expert 12 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 12" fillcolor=lightgreen shape=rectangle]
		layer15_gpu12_token_send [label="Token Send GPU12\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 12" fillcolor=lightblue shape=ellipse]
		layer15_gpu13_token_recv [label="Token Receive GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer15_gpu13_expert_mlp1 [label="Expert 13 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer15_gpu13_expert_act [label="Expert 13 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer15_gpu13_expert_mlp2 [label="Expert 13 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 13" fillcolor=lightgreen shape=rectangle]
		layer15_gpu13_token_send [label="Token Send GPU13\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 13" fillcolor=lightblue shape=ellipse]
		layer15_gpu14_token_recv [label="Token Receive GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer15_gpu14_expert_mlp1 [label="Expert 14 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer15_gpu14_expert_act [label="Expert 14 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer15_gpu14_expert_mlp2 [label="Expert 14 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 14" fillcolor=lightgreen shape=rectangle]
		layer15_gpu14_token_send [label="Token Send GPU14\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 14" fillcolor=lightblue shape=ellipse]
		layer15_gpu15_token_recv [label="Token Receive GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (routed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer15_gpu15_expert_mlp1 [label="Expert 15 MLP1\nInput: [batch_size=128, seq_len=128, hidden=1024] (filtered)\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer15_gpu15_expert_act [label="Expert 15 Activation\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, ffn=2048]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer15_gpu15_expert_mlp2 [label="Expert 15 MLP2\nInput: [batch_size=128, seq_len=128, ffn=2048]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: 15" fillcolor=lightgreen shape=rectangle]
		layer15_gpu15_token_send [label="Token Send GPU15\nInput: [batch_size=128, seq_len=128, hidden=1024] (processed)\nOutput: [batch_size=128, seq_len=128, hidden=1024] (aggregated)\nGPU: 15" fillcolor=lightblue shape=ellipse]
		layer15_expert_agg [label="Expert Aggregation\nInput: [batch_size=128, seq_len=128, hidden=1024] (x16)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
		layer15_residual2 [label="Residual Add 2\nInput: [batch_size=128, seq_len=128, hidden=1024] (x2)\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightyellow shape=parallelogram]
	}
	input -> layer0_layernorm
	layer0_layernorm -> layer0_qkv_proj
	layer0_qkv_proj -> layer0_attention
	layer0_attention -> layer0_attn_out
	layer0_attn_out -> layer0_residual1
	layer0_residual1 -> layer0_layernorm2
	layer0_layernorm2 -> layer0_global_gate
	layer0_global_gate -> layer0_routing
	layer0_routing -> layer0_gpu0_token_recv [style=dashed]
	layer0_gpu0_token_recv -> layer0_gpu0_expert_mlp1
	layer0_gpu0_expert_mlp1 -> layer0_gpu0_expert_act
	layer0_gpu0_expert_act -> layer0_gpu0_expert_mlp2
	layer0_gpu0_expert_mlp2 -> layer0_gpu0_token_send
	layer0_gpu0_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu1_token_recv [style=dashed]
	layer0_gpu1_token_recv -> layer0_gpu1_expert_mlp1
	layer0_gpu1_expert_mlp1 -> layer0_gpu1_expert_act
	layer0_gpu1_expert_act -> layer0_gpu1_expert_mlp2
	layer0_gpu1_expert_mlp2 -> layer0_gpu1_token_send
	layer0_gpu1_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu2_token_recv [style=dashed]
	layer0_gpu2_token_recv -> layer0_gpu2_expert_mlp1
	layer0_gpu2_expert_mlp1 -> layer0_gpu2_expert_act
	layer0_gpu2_expert_act -> layer0_gpu2_expert_mlp2
	layer0_gpu2_expert_mlp2 -> layer0_gpu2_token_send
	layer0_gpu2_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu3_token_recv [style=dashed]
	layer0_gpu3_token_recv -> layer0_gpu3_expert_mlp1
	layer0_gpu3_expert_mlp1 -> layer0_gpu3_expert_act
	layer0_gpu3_expert_act -> layer0_gpu3_expert_mlp2
	layer0_gpu3_expert_mlp2 -> layer0_gpu3_token_send
	layer0_gpu3_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu4_token_recv [style=dashed]
	layer0_gpu4_token_recv -> layer0_gpu4_expert_mlp1
	layer0_gpu4_expert_mlp1 -> layer0_gpu4_expert_act
	layer0_gpu4_expert_act -> layer0_gpu4_expert_mlp2
	layer0_gpu4_expert_mlp2 -> layer0_gpu4_token_send
	layer0_gpu4_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu5_token_recv [style=dashed]
	layer0_gpu5_token_recv -> layer0_gpu5_expert_mlp1
	layer0_gpu5_expert_mlp1 -> layer0_gpu5_expert_act
	layer0_gpu5_expert_act -> layer0_gpu5_expert_mlp2
	layer0_gpu5_expert_mlp2 -> layer0_gpu5_token_send
	layer0_gpu5_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu6_token_recv [style=dashed]
	layer0_gpu6_token_recv -> layer0_gpu6_expert_mlp1
	layer0_gpu6_expert_mlp1 -> layer0_gpu6_expert_act
	layer0_gpu6_expert_act -> layer0_gpu6_expert_mlp2
	layer0_gpu6_expert_mlp2 -> layer0_gpu6_token_send
	layer0_gpu6_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu7_token_recv [style=dashed]
	layer0_gpu7_token_recv -> layer0_gpu7_expert_mlp1
	layer0_gpu7_expert_mlp1 -> layer0_gpu7_expert_act
	layer0_gpu7_expert_act -> layer0_gpu7_expert_mlp2
	layer0_gpu7_expert_mlp2 -> layer0_gpu7_token_send
	layer0_gpu7_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu8_token_recv [style=dashed]
	layer0_gpu8_token_recv -> layer0_gpu8_expert_mlp1
	layer0_gpu8_expert_mlp1 -> layer0_gpu8_expert_act
	layer0_gpu8_expert_act -> layer0_gpu8_expert_mlp2
	layer0_gpu8_expert_mlp2 -> layer0_gpu8_token_send
	layer0_gpu8_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu9_token_recv [style=dashed]
	layer0_gpu9_token_recv -> layer0_gpu9_expert_mlp1
	layer0_gpu9_expert_mlp1 -> layer0_gpu9_expert_act
	layer0_gpu9_expert_act -> layer0_gpu9_expert_mlp2
	layer0_gpu9_expert_mlp2 -> layer0_gpu9_token_send
	layer0_gpu9_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu10_token_recv [style=dashed]
	layer0_gpu10_token_recv -> layer0_gpu10_expert_mlp1
	layer0_gpu10_expert_mlp1 -> layer0_gpu10_expert_act
	layer0_gpu10_expert_act -> layer0_gpu10_expert_mlp2
	layer0_gpu10_expert_mlp2 -> layer0_gpu10_token_send
	layer0_gpu10_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu11_token_recv [style=dashed]
	layer0_gpu11_token_recv -> layer0_gpu11_expert_mlp1
	layer0_gpu11_expert_mlp1 -> layer0_gpu11_expert_act
	layer0_gpu11_expert_act -> layer0_gpu11_expert_mlp2
	layer0_gpu11_expert_mlp2 -> layer0_gpu11_token_send
	layer0_gpu11_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu12_token_recv [style=dashed]
	layer0_gpu12_token_recv -> layer0_gpu12_expert_mlp1
	layer0_gpu12_expert_mlp1 -> layer0_gpu12_expert_act
	layer0_gpu12_expert_act -> layer0_gpu12_expert_mlp2
	layer0_gpu12_expert_mlp2 -> layer0_gpu12_token_send
	layer0_gpu12_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu13_token_recv [style=dashed]
	layer0_gpu13_token_recv -> layer0_gpu13_expert_mlp1
	layer0_gpu13_expert_mlp1 -> layer0_gpu13_expert_act
	layer0_gpu13_expert_act -> layer0_gpu13_expert_mlp2
	layer0_gpu13_expert_mlp2 -> layer0_gpu13_token_send
	layer0_gpu13_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu14_token_recv [style=dashed]
	layer0_gpu14_token_recv -> layer0_gpu14_expert_mlp1
	layer0_gpu14_expert_mlp1 -> layer0_gpu14_expert_act
	layer0_gpu14_expert_act -> layer0_gpu14_expert_mlp2
	layer0_gpu14_expert_mlp2 -> layer0_gpu14_token_send
	layer0_gpu14_token_send -> layer0_expert_agg
	layer0_routing -> layer0_gpu15_token_recv [style=dashed]
	layer0_gpu15_token_recv -> layer0_gpu15_expert_mlp1
	layer0_gpu15_expert_mlp1 -> layer0_gpu15_expert_act
	layer0_gpu15_expert_act -> layer0_gpu15_expert_mlp2
	layer0_gpu15_expert_mlp2 -> layer0_gpu15_token_send
	layer0_gpu15_token_send -> layer0_expert_agg
	layer0_expert_agg -> layer0_residual2
	layer0_residual2 -> layer1_layernorm
	layer1_layernorm -> layer1_qkv_proj
	layer1_qkv_proj -> layer1_attention
	layer1_attention -> layer1_attn_out
	layer1_attn_out -> layer1_residual1
	layer1_residual1 -> layer1_layernorm2
	layer1_layernorm2 -> layer1_global_gate
	layer1_global_gate -> layer1_routing
	layer1_routing -> layer1_gpu0_token_recv [style=dashed]
	layer1_gpu0_token_recv -> layer1_gpu0_expert_mlp1
	layer1_gpu0_expert_mlp1 -> layer1_gpu0_expert_act
	layer1_gpu0_expert_act -> layer1_gpu0_expert_mlp2
	layer1_gpu0_expert_mlp2 -> layer1_gpu0_token_send
	layer1_gpu0_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu1_token_recv [style=dashed]
	layer1_gpu1_token_recv -> layer1_gpu1_expert_mlp1
	layer1_gpu1_expert_mlp1 -> layer1_gpu1_expert_act
	layer1_gpu1_expert_act -> layer1_gpu1_expert_mlp2
	layer1_gpu1_expert_mlp2 -> layer1_gpu1_token_send
	layer1_gpu1_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu2_token_recv [style=dashed]
	layer1_gpu2_token_recv -> layer1_gpu2_expert_mlp1
	layer1_gpu2_expert_mlp1 -> layer1_gpu2_expert_act
	layer1_gpu2_expert_act -> layer1_gpu2_expert_mlp2
	layer1_gpu2_expert_mlp2 -> layer1_gpu2_token_send
	layer1_gpu2_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu3_token_recv [style=dashed]
	layer1_gpu3_token_recv -> layer1_gpu3_expert_mlp1
	layer1_gpu3_expert_mlp1 -> layer1_gpu3_expert_act
	layer1_gpu3_expert_act -> layer1_gpu3_expert_mlp2
	layer1_gpu3_expert_mlp2 -> layer1_gpu3_token_send
	layer1_gpu3_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu4_token_recv [style=dashed]
	layer1_gpu4_token_recv -> layer1_gpu4_expert_mlp1
	layer1_gpu4_expert_mlp1 -> layer1_gpu4_expert_act
	layer1_gpu4_expert_act -> layer1_gpu4_expert_mlp2
	layer1_gpu4_expert_mlp2 -> layer1_gpu4_token_send
	layer1_gpu4_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu5_token_recv [style=dashed]
	layer1_gpu5_token_recv -> layer1_gpu5_expert_mlp1
	layer1_gpu5_expert_mlp1 -> layer1_gpu5_expert_act
	layer1_gpu5_expert_act -> layer1_gpu5_expert_mlp2
	layer1_gpu5_expert_mlp2 -> layer1_gpu5_token_send
	layer1_gpu5_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu6_token_recv [style=dashed]
	layer1_gpu6_token_recv -> layer1_gpu6_expert_mlp1
	layer1_gpu6_expert_mlp1 -> layer1_gpu6_expert_act
	layer1_gpu6_expert_act -> layer1_gpu6_expert_mlp2
	layer1_gpu6_expert_mlp2 -> layer1_gpu6_token_send
	layer1_gpu6_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu7_token_recv [style=dashed]
	layer1_gpu7_token_recv -> layer1_gpu7_expert_mlp1
	layer1_gpu7_expert_mlp1 -> layer1_gpu7_expert_act
	layer1_gpu7_expert_act -> layer1_gpu7_expert_mlp2
	layer1_gpu7_expert_mlp2 -> layer1_gpu7_token_send
	layer1_gpu7_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu8_token_recv [style=dashed]
	layer1_gpu8_token_recv -> layer1_gpu8_expert_mlp1
	layer1_gpu8_expert_mlp1 -> layer1_gpu8_expert_act
	layer1_gpu8_expert_act -> layer1_gpu8_expert_mlp2
	layer1_gpu8_expert_mlp2 -> layer1_gpu8_token_send
	layer1_gpu8_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu9_token_recv [style=dashed]
	layer1_gpu9_token_recv -> layer1_gpu9_expert_mlp1
	layer1_gpu9_expert_mlp1 -> layer1_gpu9_expert_act
	layer1_gpu9_expert_act -> layer1_gpu9_expert_mlp2
	layer1_gpu9_expert_mlp2 -> layer1_gpu9_token_send
	layer1_gpu9_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu10_token_recv [style=dashed]
	layer1_gpu10_token_recv -> layer1_gpu10_expert_mlp1
	layer1_gpu10_expert_mlp1 -> layer1_gpu10_expert_act
	layer1_gpu10_expert_act -> layer1_gpu10_expert_mlp2
	layer1_gpu10_expert_mlp2 -> layer1_gpu10_token_send
	layer1_gpu10_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu11_token_recv [style=dashed]
	layer1_gpu11_token_recv -> layer1_gpu11_expert_mlp1
	layer1_gpu11_expert_mlp1 -> layer1_gpu11_expert_act
	layer1_gpu11_expert_act -> layer1_gpu11_expert_mlp2
	layer1_gpu11_expert_mlp2 -> layer1_gpu11_token_send
	layer1_gpu11_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu12_token_recv [style=dashed]
	layer1_gpu12_token_recv -> layer1_gpu12_expert_mlp1
	layer1_gpu12_expert_mlp1 -> layer1_gpu12_expert_act
	layer1_gpu12_expert_act -> layer1_gpu12_expert_mlp2
	layer1_gpu12_expert_mlp2 -> layer1_gpu12_token_send
	layer1_gpu12_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu13_token_recv [style=dashed]
	layer1_gpu13_token_recv -> layer1_gpu13_expert_mlp1
	layer1_gpu13_expert_mlp1 -> layer1_gpu13_expert_act
	layer1_gpu13_expert_act -> layer1_gpu13_expert_mlp2
	layer1_gpu13_expert_mlp2 -> layer1_gpu13_token_send
	layer1_gpu13_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu14_token_recv [style=dashed]
	layer1_gpu14_token_recv -> layer1_gpu14_expert_mlp1
	layer1_gpu14_expert_mlp1 -> layer1_gpu14_expert_act
	layer1_gpu14_expert_act -> layer1_gpu14_expert_mlp2
	layer1_gpu14_expert_mlp2 -> layer1_gpu14_token_send
	layer1_gpu14_token_send -> layer1_expert_agg
	layer1_routing -> layer1_gpu15_token_recv [style=dashed]
	layer1_gpu15_token_recv -> layer1_gpu15_expert_mlp1
	layer1_gpu15_expert_mlp1 -> layer1_gpu15_expert_act
	layer1_gpu15_expert_act -> layer1_gpu15_expert_mlp2
	layer1_gpu15_expert_mlp2 -> layer1_gpu15_token_send
	layer1_gpu15_token_send -> layer1_expert_agg
	layer1_expert_agg -> layer1_residual2
	layer1_residual2 -> layer2_layernorm
	layer2_layernorm -> layer2_qkv_proj
	layer2_qkv_proj -> layer2_attention
	layer2_attention -> layer2_attn_out
	layer2_attn_out -> layer2_residual1
	layer2_residual1 -> layer2_layernorm2
	layer2_layernorm2 -> layer2_global_gate
	layer2_global_gate -> layer2_routing
	layer2_routing -> layer2_gpu0_token_recv [style=dashed]
	layer2_gpu0_token_recv -> layer2_gpu0_expert_mlp1
	layer2_gpu0_expert_mlp1 -> layer2_gpu0_expert_act
	layer2_gpu0_expert_act -> layer2_gpu0_expert_mlp2
	layer2_gpu0_expert_mlp2 -> layer2_gpu0_token_send
	layer2_gpu0_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu1_token_recv [style=dashed]
	layer2_gpu1_token_recv -> layer2_gpu1_expert_mlp1
	layer2_gpu1_expert_mlp1 -> layer2_gpu1_expert_act
	layer2_gpu1_expert_act -> layer2_gpu1_expert_mlp2
	layer2_gpu1_expert_mlp2 -> layer2_gpu1_token_send
	layer2_gpu1_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu2_token_recv [style=dashed]
	layer2_gpu2_token_recv -> layer2_gpu2_expert_mlp1
	layer2_gpu2_expert_mlp1 -> layer2_gpu2_expert_act
	layer2_gpu2_expert_act -> layer2_gpu2_expert_mlp2
	layer2_gpu2_expert_mlp2 -> layer2_gpu2_token_send
	layer2_gpu2_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu3_token_recv [style=dashed]
	layer2_gpu3_token_recv -> layer2_gpu3_expert_mlp1
	layer2_gpu3_expert_mlp1 -> layer2_gpu3_expert_act
	layer2_gpu3_expert_act -> layer2_gpu3_expert_mlp2
	layer2_gpu3_expert_mlp2 -> layer2_gpu3_token_send
	layer2_gpu3_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu4_token_recv [style=dashed]
	layer2_gpu4_token_recv -> layer2_gpu4_expert_mlp1
	layer2_gpu4_expert_mlp1 -> layer2_gpu4_expert_act
	layer2_gpu4_expert_act -> layer2_gpu4_expert_mlp2
	layer2_gpu4_expert_mlp2 -> layer2_gpu4_token_send
	layer2_gpu4_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu5_token_recv [style=dashed]
	layer2_gpu5_token_recv -> layer2_gpu5_expert_mlp1
	layer2_gpu5_expert_mlp1 -> layer2_gpu5_expert_act
	layer2_gpu5_expert_act -> layer2_gpu5_expert_mlp2
	layer2_gpu5_expert_mlp2 -> layer2_gpu5_token_send
	layer2_gpu5_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu6_token_recv [style=dashed]
	layer2_gpu6_token_recv -> layer2_gpu6_expert_mlp1
	layer2_gpu6_expert_mlp1 -> layer2_gpu6_expert_act
	layer2_gpu6_expert_act -> layer2_gpu6_expert_mlp2
	layer2_gpu6_expert_mlp2 -> layer2_gpu6_token_send
	layer2_gpu6_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu7_token_recv [style=dashed]
	layer2_gpu7_token_recv -> layer2_gpu7_expert_mlp1
	layer2_gpu7_expert_mlp1 -> layer2_gpu7_expert_act
	layer2_gpu7_expert_act -> layer2_gpu7_expert_mlp2
	layer2_gpu7_expert_mlp2 -> layer2_gpu7_token_send
	layer2_gpu7_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu8_token_recv [style=dashed]
	layer2_gpu8_token_recv -> layer2_gpu8_expert_mlp1
	layer2_gpu8_expert_mlp1 -> layer2_gpu8_expert_act
	layer2_gpu8_expert_act -> layer2_gpu8_expert_mlp2
	layer2_gpu8_expert_mlp2 -> layer2_gpu8_token_send
	layer2_gpu8_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu9_token_recv [style=dashed]
	layer2_gpu9_token_recv -> layer2_gpu9_expert_mlp1
	layer2_gpu9_expert_mlp1 -> layer2_gpu9_expert_act
	layer2_gpu9_expert_act -> layer2_gpu9_expert_mlp2
	layer2_gpu9_expert_mlp2 -> layer2_gpu9_token_send
	layer2_gpu9_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu10_token_recv [style=dashed]
	layer2_gpu10_token_recv -> layer2_gpu10_expert_mlp1
	layer2_gpu10_expert_mlp1 -> layer2_gpu10_expert_act
	layer2_gpu10_expert_act -> layer2_gpu10_expert_mlp2
	layer2_gpu10_expert_mlp2 -> layer2_gpu10_token_send
	layer2_gpu10_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu11_token_recv [style=dashed]
	layer2_gpu11_token_recv -> layer2_gpu11_expert_mlp1
	layer2_gpu11_expert_mlp1 -> layer2_gpu11_expert_act
	layer2_gpu11_expert_act -> layer2_gpu11_expert_mlp2
	layer2_gpu11_expert_mlp2 -> layer2_gpu11_token_send
	layer2_gpu11_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu12_token_recv [style=dashed]
	layer2_gpu12_token_recv -> layer2_gpu12_expert_mlp1
	layer2_gpu12_expert_mlp1 -> layer2_gpu12_expert_act
	layer2_gpu12_expert_act -> layer2_gpu12_expert_mlp2
	layer2_gpu12_expert_mlp2 -> layer2_gpu12_token_send
	layer2_gpu12_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu13_token_recv [style=dashed]
	layer2_gpu13_token_recv -> layer2_gpu13_expert_mlp1
	layer2_gpu13_expert_mlp1 -> layer2_gpu13_expert_act
	layer2_gpu13_expert_act -> layer2_gpu13_expert_mlp2
	layer2_gpu13_expert_mlp2 -> layer2_gpu13_token_send
	layer2_gpu13_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu14_token_recv [style=dashed]
	layer2_gpu14_token_recv -> layer2_gpu14_expert_mlp1
	layer2_gpu14_expert_mlp1 -> layer2_gpu14_expert_act
	layer2_gpu14_expert_act -> layer2_gpu14_expert_mlp2
	layer2_gpu14_expert_mlp2 -> layer2_gpu14_token_send
	layer2_gpu14_token_send -> layer2_expert_agg
	layer2_routing -> layer2_gpu15_token_recv [style=dashed]
	layer2_gpu15_token_recv -> layer2_gpu15_expert_mlp1
	layer2_gpu15_expert_mlp1 -> layer2_gpu15_expert_act
	layer2_gpu15_expert_act -> layer2_gpu15_expert_mlp2
	layer2_gpu15_expert_mlp2 -> layer2_gpu15_token_send
	layer2_gpu15_token_send -> layer2_expert_agg
	layer2_expert_agg -> layer2_residual2
	layer2_residual2 -> layer3_layernorm
	layer3_layernorm -> layer3_qkv_proj
	layer3_qkv_proj -> layer3_attention
	layer3_attention -> layer3_attn_out
	layer3_attn_out -> layer3_residual1
	layer3_residual1 -> layer3_layernorm2
	layer3_layernorm2 -> layer3_global_gate
	layer3_global_gate -> layer3_routing
	layer3_routing -> layer3_gpu0_token_recv [style=dashed]
	layer3_gpu0_token_recv -> layer3_gpu0_expert_mlp1
	layer3_gpu0_expert_mlp1 -> layer3_gpu0_expert_act
	layer3_gpu0_expert_act -> layer3_gpu0_expert_mlp2
	layer3_gpu0_expert_mlp2 -> layer3_gpu0_token_send
	layer3_gpu0_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu1_token_recv [style=dashed]
	layer3_gpu1_token_recv -> layer3_gpu1_expert_mlp1
	layer3_gpu1_expert_mlp1 -> layer3_gpu1_expert_act
	layer3_gpu1_expert_act -> layer3_gpu1_expert_mlp2
	layer3_gpu1_expert_mlp2 -> layer3_gpu1_token_send
	layer3_gpu1_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu2_token_recv [style=dashed]
	layer3_gpu2_token_recv -> layer3_gpu2_expert_mlp1
	layer3_gpu2_expert_mlp1 -> layer3_gpu2_expert_act
	layer3_gpu2_expert_act -> layer3_gpu2_expert_mlp2
	layer3_gpu2_expert_mlp2 -> layer3_gpu2_token_send
	layer3_gpu2_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu3_token_recv [style=dashed]
	layer3_gpu3_token_recv -> layer3_gpu3_expert_mlp1
	layer3_gpu3_expert_mlp1 -> layer3_gpu3_expert_act
	layer3_gpu3_expert_act -> layer3_gpu3_expert_mlp2
	layer3_gpu3_expert_mlp2 -> layer3_gpu3_token_send
	layer3_gpu3_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu4_token_recv [style=dashed]
	layer3_gpu4_token_recv -> layer3_gpu4_expert_mlp1
	layer3_gpu4_expert_mlp1 -> layer3_gpu4_expert_act
	layer3_gpu4_expert_act -> layer3_gpu4_expert_mlp2
	layer3_gpu4_expert_mlp2 -> layer3_gpu4_token_send
	layer3_gpu4_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu5_token_recv [style=dashed]
	layer3_gpu5_token_recv -> layer3_gpu5_expert_mlp1
	layer3_gpu5_expert_mlp1 -> layer3_gpu5_expert_act
	layer3_gpu5_expert_act -> layer3_gpu5_expert_mlp2
	layer3_gpu5_expert_mlp2 -> layer3_gpu5_token_send
	layer3_gpu5_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu6_token_recv [style=dashed]
	layer3_gpu6_token_recv -> layer3_gpu6_expert_mlp1
	layer3_gpu6_expert_mlp1 -> layer3_gpu6_expert_act
	layer3_gpu6_expert_act -> layer3_gpu6_expert_mlp2
	layer3_gpu6_expert_mlp2 -> layer3_gpu6_token_send
	layer3_gpu6_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu7_token_recv [style=dashed]
	layer3_gpu7_token_recv -> layer3_gpu7_expert_mlp1
	layer3_gpu7_expert_mlp1 -> layer3_gpu7_expert_act
	layer3_gpu7_expert_act -> layer3_gpu7_expert_mlp2
	layer3_gpu7_expert_mlp2 -> layer3_gpu7_token_send
	layer3_gpu7_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu8_token_recv [style=dashed]
	layer3_gpu8_token_recv -> layer3_gpu8_expert_mlp1
	layer3_gpu8_expert_mlp1 -> layer3_gpu8_expert_act
	layer3_gpu8_expert_act -> layer3_gpu8_expert_mlp2
	layer3_gpu8_expert_mlp2 -> layer3_gpu8_token_send
	layer3_gpu8_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu9_token_recv [style=dashed]
	layer3_gpu9_token_recv -> layer3_gpu9_expert_mlp1
	layer3_gpu9_expert_mlp1 -> layer3_gpu9_expert_act
	layer3_gpu9_expert_act -> layer3_gpu9_expert_mlp2
	layer3_gpu9_expert_mlp2 -> layer3_gpu9_token_send
	layer3_gpu9_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu10_token_recv [style=dashed]
	layer3_gpu10_token_recv -> layer3_gpu10_expert_mlp1
	layer3_gpu10_expert_mlp1 -> layer3_gpu10_expert_act
	layer3_gpu10_expert_act -> layer3_gpu10_expert_mlp2
	layer3_gpu10_expert_mlp2 -> layer3_gpu10_token_send
	layer3_gpu10_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu11_token_recv [style=dashed]
	layer3_gpu11_token_recv -> layer3_gpu11_expert_mlp1
	layer3_gpu11_expert_mlp1 -> layer3_gpu11_expert_act
	layer3_gpu11_expert_act -> layer3_gpu11_expert_mlp2
	layer3_gpu11_expert_mlp2 -> layer3_gpu11_token_send
	layer3_gpu11_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu12_token_recv [style=dashed]
	layer3_gpu12_token_recv -> layer3_gpu12_expert_mlp1
	layer3_gpu12_expert_mlp1 -> layer3_gpu12_expert_act
	layer3_gpu12_expert_act -> layer3_gpu12_expert_mlp2
	layer3_gpu12_expert_mlp2 -> layer3_gpu12_token_send
	layer3_gpu12_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu13_token_recv [style=dashed]
	layer3_gpu13_token_recv -> layer3_gpu13_expert_mlp1
	layer3_gpu13_expert_mlp1 -> layer3_gpu13_expert_act
	layer3_gpu13_expert_act -> layer3_gpu13_expert_mlp2
	layer3_gpu13_expert_mlp2 -> layer3_gpu13_token_send
	layer3_gpu13_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu14_token_recv [style=dashed]
	layer3_gpu14_token_recv -> layer3_gpu14_expert_mlp1
	layer3_gpu14_expert_mlp1 -> layer3_gpu14_expert_act
	layer3_gpu14_expert_act -> layer3_gpu14_expert_mlp2
	layer3_gpu14_expert_mlp2 -> layer3_gpu14_token_send
	layer3_gpu14_token_send -> layer3_expert_agg
	layer3_routing -> layer3_gpu15_token_recv [style=dashed]
	layer3_gpu15_token_recv -> layer3_gpu15_expert_mlp1
	layer3_gpu15_expert_mlp1 -> layer3_gpu15_expert_act
	layer3_gpu15_expert_act -> layer3_gpu15_expert_mlp2
	layer3_gpu15_expert_mlp2 -> layer3_gpu15_token_send
	layer3_gpu15_token_send -> layer3_expert_agg
	layer3_expert_agg -> layer3_residual2
	layer3_residual2 -> layer4_layernorm
	layer4_layernorm -> layer4_qkv_proj
	layer4_qkv_proj -> layer4_attention
	layer4_attention -> layer4_attn_out
	layer4_attn_out -> layer4_residual1
	layer4_residual1 -> layer4_layernorm2
	layer4_layernorm2 -> layer4_global_gate
	layer4_global_gate -> layer4_routing
	layer4_routing -> layer4_gpu0_token_recv [style=dashed]
	layer4_gpu0_token_recv -> layer4_gpu0_expert_mlp1
	layer4_gpu0_expert_mlp1 -> layer4_gpu0_expert_act
	layer4_gpu0_expert_act -> layer4_gpu0_expert_mlp2
	layer4_gpu0_expert_mlp2 -> layer4_gpu0_token_send
	layer4_gpu0_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu1_token_recv [style=dashed]
	layer4_gpu1_token_recv -> layer4_gpu1_expert_mlp1
	layer4_gpu1_expert_mlp1 -> layer4_gpu1_expert_act
	layer4_gpu1_expert_act -> layer4_gpu1_expert_mlp2
	layer4_gpu1_expert_mlp2 -> layer4_gpu1_token_send
	layer4_gpu1_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu2_token_recv [style=dashed]
	layer4_gpu2_token_recv -> layer4_gpu2_expert_mlp1
	layer4_gpu2_expert_mlp1 -> layer4_gpu2_expert_act
	layer4_gpu2_expert_act -> layer4_gpu2_expert_mlp2
	layer4_gpu2_expert_mlp2 -> layer4_gpu2_token_send
	layer4_gpu2_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu3_token_recv [style=dashed]
	layer4_gpu3_token_recv -> layer4_gpu3_expert_mlp1
	layer4_gpu3_expert_mlp1 -> layer4_gpu3_expert_act
	layer4_gpu3_expert_act -> layer4_gpu3_expert_mlp2
	layer4_gpu3_expert_mlp2 -> layer4_gpu3_token_send
	layer4_gpu3_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu4_token_recv [style=dashed]
	layer4_gpu4_token_recv -> layer4_gpu4_expert_mlp1
	layer4_gpu4_expert_mlp1 -> layer4_gpu4_expert_act
	layer4_gpu4_expert_act -> layer4_gpu4_expert_mlp2
	layer4_gpu4_expert_mlp2 -> layer4_gpu4_token_send
	layer4_gpu4_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu5_token_recv [style=dashed]
	layer4_gpu5_token_recv -> layer4_gpu5_expert_mlp1
	layer4_gpu5_expert_mlp1 -> layer4_gpu5_expert_act
	layer4_gpu5_expert_act -> layer4_gpu5_expert_mlp2
	layer4_gpu5_expert_mlp2 -> layer4_gpu5_token_send
	layer4_gpu5_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu6_token_recv [style=dashed]
	layer4_gpu6_token_recv -> layer4_gpu6_expert_mlp1
	layer4_gpu6_expert_mlp1 -> layer4_gpu6_expert_act
	layer4_gpu6_expert_act -> layer4_gpu6_expert_mlp2
	layer4_gpu6_expert_mlp2 -> layer4_gpu6_token_send
	layer4_gpu6_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu7_token_recv [style=dashed]
	layer4_gpu7_token_recv -> layer4_gpu7_expert_mlp1
	layer4_gpu7_expert_mlp1 -> layer4_gpu7_expert_act
	layer4_gpu7_expert_act -> layer4_gpu7_expert_mlp2
	layer4_gpu7_expert_mlp2 -> layer4_gpu7_token_send
	layer4_gpu7_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu8_token_recv [style=dashed]
	layer4_gpu8_token_recv -> layer4_gpu8_expert_mlp1
	layer4_gpu8_expert_mlp1 -> layer4_gpu8_expert_act
	layer4_gpu8_expert_act -> layer4_gpu8_expert_mlp2
	layer4_gpu8_expert_mlp2 -> layer4_gpu8_token_send
	layer4_gpu8_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu9_token_recv [style=dashed]
	layer4_gpu9_token_recv -> layer4_gpu9_expert_mlp1
	layer4_gpu9_expert_mlp1 -> layer4_gpu9_expert_act
	layer4_gpu9_expert_act -> layer4_gpu9_expert_mlp2
	layer4_gpu9_expert_mlp2 -> layer4_gpu9_token_send
	layer4_gpu9_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu10_token_recv [style=dashed]
	layer4_gpu10_token_recv -> layer4_gpu10_expert_mlp1
	layer4_gpu10_expert_mlp1 -> layer4_gpu10_expert_act
	layer4_gpu10_expert_act -> layer4_gpu10_expert_mlp2
	layer4_gpu10_expert_mlp2 -> layer4_gpu10_token_send
	layer4_gpu10_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu11_token_recv [style=dashed]
	layer4_gpu11_token_recv -> layer4_gpu11_expert_mlp1
	layer4_gpu11_expert_mlp1 -> layer4_gpu11_expert_act
	layer4_gpu11_expert_act -> layer4_gpu11_expert_mlp2
	layer4_gpu11_expert_mlp2 -> layer4_gpu11_token_send
	layer4_gpu11_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu12_token_recv [style=dashed]
	layer4_gpu12_token_recv -> layer4_gpu12_expert_mlp1
	layer4_gpu12_expert_mlp1 -> layer4_gpu12_expert_act
	layer4_gpu12_expert_act -> layer4_gpu12_expert_mlp2
	layer4_gpu12_expert_mlp2 -> layer4_gpu12_token_send
	layer4_gpu12_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu13_token_recv [style=dashed]
	layer4_gpu13_token_recv -> layer4_gpu13_expert_mlp1
	layer4_gpu13_expert_mlp1 -> layer4_gpu13_expert_act
	layer4_gpu13_expert_act -> layer4_gpu13_expert_mlp2
	layer4_gpu13_expert_mlp2 -> layer4_gpu13_token_send
	layer4_gpu13_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu14_token_recv [style=dashed]
	layer4_gpu14_token_recv -> layer4_gpu14_expert_mlp1
	layer4_gpu14_expert_mlp1 -> layer4_gpu14_expert_act
	layer4_gpu14_expert_act -> layer4_gpu14_expert_mlp2
	layer4_gpu14_expert_mlp2 -> layer4_gpu14_token_send
	layer4_gpu14_token_send -> layer4_expert_agg
	layer4_routing -> layer4_gpu15_token_recv [style=dashed]
	layer4_gpu15_token_recv -> layer4_gpu15_expert_mlp1
	layer4_gpu15_expert_mlp1 -> layer4_gpu15_expert_act
	layer4_gpu15_expert_act -> layer4_gpu15_expert_mlp2
	layer4_gpu15_expert_mlp2 -> layer4_gpu15_token_send
	layer4_gpu15_token_send -> layer4_expert_agg
	layer4_expert_agg -> layer4_residual2
	layer4_residual2 -> layer5_layernorm
	layer5_layernorm -> layer5_qkv_proj
	layer5_qkv_proj -> layer5_attention
	layer5_attention -> layer5_attn_out
	layer5_attn_out -> layer5_residual1
	layer5_residual1 -> layer5_layernorm2
	layer5_layernorm2 -> layer5_global_gate
	layer5_global_gate -> layer5_routing
	layer5_routing -> layer5_gpu0_token_recv [style=dashed]
	layer5_gpu0_token_recv -> layer5_gpu0_expert_mlp1
	layer5_gpu0_expert_mlp1 -> layer5_gpu0_expert_act
	layer5_gpu0_expert_act -> layer5_gpu0_expert_mlp2
	layer5_gpu0_expert_mlp2 -> layer5_gpu0_token_send
	layer5_gpu0_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu1_token_recv [style=dashed]
	layer5_gpu1_token_recv -> layer5_gpu1_expert_mlp1
	layer5_gpu1_expert_mlp1 -> layer5_gpu1_expert_act
	layer5_gpu1_expert_act -> layer5_gpu1_expert_mlp2
	layer5_gpu1_expert_mlp2 -> layer5_gpu1_token_send
	layer5_gpu1_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu2_token_recv [style=dashed]
	layer5_gpu2_token_recv -> layer5_gpu2_expert_mlp1
	layer5_gpu2_expert_mlp1 -> layer5_gpu2_expert_act
	layer5_gpu2_expert_act -> layer5_gpu2_expert_mlp2
	layer5_gpu2_expert_mlp2 -> layer5_gpu2_token_send
	layer5_gpu2_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu3_token_recv [style=dashed]
	layer5_gpu3_token_recv -> layer5_gpu3_expert_mlp1
	layer5_gpu3_expert_mlp1 -> layer5_gpu3_expert_act
	layer5_gpu3_expert_act -> layer5_gpu3_expert_mlp2
	layer5_gpu3_expert_mlp2 -> layer5_gpu3_token_send
	layer5_gpu3_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu4_token_recv [style=dashed]
	layer5_gpu4_token_recv -> layer5_gpu4_expert_mlp1
	layer5_gpu4_expert_mlp1 -> layer5_gpu4_expert_act
	layer5_gpu4_expert_act -> layer5_gpu4_expert_mlp2
	layer5_gpu4_expert_mlp2 -> layer5_gpu4_token_send
	layer5_gpu4_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu5_token_recv [style=dashed]
	layer5_gpu5_token_recv -> layer5_gpu5_expert_mlp1
	layer5_gpu5_expert_mlp1 -> layer5_gpu5_expert_act
	layer5_gpu5_expert_act -> layer5_gpu5_expert_mlp2
	layer5_gpu5_expert_mlp2 -> layer5_gpu5_token_send
	layer5_gpu5_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu6_token_recv [style=dashed]
	layer5_gpu6_token_recv -> layer5_gpu6_expert_mlp1
	layer5_gpu6_expert_mlp1 -> layer5_gpu6_expert_act
	layer5_gpu6_expert_act -> layer5_gpu6_expert_mlp2
	layer5_gpu6_expert_mlp2 -> layer5_gpu6_token_send
	layer5_gpu6_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu7_token_recv [style=dashed]
	layer5_gpu7_token_recv -> layer5_gpu7_expert_mlp1
	layer5_gpu7_expert_mlp1 -> layer5_gpu7_expert_act
	layer5_gpu7_expert_act -> layer5_gpu7_expert_mlp2
	layer5_gpu7_expert_mlp2 -> layer5_gpu7_token_send
	layer5_gpu7_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu8_token_recv [style=dashed]
	layer5_gpu8_token_recv -> layer5_gpu8_expert_mlp1
	layer5_gpu8_expert_mlp1 -> layer5_gpu8_expert_act
	layer5_gpu8_expert_act -> layer5_gpu8_expert_mlp2
	layer5_gpu8_expert_mlp2 -> layer5_gpu8_token_send
	layer5_gpu8_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu9_token_recv [style=dashed]
	layer5_gpu9_token_recv -> layer5_gpu9_expert_mlp1
	layer5_gpu9_expert_mlp1 -> layer5_gpu9_expert_act
	layer5_gpu9_expert_act -> layer5_gpu9_expert_mlp2
	layer5_gpu9_expert_mlp2 -> layer5_gpu9_token_send
	layer5_gpu9_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu10_token_recv [style=dashed]
	layer5_gpu10_token_recv -> layer5_gpu10_expert_mlp1
	layer5_gpu10_expert_mlp1 -> layer5_gpu10_expert_act
	layer5_gpu10_expert_act -> layer5_gpu10_expert_mlp2
	layer5_gpu10_expert_mlp2 -> layer5_gpu10_token_send
	layer5_gpu10_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu11_token_recv [style=dashed]
	layer5_gpu11_token_recv -> layer5_gpu11_expert_mlp1
	layer5_gpu11_expert_mlp1 -> layer5_gpu11_expert_act
	layer5_gpu11_expert_act -> layer5_gpu11_expert_mlp2
	layer5_gpu11_expert_mlp2 -> layer5_gpu11_token_send
	layer5_gpu11_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu12_token_recv [style=dashed]
	layer5_gpu12_token_recv -> layer5_gpu12_expert_mlp1
	layer5_gpu12_expert_mlp1 -> layer5_gpu12_expert_act
	layer5_gpu12_expert_act -> layer5_gpu12_expert_mlp2
	layer5_gpu12_expert_mlp2 -> layer5_gpu12_token_send
	layer5_gpu12_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu13_token_recv [style=dashed]
	layer5_gpu13_token_recv -> layer5_gpu13_expert_mlp1
	layer5_gpu13_expert_mlp1 -> layer5_gpu13_expert_act
	layer5_gpu13_expert_act -> layer5_gpu13_expert_mlp2
	layer5_gpu13_expert_mlp2 -> layer5_gpu13_token_send
	layer5_gpu13_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu14_token_recv [style=dashed]
	layer5_gpu14_token_recv -> layer5_gpu14_expert_mlp1
	layer5_gpu14_expert_mlp1 -> layer5_gpu14_expert_act
	layer5_gpu14_expert_act -> layer5_gpu14_expert_mlp2
	layer5_gpu14_expert_mlp2 -> layer5_gpu14_token_send
	layer5_gpu14_token_send -> layer5_expert_agg
	layer5_routing -> layer5_gpu15_token_recv [style=dashed]
	layer5_gpu15_token_recv -> layer5_gpu15_expert_mlp1
	layer5_gpu15_expert_mlp1 -> layer5_gpu15_expert_act
	layer5_gpu15_expert_act -> layer5_gpu15_expert_mlp2
	layer5_gpu15_expert_mlp2 -> layer5_gpu15_token_send
	layer5_gpu15_token_send -> layer5_expert_agg
	layer5_expert_agg -> layer5_residual2
	layer5_residual2 -> layer6_layernorm
	layer6_layernorm -> layer6_qkv_proj
	layer6_qkv_proj -> layer6_attention
	layer6_attention -> layer6_attn_out
	layer6_attn_out -> layer6_residual1
	layer6_residual1 -> layer6_layernorm2
	layer6_layernorm2 -> layer6_global_gate
	layer6_global_gate -> layer6_routing
	layer6_routing -> layer6_gpu0_token_recv [style=dashed]
	layer6_gpu0_token_recv -> layer6_gpu0_expert_mlp1
	layer6_gpu0_expert_mlp1 -> layer6_gpu0_expert_act
	layer6_gpu0_expert_act -> layer6_gpu0_expert_mlp2
	layer6_gpu0_expert_mlp2 -> layer6_gpu0_token_send
	layer6_gpu0_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu1_token_recv [style=dashed]
	layer6_gpu1_token_recv -> layer6_gpu1_expert_mlp1
	layer6_gpu1_expert_mlp1 -> layer6_gpu1_expert_act
	layer6_gpu1_expert_act -> layer6_gpu1_expert_mlp2
	layer6_gpu1_expert_mlp2 -> layer6_gpu1_token_send
	layer6_gpu1_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu2_token_recv [style=dashed]
	layer6_gpu2_token_recv -> layer6_gpu2_expert_mlp1
	layer6_gpu2_expert_mlp1 -> layer6_gpu2_expert_act
	layer6_gpu2_expert_act -> layer6_gpu2_expert_mlp2
	layer6_gpu2_expert_mlp2 -> layer6_gpu2_token_send
	layer6_gpu2_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu3_token_recv [style=dashed]
	layer6_gpu3_token_recv -> layer6_gpu3_expert_mlp1
	layer6_gpu3_expert_mlp1 -> layer6_gpu3_expert_act
	layer6_gpu3_expert_act -> layer6_gpu3_expert_mlp2
	layer6_gpu3_expert_mlp2 -> layer6_gpu3_token_send
	layer6_gpu3_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu4_token_recv [style=dashed]
	layer6_gpu4_token_recv -> layer6_gpu4_expert_mlp1
	layer6_gpu4_expert_mlp1 -> layer6_gpu4_expert_act
	layer6_gpu4_expert_act -> layer6_gpu4_expert_mlp2
	layer6_gpu4_expert_mlp2 -> layer6_gpu4_token_send
	layer6_gpu4_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu5_token_recv [style=dashed]
	layer6_gpu5_token_recv -> layer6_gpu5_expert_mlp1
	layer6_gpu5_expert_mlp1 -> layer6_gpu5_expert_act
	layer6_gpu5_expert_act -> layer6_gpu5_expert_mlp2
	layer6_gpu5_expert_mlp2 -> layer6_gpu5_token_send
	layer6_gpu5_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu6_token_recv [style=dashed]
	layer6_gpu6_token_recv -> layer6_gpu6_expert_mlp1
	layer6_gpu6_expert_mlp1 -> layer6_gpu6_expert_act
	layer6_gpu6_expert_act -> layer6_gpu6_expert_mlp2
	layer6_gpu6_expert_mlp2 -> layer6_gpu6_token_send
	layer6_gpu6_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu7_token_recv [style=dashed]
	layer6_gpu7_token_recv -> layer6_gpu7_expert_mlp1
	layer6_gpu7_expert_mlp1 -> layer6_gpu7_expert_act
	layer6_gpu7_expert_act -> layer6_gpu7_expert_mlp2
	layer6_gpu7_expert_mlp2 -> layer6_gpu7_token_send
	layer6_gpu7_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu8_token_recv [style=dashed]
	layer6_gpu8_token_recv -> layer6_gpu8_expert_mlp1
	layer6_gpu8_expert_mlp1 -> layer6_gpu8_expert_act
	layer6_gpu8_expert_act -> layer6_gpu8_expert_mlp2
	layer6_gpu8_expert_mlp2 -> layer6_gpu8_token_send
	layer6_gpu8_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu9_token_recv [style=dashed]
	layer6_gpu9_token_recv -> layer6_gpu9_expert_mlp1
	layer6_gpu9_expert_mlp1 -> layer6_gpu9_expert_act
	layer6_gpu9_expert_act -> layer6_gpu9_expert_mlp2
	layer6_gpu9_expert_mlp2 -> layer6_gpu9_token_send
	layer6_gpu9_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu10_token_recv [style=dashed]
	layer6_gpu10_token_recv -> layer6_gpu10_expert_mlp1
	layer6_gpu10_expert_mlp1 -> layer6_gpu10_expert_act
	layer6_gpu10_expert_act -> layer6_gpu10_expert_mlp2
	layer6_gpu10_expert_mlp2 -> layer6_gpu10_token_send
	layer6_gpu10_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu11_token_recv [style=dashed]
	layer6_gpu11_token_recv -> layer6_gpu11_expert_mlp1
	layer6_gpu11_expert_mlp1 -> layer6_gpu11_expert_act
	layer6_gpu11_expert_act -> layer6_gpu11_expert_mlp2
	layer6_gpu11_expert_mlp2 -> layer6_gpu11_token_send
	layer6_gpu11_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu12_token_recv [style=dashed]
	layer6_gpu12_token_recv -> layer6_gpu12_expert_mlp1
	layer6_gpu12_expert_mlp1 -> layer6_gpu12_expert_act
	layer6_gpu12_expert_act -> layer6_gpu12_expert_mlp2
	layer6_gpu12_expert_mlp2 -> layer6_gpu12_token_send
	layer6_gpu12_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu13_token_recv [style=dashed]
	layer6_gpu13_token_recv -> layer6_gpu13_expert_mlp1
	layer6_gpu13_expert_mlp1 -> layer6_gpu13_expert_act
	layer6_gpu13_expert_act -> layer6_gpu13_expert_mlp2
	layer6_gpu13_expert_mlp2 -> layer6_gpu13_token_send
	layer6_gpu13_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu14_token_recv [style=dashed]
	layer6_gpu14_token_recv -> layer6_gpu14_expert_mlp1
	layer6_gpu14_expert_mlp1 -> layer6_gpu14_expert_act
	layer6_gpu14_expert_act -> layer6_gpu14_expert_mlp2
	layer6_gpu14_expert_mlp2 -> layer6_gpu14_token_send
	layer6_gpu14_token_send -> layer6_expert_agg
	layer6_routing -> layer6_gpu15_token_recv [style=dashed]
	layer6_gpu15_token_recv -> layer6_gpu15_expert_mlp1
	layer6_gpu15_expert_mlp1 -> layer6_gpu15_expert_act
	layer6_gpu15_expert_act -> layer6_gpu15_expert_mlp2
	layer6_gpu15_expert_mlp2 -> layer6_gpu15_token_send
	layer6_gpu15_token_send -> layer6_expert_agg
	layer6_expert_agg -> layer6_residual2
	layer6_residual2 -> layer7_layernorm
	layer7_layernorm -> layer7_qkv_proj
	layer7_qkv_proj -> layer7_attention
	layer7_attention -> layer7_attn_out
	layer7_attn_out -> layer7_residual1
	layer7_residual1 -> layer7_layernorm2
	layer7_layernorm2 -> layer7_global_gate
	layer7_global_gate -> layer7_routing
	layer7_routing -> layer7_gpu0_token_recv [style=dashed]
	layer7_gpu0_token_recv -> layer7_gpu0_expert_mlp1
	layer7_gpu0_expert_mlp1 -> layer7_gpu0_expert_act
	layer7_gpu0_expert_act -> layer7_gpu0_expert_mlp2
	layer7_gpu0_expert_mlp2 -> layer7_gpu0_token_send
	layer7_gpu0_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu1_token_recv [style=dashed]
	layer7_gpu1_token_recv -> layer7_gpu1_expert_mlp1
	layer7_gpu1_expert_mlp1 -> layer7_gpu1_expert_act
	layer7_gpu1_expert_act -> layer7_gpu1_expert_mlp2
	layer7_gpu1_expert_mlp2 -> layer7_gpu1_token_send
	layer7_gpu1_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu2_token_recv [style=dashed]
	layer7_gpu2_token_recv -> layer7_gpu2_expert_mlp1
	layer7_gpu2_expert_mlp1 -> layer7_gpu2_expert_act
	layer7_gpu2_expert_act -> layer7_gpu2_expert_mlp2
	layer7_gpu2_expert_mlp2 -> layer7_gpu2_token_send
	layer7_gpu2_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu3_token_recv [style=dashed]
	layer7_gpu3_token_recv -> layer7_gpu3_expert_mlp1
	layer7_gpu3_expert_mlp1 -> layer7_gpu3_expert_act
	layer7_gpu3_expert_act -> layer7_gpu3_expert_mlp2
	layer7_gpu3_expert_mlp2 -> layer7_gpu3_token_send
	layer7_gpu3_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu4_token_recv [style=dashed]
	layer7_gpu4_token_recv -> layer7_gpu4_expert_mlp1
	layer7_gpu4_expert_mlp1 -> layer7_gpu4_expert_act
	layer7_gpu4_expert_act -> layer7_gpu4_expert_mlp2
	layer7_gpu4_expert_mlp2 -> layer7_gpu4_token_send
	layer7_gpu4_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu5_token_recv [style=dashed]
	layer7_gpu5_token_recv -> layer7_gpu5_expert_mlp1
	layer7_gpu5_expert_mlp1 -> layer7_gpu5_expert_act
	layer7_gpu5_expert_act -> layer7_gpu5_expert_mlp2
	layer7_gpu5_expert_mlp2 -> layer7_gpu5_token_send
	layer7_gpu5_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu6_token_recv [style=dashed]
	layer7_gpu6_token_recv -> layer7_gpu6_expert_mlp1
	layer7_gpu6_expert_mlp1 -> layer7_gpu6_expert_act
	layer7_gpu6_expert_act -> layer7_gpu6_expert_mlp2
	layer7_gpu6_expert_mlp2 -> layer7_gpu6_token_send
	layer7_gpu6_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu7_token_recv [style=dashed]
	layer7_gpu7_token_recv -> layer7_gpu7_expert_mlp1
	layer7_gpu7_expert_mlp1 -> layer7_gpu7_expert_act
	layer7_gpu7_expert_act -> layer7_gpu7_expert_mlp2
	layer7_gpu7_expert_mlp2 -> layer7_gpu7_token_send
	layer7_gpu7_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu8_token_recv [style=dashed]
	layer7_gpu8_token_recv -> layer7_gpu8_expert_mlp1
	layer7_gpu8_expert_mlp1 -> layer7_gpu8_expert_act
	layer7_gpu8_expert_act -> layer7_gpu8_expert_mlp2
	layer7_gpu8_expert_mlp2 -> layer7_gpu8_token_send
	layer7_gpu8_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu9_token_recv [style=dashed]
	layer7_gpu9_token_recv -> layer7_gpu9_expert_mlp1
	layer7_gpu9_expert_mlp1 -> layer7_gpu9_expert_act
	layer7_gpu9_expert_act -> layer7_gpu9_expert_mlp2
	layer7_gpu9_expert_mlp2 -> layer7_gpu9_token_send
	layer7_gpu9_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu10_token_recv [style=dashed]
	layer7_gpu10_token_recv -> layer7_gpu10_expert_mlp1
	layer7_gpu10_expert_mlp1 -> layer7_gpu10_expert_act
	layer7_gpu10_expert_act -> layer7_gpu10_expert_mlp2
	layer7_gpu10_expert_mlp2 -> layer7_gpu10_token_send
	layer7_gpu10_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu11_token_recv [style=dashed]
	layer7_gpu11_token_recv -> layer7_gpu11_expert_mlp1
	layer7_gpu11_expert_mlp1 -> layer7_gpu11_expert_act
	layer7_gpu11_expert_act -> layer7_gpu11_expert_mlp2
	layer7_gpu11_expert_mlp2 -> layer7_gpu11_token_send
	layer7_gpu11_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu12_token_recv [style=dashed]
	layer7_gpu12_token_recv -> layer7_gpu12_expert_mlp1
	layer7_gpu12_expert_mlp1 -> layer7_gpu12_expert_act
	layer7_gpu12_expert_act -> layer7_gpu12_expert_mlp2
	layer7_gpu12_expert_mlp2 -> layer7_gpu12_token_send
	layer7_gpu12_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu13_token_recv [style=dashed]
	layer7_gpu13_token_recv -> layer7_gpu13_expert_mlp1
	layer7_gpu13_expert_mlp1 -> layer7_gpu13_expert_act
	layer7_gpu13_expert_act -> layer7_gpu13_expert_mlp2
	layer7_gpu13_expert_mlp2 -> layer7_gpu13_token_send
	layer7_gpu13_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu14_token_recv [style=dashed]
	layer7_gpu14_token_recv -> layer7_gpu14_expert_mlp1
	layer7_gpu14_expert_mlp1 -> layer7_gpu14_expert_act
	layer7_gpu14_expert_act -> layer7_gpu14_expert_mlp2
	layer7_gpu14_expert_mlp2 -> layer7_gpu14_token_send
	layer7_gpu14_token_send -> layer7_expert_agg
	layer7_routing -> layer7_gpu15_token_recv [style=dashed]
	layer7_gpu15_token_recv -> layer7_gpu15_expert_mlp1
	layer7_gpu15_expert_mlp1 -> layer7_gpu15_expert_act
	layer7_gpu15_expert_act -> layer7_gpu15_expert_mlp2
	layer7_gpu15_expert_mlp2 -> layer7_gpu15_token_send
	layer7_gpu15_token_send -> layer7_expert_agg
	layer7_expert_agg -> layer7_residual2
	layer7_residual2 -> layer8_layernorm
	layer8_layernorm -> layer8_qkv_proj
	layer8_qkv_proj -> layer8_attention
	layer8_attention -> layer8_attn_out
	layer8_attn_out -> layer8_residual1
	layer8_residual1 -> layer8_layernorm2
	layer8_layernorm2 -> layer8_global_gate
	layer8_global_gate -> layer8_routing
	layer8_routing -> layer8_gpu0_token_recv [style=dashed]
	layer8_gpu0_token_recv -> layer8_gpu0_expert_mlp1
	layer8_gpu0_expert_mlp1 -> layer8_gpu0_expert_act
	layer8_gpu0_expert_act -> layer8_gpu0_expert_mlp2
	layer8_gpu0_expert_mlp2 -> layer8_gpu0_token_send
	layer8_gpu0_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu1_token_recv [style=dashed]
	layer8_gpu1_token_recv -> layer8_gpu1_expert_mlp1
	layer8_gpu1_expert_mlp1 -> layer8_gpu1_expert_act
	layer8_gpu1_expert_act -> layer8_gpu1_expert_mlp2
	layer8_gpu1_expert_mlp2 -> layer8_gpu1_token_send
	layer8_gpu1_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu2_token_recv [style=dashed]
	layer8_gpu2_token_recv -> layer8_gpu2_expert_mlp1
	layer8_gpu2_expert_mlp1 -> layer8_gpu2_expert_act
	layer8_gpu2_expert_act -> layer8_gpu2_expert_mlp2
	layer8_gpu2_expert_mlp2 -> layer8_gpu2_token_send
	layer8_gpu2_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu3_token_recv [style=dashed]
	layer8_gpu3_token_recv -> layer8_gpu3_expert_mlp1
	layer8_gpu3_expert_mlp1 -> layer8_gpu3_expert_act
	layer8_gpu3_expert_act -> layer8_gpu3_expert_mlp2
	layer8_gpu3_expert_mlp2 -> layer8_gpu3_token_send
	layer8_gpu3_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu4_token_recv [style=dashed]
	layer8_gpu4_token_recv -> layer8_gpu4_expert_mlp1
	layer8_gpu4_expert_mlp1 -> layer8_gpu4_expert_act
	layer8_gpu4_expert_act -> layer8_gpu4_expert_mlp2
	layer8_gpu4_expert_mlp2 -> layer8_gpu4_token_send
	layer8_gpu4_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu5_token_recv [style=dashed]
	layer8_gpu5_token_recv -> layer8_gpu5_expert_mlp1
	layer8_gpu5_expert_mlp1 -> layer8_gpu5_expert_act
	layer8_gpu5_expert_act -> layer8_gpu5_expert_mlp2
	layer8_gpu5_expert_mlp2 -> layer8_gpu5_token_send
	layer8_gpu5_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu6_token_recv [style=dashed]
	layer8_gpu6_token_recv -> layer8_gpu6_expert_mlp1
	layer8_gpu6_expert_mlp1 -> layer8_gpu6_expert_act
	layer8_gpu6_expert_act -> layer8_gpu6_expert_mlp2
	layer8_gpu6_expert_mlp2 -> layer8_gpu6_token_send
	layer8_gpu6_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu7_token_recv [style=dashed]
	layer8_gpu7_token_recv -> layer8_gpu7_expert_mlp1
	layer8_gpu7_expert_mlp1 -> layer8_gpu7_expert_act
	layer8_gpu7_expert_act -> layer8_gpu7_expert_mlp2
	layer8_gpu7_expert_mlp2 -> layer8_gpu7_token_send
	layer8_gpu7_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu8_token_recv [style=dashed]
	layer8_gpu8_token_recv -> layer8_gpu8_expert_mlp1
	layer8_gpu8_expert_mlp1 -> layer8_gpu8_expert_act
	layer8_gpu8_expert_act -> layer8_gpu8_expert_mlp2
	layer8_gpu8_expert_mlp2 -> layer8_gpu8_token_send
	layer8_gpu8_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu9_token_recv [style=dashed]
	layer8_gpu9_token_recv -> layer8_gpu9_expert_mlp1
	layer8_gpu9_expert_mlp1 -> layer8_gpu9_expert_act
	layer8_gpu9_expert_act -> layer8_gpu9_expert_mlp2
	layer8_gpu9_expert_mlp2 -> layer8_gpu9_token_send
	layer8_gpu9_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu10_token_recv [style=dashed]
	layer8_gpu10_token_recv -> layer8_gpu10_expert_mlp1
	layer8_gpu10_expert_mlp1 -> layer8_gpu10_expert_act
	layer8_gpu10_expert_act -> layer8_gpu10_expert_mlp2
	layer8_gpu10_expert_mlp2 -> layer8_gpu10_token_send
	layer8_gpu10_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu11_token_recv [style=dashed]
	layer8_gpu11_token_recv -> layer8_gpu11_expert_mlp1
	layer8_gpu11_expert_mlp1 -> layer8_gpu11_expert_act
	layer8_gpu11_expert_act -> layer8_gpu11_expert_mlp2
	layer8_gpu11_expert_mlp2 -> layer8_gpu11_token_send
	layer8_gpu11_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu12_token_recv [style=dashed]
	layer8_gpu12_token_recv -> layer8_gpu12_expert_mlp1
	layer8_gpu12_expert_mlp1 -> layer8_gpu12_expert_act
	layer8_gpu12_expert_act -> layer8_gpu12_expert_mlp2
	layer8_gpu12_expert_mlp2 -> layer8_gpu12_token_send
	layer8_gpu12_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu13_token_recv [style=dashed]
	layer8_gpu13_token_recv -> layer8_gpu13_expert_mlp1
	layer8_gpu13_expert_mlp1 -> layer8_gpu13_expert_act
	layer8_gpu13_expert_act -> layer8_gpu13_expert_mlp2
	layer8_gpu13_expert_mlp2 -> layer8_gpu13_token_send
	layer8_gpu13_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu14_token_recv [style=dashed]
	layer8_gpu14_token_recv -> layer8_gpu14_expert_mlp1
	layer8_gpu14_expert_mlp1 -> layer8_gpu14_expert_act
	layer8_gpu14_expert_act -> layer8_gpu14_expert_mlp2
	layer8_gpu14_expert_mlp2 -> layer8_gpu14_token_send
	layer8_gpu14_token_send -> layer8_expert_agg
	layer8_routing -> layer8_gpu15_token_recv [style=dashed]
	layer8_gpu15_token_recv -> layer8_gpu15_expert_mlp1
	layer8_gpu15_expert_mlp1 -> layer8_gpu15_expert_act
	layer8_gpu15_expert_act -> layer8_gpu15_expert_mlp2
	layer8_gpu15_expert_mlp2 -> layer8_gpu15_token_send
	layer8_gpu15_token_send -> layer8_expert_agg
	layer8_expert_agg -> layer8_residual2
	layer8_residual2 -> layer9_layernorm
	layer9_layernorm -> layer9_qkv_proj
	layer9_qkv_proj -> layer9_attention
	layer9_attention -> layer9_attn_out
	layer9_attn_out -> layer9_residual1
	layer9_residual1 -> layer9_layernorm2
	layer9_layernorm2 -> layer9_global_gate
	layer9_global_gate -> layer9_routing
	layer9_routing -> layer9_gpu0_token_recv [style=dashed]
	layer9_gpu0_token_recv -> layer9_gpu0_expert_mlp1
	layer9_gpu0_expert_mlp1 -> layer9_gpu0_expert_act
	layer9_gpu0_expert_act -> layer9_gpu0_expert_mlp2
	layer9_gpu0_expert_mlp2 -> layer9_gpu0_token_send
	layer9_gpu0_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu1_token_recv [style=dashed]
	layer9_gpu1_token_recv -> layer9_gpu1_expert_mlp1
	layer9_gpu1_expert_mlp1 -> layer9_gpu1_expert_act
	layer9_gpu1_expert_act -> layer9_gpu1_expert_mlp2
	layer9_gpu1_expert_mlp2 -> layer9_gpu1_token_send
	layer9_gpu1_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu2_token_recv [style=dashed]
	layer9_gpu2_token_recv -> layer9_gpu2_expert_mlp1
	layer9_gpu2_expert_mlp1 -> layer9_gpu2_expert_act
	layer9_gpu2_expert_act -> layer9_gpu2_expert_mlp2
	layer9_gpu2_expert_mlp2 -> layer9_gpu2_token_send
	layer9_gpu2_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu3_token_recv [style=dashed]
	layer9_gpu3_token_recv -> layer9_gpu3_expert_mlp1
	layer9_gpu3_expert_mlp1 -> layer9_gpu3_expert_act
	layer9_gpu3_expert_act -> layer9_gpu3_expert_mlp2
	layer9_gpu3_expert_mlp2 -> layer9_gpu3_token_send
	layer9_gpu3_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu4_token_recv [style=dashed]
	layer9_gpu4_token_recv -> layer9_gpu4_expert_mlp1
	layer9_gpu4_expert_mlp1 -> layer9_gpu4_expert_act
	layer9_gpu4_expert_act -> layer9_gpu4_expert_mlp2
	layer9_gpu4_expert_mlp2 -> layer9_gpu4_token_send
	layer9_gpu4_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu5_token_recv [style=dashed]
	layer9_gpu5_token_recv -> layer9_gpu5_expert_mlp1
	layer9_gpu5_expert_mlp1 -> layer9_gpu5_expert_act
	layer9_gpu5_expert_act -> layer9_gpu5_expert_mlp2
	layer9_gpu5_expert_mlp2 -> layer9_gpu5_token_send
	layer9_gpu5_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu6_token_recv [style=dashed]
	layer9_gpu6_token_recv -> layer9_gpu6_expert_mlp1
	layer9_gpu6_expert_mlp1 -> layer9_gpu6_expert_act
	layer9_gpu6_expert_act -> layer9_gpu6_expert_mlp2
	layer9_gpu6_expert_mlp2 -> layer9_gpu6_token_send
	layer9_gpu6_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu7_token_recv [style=dashed]
	layer9_gpu7_token_recv -> layer9_gpu7_expert_mlp1
	layer9_gpu7_expert_mlp1 -> layer9_gpu7_expert_act
	layer9_gpu7_expert_act -> layer9_gpu7_expert_mlp2
	layer9_gpu7_expert_mlp2 -> layer9_gpu7_token_send
	layer9_gpu7_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu8_token_recv [style=dashed]
	layer9_gpu8_token_recv -> layer9_gpu8_expert_mlp1
	layer9_gpu8_expert_mlp1 -> layer9_gpu8_expert_act
	layer9_gpu8_expert_act -> layer9_gpu8_expert_mlp2
	layer9_gpu8_expert_mlp2 -> layer9_gpu8_token_send
	layer9_gpu8_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu9_token_recv [style=dashed]
	layer9_gpu9_token_recv -> layer9_gpu9_expert_mlp1
	layer9_gpu9_expert_mlp1 -> layer9_gpu9_expert_act
	layer9_gpu9_expert_act -> layer9_gpu9_expert_mlp2
	layer9_gpu9_expert_mlp2 -> layer9_gpu9_token_send
	layer9_gpu9_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu10_token_recv [style=dashed]
	layer9_gpu10_token_recv -> layer9_gpu10_expert_mlp1
	layer9_gpu10_expert_mlp1 -> layer9_gpu10_expert_act
	layer9_gpu10_expert_act -> layer9_gpu10_expert_mlp2
	layer9_gpu10_expert_mlp2 -> layer9_gpu10_token_send
	layer9_gpu10_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu11_token_recv [style=dashed]
	layer9_gpu11_token_recv -> layer9_gpu11_expert_mlp1
	layer9_gpu11_expert_mlp1 -> layer9_gpu11_expert_act
	layer9_gpu11_expert_act -> layer9_gpu11_expert_mlp2
	layer9_gpu11_expert_mlp2 -> layer9_gpu11_token_send
	layer9_gpu11_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu12_token_recv [style=dashed]
	layer9_gpu12_token_recv -> layer9_gpu12_expert_mlp1
	layer9_gpu12_expert_mlp1 -> layer9_gpu12_expert_act
	layer9_gpu12_expert_act -> layer9_gpu12_expert_mlp2
	layer9_gpu12_expert_mlp2 -> layer9_gpu12_token_send
	layer9_gpu12_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu13_token_recv [style=dashed]
	layer9_gpu13_token_recv -> layer9_gpu13_expert_mlp1
	layer9_gpu13_expert_mlp1 -> layer9_gpu13_expert_act
	layer9_gpu13_expert_act -> layer9_gpu13_expert_mlp2
	layer9_gpu13_expert_mlp2 -> layer9_gpu13_token_send
	layer9_gpu13_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu14_token_recv [style=dashed]
	layer9_gpu14_token_recv -> layer9_gpu14_expert_mlp1
	layer9_gpu14_expert_mlp1 -> layer9_gpu14_expert_act
	layer9_gpu14_expert_act -> layer9_gpu14_expert_mlp2
	layer9_gpu14_expert_mlp2 -> layer9_gpu14_token_send
	layer9_gpu14_token_send -> layer9_expert_agg
	layer9_routing -> layer9_gpu15_token_recv [style=dashed]
	layer9_gpu15_token_recv -> layer9_gpu15_expert_mlp1
	layer9_gpu15_expert_mlp1 -> layer9_gpu15_expert_act
	layer9_gpu15_expert_act -> layer9_gpu15_expert_mlp2
	layer9_gpu15_expert_mlp2 -> layer9_gpu15_token_send
	layer9_gpu15_token_send -> layer9_expert_agg
	layer9_expert_agg -> layer9_residual2
	layer9_residual2 -> layer10_layernorm
	layer10_layernorm -> layer10_qkv_proj
	layer10_qkv_proj -> layer10_attention
	layer10_attention -> layer10_attn_out
	layer10_attn_out -> layer10_residual1
	layer10_residual1 -> layer10_layernorm2
	layer10_layernorm2 -> layer10_global_gate
	layer10_global_gate -> layer10_routing
	layer10_routing -> layer10_gpu0_token_recv [style=dashed]
	layer10_gpu0_token_recv -> layer10_gpu0_expert_mlp1
	layer10_gpu0_expert_mlp1 -> layer10_gpu0_expert_act
	layer10_gpu0_expert_act -> layer10_gpu0_expert_mlp2
	layer10_gpu0_expert_mlp2 -> layer10_gpu0_token_send
	layer10_gpu0_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu1_token_recv [style=dashed]
	layer10_gpu1_token_recv -> layer10_gpu1_expert_mlp1
	layer10_gpu1_expert_mlp1 -> layer10_gpu1_expert_act
	layer10_gpu1_expert_act -> layer10_gpu1_expert_mlp2
	layer10_gpu1_expert_mlp2 -> layer10_gpu1_token_send
	layer10_gpu1_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu2_token_recv [style=dashed]
	layer10_gpu2_token_recv -> layer10_gpu2_expert_mlp1
	layer10_gpu2_expert_mlp1 -> layer10_gpu2_expert_act
	layer10_gpu2_expert_act -> layer10_gpu2_expert_mlp2
	layer10_gpu2_expert_mlp2 -> layer10_gpu2_token_send
	layer10_gpu2_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu3_token_recv [style=dashed]
	layer10_gpu3_token_recv -> layer10_gpu3_expert_mlp1
	layer10_gpu3_expert_mlp1 -> layer10_gpu3_expert_act
	layer10_gpu3_expert_act -> layer10_gpu3_expert_mlp2
	layer10_gpu3_expert_mlp2 -> layer10_gpu3_token_send
	layer10_gpu3_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu4_token_recv [style=dashed]
	layer10_gpu4_token_recv -> layer10_gpu4_expert_mlp1
	layer10_gpu4_expert_mlp1 -> layer10_gpu4_expert_act
	layer10_gpu4_expert_act -> layer10_gpu4_expert_mlp2
	layer10_gpu4_expert_mlp2 -> layer10_gpu4_token_send
	layer10_gpu4_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu5_token_recv [style=dashed]
	layer10_gpu5_token_recv -> layer10_gpu5_expert_mlp1
	layer10_gpu5_expert_mlp1 -> layer10_gpu5_expert_act
	layer10_gpu5_expert_act -> layer10_gpu5_expert_mlp2
	layer10_gpu5_expert_mlp2 -> layer10_gpu5_token_send
	layer10_gpu5_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu6_token_recv [style=dashed]
	layer10_gpu6_token_recv -> layer10_gpu6_expert_mlp1
	layer10_gpu6_expert_mlp1 -> layer10_gpu6_expert_act
	layer10_gpu6_expert_act -> layer10_gpu6_expert_mlp2
	layer10_gpu6_expert_mlp2 -> layer10_gpu6_token_send
	layer10_gpu6_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu7_token_recv [style=dashed]
	layer10_gpu7_token_recv -> layer10_gpu7_expert_mlp1
	layer10_gpu7_expert_mlp1 -> layer10_gpu7_expert_act
	layer10_gpu7_expert_act -> layer10_gpu7_expert_mlp2
	layer10_gpu7_expert_mlp2 -> layer10_gpu7_token_send
	layer10_gpu7_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu8_token_recv [style=dashed]
	layer10_gpu8_token_recv -> layer10_gpu8_expert_mlp1
	layer10_gpu8_expert_mlp1 -> layer10_gpu8_expert_act
	layer10_gpu8_expert_act -> layer10_gpu8_expert_mlp2
	layer10_gpu8_expert_mlp2 -> layer10_gpu8_token_send
	layer10_gpu8_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu9_token_recv [style=dashed]
	layer10_gpu9_token_recv -> layer10_gpu9_expert_mlp1
	layer10_gpu9_expert_mlp1 -> layer10_gpu9_expert_act
	layer10_gpu9_expert_act -> layer10_gpu9_expert_mlp2
	layer10_gpu9_expert_mlp2 -> layer10_gpu9_token_send
	layer10_gpu9_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu10_token_recv [style=dashed]
	layer10_gpu10_token_recv -> layer10_gpu10_expert_mlp1
	layer10_gpu10_expert_mlp1 -> layer10_gpu10_expert_act
	layer10_gpu10_expert_act -> layer10_gpu10_expert_mlp2
	layer10_gpu10_expert_mlp2 -> layer10_gpu10_token_send
	layer10_gpu10_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu11_token_recv [style=dashed]
	layer10_gpu11_token_recv -> layer10_gpu11_expert_mlp1
	layer10_gpu11_expert_mlp1 -> layer10_gpu11_expert_act
	layer10_gpu11_expert_act -> layer10_gpu11_expert_mlp2
	layer10_gpu11_expert_mlp2 -> layer10_gpu11_token_send
	layer10_gpu11_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu12_token_recv [style=dashed]
	layer10_gpu12_token_recv -> layer10_gpu12_expert_mlp1
	layer10_gpu12_expert_mlp1 -> layer10_gpu12_expert_act
	layer10_gpu12_expert_act -> layer10_gpu12_expert_mlp2
	layer10_gpu12_expert_mlp2 -> layer10_gpu12_token_send
	layer10_gpu12_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu13_token_recv [style=dashed]
	layer10_gpu13_token_recv -> layer10_gpu13_expert_mlp1
	layer10_gpu13_expert_mlp1 -> layer10_gpu13_expert_act
	layer10_gpu13_expert_act -> layer10_gpu13_expert_mlp2
	layer10_gpu13_expert_mlp2 -> layer10_gpu13_token_send
	layer10_gpu13_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu14_token_recv [style=dashed]
	layer10_gpu14_token_recv -> layer10_gpu14_expert_mlp1
	layer10_gpu14_expert_mlp1 -> layer10_gpu14_expert_act
	layer10_gpu14_expert_act -> layer10_gpu14_expert_mlp2
	layer10_gpu14_expert_mlp2 -> layer10_gpu14_token_send
	layer10_gpu14_token_send -> layer10_expert_agg
	layer10_routing -> layer10_gpu15_token_recv [style=dashed]
	layer10_gpu15_token_recv -> layer10_gpu15_expert_mlp1
	layer10_gpu15_expert_mlp1 -> layer10_gpu15_expert_act
	layer10_gpu15_expert_act -> layer10_gpu15_expert_mlp2
	layer10_gpu15_expert_mlp2 -> layer10_gpu15_token_send
	layer10_gpu15_token_send -> layer10_expert_agg
	layer10_expert_agg -> layer10_residual2
	layer10_residual2 -> layer11_layernorm
	layer11_layernorm -> layer11_qkv_proj
	layer11_qkv_proj -> layer11_attention
	layer11_attention -> layer11_attn_out
	layer11_attn_out -> layer11_residual1
	layer11_residual1 -> layer11_layernorm2
	layer11_layernorm2 -> layer11_global_gate
	layer11_global_gate -> layer11_routing
	layer11_routing -> layer11_gpu0_token_recv [style=dashed]
	layer11_gpu0_token_recv -> layer11_gpu0_expert_mlp1
	layer11_gpu0_expert_mlp1 -> layer11_gpu0_expert_act
	layer11_gpu0_expert_act -> layer11_gpu0_expert_mlp2
	layer11_gpu0_expert_mlp2 -> layer11_gpu0_token_send
	layer11_gpu0_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu1_token_recv [style=dashed]
	layer11_gpu1_token_recv -> layer11_gpu1_expert_mlp1
	layer11_gpu1_expert_mlp1 -> layer11_gpu1_expert_act
	layer11_gpu1_expert_act -> layer11_gpu1_expert_mlp2
	layer11_gpu1_expert_mlp2 -> layer11_gpu1_token_send
	layer11_gpu1_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu2_token_recv [style=dashed]
	layer11_gpu2_token_recv -> layer11_gpu2_expert_mlp1
	layer11_gpu2_expert_mlp1 -> layer11_gpu2_expert_act
	layer11_gpu2_expert_act -> layer11_gpu2_expert_mlp2
	layer11_gpu2_expert_mlp2 -> layer11_gpu2_token_send
	layer11_gpu2_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu3_token_recv [style=dashed]
	layer11_gpu3_token_recv -> layer11_gpu3_expert_mlp1
	layer11_gpu3_expert_mlp1 -> layer11_gpu3_expert_act
	layer11_gpu3_expert_act -> layer11_gpu3_expert_mlp2
	layer11_gpu3_expert_mlp2 -> layer11_gpu3_token_send
	layer11_gpu3_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu4_token_recv [style=dashed]
	layer11_gpu4_token_recv -> layer11_gpu4_expert_mlp1
	layer11_gpu4_expert_mlp1 -> layer11_gpu4_expert_act
	layer11_gpu4_expert_act -> layer11_gpu4_expert_mlp2
	layer11_gpu4_expert_mlp2 -> layer11_gpu4_token_send
	layer11_gpu4_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu5_token_recv [style=dashed]
	layer11_gpu5_token_recv -> layer11_gpu5_expert_mlp1
	layer11_gpu5_expert_mlp1 -> layer11_gpu5_expert_act
	layer11_gpu5_expert_act -> layer11_gpu5_expert_mlp2
	layer11_gpu5_expert_mlp2 -> layer11_gpu5_token_send
	layer11_gpu5_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu6_token_recv [style=dashed]
	layer11_gpu6_token_recv -> layer11_gpu6_expert_mlp1
	layer11_gpu6_expert_mlp1 -> layer11_gpu6_expert_act
	layer11_gpu6_expert_act -> layer11_gpu6_expert_mlp2
	layer11_gpu6_expert_mlp2 -> layer11_gpu6_token_send
	layer11_gpu6_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu7_token_recv [style=dashed]
	layer11_gpu7_token_recv -> layer11_gpu7_expert_mlp1
	layer11_gpu7_expert_mlp1 -> layer11_gpu7_expert_act
	layer11_gpu7_expert_act -> layer11_gpu7_expert_mlp2
	layer11_gpu7_expert_mlp2 -> layer11_gpu7_token_send
	layer11_gpu7_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu8_token_recv [style=dashed]
	layer11_gpu8_token_recv -> layer11_gpu8_expert_mlp1
	layer11_gpu8_expert_mlp1 -> layer11_gpu8_expert_act
	layer11_gpu8_expert_act -> layer11_gpu8_expert_mlp2
	layer11_gpu8_expert_mlp2 -> layer11_gpu8_token_send
	layer11_gpu8_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu9_token_recv [style=dashed]
	layer11_gpu9_token_recv -> layer11_gpu9_expert_mlp1
	layer11_gpu9_expert_mlp1 -> layer11_gpu9_expert_act
	layer11_gpu9_expert_act -> layer11_gpu9_expert_mlp2
	layer11_gpu9_expert_mlp2 -> layer11_gpu9_token_send
	layer11_gpu9_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu10_token_recv [style=dashed]
	layer11_gpu10_token_recv -> layer11_gpu10_expert_mlp1
	layer11_gpu10_expert_mlp1 -> layer11_gpu10_expert_act
	layer11_gpu10_expert_act -> layer11_gpu10_expert_mlp2
	layer11_gpu10_expert_mlp2 -> layer11_gpu10_token_send
	layer11_gpu10_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu11_token_recv [style=dashed]
	layer11_gpu11_token_recv -> layer11_gpu11_expert_mlp1
	layer11_gpu11_expert_mlp1 -> layer11_gpu11_expert_act
	layer11_gpu11_expert_act -> layer11_gpu11_expert_mlp2
	layer11_gpu11_expert_mlp2 -> layer11_gpu11_token_send
	layer11_gpu11_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu12_token_recv [style=dashed]
	layer11_gpu12_token_recv -> layer11_gpu12_expert_mlp1
	layer11_gpu12_expert_mlp1 -> layer11_gpu12_expert_act
	layer11_gpu12_expert_act -> layer11_gpu12_expert_mlp2
	layer11_gpu12_expert_mlp2 -> layer11_gpu12_token_send
	layer11_gpu12_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu13_token_recv [style=dashed]
	layer11_gpu13_token_recv -> layer11_gpu13_expert_mlp1
	layer11_gpu13_expert_mlp1 -> layer11_gpu13_expert_act
	layer11_gpu13_expert_act -> layer11_gpu13_expert_mlp2
	layer11_gpu13_expert_mlp2 -> layer11_gpu13_token_send
	layer11_gpu13_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu14_token_recv [style=dashed]
	layer11_gpu14_token_recv -> layer11_gpu14_expert_mlp1
	layer11_gpu14_expert_mlp1 -> layer11_gpu14_expert_act
	layer11_gpu14_expert_act -> layer11_gpu14_expert_mlp2
	layer11_gpu14_expert_mlp2 -> layer11_gpu14_token_send
	layer11_gpu14_token_send -> layer11_expert_agg
	layer11_routing -> layer11_gpu15_token_recv [style=dashed]
	layer11_gpu15_token_recv -> layer11_gpu15_expert_mlp1
	layer11_gpu15_expert_mlp1 -> layer11_gpu15_expert_act
	layer11_gpu15_expert_act -> layer11_gpu15_expert_mlp2
	layer11_gpu15_expert_mlp2 -> layer11_gpu15_token_send
	layer11_gpu15_token_send -> layer11_expert_agg
	layer11_expert_agg -> layer11_residual2
	layer11_residual2 -> layer12_layernorm
	layer12_layernorm -> layer12_qkv_proj
	layer12_qkv_proj -> layer12_attention
	layer12_attention -> layer12_attn_out
	layer12_attn_out -> layer12_residual1
	layer12_residual1 -> layer12_layernorm2
	layer12_layernorm2 -> layer12_global_gate
	layer12_global_gate -> layer12_routing
	layer12_routing -> layer12_gpu0_token_recv [style=dashed]
	layer12_gpu0_token_recv -> layer12_gpu0_expert_mlp1
	layer12_gpu0_expert_mlp1 -> layer12_gpu0_expert_act
	layer12_gpu0_expert_act -> layer12_gpu0_expert_mlp2
	layer12_gpu0_expert_mlp2 -> layer12_gpu0_token_send
	layer12_gpu0_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu1_token_recv [style=dashed]
	layer12_gpu1_token_recv -> layer12_gpu1_expert_mlp1
	layer12_gpu1_expert_mlp1 -> layer12_gpu1_expert_act
	layer12_gpu1_expert_act -> layer12_gpu1_expert_mlp2
	layer12_gpu1_expert_mlp2 -> layer12_gpu1_token_send
	layer12_gpu1_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu2_token_recv [style=dashed]
	layer12_gpu2_token_recv -> layer12_gpu2_expert_mlp1
	layer12_gpu2_expert_mlp1 -> layer12_gpu2_expert_act
	layer12_gpu2_expert_act -> layer12_gpu2_expert_mlp2
	layer12_gpu2_expert_mlp2 -> layer12_gpu2_token_send
	layer12_gpu2_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu3_token_recv [style=dashed]
	layer12_gpu3_token_recv -> layer12_gpu3_expert_mlp1
	layer12_gpu3_expert_mlp1 -> layer12_gpu3_expert_act
	layer12_gpu3_expert_act -> layer12_gpu3_expert_mlp2
	layer12_gpu3_expert_mlp2 -> layer12_gpu3_token_send
	layer12_gpu3_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu4_token_recv [style=dashed]
	layer12_gpu4_token_recv -> layer12_gpu4_expert_mlp1
	layer12_gpu4_expert_mlp1 -> layer12_gpu4_expert_act
	layer12_gpu4_expert_act -> layer12_gpu4_expert_mlp2
	layer12_gpu4_expert_mlp2 -> layer12_gpu4_token_send
	layer12_gpu4_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu5_token_recv [style=dashed]
	layer12_gpu5_token_recv -> layer12_gpu5_expert_mlp1
	layer12_gpu5_expert_mlp1 -> layer12_gpu5_expert_act
	layer12_gpu5_expert_act -> layer12_gpu5_expert_mlp2
	layer12_gpu5_expert_mlp2 -> layer12_gpu5_token_send
	layer12_gpu5_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu6_token_recv [style=dashed]
	layer12_gpu6_token_recv -> layer12_gpu6_expert_mlp1
	layer12_gpu6_expert_mlp1 -> layer12_gpu6_expert_act
	layer12_gpu6_expert_act -> layer12_gpu6_expert_mlp2
	layer12_gpu6_expert_mlp2 -> layer12_gpu6_token_send
	layer12_gpu6_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu7_token_recv [style=dashed]
	layer12_gpu7_token_recv -> layer12_gpu7_expert_mlp1
	layer12_gpu7_expert_mlp1 -> layer12_gpu7_expert_act
	layer12_gpu7_expert_act -> layer12_gpu7_expert_mlp2
	layer12_gpu7_expert_mlp2 -> layer12_gpu7_token_send
	layer12_gpu7_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu8_token_recv [style=dashed]
	layer12_gpu8_token_recv -> layer12_gpu8_expert_mlp1
	layer12_gpu8_expert_mlp1 -> layer12_gpu8_expert_act
	layer12_gpu8_expert_act -> layer12_gpu8_expert_mlp2
	layer12_gpu8_expert_mlp2 -> layer12_gpu8_token_send
	layer12_gpu8_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu9_token_recv [style=dashed]
	layer12_gpu9_token_recv -> layer12_gpu9_expert_mlp1
	layer12_gpu9_expert_mlp1 -> layer12_gpu9_expert_act
	layer12_gpu9_expert_act -> layer12_gpu9_expert_mlp2
	layer12_gpu9_expert_mlp2 -> layer12_gpu9_token_send
	layer12_gpu9_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu10_token_recv [style=dashed]
	layer12_gpu10_token_recv -> layer12_gpu10_expert_mlp1
	layer12_gpu10_expert_mlp1 -> layer12_gpu10_expert_act
	layer12_gpu10_expert_act -> layer12_gpu10_expert_mlp2
	layer12_gpu10_expert_mlp2 -> layer12_gpu10_token_send
	layer12_gpu10_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu11_token_recv [style=dashed]
	layer12_gpu11_token_recv -> layer12_gpu11_expert_mlp1
	layer12_gpu11_expert_mlp1 -> layer12_gpu11_expert_act
	layer12_gpu11_expert_act -> layer12_gpu11_expert_mlp2
	layer12_gpu11_expert_mlp2 -> layer12_gpu11_token_send
	layer12_gpu11_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu12_token_recv [style=dashed]
	layer12_gpu12_token_recv -> layer12_gpu12_expert_mlp1
	layer12_gpu12_expert_mlp1 -> layer12_gpu12_expert_act
	layer12_gpu12_expert_act -> layer12_gpu12_expert_mlp2
	layer12_gpu12_expert_mlp2 -> layer12_gpu12_token_send
	layer12_gpu12_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu13_token_recv [style=dashed]
	layer12_gpu13_token_recv -> layer12_gpu13_expert_mlp1
	layer12_gpu13_expert_mlp1 -> layer12_gpu13_expert_act
	layer12_gpu13_expert_act -> layer12_gpu13_expert_mlp2
	layer12_gpu13_expert_mlp2 -> layer12_gpu13_token_send
	layer12_gpu13_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu14_token_recv [style=dashed]
	layer12_gpu14_token_recv -> layer12_gpu14_expert_mlp1
	layer12_gpu14_expert_mlp1 -> layer12_gpu14_expert_act
	layer12_gpu14_expert_act -> layer12_gpu14_expert_mlp2
	layer12_gpu14_expert_mlp2 -> layer12_gpu14_token_send
	layer12_gpu14_token_send -> layer12_expert_agg
	layer12_routing -> layer12_gpu15_token_recv [style=dashed]
	layer12_gpu15_token_recv -> layer12_gpu15_expert_mlp1
	layer12_gpu15_expert_mlp1 -> layer12_gpu15_expert_act
	layer12_gpu15_expert_act -> layer12_gpu15_expert_mlp2
	layer12_gpu15_expert_mlp2 -> layer12_gpu15_token_send
	layer12_gpu15_token_send -> layer12_expert_agg
	layer12_expert_agg -> layer12_residual2
	layer12_residual2 -> layer13_layernorm
	layer13_layernorm -> layer13_qkv_proj
	layer13_qkv_proj -> layer13_attention
	layer13_attention -> layer13_attn_out
	layer13_attn_out -> layer13_residual1
	layer13_residual1 -> layer13_layernorm2
	layer13_layernorm2 -> layer13_global_gate
	layer13_global_gate -> layer13_routing
	layer13_routing -> layer13_gpu0_token_recv [style=dashed]
	layer13_gpu0_token_recv -> layer13_gpu0_expert_mlp1
	layer13_gpu0_expert_mlp1 -> layer13_gpu0_expert_act
	layer13_gpu0_expert_act -> layer13_gpu0_expert_mlp2
	layer13_gpu0_expert_mlp2 -> layer13_gpu0_token_send
	layer13_gpu0_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu1_token_recv [style=dashed]
	layer13_gpu1_token_recv -> layer13_gpu1_expert_mlp1
	layer13_gpu1_expert_mlp1 -> layer13_gpu1_expert_act
	layer13_gpu1_expert_act -> layer13_gpu1_expert_mlp2
	layer13_gpu1_expert_mlp2 -> layer13_gpu1_token_send
	layer13_gpu1_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu2_token_recv [style=dashed]
	layer13_gpu2_token_recv -> layer13_gpu2_expert_mlp1
	layer13_gpu2_expert_mlp1 -> layer13_gpu2_expert_act
	layer13_gpu2_expert_act -> layer13_gpu2_expert_mlp2
	layer13_gpu2_expert_mlp2 -> layer13_gpu2_token_send
	layer13_gpu2_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu3_token_recv [style=dashed]
	layer13_gpu3_token_recv -> layer13_gpu3_expert_mlp1
	layer13_gpu3_expert_mlp1 -> layer13_gpu3_expert_act
	layer13_gpu3_expert_act -> layer13_gpu3_expert_mlp2
	layer13_gpu3_expert_mlp2 -> layer13_gpu3_token_send
	layer13_gpu3_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu4_token_recv [style=dashed]
	layer13_gpu4_token_recv -> layer13_gpu4_expert_mlp1
	layer13_gpu4_expert_mlp1 -> layer13_gpu4_expert_act
	layer13_gpu4_expert_act -> layer13_gpu4_expert_mlp2
	layer13_gpu4_expert_mlp2 -> layer13_gpu4_token_send
	layer13_gpu4_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu5_token_recv [style=dashed]
	layer13_gpu5_token_recv -> layer13_gpu5_expert_mlp1
	layer13_gpu5_expert_mlp1 -> layer13_gpu5_expert_act
	layer13_gpu5_expert_act -> layer13_gpu5_expert_mlp2
	layer13_gpu5_expert_mlp2 -> layer13_gpu5_token_send
	layer13_gpu5_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu6_token_recv [style=dashed]
	layer13_gpu6_token_recv -> layer13_gpu6_expert_mlp1
	layer13_gpu6_expert_mlp1 -> layer13_gpu6_expert_act
	layer13_gpu6_expert_act -> layer13_gpu6_expert_mlp2
	layer13_gpu6_expert_mlp2 -> layer13_gpu6_token_send
	layer13_gpu6_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu7_token_recv [style=dashed]
	layer13_gpu7_token_recv -> layer13_gpu7_expert_mlp1
	layer13_gpu7_expert_mlp1 -> layer13_gpu7_expert_act
	layer13_gpu7_expert_act -> layer13_gpu7_expert_mlp2
	layer13_gpu7_expert_mlp2 -> layer13_gpu7_token_send
	layer13_gpu7_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu8_token_recv [style=dashed]
	layer13_gpu8_token_recv -> layer13_gpu8_expert_mlp1
	layer13_gpu8_expert_mlp1 -> layer13_gpu8_expert_act
	layer13_gpu8_expert_act -> layer13_gpu8_expert_mlp2
	layer13_gpu8_expert_mlp2 -> layer13_gpu8_token_send
	layer13_gpu8_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu9_token_recv [style=dashed]
	layer13_gpu9_token_recv -> layer13_gpu9_expert_mlp1
	layer13_gpu9_expert_mlp1 -> layer13_gpu9_expert_act
	layer13_gpu9_expert_act -> layer13_gpu9_expert_mlp2
	layer13_gpu9_expert_mlp2 -> layer13_gpu9_token_send
	layer13_gpu9_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu10_token_recv [style=dashed]
	layer13_gpu10_token_recv -> layer13_gpu10_expert_mlp1
	layer13_gpu10_expert_mlp1 -> layer13_gpu10_expert_act
	layer13_gpu10_expert_act -> layer13_gpu10_expert_mlp2
	layer13_gpu10_expert_mlp2 -> layer13_gpu10_token_send
	layer13_gpu10_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu11_token_recv [style=dashed]
	layer13_gpu11_token_recv -> layer13_gpu11_expert_mlp1
	layer13_gpu11_expert_mlp1 -> layer13_gpu11_expert_act
	layer13_gpu11_expert_act -> layer13_gpu11_expert_mlp2
	layer13_gpu11_expert_mlp2 -> layer13_gpu11_token_send
	layer13_gpu11_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu12_token_recv [style=dashed]
	layer13_gpu12_token_recv -> layer13_gpu12_expert_mlp1
	layer13_gpu12_expert_mlp1 -> layer13_gpu12_expert_act
	layer13_gpu12_expert_act -> layer13_gpu12_expert_mlp2
	layer13_gpu12_expert_mlp2 -> layer13_gpu12_token_send
	layer13_gpu12_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu13_token_recv [style=dashed]
	layer13_gpu13_token_recv -> layer13_gpu13_expert_mlp1
	layer13_gpu13_expert_mlp1 -> layer13_gpu13_expert_act
	layer13_gpu13_expert_act -> layer13_gpu13_expert_mlp2
	layer13_gpu13_expert_mlp2 -> layer13_gpu13_token_send
	layer13_gpu13_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu14_token_recv [style=dashed]
	layer13_gpu14_token_recv -> layer13_gpu14_expert_mlp1
	layer13_gpu14_expert_mlp1 -> layer13_gpu14_expert_act
	layer13_gpu14_expert_act -> layer13_gpu14_expert_mlp2
	layer13_gpu14_expert_mlp2 -> layer13_gpu14_token_send
	layer13_gpu14_token_send -> layer13_expert_agg
	layer13_routing -> layer13_gpu15_token_recv [style=dashed]
	layer13_gpu15_token_recv -> layer13_gpu15_expert_mlp1
	layer13_gpu15_expert_mlp1 -> layer13_gpu15_expert_act
	layer13_gpu15_expert_act -> layer13_gpu15_expert_mlp2
	layer13_gpu15_expert_mlp2 -> layer13_gpu15_token_send
	layer13_gpu15_token_send -> layer13_expert_agg
	layer13_expert_agg -> layer13_residual2
	layer13_residual2 -> layer14_layernorm
	layer14_layernorm -> layer14_qkv_proj
	layer14_qkv_proj -> layer14_attention
	layer14_attention -> layer14_attn_out
	layer14_attn_out -> layer14_residual1
	layer14_residual1 -> layer14_layernorm2
	layer14_layernorm2 -> layer14_global_gate
	layer14_global_gate -> layer14_routing
	layer14_routing -> layer14_gpu0_token_recv [style=dashed]
	layer14_gpu0_token_recv -> layer14_gpu0_expert_mlp1
	layer14_gpu0_expert_mlp1 -> layer14_gpu0_expert_act
	layer14_gpu0_expert_act -> layer14_gpu0_expert_mlp2
	layer14_gpu0_expert_mlp2 -> layer14_gpu0_token_send
	layer14_gpu0_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu1_token_recv [style=dashed]
	layer14_gpu1_token_recv -> layer14_gpu1_expert_mlp1
	layer14_gpu1_expert_mlp1 -> layer14_gpu1_expert_act
	layer14_gpu1_expert_act -> layer14_gpu1_expert_mlp2
	layer14_gpu1_expert_mlp2 -> layer14_gpu1_token_send
	layer14_gpu1_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu2_token_recv [style=dashed]
	layer14_gpu2_token_recv -> layer14_gpu2_expert_mlp1
	layer14_gpu2_expert_mlp1 -> layer14_gpu2_expert_act
	layer14_gpu2_expert_act -> layer14_gpu2_expert_mlp2
	layer14_gpu2_expert_mlp2 -> layer14_gpu2_token_send
	layer14_gpu2_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu3_token_recv [style=dashed]
	layer14_gpu3_token_recv -> layer14_gpu3_expert_mlp1
	layer14_gpu3_expert_mlp1 -> layer14_gpu3_expert_act
	layer14_gpu3_expert_act -> layer14_gpu3_expert_mlp2
	layer14_gpu3_expert_mlp2 -> layer14_gpu3_token_send
	layer14_gpu3_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu4_token_recv [style=dashed]
	layer14_gpu4_token_recv -> layer14_gpu4_expert_mlp1
	layer14_gpu4_expert_mlp1 -> layer14_gpu4_expert_act
	layer14_gpu4_expert_act -> layer14_gpu4_expert_mlp2
	layer14_gpu4_expert_mlp2 -> layer14_gpu4_token_send
	layer14_gpu4_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu5_token_recv [style=dashed]
	layer14_gpu5_token_recv -> layer14_gpu5_expert_mlp1
	layer14_gpu5_expert_mlp1 -> layer14_gpu5_expert_act
	layer14_gpu5_expert_act -> layer14_gpu5_expert_mlp2
	layer14_gpu5_expert_mlp2 -> layer14_gpu5_token_send
	layer14_gpu5_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu6_token_recv [style=dashed]
	layer14_gpu6_token_recv -> layer14_gpu6_expert_mlp1
	layer14_gpu6_expert_mlp1 -> layer14_gpu6_expert_act
	layer14_gpu6_expert_act -> layer14_gpu6_expert_mlp2
	layer14_gpu6_expert_mlp2 -> layer14_gpu6_token_send
	layer14_gpu6_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu7_token_recv [style=dashed]
	layer14_gpu7_token_recv -> layer14_gpu7_expert_mlp1
	layer14_gpu7_expert_mlp1 -> layer14_gpu7_expert_act
	layer14_gpu7_expert_act -> layer14_gpu7_expert_mlp2
	layer14_gpu7_expert_mlp2 -> layer14_gpu7_token_send
	layer14_gpu7_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu8_token_recv [style=dashed]
	layer14_gpu8_token_recv -> layer14_gpu8_expert_mlp1
	layer14_gpu8_expert_mlp1 -> layer14_gpu8_expert_act
	layer14_gpu8_expert_act -> layer14_gpu8_expert_mlp2
	layer14_gpu8_expert_mlp2 -> layer14_gpu8_token_send
	layer14_gpu8_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu9_token_recv [style=dashed]
	layer14_gpu9_token_recv -> layer14_gpu9_expert_mlp1
	layer14_gpu9_expert_mlp1 -> layer14_gpu9_expert_act
	layer14_gpu9_expert_act -> layer14_gpu9_expert_mlp2
	layer14_gpu9_expert_mlp2 -> layer14_gpu9_token_send
	layer14_gpu9_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu10_token_recv [style=dashed]
	layer14_gpu10_token_recv -> layer14_gpu10_expert_mlp1
	layer14_gpu10_expert_mlp1 -> layer14_gpu10_expert_act
	layer14_gpu10_expert_act -> layer14_gpu10_expert_mlp2
	layer14_gpu10_expert_mlp2 -> layer14_gpu10_token_send
	layer14_gpu10_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu11_token_recv [style=dashed]
	layer14_gpu11_token_recv -> layer14_gpu11_expert_mlp1
	layer14_gpu11_expert_mlp1 -> layer14_gpu11_expert_act
	layer14_gpu11_expert_act -> layer14_gpu11_expert_mlp2
	layer14_gpu11_expert_mlp2 -> layer14_gpu11_token_send
	layer14_gpu11_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu12_token_recv [style=dashed]
	layer14_gpu12_token_recv -> layer14_gpu12_expert_mlp1
	layer14_gpu12_expert_mlp1 -> layer14_gpu12_expert_act
	layer14_gpu12_expert_act -> layer14_gpu12_expert_mlp2
	layer14_gpu12_expert_mlp2 -> layer14_gpu12_token_send
	layer14_gpu12_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu13_token_recv [style=dashed]
	layer14_gpu13_token_recv -> layer14_gpu13_expert_mlp1
	layer14_gpu13_expert_mlp1 -> layer14_gpu13_expert_act
	layer14_gpu13_expert_act -> layer14_gpu13_expert_mlp2
	layer14_gpu13_expert_mlp2 -> layer14_gpu13_token_send
	layer14_gpu13_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu14_token_recv [style=dashed]
	layer14_gpu14_token_recv -> layer14_gpu14_expert_mlp1
	layer14_gpu14_expert_mlp1 -> layer14_gpu14_expert_act
	layer14_gpu14_expert_act -> layer14_gpu14_expert_mlp2
	layer14_gpu14_expert_mlp2 -> layer14_gpu14_token_send
	layer14_gpu14_token_send -> layer14_expert_agg
	layer14_routing -> layer14_gpu15_token_recv [style=dashed]
	layer14_gpu15_token_recv -> layer14_gpu15_expert_mlp1
	layer14_gpu15_expert_mlp1 -> layer14_gpu15_expert_act
	layer14_gpu15_expert_act -> layer14_gpu15_expert_mlp2
	layer14_gpu15_expert_mlp2 -> layer14_gpu15_token_send
	layer14_gpu15_token_send -> layer14_expert_agg
	layer14_expert_agg -> layer14_residual2
	layer14_residual2 -> layer15_layernorm
	layer15_layernorm -> layer15_qkv_proj
	layer15_qkv_proj -> layer15_attention
	layer15_attention -> layer15_attn_out
	layer15_attn_out -> layer15_residual1
	layer15_residual1 -> layer15_layernorm2
	layer15_layernorm2 -> layer15_global_gate
	layer15_global_gate -> layer15_routing
	layer15_routing -> layer15_gpu0_token_recv [style=dashed]
	layer15_gpu0_token_recv -> layer15_gpu0_expert_mlp1
	layer15_gpu0_expert_mlp1 -> layer15_gpu0_expert_act
	layer15_gpu0_expert_act -> layer15_gpu0_expert_mlp2
	layer15_gpu0_expert_mlp2 -> layer15_gpu0_token_send
	layer15_gpu0_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu1_token_recv [style=dashed]
	layer15_gpu1_token_recv -> layer15_gpu1_expert_mlp1
	layer15_gpu1_expert_mlp1 -> layer15_gpu1_expert_act
	layer15_gpu1_expert_act -> layer15_gpu1_expert_mlp2
	layer15_gpu1_expert_mlp2 -> layer15_gpu1_token_send
	layer15_gpu1_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu2_token_recv [style=dashed]
	layer15_gpu2_token_recv -> layer15_gpu2_expert_mlp1
	layer15_gpu2_expert_mlp1 -> layer15_gpu2_expert_act
	layer15_gpu2_expert_act -> layer15_gpu2_expert_mlp2
	layer15_gpu2_expert_mlp2 -> layer15_gpu2_token_send
	layer15_gpu2_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu3_token_recv [style=dashed]
	layer15_gpu3_token_recv -> layer15_gpu3_expert_mlp1
	layer15_gpu3_expert_mlp1 -> layer15_gpu3_expert_act
	layer15_gpu3_expert_act -> layer15_gpu3_expert_mlp2
	layer15_gpu3_expert_mlp2 -> layer15_gpu3_token_send
	layer15_gpu3_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu4_token_recv [style=dashed]
	layer15_gpu4_token_recv -> layer15_gpu4_expert_mlp1
	layer15_gpu4_expert_mlp1 -> layer15_gpu4_expert_act
	layer15_gpu4_expert_act -> layer15_gpu4_expert_mlp2
	layer15_gpu4_expert_mlp2 -> layer15_gpu4_token_send
	layer15_gpu4_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu5_token_recv [style=dashed]
	layer15_gpu5_token_recv -> layer15_gpu5_expert_mlp1
	layer15_gpu5_expert_mlp1 -> layer15_gpu5_expert_act
	layer15_gpu5_expert_act -> layer15_gpu5_expert_mlp2
	layer15_gpu5_expert_mlp2 -> layer15_gpu5_token_send
	layer15_gpu5_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu6_token_recv [style=dashed]
	layer15_gpu6_token_recv -> layer15_gpu6_expert_mlp1
	layer15_gpu6_expert_mlp1 -> layer15_gpu6_expert_act
	layer15_gpu6_expert_act -> layer15_gpu6_expert_mlp2
	layer15_gpu6_expert_mlp2 -> layer15_gpu6_token_send
	layer15_gpu6_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu7_token_recv [style=dashed]
	layer15_gpu7_token_recv -> layer15_gpu7_expert_mlp1
	layer15_gpu7_expert_mlp1 -> layer15_gpu7_expert_act
	layer15_gpu7_expert_act -> layer15_gpu7_expert_mlp2
	layer15_gpu7_expert_mlp2 -> layer15_gpu7_token_send
	layer15_gpu7_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu8_token_recv [style=dashed]
	layer15_gpu8_token_recv -> layer15_gpu8_expert_mlp1
	layer15_gpu8_expert_mlp1 -> layer15_gpu8_expert_act
	layer15_gpu8_expert_act -> layer15_gpu8_expert_mlp2
	layer15_gpu8_expert_mlp2 -> layer15_gpu8_token_send
	layer15_gpu8_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu9_token_recv [style=dashed]
	layer15_gpu9_token_recv -> layer15_gpu9_expert_mlp1
	layer15_gpu9_expert_mlp1 -> layer15_gpu9_expert_act
	layer15_gpu9_expert_act -> layer15_gpu9_expert_mlp2
	layer15_gpu9_expert_mlp2 -> layer15_gpu9_token_send
	layer15_gpu9_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu10_token_recv [style=dashed]
	layer15_gpu10_token_recv -> layer15_gpu10_expert_mlp1
	layer15_gpu10_expert_mlp1 -> layer15_gpu10_expert_act
	layer15_gpu10_expert_act -> layer15_gpu10_expert_mlp2
	layer15_gpu10_expert_mlp2 -> layer15_gpu10_token_send
	layer15_gpu10_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu11_token_recv [style=dashed]
	layer15_gpu11_token_recv -> layer15_gpu11_expert_mlp1
	layer15_gpu11_expert_mlp1 -> layer15_gpu11_expert_act
	layer15_gpu11_expert_act -> layer15_gpu11_expert_mlp2
	layer15_gpu11_expert_mlp2 -> layer15_gpu11_token_send
	layer15_gpu11_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu12_token_recv [style=dashed]
	layer15_gpu12_token_recv -> layer15_gpu12_expert_mlp1
	layer15_gpu12_expert_mlp1 -> layer15_gpu12_expert_act
	layer15_gpu12_expert_act -> layer15_gpu12_expert_mlp2
	layer15_gpu12_expert_mlp2 -> layer15_gpu12_token_send
	layer15_gpu12_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu13_token_recv [style=dashed]
	layer15_gpu13_token_recv -> layer15_gpu13_expert_mlp1
	layer15_gpu13_expert_mlp1 -> layer15_gpu13_expert_act
	layer15_gpu13_expert_act -> layer15_gpu13_expert_mlp2
	layer15_gpu13_expert_mlp2 -> layer15_gpu13_token_send
	layer15_gpu13_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu14_token_recv [style=dashed]
	layer15_gpu14_token_recv -> layer15_gpu14_expert_mlp1
	layer15_gpu14_expert_mlp1 -> layer15_gpu14_expert_act
	layer15_gpu14_expert_act -> layer15_gpu14_expert_mlp2
	layer15_gpu14_expert_mlp2 -> layer15_gpu14_token_send
	layer15_gpu14_token_send -> layer15_expert_agg
	layer15_routing -> layer15_gpu15_token_recv [style=dashed]
	layer15_gpu15_token_recv -> layer15_gpu15_expert_mlp1
	layer15_gpu15_expert_mlp1 -> layer15_gpu15_expert_act
	layer15_gpu15_expert_act -> layer15_gpu15_expert_mlp2
	layer15_gpu15_expert_mlp2 -> layer15_gpu15_token_send
	layer15_gpu15_token_send -> layer15_expert_agg
	layer15_expert_agg -> layer15_residual2
	output [label="Output\nInput: [batch_size=128, seq_len=128, hidden=1024]\nOutput: [batch_size=128, seq_len=128, hidden=1024]\nGPU: All" fillcolor=lightblue shape=ellipse]
	layer15_residual2 -> output
}
