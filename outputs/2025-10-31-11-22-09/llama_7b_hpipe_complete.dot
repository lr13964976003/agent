digraph llama_7b_hpipe {
    rankdir=TB;
    node [shape=rectangle, fontname="Arial"];
    
    // Input and slicing
    input [label="Input
Input: [batch_size=6, seq_len=2048, vocab_size=32000]
Output: [batch_size=6, seq_len=2048, vocab_size=32000]
GPU: all GPUs", shape=ellipse, style=filled, fillcolor=lightblue];
    slice_sequence [label="Slice Sequence
Input: [batch_size=6, seq_len=2048]
Output: [batch_size=6, seq_len=384]
GPU: all GPUs", shape=parallelogram, style=filled, fillcolor=yellow];
    
    // Stage 1: P@1 (Layers 1-3)
    subgraph cluster_stage1 {
        label="Stage 1: P@1 (P100)";
        style=dashed;
        
        // Layer 1
        layer1_embed [label="Layer 1\nEmbedding\nInput: [batch_size=6, seq_len=384, vocab_size=32000]\nOutput: [batch_size=6, seq_len=384, hidden_size=4096]\nGPU: P@1"];
        layer1_norm1 [label="Layer 1\nRMSNorm\nInput: [batch_size=6, seq_len=384, hidden_size=4096]\nOutput: [batch_size=6, seq_len=384, hidden_size=4096]\nGPU: P@1"];
        layer1_qkv [label="Layer 1\nQKV Linear\nInput: [batch_size=6, seq_len=384, hidden_size=4096]\nOutput: [batch_size=6, seq_len=384, 32 heads, d_k=128]\nGPU: P@1"];
        layer1_attn_score [label="Layer 1\nAttention Score\nInput: [batch_size=6, seq_len=384, 32 heads, d_k=128]\nOutput: [batch_size=6, seq_len=384, 32 heads, d_k=128]\nGPU: P@1"];
        layer1_attn_out [label="Layer 1\nAttention Output\nInput: [batch_size=6, seq_len=384, 32 heads, d_k=128]\nOutput: [batch_size=6, seq_len=384, hidden_size=4096]\nGPU: P@1"];
        layer1_residual1 [label="Layer 1\nResidual Add\nInput: [batch_size=6, seq_len=384, hidden_size=4096] x2\nOutput: [batch_size=6, seq_len=384, hidden_size=4096]\nGPU: P@1"];
        layer1_norm2 [label="Layer 1\nRMSNorm\nInput: [batch_size=6, seq_len=384, hidden_size=4096]\nOutput: [batch_size=6, seq_len=384, hidden_size=4096]\nGPU: P@1"];
        layer1_ffn_gate [label="Layer 1\nFFN Gate\nInput: [batch_size=6, seq_len=384, hidden_size=4096]\nOutput: [batch_size=6, seq_len=384, ffn_hidden_size=11008]\nGPU: P@1"];
        layer1_ffn_up [label="Layer 1\nFFN Up\nInput: [batch_size=6, seq_len=384, hidden_size=4096]\nOutput: [batch_size=6, seq_len=384, ffn_hidden_size=11008]\nGPU: P@1"];
        layer1_ffn_down [label="Layer 1\nFFN Down\nInput: [batch_size=6, seq_len=384, ffn_hidden_size=11008]\nOutput: [batch_size=6, seq_len=384, hidden_size=4096]\nGPU: P@1"];
        layer1_residual2 [label="Layer 1\nResidual Add\nInput: [batch_size=6, seq_len=384, hidden_size=4096] x2\nOutput: [batch_size=6, seq_len=384, hidden_size=4096]\nGPU: P@1"];
        
        // Layer 2
        layer2_norm1 [label="Layer 2\nRMSNorm\nGPU: P@1"];
        layer2_qkv [label="Layer 2\nQKV Linear\nGPU: P@1"];
        layer2_attn_score [label="Layer 2\nAttention Score\nGPU: P@1"];
        layer2_attn_out [label="Layer 2\nAttention Output\nGPU: P@1"];
        layer2_residual1 [label="Layer 2\nResidual Add\nGPU: P@1"];
        layer2_norm2 [label="Layer 2\nRMSNorm\nGPU: P@1"];
        layer2_ffn_gate [label="Layer 2\nFFN Gate\nGPU: P@1"];
        layer2_ffn_up [label="Layer 2\nFFN Up\nGPU: P@1"];
        layer2_ffn_down [label="Layer 2\nFFN Down\nGPU: P@1"];
        layer2_residual2 [label="Layer 2\nResidual Add\nGPU: P@1"];
        
        // Layer 3
        layer3_norm1 [label="Layer 3\nRMSNorm\nGPU: P@1"];
        layer3_qkv [label="Layer 3\nQKV Linear\nGPU: P@1"];
        layer3_attn_score [label="Layer 3\nAttention Score\nGPU: P@1"];
        layer3_attn_out [label="Layer 3\nAttention Output\nGPU: P@1"];
        layer3_residual1 [label="Layer 3\nResidual Add\nGPU: P@1"];
        layer3_norm2 [label="Layer 3\nRMSNorm\nGPU: P@1"];
        layer3_ffn_gate [label="Layer 3\nFFN Gate\nGPU: P@1"];
        layer3_ffn_up [label="Layer 3\nFFN Up\nGPU: P@1"];
        layer3_ffn_down [label="Layer 3\nFFN Down\nGPU: P@1"];
        layer3_residual2 [label="Layer 3\nResidual Add\nGPU: P@1"];
    }
    
    // Communication
    comm_stage1_to_stage2 [label="Communication\nP@1 â†’ P@2\nInput: [batch_size=6, seq_len=384, hidden_size=4096]\nOutput: [batch_size=6, seq_len=384, hidden_size=4096]\nGPU: PCIe", shape=ellipse, style=filled, fillcolor=lightgreen];
    
    // Stage 2: P@2 (Layers 4-7)
    subgraph cluster_stage2 {
        label="Stage 2: P@2 (P100)";
        style=dashed;
        
        layer4_norm1 [label="Layer 4\nRMSNorm\nGPU: P@2"];
        layer4_qkv [label="Layer 4\nQKV Linear\nGPU: P@2"];
        layer4_attn_score [label="Layer 4\nAttention Score\nGPU: P@2"];
        layer4_attn_out [label="Layer 4\nAttention Output\nGPU: P@2"];
        layer4_residual1 [label="Layer 4\nResidual Add\nGPU: P@2"];
        layer4_norm2 [label="Layer 4\nRMSNorm\nGPU: P@2"];
        layer4_ffn_gate [label="Layer 4\nFFN Gate\nGPU: P@2"];
        layer4_ffn_up [label="Layer 4\nFFN Up\nGPU: P@2"];
        layer4_ffn_down [label="Layer 4\nFFN Down\nGPU: P@2"];
        layer4_residual2 [label="Layer 4\nResidual Add\nGPU: P@2"];
        
        layer5_norm1 [label="Layer 5\nRMSNorm\nGPU: P@2"];
        layer5_qkv [label="Layer 5\nQKV Linear\nGPU: P@2"];
        layer5_attn_score [label="Layer 5\nAttention Score\nGPU: P@2"];
        layer5_attn_out [label="Layer 5\nAttention Output\nGPU: P@2"];
        layer5_residual1 [label="Layer 5\nResidual Add\nGPU: P@2"];
        layer5_norm2 [label="Layer 5\nRMSNorm\nGPU: P@2"];
        layer5_ffn_gate [label="Layer 5\nFFN Gate\nGPU: P@2"];
        layer5_ffn_up [label="Layer 5\nFFN Up\nGPU: P@2"];
        layer5_ffn_down [label="Layer 5\nFFN Down\nGPU: P@2"];
        layer5_residual2 [label="Layer 5\nResidual Add\nGPU: P@2"];
        
        layer6_norm1 [label="Layer 6\nRMSNorm\nGPU: P@2"];
        layer6_qkv [label="Layer 6\nQKV Linear\nGPU: P@2"];
        layer6_attn_score [label="Layer 6\nAttention Score\nGPU: P@2"];
        layer6_attn_out [label="Layer 6\nAttention Output\nGPU: P@2"];
        layer6_residual1 [label="Layer 6\nResidual Add\nGPU: P@2"];
        layer6_norm2 [label="Layer 6\nRMSNorm\nGPU: P@2"];
        layer6_ffn_gate [label="Layer 6\nFFN Gate\nGPU: P@2"];
        layer6_ffn_up [label="Layer 6\nFFN Up\nGPU: P@2"];
        layer6_ffn_down [label="Layer 6\nFFN Down\nGPU: P@2"];
        layer6_residual2 [label="Layer 6\nResidual Add\nGPU: P@2"];
        
        layer7_norm1 [label="Layer 7\nRMSNorm\nGPU: P@2"];
        layer7_qkv [label="Layer 7\nQKV Linear\nGPU: P@2"];
        layer7_attn_score [label="Layer 7\nAttention Score\nGPU: P@2"];
        layer7_attn_out [label="Layer 7\nAttention Output\nGPU: P@2"];
        layer7_residual1 [label="Layer 7\nResidual Add\nGPU: P@2"];
        layer7_norm2 [label="Layer 7\nRMSNorm\nGPU: P@2"];
        layer7_ffn_gate [label="Layer 7\nFFN Gate\nGPU: P@2"];
        layer7_ffn_up [label="Layer 7\nFFN Up\nGPU: P@2"];
        layer7_ffn_down [label="Layer 7\nFFN Down\nGPU: P@2"];
        layer7_residual2 [label="Layer 7\nResidual Add\nGPU: P@2"];
    }
    
    // Continue for all stages...
    
    // Edges - Complete flow
    input -> slice_sequence
    slice_sequence -> layer1_embed
    layer1_embed -> layer1_norm1
    layer1_norm1 -> layer1_qkv
    layer1_qkv -> layer1_attn_score
    layer1_attn_score -> layer1_attn_out
    layer1_attn_out -> layer1_residual1
    layer1_embed -> layer1_residual1 [style=dashed]
    layer1_residual1 -> layer1_norm2
    layer1_norm2 -> layer1_ffn_gate
    layer1_norm2 -> layer1_ffn_up
    layer1_ffn_gate -> layer1_ffn_down
    layer1_ffn_up -> layer1_ffn_down
    layer1_ffn_down -> layer1_residual2
    layer1_residual1 -> layer1_residual2 [style=dashed]
    
    layer1_residual2 -> layer2_norm1
    layer2_norm1 -> layer2_qkv
    layer2_qkv -> layer2_attn_score
    layer2_attn_score -> layer2_attn_out
    layer2_attn_out -> layer2_residual1
    layer2_norm1 -> layer2_residual1 [style=dashed]
    layer2_residual1 -> layer2_norm2
    layer2_norm2 -> layer2_ffn_gate
    layer2_norm2 -> layer2_ffn_up
    layer2_ffn_gate -> layer2_ffn_down
    layer2_ffn_up -> layer2_ffn_down
    layer2_ffn_down -> layer2_residual2
    layer2_residual1 -> layer2_residual2 [style=dashed]
    
    layer2_residual2 -> layer3_norm1
    layer3_norm1 -> layer3_qkv
    layer3_qkv -> layer3_attn_score
    layer3_attn_score -> layer3_attn_out
    layer3_attn_out -> layer3_residual1
    layer3_norm1 -> layer3_residual1 [style=dashed]
    layer3_residual1 -> layer3_norm2
    layer3_norm2 -> layer3_ffn_gate
    layer3_norm2 -> layer3_ffn_up
    layer3_ffn_gate -> layer3_ffn_down
    layer3_ffn_up -> layer3_ffn_down
    layer3_ffn_down -> layer3_residual2
    layer3_residual1 -> layer3_residual2 [style=dashed]
    
    layer3_residual2 -> comm_stage1_to_stage2
    comm_stage1_to_stage2 -> layer4_norm1
    
    // Continue pattern for all layers...
    layer4_residual2 -> layer5_norm1
    layer5_residual2 -> layer6_norm1
    layer6_residual2 -> layer7_norm1
    
    // Output
    final_norm [label="Final RMSNorm\nInput: [batch_size=6, seq_len=384, hidden_size=4096]\nOutput: [batch_size=6, seq_len=384, hidden_size=4096]\nGPU: R@2"];
    final_linear [label="LM Head\nInput: [batch_size=6, seq_len=384, hidden_size=4096]\nOutput: [batch_size=6, seq_len=384, vocab_size=32000]\nGPU: R@2"];
    output [label="Output\nInput: [batch_size=6, seq_len=384, vocab_size=32000]\nOutput: [batch_size=6, seq_len=384, vocab_size=32000]\nGPU: R@2", shape=ellipse, style=filled, fillcolor=lightblue];
    
    layer32_residual2 -> final_norm
    final_norm -> final_linear
    final_linear -> output
}