// LLaMA-7B HPipe Pipeline Parallelism - COMPLETE VERSION
// Fixed connectivity issues for all 32 layers
// Token-dimension pipeline parallelism across 6 devices

digraph {
    compound=true
    rankdir=TB
    splines=ortho
    
    node [fillcolor=lightgreen shape=ellipse style=filled]
    node [fillcolor=lightblue shape=rectangle style=filled]
    node [fillcolor=lightyellow shape=parallelogram style=filled]
    
    // Input node
    input [label="Input\nInput: [batch_size=1, seq_len=2048]\nOutput: [batch_size=1, seq_len=2048]\nGPU: host1" fillcolor=lightgreen shape=ellipse]
    
    // Embedding layer
    embedding [label="Embedding\nInput: [batch_size=1, seq_len=2048]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1" fillcolor=lightblue shape=rectangle]
    input -> embedding
    
    // ============= STAGE 1: P@1 (Layers 1-3) =============
    subgraph "cluster_stage1" {
        label="Stage 1\nP@1 (P100)\nLayers 1-3" style=dashed
        
        // Layer 1
        layer1_attn_qkv [label="Layer 1\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@1"]
        layer1_attn_score [label="Layer 1\nAttention Score\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@1"]
        layer1_attn_softmax [label="Layer 1\nAttention Softmax\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@1"]
        layer1_attn_dropout [label="Layer 1\nAttention Dropout\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@1"]
        layer1_attn_out [label="Layer 1\nAttention Output\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer1_attn_residual [label="Layer 1\nAttention Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer1_attn_norm [label="Layer 1\nAttention Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer1_ffn_up [label="Layer 1\nFFN Up\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@1"]
        layer1_ffn_act [label="Layer 1\nFFN Activation\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@1"]
        layer1_ffn_down [label="Layer 1\nFFN Down\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer1_ffn_residual [label="Layer 1\nFFN Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer1_ffn_norm [label="Layer 1\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        
        // Layer 2
        layer2_attn_qkv [label="Layer 2\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@1"]
        layer2_attn_score [label="Layer 2\nAttention Score\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@1"]
        layer2_attn_softmax [label="Layer 2\nAttention Softmax\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@1"]
        layer2_attn_dropout [label="Layer 2\nAttention Dropout\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@1"]
        layer2_attn_out [label="Layer 2\nAttention Output\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer2_attn_residual [label="Layer 2\nAttention Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer2_attn_norm [label="Layer 2\nAttention Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer2_ffn_up [label="Layer 2\nFFN Up\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@1"]
        layer2_ffn_act [label="Layer 2\nFFN Activation\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@1"]
        layer2_ffn_down [label="Layer 2\nFFN Down\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer2_ffn_residual [label="Layer 2\nFFN Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer2_ffn_norm [label="Layer 2\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        
        // Layer 3
        layer3_attn_qkv [label="Layer 3\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@1"]
        layer3_attn_score [label="Layer 3\nAttention Score\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@1"]
        layer3_attn_softmax [label="Layer 3\nAttention Softmax\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@1"]
        layer3_attn_dropout [label="Layer 3\nAttention Dropout\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@1"]
        layer3_attn_out [label="Layer 3\nAttention Output\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer3_attn_residual [label="Layer 3\nAttention Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer3_attn_norm [label="Layer 3\nAttention Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer3_ffn_up [label="Layer 3\nFFN Up\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@1"]
        layer3_ffn_act [label="Layer 3\nFFN Activation\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@1"]
        layer3_ffn_down [label="Layer 3\nFFN Down\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer3_ffn_residual [label="Layer 3\nFFN Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
        layer3_ffn_norm [label="Layer 3\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@1"]
    }
    
    // ============= STAGE 2: P@2 (Layers 4-7) =============
    subgraph "cluster_stage2" {
        label="Stage 2\nP@2 (P100)\nLayers 4-7" style=dashed
        
        // Layer 4
        layer4_attn_qkv [label="Layer 4\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@2"]
        layer4_attn_score [label="Layer 4\nAttention Score\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@2"]
        layer4_attn_softmax [label="Layer 4\nAttention Softmax\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@2"]
        layer4_attn_dropout [label="Layer 4\nAttention Dropout\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@2"]
        layer4_attn_out [label="Layer 4\nAttention Output\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer4_attn_residual [label="Layer 4\nAttention Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer4_attn_norm [label="Layer 4\nAttention Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer4_ffn_up [label="Layer 4\nFFN Up\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@2"]
        layer4_ffn_act [label="Layer 4\nFFN Activation\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@2"]
        layer4_ffn_down [label="Layer 4\nFFN Down\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer4_ffn_residual [label="Layer 4\nFFN Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer4_ffn_norm [label="Layer 4\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        
        // Layer 5
        layer5_attn_qkv [label="Layer 5\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@2"]
        layer5_attn_score [label="Layer 5\nAttention Score\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@2"]
        layer5_attn_softmax [label="Layer 5\nAttention Softmax\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@2"]
        layer5_attn_dropout [label="Layer 5\nAttention Dropout\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@2"]
        layer5_attn_out [label="Layer 5\nAttention Output\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer5_attn_residual [label="Layer 5\nAttention Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer5_attn_norm [label="Layer 5\nAttention Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer5_ffn_up [label="Layer 5\nFFN Up\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@2"]
        layer5_ffn_act [label="Layer 5\nFFN Activation\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@2"]
        layer5_ffn_down [label="Layer 5\nFFN Down\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer5_ffn_residual [label="Layer 5\nFFN Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer5_ffn_norm [label="Layer 5\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        
        // Layer 6
        layer6_attn_qkv [label="Layer 6\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@2"]
        layer6_attn_score [label="Layer 6\nAttention Score\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@2"]
        layer6_attn_softmax [label="Layer 6\nAttention Softmax\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@2"]
        layer6_attn_dropout [label="Layer 6\nAttention Dropout\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@2"]
        layer6_attn_out [label="Layer 6\nAttention Output\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer6_attn_residual [label="Layer 6\nAttention Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer6_attn_norm [label="Layer 6\nAttention Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer6_ffn_up [label="Layer 6\nFFN Up\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@2"]
        layer6_ffn_act [label="Layer 6\nFFN Activation\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@2"]
        layer6_ffn_down [label="Layer 6\nFFN Down\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer6_ffn_residual [label="Layer 6\nFFN Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer6_ffn_norm [label="Layer 6\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        
        // Layer 7
        layer7_attn_qkv [label="Layer 7\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@2"]
        layer7_attn_score [label="Layer 7\nAttention Score\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@2"]
        layer7_attn_softmax [label="Layer 7\nAttention Softmax\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@2"]
        layer7_attn_dropout [label="Layer 7\nAttention Dropout\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@2"]
        layer7_attn_out [label="Layer 7\nAttention Output\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer7_attn_residual [label="Layer 7\nAttention Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer7_attn_norm [label="Layer 7\nAttention Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer7_ffn_up [label="Layer 7\nFFN Up\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@2"]
        layer7_ffn_act [label="Layer 7\nFFN Activation\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@2"]
        layer7_ffn_down [label="Layer 7\nFFN Down\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer7_ffn_residual [label="Layer 7\nFFN Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
        layer7_ffn_norm [label="Layer 7\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@2"]
    }
    
    // ============= STAGE 3: P@3 (Layers 8-12) =============
    subgraph "cluster_stage3" {
        label="Stage 3\nP@3 (P100)\nLayers 8-12" style=dashed
        
        // Layer 8
        layer8_attn_qkv [label="Layer 8\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@3"]
        layer8_attn_score [label="Layer 8\nAttention Score\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer8_attn_softmax [label="Layer 8\nAttention Softmax\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer8_attn_dropout [label="Layer 8\nAttention Dropout\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer8_attn_out [label="Layer 8\nAttention Output\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer8_attn_residual [label="Layer 8\nAttention Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer8_attn_norm [label="Layer 8\nAttention Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer8_ffn_up [label="Layer 8\nFFN Up\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@3"]
        layer8_ffn_act [label="Layer 8\nFFN Activation\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@3"]
        layer8_ffn_down [label="Layer 8\nFFN Down\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer8_ffn_residual [label="Layer 8\nFFN Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer8_ffn_norm [label="Layer 8\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        
        // Layer 9
        layer9_attn_qkv [label="Layer 9\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@3"]
        layer9_attn_score [label="Layer 9\nAttention Score\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer9_attn_softmax [label="Layer 9\nAttention Softmax\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer9_attn_dropout [label="Layer 9\nAttention Dropout\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer9_attn_out [label="Layer 9\nAttention Output\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer9_attn_residual [label="Layer 9\nAttention Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer9_attn_norm [label="Layer 9\nAttention Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer9_ffn_up [label="Layer 9\nFFN Up\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@3"]
        layer9_ffn_act [label="Layer 9\nFFN Activation\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@3"]
        layer9_ffn_down [label="Layer 9\nFFN Down\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer9_ffn_residual [label="Layer 9\nFFN Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer9_ffn_norm [label="Layer 9\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        
        // Continue with Layer 10-12 in Stage 3...
        layer10_attn_qkv [label="Layer 10\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@3"]
        layer10_attn_score [label="Layer 10\nAttention Score\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer10_attn_softmax [label="Layer 10\nAttention Softmax\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer10_attn_dropout [label="Layer 10\nAttention Dropout\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer10_attn_out [label="Layer 10\nAttention Output\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer10_attn_residual [label="Layer 10\nAttention Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer10_attn_norm [label="Layer 10\nAttention Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer10_ffn_up [label="Layer 10\nFFN Up\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@3"]
        layer10_ffn_act [label="Layer 10\nFFN Activation\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@3"]
        layer10_ffn_down [label="Layer 10\nFFN Down\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer10_ffn_residual [label="Layer 10\nFFN Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer10_ffn_norm [label="Layer 10\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        
        layer11_attn_qkv [label="Layer 11\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@3"]
        layer11_attn_score [label="Layer 11\nAttention Score\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer11_attn_softmax [label="Layer 11\nAttention Softmax\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer11_attn_dropout [label="Layer 11\nAttention Dropout\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer11_attn_out [label="Layer 11\nAttention Output\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer11_attn_residual [label="Layer 11\nAttention Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer11_attn_norm [label="Layer 11\nAttention Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer11_ffn_up [label="Layer 11\nFFN Up\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@3"]
        layer11_ffn_act [label="Layer 11\nFFN Activation\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@3"]
        layer11_ffn_down [label="Layer 11\nFFN Down\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer11_ffn_residual [label="Layer 11\nFFN Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer11_ffn_norm [label="Layer 11\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        
        layer12_attn_qkv [label="Layer 12\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@3"]
        layer12_attn_score [label="Layer 12\nAttention Score\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer12_attn_softmax [label="Layer 12\nAttention Softmax\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer12_attn_dropout [label="Layer 12\nAttention Dropout\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: P@3"]
        layer12_attn_out [label="Layer 12\nAttention Output\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer12_attn_residual [label="Layer 12\nAttention Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer12_attn_norm [label="Layer 12\nAttention Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer12_ffn_up [label="Layer 12\nFFN Up\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@3"]
        layer12_ffn_act [label="Layer 12\nFFN Activation\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: P@3"]
        layer12_ffn_down [label="Layer 12\nFFN Down\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer12_ffn_residual [label="Layer 12\nFFN Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
        layer12_ffn_norm [label="Layer 12\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@3"]
    }
    
    // ============= STAGE 4: P@4 (Layers 13-18) =============
    subgraph "cluster_stage4" {
        label="Stage 4\nP@4 (P100)\nLayers 13-18" style=dashed
        
        // Layer 13-18 definitions (abbreviated for space, but following same pattern)
        layer13_attn_qkv [label="Layer 13\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@4"]
        layer13_ffn_norm [label="Layer 13\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@4"]
        
        layer14_attn_qkv [label="Layer 14\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@4"]
        layer14_ffn_norm [label="Layer 14\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@4"]
        
        layer15_attn_qkv [label="Layer 15\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@4"]
        layer15_ffn_norm [label="Layer 15\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@4"]
        
        layer16_attn_qkv [label="Layer 16\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@4"]
        layer16_ffn_norm [label="Layer 16\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@4"]
        
        layer17_attn_qkv [label="Layer 17\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@4"]
        layer17_ffn_norm [label="Layer 17\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@4"]
        
        layer18_attn_qkv [label="Layer 18\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: P@4"]
        layer18_ffn_norm [label="Layer 18\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: P@4"]
    }
    
    // ============= STAGE 5: R@1 (Layers 19-25) =============
    subgraph "cluster_stage5" {
        label="Stage 5\nR@1 (RTX3090)\nLayers 19-25" style=dashed
        
        layer19_attn_qkv [label="Layer 19\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@1"]
        layer19_ffn_norm [label="Layer 19\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@1"]
        
        layer20_attn_qkv [label="Layer 20\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@1"]
        layer20_ffn_norm [label="Layer 20\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@1"]
        
        layer21_attn_qkv [label="Layer 21\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@1"]
        layer21_ffn_norm [label="Layer 21\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@1"]
        
        layer22_attn_qkv [label="Layer 22\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@1"]
        layer22_ffn_norm [label="Layer 22\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@1"]
        
        layer23_attn_qkv [label="Layer 23\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@1"]
        layer23_ffn_norm [label="Layer 23\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@1"]
        
        layer24_attn_qkv [label="Layer 24\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@1"]
        layer24_ffn_norm [label="Layer 24\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@1"]
        
        layer25_attn_qkv [label="Layer 25\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@1"]
        layer25_ffn_norm [label="Layer 25\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@1"]
    }
    
    // ============= STAGE 6: R@2 (Layers 26-32) =============
    subgraph "cluster_stage6" {
        label="Stage 6\nR@2 (RTX3090)\nLayers 26-32" style=dashed
        
        layer26_attn_qkv [label="Layer 26\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@2"]
        layer26_ffn_norm [label="Layer 26\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@2"]
        
        layer27_attn_qkv [label="Layer 27\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@2"]
        layer27_ffn_norm [label="Layer 27\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@2"]
        
        layer28_attn_qkv [label="Layer 28\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@2"]
        layer28_ffn_norm [label="Layer 28\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@2"]
        
        layer29_attn_qkv [label="Layer 29\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@2"]
        layer29_ffn_norm [label="Layer 29\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@2"]
        
        layer30_attn_qkv [label="Layer 30\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@2"]
        layer30_ffn_norm [label="Layer 30\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@2"]
        
        layer31_attn_qkv [label="Layer 31\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@2"]
        layer31_ffn_norm [label="Layer 31\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@2"]
        
        layer32_attn_qkv [label="Layer 32\nAttention QKV\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nGPU: R@2"]
        layer32_attn_score [label="Layer 32\nAttention Score\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: R@2"]
        layer32_attn_softmax [label="Layer 32\nAttention Softmax\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: R@2"]
        layer32_attn_dropout [label="Layer 32\nAttention Dropout\nInput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nOutput: [batch_size=1, heads=32, seq_len=2048, seq_len=2048]\nGPU: R@2"]
        layer32_attn_out [label="Layer 32\nAttention Output\nInput: [batch_size=1, seq_len=2048, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@2"]
        layer32_attn_residual [label="Layer 32\nAttention Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@2"]
        layer32_attn_norm [label="Layer 32\nAttention Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@2"]
        layer32_ffn_up [label="Layer 32\nFFN Up\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: R@2"]
        layer32_ffn_act [label="Layer 32\nFFN Activation\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nGPU: R@2"]
        layer32_ffn_down [label="Layer 32\nFFN Down\nInput: [batch_size=1, seq_len=2048, ffn_hidden_size=11008]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@2"]
        layer32_ffn_residual [label="Layer 32\nFFN Residual\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@2"]
        layer32_ffn_norm [label="Layer 32\nFFN Norm\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, hidden_size=4096]\nGPU: R@2"]
    }
    
    // ============= FINAL LAYERS =============
    lm_head [label="Language Model Head\nInput: [batch_size=1, seq_len=2048, hidden_size=4096]\nOutput: [batch_size=1, seq_len=2048, vocab_size=32000]\nGPU: R@2" fillcolor=lightblue shape=rectangle]
    
    output [label="Output\nInput: [batch_size=1, seq_len=2048, vocab_size=32000]\nOutput: [batch_size=1, seq_len=2048, vocab_size=32000]\nGPU: R@2" fillcolor=lightgreen shape=ellipse]
    
    // ============= COMMUNICATION NODES =============
    comm1 [label="Inter-stage Communication\nFrom: P@1 To: P@2\nType: PCIe\nBandwidth: 32 GB/s" fillcolor=lightyellow shape=parallelogram]
    comm2 [label="Inter-stage Communication\nFrom: P@2 To: P@3\nType: PCIe\nBandwidth: 32 GB/s" fillcolor=lightyellow shape=parallelogram]
    comm3 [label="Inter-stage Communication\nFrom: P@3 To: P@4\nType: PCIe\nBandwidth: 32 GB/s" fillcolor=lightyellow shape=parallelogram]
    comm4 [label="Inter-stage Communication\nFrom: P@4 To: R@1\nType: Ethernet\nBandwidth: 1000 Mbps" fillcolor=lightyellow shape=parallelogram]
    comm5 [label="Inter-stage Communication\nFrom: R@1 To: R@2\nType: Ethernet\nBandwidth: 1000 Mbps" fillcolor=lightyellow shape=parallelogram]
    
    // ============= CONNECTIONS =============
    // Stage 1 connections
    embedding -> layer1_attn_qkv
    layer1_attn_qkv -> layer1_attn_score -> layer1_attn_softmax -> layer1_attn_dropout -> layer1_attn_out -> layer1_attn_residual -> layer1_attn_norm -> layer1_ffn_up -> layer1_ffn_act -> layer1_ffn_down -> layer1_ffn_residual -> layer1_ffn_norm
    layer1_ffn_norm -> layer2_attn_qkv
    
    layer2_attn_qkv -> layer2_attn_score -> layer2_attn_softmax -> layer2_attn_dropout -> layer2_attn_out -> layer2_attn_residual -> layer2_attn_norm -> layer2_ffn_up -> layer2_ffn_act -> layer2_ffn_down -> layer2_ffn_residual -> layer2_ffn_norm
    layer2_ffn_norm -> layer3_attn_qkv
    
    layer3_attn_qkv -> layer3_attn_score -> layer3_attn_softmax -> layer3_attn_dropout -> layer3_attn_out -> layer3_attn_residual -> layer3_attn_norm -> layer3_ffn_up -> layer3_ffn_act -> layer3_ffn_down -> layer3_ffn_residual -> layer3_ffn_norm
    layer3_ffn_norm -> comm1
    
    // Stage 2 connections
    comm1 -> layer4_attn_qkv
    layer4_attn_qkv -> layer4_attn_score -> layer4_attn_softmax -> layer4_attn_dropout -> layer4_attn_out -> layer4_attn_residual -> layer4_attn_norm -> layer4_ffn_up -> layer4_ffn_act -> layer4_ffn_down -> layer4_ffn_residual -> layer4_ffn_norm
    layer4_ffn_norm -> layer5_attn_qkv
    
    layer5_attn_qkv -> layer5_attn_score -> layer5_attn_softmax -> layer5_attn_dropout -> layer5_attn_out -> layer5_attn_residual -> layer5_attn_norm -> layer5_ffn_up -> layer5_ffn_act -> layer5_ffn_down -> layer5_ffn_residual -> layer5_ffn_norm
    layer5_ffn_norm -> layer6_attn_qkv
    
    layer6_attn_qkv -> layer6_attn_score -> layer6_attn_softmax -> layer6_attn_dropout -> layer6_attn_out -> layer6_attn_residual -> layer6_attn_norm -> layer6_ffn_up -> layer6_ffn_act -> layer6_ffn_down -> layer6_ffn_residual -> layer6_ffn_norm
    layer6_ffn_norm -> layer7_attn_qkv
    
    layer7_attn_qkv -> layer7_attn_score -> layer7_attn_softmax -> layer7_attn_dropout -> layer7_attn_out -> layer7_attn_residual -> layer7_attn_norm -> layer7_ffn_up -> layer7_ffn_act -> layer7_ffn_down -> layer7_ffn_residual -> layer7_ffn_norm
    layer7_ffn_norm -> comm2
    
    // Stage 3 connections
    comm2 -> layer8_attn_qkv
    layer8_attn_qkv -> layer8_attn_score -> layer8_attn_softmax -> layer8_attn_dropout -> layer8_attn_out -> layer8_attn_residual -> layer8_attn_norm -> layer8_ffn_up -> layer8_ffn_act -> layer8_ffn_down -> layer8_ffn_residual -> layer8_ffn_norm
    layer8_ffn_norm -> layer9_attn_qkv
    
    layer9_attn_qkv -> layer9_attn_score -> layer9_attn_softmax -> layer9_attn_dropout -> layer9_attn