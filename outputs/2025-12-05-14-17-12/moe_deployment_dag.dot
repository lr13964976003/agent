// 30B MoE Model Deployment DAG
digraph {
	bgcolor=white rankdir=TB splines=ortho
	node [fontname=Arial fontsize=10 shape=rectangle style=filled]
	edge [fontname=Arial fontsize=9]
	node [shape=ellipse]
	node [shape=rectangle style="filled,rounded"]
	node [shape=parallelogram style=filled]
	subgraph cluster_input {
		bgcolor="#F0F0F0" label="Input Layer" style=rounded
		input [label="Input Tokens\nInput: [batch_size=128, seq_len=1024]\nOutput: [batch_size=128, seq_len=1024]" fillcolor=lightblue shape=ellipse]
	}
	subgraph cluster_stage0 {
		bgcolor="#FFCCCC" label="Pipeline Stage 0 (Layers 0-1)\nGPUs: 0-127" style=rounded
		stage0_routing [label="Expert Routing\nAll-to-All Communication\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage0_expert0_qkv [label="Expert 0 QKV Proj (TP)\nGPUs: 0,1\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert0_attn [label="Expert 0 Attention (TP)\nGPUs: 0,1\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert0_attn_out [label="Expert 0 Attn Output (TP)\nGPUs: 0,1\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert0_mlp1 [label="Expert 0 MLP Layer1 (TP)\nGPUs: 0,1\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert0_mlp_act [label="Expert 0 MLP GELU (TP)\nGPUs: 0,1\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert0_mlp2 [label="Expert 0 MLP Layer2 (TP)\nGPUs: 0,1\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert1_qkv [label="Expert 1 QKV Proj (TP)\nGPUs: 2,3\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert1_attn [label="Expert 1 Attention (TP)\nGPUs: 2,3\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert1_attn_out [label="Expert 1 Attn Output (TP)\nGPUs: 2,3\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert1_mlp1 [label="Expert 1 MLP Layer1 (TP)\nGPUs: 2,3\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert1_mlp_act [label="Expert 1 MLP GELU (TP)\nGPUs: 2,3\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert1_mlp2 [label="Expert 1 MLP Layer2 (TP)\nGPUs: 2,3\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert2_qkv [label="Expert 2 QKV Proj (TP)\nGPUs: 4,5\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert2_attn [label="Expert 2 Attention (TP)\nGPUs: 4,5\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert2_attn_out [label="Expert 2 Attn Output (TP)\nGPUs: 4,5\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert2_mlp1 [label="Expert 2 MLP Layer1 (TP)\nGPUs: 4,5\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert2_mlp_act [label="Expert 2 MLP GELU (TP)\nGPUs: 4,5\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert2_mlp2 [label="Expert 2 MLP Layer2 (TP)\nGPUs: 4,5\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert3_qkv [label="Expert 3 QKV Proj (TP)\nGPUs: 6,7\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert3_attn [label="Expert 3 Attention (TP)\nGPUs: 6,7\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert3_attn_out [label="Expert 3 Attn Output (TP)\nGPUs: 6,7\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert3_mlp1 [label="Expert 3 MLP Layer1 (TP)\nGPUs: 6,7\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert3_mlp_act [label="Expert 3 MLP GELU (TP)\nGPUs: 6,7\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert3_mlp2 [label="Expert 3 MLP Layer2 (TP)\nGPUs: 6,7\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert4_qkv [label="Expert 4 QKV Proj (TP)\nGPUs: 8,9\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert4_attn [label="Expert 4 Attention (TP)\nGPUs: 8,9\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert4_attn_out [label="Expert 4 Attn Output (TP)\nGPUs: 8,9\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert4_mlp1 [label="Expert 4 MLP Layer1 (TP)\nGPUs: 8,9\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert4_mlp_act [label="Expert 4 MLP GELU (TP)\nGPUs: 8,9\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert4_mlp2 [label="Expert 4 MLP Layer2 (TP)\nGPUs: 8,9\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert5_qkv [label="Expert 5 QKV Proj (TP)\nGPUs: 10,11\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert5_attn [label="Expert 5 Attention (TP)\nGPUs: 10,11\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert5_attn_out [label="Expert 5 Attn Output (TP)\nGPUs: 10,11\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert5_mlp1 [label="Expert 5 MLP Layer1 (TP)\nGPUs: 10,11\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert5_mlp_act [label="Expert 5 MLP GELU (TP)\nGPUs: 10,11\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert5_mlp2 [label="Expert 5 MLP Layer2 (TP)\nGPUs: 10,11\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert6_qkv [label="Expert 6 QKV Proj (TP)\nGPUs: 12,13\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert6_attn [label="Expert 6 Attention (TP)\nGPUs: 12,13\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert6_attn_out [label="Expert 6 Attn Output (TP)\nGPUs: 12,13\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert6_mlp1 [label="Expert 6 MLP Layer1 (TP)\nGPUs: 12,13\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert6_mlp_act [label="Expert 6 MLP GELU (TP)\nGPUs: 12,13\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert6_mlp2 [label="Expert 6 MLP Layer2 (TP)\nGPUs: 12,13\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert7_qkv [label="Expert 7 QKV Proj (TP)\nGPUs: 14,15\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert7_attn [label="Expert 7 Attention (TP)\nGPUs: 14,15\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert7_attn_out [label="Expert 7 Attn Output (TP)\nGPUs: 14,15\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert7_mlp1 [label="Expert 7 MLP Layer1 (TP)\nGPUs: 14,15\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert7_mlp_act [label="Expert 7 MLP GELU (TP)\nGPUs: 14,15\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert7_mlp2 [label="Expert 7 MLP Layer2 (TP)\nGPUs: 14,15\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert8_qkv [label="Expert 8 QKV Proj (TP)\nGPUs: 16,17\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert8_attn [label="Expert 8 Attention (TP)\nGPUs: 16,17\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert8_attn_out [label="Expert 8 Attn Output (TP)\nGPUs: 16,17\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert8_mlp1 [label="Expert 8 MLP Layer1 (TP)\nGPUs: 16,17\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert8_mlp_act [label="Expert 8 MLP GELU (TP)\nGPUs: 16,17\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert8_mlp2 [label="Expert 8 MLP Layer2 (TP)\nGPUs: 16,17\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert9_qkv [label="Expert 9 QKV Proj (TP)\nGPUs: 18,19\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert9_attn [label="Expert 9 Attention (TP)\nGPUs: 18,19\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert9_attn_out [label="Expert 9 Attn Output (TP)\nGPUs: 18,19\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert9_mlp1 [label="Expert 9 MLP Layer1 (TP)\nGPUs: 18,19\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert9_mlp_act [label="Expert 9 MLP GELU (TP)\nGPUs: 18,19\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert9_mlp2 [label="Expert 9 MLP Layer2 (TP)\nGPUs: 18,19\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert10_qkv [label="Expert 10 QKV Proj (TP)\nGPUs: 20,21\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert10_attn [label="Expert 10 Attention (TP)\nGPUs: 20,21\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert10_attn_out [label="Expert 10 Attn Output (TP)\nGPUs: 20,21\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert10_mlp1 [label="Expert 10 MLP Layer1 (TP)\nGPUs: 20,21\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert10_mlp_act [label="Expert 10 MLP GELU (TP)\nGPUs: 20,21\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert10_mlp2 [label="Expert 10 MLP Layer2 (TP)\nGPUs: 20,21\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert11_qkv [label="Expert 11 QKV Proj (TP)\nGPUs: 22,23\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert11_attn [label="Expert 11 Attention (TP)\nGPUs: 22,23\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert11_attn_out [label="Expert 11 Attn Output (TP)\nGPUs: 22,23\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert11_mlp1 [label="Expert 11 MLP Layer1 (TP)\nGPUs: 22,23\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert11_mlp_act [label="Expert 11 MLP GELU (TP)\nGPUs: 22,23\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert11_mlp2 [label="Expert 11 MLP Layer2 (TP)\nGPUs: 22,23\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert12_qkv [label="Expert 12 QKV Proj (TP)\nGPUs: 24,25\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert12_attn [label="Expert 12 Attention (TP)\nGPUs: 24,25\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert12_attn_out [label="Expert 12 Attn Output (TP)\nGPUs: 24,25\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert12_mlp1 [label="Expert 12 MLP Layer1 (TP)\nGPUs: 24,25\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert12_mlp_act [label="Expert 12 MLP GELU (TP)\nGPUs: 24,25\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert12_mlp2 [label="Expert 12 MLP Layer2 (TP)\nGPUs: 24,25\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert13_qkv [label="Expert 13 QKV Proj (TP)\nGPUs: 26,27\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert13_attn [label="Expert 13 Attention (TP)\nGPUs: 26,27\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert13_attn_out [label="Expert 13 Attn Output (TP)\nGPUs: 26,27\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert13_mlp1 [label="Expert 13 MLP Layer1 (TP)\nGPUs: 26,27\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert13_mlp_act [label="Expert 13 MLP GELU (TP)\nGPUs: 26,27\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert13_mlp2 [label="Expert 13 MLP Layer2 (TP)\nGPUs: 26,27\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert14_qkv [label="Expert 14 QKV Proj (TP)\nGPUs: 28,29\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert14_attn [label="Expert 14 Attention (TP)\nGPUs: 28,29\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert14_attn_out [label="Expert 14 Attn Output (TP)\nGPUs: 28,29\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert14_mlp1 [label="Expert 14 MLP Layer1 (TP)\nGPUs: 28,29\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert14_mlp_act [label="Expert 14 MLP GELU (TP)\nGPUs: 28,29\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert14_mlp2 [label="Expert 14 MLP Layer2 (TP)\nGPUs: 28,29\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert15_qkv [label="Expert 15 QKV Proj (TP)\nGPUs: 30,31\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert15_attn [label="Expert 15 Attention (TP)\nGPUs: 30,31\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert15_attn_out [label="Expert 15 Attn Output (TP)\nGPUs: 30,31\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert15_mlp1 [label="Expert 15 MLP Layer1 (TP)\nGPUs: 30,31\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert15_mlp_act [label="Expert 15 MLP GELU (TP)\nGPUs: 30,31\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert15_mlp2 [label="Expert 15 MLP Layer2 (TP)\nGPUs: 30,31\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert16_qkv [label="Expert 16 QKV Proj (TP)\nGPUs: 32,33\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert16_attn [label="Expert 16 Attention (TP)\nGPUs: 32,33\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert16_attn_out [label="Expert 16 Attn Output (TP)\nGPUs: 32,33\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert16_mlp1 [label="Expert 16 MLP Layer1 (TP)\nGPUs: 32,33\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert16_mlp_act [label="Expert 16 MLP GELU (TP)\nGPUs: 32,33\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert16_mlp2 [label="Expert 16 MLP Layer2 (TP)\nGPUs: 32,33\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert17_qkv [label="Expert 17 QKV Proj (TP)\nGPUs: 34,35\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert17_attn [label="Expert 17 Attention (TP)\nGPUs: 34,35\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert17_attn_out [label="Expert 17 Attn Output (TP)\nGPUs: 34,35\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert17_mlp1 [label="Expert 17 MLP Layer1 (TP)\nGPUs: 34,35\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert17_mlp_act [label="Expert 17 MLP GELU (TP)\nGPUs: 34,35\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert17_mlp2 [label="Expert 17 MLP Layer2 (TP)\nGPUs: 34,35\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert18_qkv [label="Expert 18 QKV Proj (TP)\nGPUs: 36,37\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert18_attn [label="Expert 18 Attention (TP)\nGPUs: 36,37\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert18_attn_out [label="Expert 18 Attn Output (TP)\nGPUs: 36,37\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert18_mlp1 [label="Expert 18 MLP Layer1 (TP)\nGPUs: 36,37\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert18_mlp_act [label="Expert 18 MLP GELU (TP)\nGPUs: 36,37\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert18_mlp2 [label="Expert 18 MLP Layer2 (TP)\nGPUs: 36,37\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert19_qkv [label="Expert 19 QKV Proj (TP)\nGPUs: 38,39\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert19_attn [label="Expert 19 Attention (TP)\nGPUs: 38,39\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert19_attn_out [label="Expert 19 Attn Output (TP)\nGPUs: 38,39\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert19_mlp1 [label="Expert 19 MLP Layer1 (TP)\nGPUs: 38,39\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert19_mlp_act [label="Expert 19 MLP GELU (TP)\nGPUs: 38,39\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert19_mlp2 [label="Expert 19 MLP Layer2 (TP)\nGPUs: 38,39\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert20_qkv [label="Expert 20 QKV Proj (TP)\nGPUs: 40,41\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert20_attn [label="Expert 20 Attention (TP)\nGPUs: 40,41\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert20_attn_out [label="Expert 20 Attn Output (TP)\nGPUs: 40,41\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert20_mlp1 [label="Expert 20 MLP Layer1 (TP)\nGPUs: 40,41\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert20_mlp_act [label="Expert 20 MLP GELU (TP)\nGPUs: 40,41\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert20_mlp2 [label="Expert 20 MLP Layer2 (TP)\nGPUs: 40,41\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert21_qkv [label="Expert 21 QKV Proj (TP)\nGPUs: 42,43\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert21_attn [label="Expert 21 Attention (TP)\nGPUs: 42,43\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert21_attn_out [label="Expert 21 Attn Output (TP)\nGPUs: 42,43\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert21_mlp1 [label="Expert 21 MLP Layer1 (TP)\nGPUs: 42,43\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert21_mlp_act [label="Expert 21 MLP GELU (TP)\nGPUs: 42,43\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert21_mlp2 [label="Expert 21 MLP Layer2 (TP)\nGPUs: 42,43\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert22_qkv [label="Expert 22 QKV Proj (TP)\nGPUs: 44,45\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert22_attn [label="Expert 22 Attention (TP)\nGPUs: 44,45\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert22_attn_out [label="Expert 22 Attn Output (TP)\nGPUs: 44,45\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert22_mlp1 [label="Expert 22 MLP Layer1 (TP)\nGPUs: 44,45\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert22_mlp_act [label="Expert 22 MLP GELU (TP)\nGPUs: 44,45\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert22_mlp2 [label="Expert 22 MLP Layer2 (TP)\nGPUs: 44,45\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert23_qkv [label="Expert 23 QKV Proj (TP)\nGPUs: 46,47\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert23_attn [label="Expert 23 Attention (TP)\nGPUs: 46,47\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert23_attn_out [label="Expert 23 Attn Output (TP)\nGPUs: 46,47\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert23_mlp1 [label="Expert 23 MLP Layer1 (TP)\nGPUs: 46,47\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert23_mlp_act [label="Expert 23 MLP GELU (TP)\nGPUs: 46,47\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert23_mlp2 [label="Expert 23 MLP Layer2 (TP)\nGPUs: 46,47\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert24_qkv [label="Expert 24 QKV Proj (TP)\nGPUs: 48,49\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert24_attn [label="Expert 24 Attention (TP)\nGPUs: 48,49\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert24_attn_out [label="Expert 24 Attn Output (TP)\nGPUs: 48,49\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert24_mlp1 [label="Expert 24 MLP Layer1 (TP)\nGPUs: 48,49\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert24_mlp_act [label="Expert 24 MLP GELU (TP)\nGPUs: 48,49\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert24_mlp2 [label="Expert 24 MLP Layer2 (TP)\nGPUs: 48,49\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert25_qkv [label="Expert 25 QKV Proj (TP)\nGPUs: 50,51\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert25_attn [label="Expert 25 Attention (TP)\nGPUs: 50,51\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert25_attn_out [label="Expert 25 Attn Output (TP)\nGPUs: 50,51\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert25_mlp1 [label="Expert 25 MLP Layer1 (TP)\nGPUs: 50,51\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert25_mlp_act [label="Expert 25 MLP GELU (TP)\nGPUs: 50,51\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert25_mlp2 [label="Expert 25 MLP Layer2 (TP)\nGPUs: 50,51\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert26_qkv [label="Expert 26 QKV Proj (TP)\nGPUs: 52,53\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert26_attn [label="Expert 26 Attention (TP)\nGPUs: 52,53\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert26_attn_out [label="Expert 26 Attn Output (TP)\nGPUs: 52,53\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert26_mlp1 [label="Expert 26 MLP Layer1 (TP)\nGPUs: 52,53\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert26_mlp_act [label="Expert 26 MLP GELU (TP)\nGPUs: 52,53\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert26_mlp2 [label="Expert 26 MLP Layer2 (TP)\nGPUs: 52,53\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert27_qkv [label="Expert 27 QKV Proj (TP)\nGPUs: 54,55\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert27_attn [label="Expert 27 Attention (TP)\nGPUs: 54,55\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert27_attn_out [label="Expert 27 Attn Output (TP)\nGPUs: 54,55\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert27_mlp1 [label="Expert 27 MLP Layer1 (TP)\nGPUs: 54,55\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert27_mlp_act [label="Expert 27 MLP GELU (TP)\nGPUs: 54,55\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert27_mlp2 [label="Expert 27 MLP Layer2 (TP)\nGPUs: 54,55\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert28_qkv [label="Expert 28 QKV Proj (TP)\nGPUs: 56,57\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert28_attn [label="Expert 28 Attention (TP)\nGPUs: 56,57\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert28_attn_out [label="Expert 28 Attn Output (TP)\nGPUs: 56,57\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert28_mlp1 [label="Expert 28 MLP Layer1 (TP)\nGPUs: 56,57\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert28_mlp_act [label="Expert 28 MLP GELU (TP)\nGPUs: 56,57\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert28_mlp2 [label="Expert 28 MLP Layer2 (TP)\nGPUs: 56,57\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert29_qkv [label="Expert 29 QKV Proj (TP)\nGPUs: 58,59\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert29_attn [label="Expert 29 Attention (TP)\nGPUs: 58,59\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert29_attn_out [label="Expert 29 Attn Output (TP)\nGPUs: 58,59\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert29_mlp1 [label="Expert 29 MLP Layer1 (TP)\nGPUs: 58,59\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert29_mlp_act [label="Expert 29 MLP GELU (TP)\nGPUs: 58,59\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert29_mlp2 [label="Expert 29 MLP Layer2 (TP)\nGPUs: 58,59\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert30_qkv [label="Expert 30 QKV Proj (TP)\nGPUs: 60,61\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert30_attn [label="Expert 30 Attention (TP)\nGPUs: 60,61\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert30_attn_out [label="Expert 30 Attn Output (TP)\nGPUs: 60,61\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert30_mlp1 [label="Expert 30 MLP Layer1 (TP)\nGPUs: 60,61\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert30_mlp_act [label="Expert 30 MLP GELU (TP)\nGPUs: 60,61\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert30_mlp2 [label="Expert 30 MLP Layer2 (TP)\nGPUs: 60,61\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert31_qkv [label="Expert 31 QKV Proj (TP)\nGPUs: 62,63\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert31_attn [label="Expert 31 Attention (TP)\nGPUs: 62,63\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert31_attn_out [label="Expert 31 Attn Output (TP)\nGPUs: 62,63\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert31_mlp1 [label="Expert 31 MLP Layer1 (TP)\nGPUs: 62,63\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert31_mlp_act [label="Expert 31 MLP GELU (TP)\nGPUs: 62,63\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert31_mlp2 [label="Expert 31 MLP Layer2 (TP)\nGPUs: 62,63\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert32_qkv [label="Expert 32 QKV Proj (TP)\nGPUs: 64,65\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert32_attn [label="Expert 32 Attention (TP)\nGPUs: 64,65\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert32_attn_out [label="Expert 32 Attn Output (TP)\nGPUs: 64,65\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert32_mlp1 [label="Expert 32 MLP Layer1 (TP)\nGPUs: 64,65\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert32_mlp_act [label="Expert 32 MLP GELU (TP)\nGPUs: 64,65\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert32_mlp2 [label="Expert 32 MLP Layer2 (TP)\nGPUs: 64,65\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert33_qkv [label="Expert 33 QKV Proj (TP)\nGPUs: 66,67\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert33_attn [label="Expert 33 Attention (TP)\nGPUs: 66,67\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert33_attn_out [label="Expert 33 Attn Output (TP)\nGPUs: 66,67\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert33_mlp1 [label="Expert 33 MLP Layer1 (TP)\nGPUs: 66,67\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert33_mlp_act [label="Expert 33 MLP GELU (TP)\nGPUs: 66,67\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert33_mlp2 [label="Expert 33 MLP Layer2 (TP)\nGPUs: 66,67\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert34_qkv [label="Expert 34 QKV Proj (TP)\nGPUs: 68,69\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert34_attn [label="Expert 34 Attention (TP)\nGPUs: 68,69\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert34_attn_out [label="Expert 34 Attn Output (TP)\nGPUs: 68,69\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert34_mlp1 [label="Expert 34 MLP Layer1 (TP)\nGPUs: 68,69\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert34_mlp_act [label="Expert 34 MLP GELU (TP)\nGPUs: 68,69\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert34_mlp2 [label="Expert 34 MLP Layer2 (TP)\nGPUs: 68,69\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert35_qkv [label="Expert 35 QKV Proj (TP)\nGPUs: 70,71\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert35_attn [label="Expert 35 Attention (TP)\nGPUs: 70,71\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert35_attn_out [label="Expert 35 Attn Output (TP)\nGPUs: 70,71\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert35_mlp1 [label="Expert 35 MLP Layer1 (TP)\nGPUs: 70,71\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert35_mlp_act [label="Expert 35 MLP GELU (TP)\nGPUs: 70,71\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert35_mlp2 [label="Expert 35 MLP Layer2 (TP)\nGPUs: 70,71\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert36_qkv [label="Expert 36 QKV Proj (TP)\nGPUs: 72,73\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert36_attn [label="Expert 36 Attention (TP)\nGPUs: 72,73\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert36_attn_out [label="Expert 36 Attn Output (TP)\nGPUs: 72,73\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert36_mlp1 [label="Expert 36 MLP Layer1 (TP)\nGPUs: 72,73\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert36_mlp_act [label="Expert 36 MLP GELU (TP)\nGPUs: 72,73\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert36_mlp2 [label="Expert 36 MLP Layer2 (TP)\nGPUs: 72,73\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert37_qkv [label="Expert 37 QKV Proj (TP)\nGPUs: 74,75\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert37_attn [label="Expert 37 Attention (TP)\nGPUs: 74,75\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert37_attn_out [label="Expert 37 Attn Output (TP)\nGPUs: 74,75\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert37_mlp1 [label="Expert 37 MLP Layer1 (TP)\nGPUs: 74,75\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert37_mlp_act [label="Expert 37 MLP GELU (TP)\nGPUs: 74,75\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert37_mlp2 [label="Expert 37 MLP Layer2 (TP)\nGPUs: 74,75\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert38_qkv [label="Expert 38 QKV Proj (TP)\nGPUs: 76,77\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert38_attn [label="Expert 38 Attention (TP)\nGPUs: 76,77\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert38_attn_out [label="Expert 38 Attn Output (TP)\nGPUs: 76,77\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert38_mlp1 [label="Expert 38 MLP Layer1 (TP)\nGPUs: 76,77\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert38_mlp_act [label="Expert 38 MLP GELU (TP)\nGPUs: 76,77\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert38_mlp2 [label="Expert 38 MLP Layer2 (TP)\nGPUs: 76,77\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert39_qkv [label="Expert 39 QKV Proj (TP)\nGPUs: 78,79\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert39_attn [label="Expert 39 Attention (TP)\nGPUs: 78,79\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert39_attn_out [label="Expert 39 Attn Output (TP)\nGPUs: 78,79\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert39_mlp1 [label="Expert 39 MLP Layer1 (TP)\nGPUs: 78,79\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert39_mlp_act [label="Expert 39 MLP GELU (TP)\nGPUs: 78,79\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert39_mlp2 [label="Expert 39 MLP Layer2 (TP)\nGPUs: 78,79\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert40_qkv [label="Expert 40 QKV Proj (TP)\nGPUs: 80,81\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert40_attn [label="Expert 40 Attention (TP)\nGPUs: 80,81\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert40_attn_out [label="Expert 40 Attn Output (TP)\nGPUs: 80,81\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert40_mlp1 [label="Expert 40 MLP Layer1 (TP)\nGPUs: 80,81\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert40_mlp_act [label="Expert 40 MLP GELU (TP)\nGPUs: 80,81\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert40_mlp2 [label="Expert 40 MLP Layer2 (TP)\nGPUs: 80,81\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert41_qkv [label="Expert 41 QKV Proj (TP)\nGPUs: 82,83\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert41_attn [label="Expert 41 Attention (TP)\nGPUs: 82,83\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert41_attn_out [label="Expert 41 Attn Output (TP)\nGPUs: 82,83\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert41_mlp1 [label="Expert 41 MLP Layer1 (TP)\nGPUs: 82,83\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert41_mlp_act [label="Expert 41 MLP GELU (TP)\nGPUs: 82,83\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert41_mlp2 [label="Expert 41 MLP Layer2 (TP)\nGPUs: 82,83\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert42_qkv [label="Expert 42 QKV Proj (TP)\nGPUs: 84,85\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert42_attn [label="Expert 42 Attention (TP)\nGPUs: 84,85\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert42_attn_out [label="Expert 42 Attn Output (TP)\nGPUs: 84,85\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert42_mlp1 [label="Expert 42 MLP Layer1 (TP)\nGPUs: 84,85\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert42_mlp_act [label="Expert 42 MLP GELU (TP)\nGPUs: 84,85\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert42_mlp2 [label="Expert 42 MLP Layer2 (TP)\nGPUs: 84,85\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert43_qkv [label="Expert 43 QKV Proj (TP)\nGPUs: 86,87\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert43_attn [label="Expert 43 Attention (TP)\nGPUs: 86,87\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert43_attn_out [label="Expert 43 Attn Output (TP)\nGPUs: 86,87\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert43_mlp1 [label="Expert 43 MLP Layer1 (TP)\nGPUs: 86,87\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert43_mlp_act [label="Expert 43 MLP GELU (TP)\nGPUs: 86,87\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert43_mlp2 [label="Expert 43 MLP Layer2 (TP)\nGPUs: 86,87\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert44_qkv [label="Expert 44 QKV Proj (TP)\nGPUs: 88,89\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert44_attn [label="Expert 44 Attention (TP)\nGPUs: 88,89\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert44_attn_out [label="Expert 44 Attn Output (TP)\nGPUs: 88,89\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert44_mlp1 [label="Expert 44 MLP Layer1 (TP)\nGPUs: 88,89\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert44_mlp_act [label="Expert 44 MLP GELU (TP)\nGPUs: 88,89\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert44_mlp2 [label="Expert 44 MLP Layer2 (TP)\nGPUs: 88,89\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert45_qkv [label="Expert 45 QKV Proj (TP)\nGPUs: 90,91\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert45_attn [label="Expert 45 Attention (TP)\nGPUs: 90,91\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert45_attn_out [label="Expert 45 Attn Output (TP)\nGPUs: 90,91\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert45_mlp1 [label="Expert 45 MLP Layer1 (TP)\nGPUs: 90,91\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert45_mlp_act [label="Expert 45 MLP GELU (TP)\nGPUs: 90,91\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert45_mlp2 [label="Expert 45 MLP Layer2 (TP)\nGPUs: 90,91\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert46_qkv [label="Expert 46 QKV Proj (TP)\nGPUs: 92,93\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert46_attn [label="Expert 46 Attention (TP)\nGPUs: 92,93\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert46_attn_out [label="Expert 46 Attn Output (TP)\nGPUs: 92,93\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert46_mlp1 [label="Expert 46 MLP Layer1 (TP)\nGPUs: 92,93\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert46_mlp_act [label="Expert 46 MLP GELU (TP)\nGPUs: 92,93\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert46_mlp2 [label="Expert 46 MLP Layer2 (TP)\nGPUs: 92,93\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert47_qkv [label="Expert 47 QKV Proj (TP)\nGPUs: 94,95\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert47_attn [label="Expert 47 Attention (TP)\nGPUs: 94,95\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert47_attn_out [label="Expert 47 Attn Output (TP)\nGPUs: 94,95\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert47_mlp1 [label="Expert 47 MLP Layer1 (TP)\nGPUs: 94,95\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert47_mlp_act [label="Expert 47 MLP GELU (TP)\nGPUs: 94,95\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert47_mlp2 [label="Expert 47 MLP Layer2 (TP)\nGPUs: 94,95\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert48_qkv [label="Expert 48 QKV Proj (TP)\nGPUs: 96,97\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert48_attn [label="Expert 48 Attention (TP)\nGPUs: 96,97\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert48_attn_out [label="Expert 48 Attn Output (TP)\nGPUs: 96,97\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert48_mlp1 [label="Expert 48 MLP Layer1 (TP)\nGPUs: 96,97\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert48_mlp_act [label="Expert 48 MLP GELU (TP)\nGPUs: 96,97\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert48_mlp2 [label="Expert 48 MLP Layer2 (TP)\nGPUs: 96,97\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert49_qkv [label="Expert 49 QKV Proj (TP)\nGPUs: 98,99\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert49_attn [label="Expert 49 Attention (TP)\nGPUs: 98,99\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert49_attn_out [label="Expert 49 Attn Output (TP)\nGPUs: 98,99\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert49_mlp1 [label="Expert 49 MLP Layer1 (TP)\nGPUs: 98,99\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert49_mlp_act [label="Expert 49 MLP GELU (TP)\nGPUs: 98,99\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert49_mlp2 [label="Expert 49 MLP Layer2 (TP)\nGPUs: 98,99\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert50_qkv [label="Expert 50 QKV Proj (TP)\nGPUs: 100,101\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert50_attn [label="Expert 50 Attention (TP)\nGPUs: 100,101\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert50_attn_out [label="Expert 50 Attn Output (TP)\nGPUs: 100,101\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert50_mlp1 [label="Expert 50 MLP Layer1 (TP)\nGPUs: 100,101\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert50_mlp_act [label="Expert 50 MLP GELU (TP)\nGPUs: 100,101\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert50_mlp2 [label="Expert 50 MLP Layer2 (TP)\nGPUs: 100,101\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert51_qkv [label="Expert 51 QKV Proj (TP)\nGPUs: 102,103\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert51_attn [label="Expert 51 Attention (TP)\nGPUs: 102,103\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert51_attn_out [label="Expert 51 Attn Output (TP)\nGPUs: 102,103\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert51_mlp1 [label="Expert 51 MLP Layer1 (TP)\nGPUs: 102,103\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert51_mlp_act [label="Expert 51 MLP GELU (TP)\nGPUs: 102,103\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert51_mlp2 [label="Expert 51 MLP Layer2 (TP)\nGPUs: 102,103\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert52_qkv [label="Expert 52 QKV Proj (TP)\nGPUs: 104,105\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert52_attn [label="Expert 52 Attention (TP)\nGPUs: 104,105\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert52_attn_out [label="Expert 52 Attn Output (TP)\nGPUs: 104,105\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert52_mlp1 [label="Expert 52 MLP Layer1 (TP)\nGPUs: 104,105\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert52_mlp_act [label="Expert 52 MLP GELU (TP)\nGPUs: 104,105\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert52_mlp2 [label="Expert 52 MLP Layer2 (TP)\nGPUs: 104,105\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert53_qkv [label="Expert 53 QKV Proj (TP)\nGPUs: 106,107\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert53_attn [label="Expert 53 Attention (TP)\nGPUs: 106,107\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert53_attn_out [label="Expert 53 Attn Output (TP)\nGPUs: 106,107\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert53_mlp1 [label="Expert 53 MLP Layer1 (TP)\nGPUs: 106,107\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert53_mlp_act [label="Expert 53 MLP GELU (TP)\nGPUs: 106,107\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert53_mlp2 [label="Expert 53 MLP Layer2 (TP)\nGPUs: 106,107\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert54_qkv [label="Expert 54 QKV Proj (TP)\nGPUs: 108,109\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert54_attn [label="Expert 54 Attention (TP)\nGPUs: 108,109\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert54_attn_out [label="Expert 54 Attn Output (TP)\nGPUs: 108,109\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert54_mlp1 [label="Expert 54 MLP Layer1 (TP)\nGPUs: 108,109\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert54_mlp_act [label="Expert 54 MLP GELU (TP)\nGPUs: 108,109\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert54_mlp2 [label="Expert 54 MLP Layer2 (TP)\nGPUs: 108,109\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert55_qkv [label="Expert 55 QKV Proj (TP)\nGPUs: 110,111\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert55_attn [label="Expert 55 Attention (TP)\nGPUs: 110,111\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert55_attn_out [label="Expert 55 Attn Output (TP)\nGPUs: 110,111\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert55_mlp1 [label="Expert 55 MLP Layer1 (TP)\nGPUs: 110,111\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert55_mlp_act [label="Expert 55 MLP GELU (TP)\nGPUs: 110,111\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert55_mlp2 [label="Expert 55 MLP Layer2 (TP)\nGPUs: 110,111\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert56_qkv [label="Expert 56 QKV Proj (TP)\nGPUs: 112,113\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert56_attn [label="Expert 56 Attention (TP)\nGPUs: 112,113\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert56_attn_out [label="Expert 56 Attn Output (TP)\nGPUs: 112,113\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert56_mlp1 [label="Expert 56 MLP Layer1 (TP)\nGPUs: 112,113\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert56_mlp_act [label="Expert 56 MLP GELU (TP)\nGPUs: 112,113\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert56_mlp2 [label="Expert 56 MLP Layer2 (TP)\nGPUs: 112,113\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert57_qkv [label="Expert 57 QKV Proj (TP)\nGPUs: 114,115\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert57_attn [label="Expert 57 Attention (TP)\nGPUs: 114,115\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert57_attn_out [label="Expert 57 Attn Output (TP)\nGPUs: 114,115\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert57_mlp1 [label="Expert 57 MLP Layer1 (TP)\nGPUs: 114,115\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert57_mlp_act [label="Expert 57 MLP GELU (TP)\nGPUs: 114,115\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert57_mlp2 [label="Expert 57 MLP Layer2 (TP)\nGPUs: 114,115\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert58_qkv [label="Expert 58 QKV Proj (TP)\nGPUs: 116,117\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert58_attn [label="Expert 58 Attention (TP)\nGPUs: 116,117\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert58_attn_out [label="Expert 58 Attn Output (TP)\nGPUs: 116,117\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert58_mlp1 [label="Expert 58 MLP Layer1 (TP)\nGPUs: 116,117\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert58_mlp_act [label="Expert 58 MLP GELU (TP)\nGPUs: 116,117\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert58_mlp2 [label="Expert 58 MLP Layer2 (TP)\nGPUs: 116,117\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert59_qkv [label="Expert 59 QKV Proj (TP)\nGPUs: 118,119\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert59_attn [label="Expert 59 Attention (TP)\nGPUs: 118,119\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert59_attn_out [label="Expert 59 Attn Output (TP)\nGPUs: 118,119\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert59_mlp1 [label="Expert 59 MLP Layer1 (TP)\nGPUs: 118,119\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert59_mlp_act [label="Expert 59 MLP GELU (TP)\nGPUs: 118,119\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert59_mlp2 [label="Expert 59 MLP Layer2 (TP)\nGPUs: 118,119\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert60_qkv [label="Expert 60 QKV Proj (TP)\nGPUs: 120,121\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert60_attn [label="Expert 60 Attention (TP)\nGPUs: 120,121\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert60_attn_out [label="Expert 60 Attn Output (TP)\nGPUs: 120,121\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert60_mlp1 [label="Expert 60 MLP Layer1 (TP)\nGPUs: 120,121\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert60_mlp_act [label="Expert 60 MLP GELU (TP)\nGPUs: 120,121\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert60_mlp2 [label="Expert 60 MLP Layer2 (TP)\nGPUs: 120,121\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert61_qkv [label="Expert 61 QKV Proj (TP)\nGPUs: 122,123\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert61_attn [label="Expert 61 Attention (TP)\nGPUs: 122,123\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert61_attn_out [label="Expert 61 Attn Output (TP)\nGPUs: 122,123\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert61_mlp1 [label="Expert 61 MLP Layer1 (TP)\nGPUs: 122,123\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert61_mlp_act [label="Expert 61 MLP GELU (TP)\nGPUs: 122,123\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert61_mlp2 [label="Expert 61 MLP Layer2 (TP)\nGPUs: 122,123\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert62_qkv [label="Expert 62 QKV Proj (TP)\nGPUs: 124,125\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert62_attn [label="Expert 62 Attention (TP)\nGPUs: 124,125\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert62_attn_out [label="Expert 62 Attn Output (TP)\nGPUs: 124,125\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert62_mlp1 [label="Expert 62 MLP Layer1 (TP)\nGPUs: 124,125\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert62_mlp_act [label="Expert 62 MLP GELU (TP)\nGPUs: 124,125\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert62_mlp2 [label="Expert 62 MLP Layer2 (TP)\nGPUs: 124,125\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_expert63_qkv [label="Expert 63 QKV Proj (TP)\nGPUs: 126,127\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert63_attn [label="Expert 63 Attention (TP)\nGPUs: 126,127\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage0_expert63_attn_out [label="Expert 63 Attn Output (TP)\nGPUs: 126,127\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage0_expert63_mlp1 [label="Expert 63 MLP Layer1 (TP)\nGPUs: 126,127\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert63_mlp_act [label="Expert 63 MLP GELU (TP)\nGPUs: 126,127\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage0_expert63_mlp2 [label="Expert 63 MLP Layer2 (TP)\nGPUs: 126,127\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage0_aggregate [label="Expert Output Aggregation\nGPUs: 0-127\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage0_layernorm1 [label="LayerNorm 1\nGPUs: 0-127\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
		stage0_layernorm2 [label="LayerNorm 2\nGPUs: 0-127\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
	}
	pipe_comm_0_1 [label="Pipeline Communication\nStage 0  Stage 1\nGPUs: 0-127  128-255\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=orange shape=ellipse]
	subgraph cluster_stage1 {
		bgcolor="#CCFFCC" label="Pipeline Stage 1 (Layers 2-3)\nGPUs: 128-255" style=rounded
		stage1_routing [label="Expert Routing\nAll-to-All Communication\nGPUs: 128-255\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage1_expert0_qkv [label="Expert 0 QKV Proj (TP)\nGPUs: 128,129\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert0_attn [label="Expert 0 Attention (TP)\nGPUs: 128,129\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert0_attn_out [label="Expert 0 Attn Output (TP)\nGPUs: 128,129\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert0_mlp1 [label="Expert 0 MLP Layer1 (TP)\nGPUs: 128,129\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert0_mlp_act [label="Expert 0 MLP GELU (TP)\nGPUs: 128,129\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert0_mlp2 [label="Expert 0 MLP Layer2 (TP)\nGPUs: 128,129\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert1_qkv [label="Expert 1 QKV Proj (TP)\nGPUs: 130,131\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert1_attn [label="Expert 1 Attention (TP)\nGPUs: 130,131\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert1_attn_out [label="Expert 1 Attn Output (TP)\nGPUs: 130,131\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert1_mlp1 [label="Expert 1 MLP Layer1 (TP)\nGPUs: 130,131\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert1_mlp_act [label="Expert 1 MLP GELU (TP)\nGPUs: 130,131\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert1_mlp2 [label="Expert 1 MLP Layer2 (TP)\nGPUs: 130,131\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert2_qkv [label="Expert 2 QKV Proj (TP)\nGPUs: 132,133\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert2_attn [label="Expert 2 Attention (TP)\nGPUs: 132,133\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert2_attn_out [label="Expert 2 Attn Output (TP)\nGPUs: 132,133\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert2_mlp1 [label="Expert 2 MLP Layer1 (TP)\nGPUs: 132,133\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert2_mlp_act [label="Expert 2 MLP GELU (TP)\nGPUs: 132,133\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert2_mlp2 [label="Expert 2 MLP Layer2 (TP)\nGPUs: 132,133\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert3_qkv [label="Expert 3 QKV Proj (TP)\nGPUs: 134,135\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert3_attn [label="Expert 3 Attention (TP)\nGPUs: 134,135\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert3_attn_out [label="Expert 3 Attn Output (TP)\nGPUs: 134,135\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert3_mlp1 [label="Expert 3 MLP Layer1 (TP)\nGPUs: 134,135\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert3_mlp_act [label="Expert 3 MLP GELU (TP)\nGPUs: 134,135\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert3_mlp2 [label="Expert 3 MLP Layer2 (TP)\nGPUs: 134,135\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert4_qkv [label="Expert 4 QKV Proj (TP)\nGPUs: 136,137\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert4_attn [label="Expert 4 Attention (TP)\nGPUs: 136,137\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert4_attn_out [label="Expert 4 Attn Output (TP)\nGPUs: 136,137\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert4_mlp1 [label="Expert 4 MLP Layer1 (TP)\nGPUs: 136,137\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert4_mlp_act [label="Expert 4 MLP GELU (TP)\nGPUs: 136,137\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert4_mlp2 [label="Expert 4 MLP Layer2 (TP)\nGPUs: 136,137\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert5_qkv [label="Expert 5 QKV Proj (TP)\nGPUs: 138,139\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert5_attn [label="Expert 5 Attention (TP)\nGPUs: 138,139\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert5_attn_out [label="Expert 5 Attn Output (TP)\nGPUs: 138,139\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert5_mlp1 [label="Expert 5 MLP Layer1 (TP)\nGPUs: 138,139\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert5_mlp_act [label="Expert 5 MLP GELU (TP)\nGPUs: 138,139\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert5_mlp2 [label="Expert 5 MLP Layer2 (TP)\nGPUs: 138,139\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert6_qkv [label="Expert 6 QKV Proj (TP)\nGPUs: 140,141\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert6_attn [label="Expert 6 Attention (TP)\nGPUs: 140,141\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert6_attn_out [label="Expert 6 Attn Output (TP)\nGPUs: 140,141\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert6_mlp1 [label="Expert 6 MLP Layer1 (TP)\nGPUs: 140,141\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert6_mlp_act [label="Expert 6 MLP GELU (TP)\nGPUs: 140,141\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert6_mlp2 [label="Expert 6 MLP Layer2 (TP)\nGPUs: 140,141\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert7_qkv [label="Expert 7 QKV Proj (TP)\nGPUs: 142,143\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert7_attn [label="Expert 7 Attention (TP)\nGPUs: 142,143\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert7_attn_out [label="Expert 7 Attn Output (TP)\nGPUs: 142,143\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert7_mlp1 [label="Expert 7 MLP Layer1 (TP)\nGPUs: 142,143\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert7_mlp_act [label="Expert 7 MLP GELU (TP)\nGPUs: 142,143\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert7_mlp2 [label="Expert 7 MLP Layer2 (TP)\nGPUs: 142,143\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert8_qkv [label="Expert 8 QKV Proj (TP)\nGPUs: 144,145\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert8_attn [label="Expert 8 Attention (TP)\nGPUs: 144,145\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert8_attn_out [label="Expert 8 Attn Output (TP)\nGPUs: 144,145\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert8_mlp1 [label="Expert 8 MLP Layer1 (TP)\nGPUs: 144,145\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert8_mlp_act [label="Expert 8 MLP GELU (TP)\nGPUs: 144,145\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert8_mlp2 [label="Expert 8 MLP Layer2 (TP)\nGPUs: 144,145\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert9_qkv [label="Expert 9 QKV Proj (TP)\nGPUs: 146,147\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert9_attn [label="Expert 9 Attention (TP)\nGPUs: 146,147\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert9_attn_out [label="Expert 9 Attn Output (TP)\nGPUs: 146,147\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert9_mlp1 [label="Expert 9 MLP Layer1 (TP)\nGPUs: 146,147\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert9_mlp_act [label="Expert 9 MLP GELU (TP)\nGPUs: 146,147\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert9_mlp2 [label="Expert 9 MLP Layer2 (TP)\nGPUs: 146,147\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert10_qkv [label="Expert 10 QKV Proj (TP)\nGPUs: 148,149\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert10_attn [label="Expert 10 Attention (TP)\nGPUs: 148,149\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert10_attn_out [label="Expert 10 Attn Output (TP)\nGPUs: 148,149\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert10_mlp1 [label="Expert 10 MLP Layer1 (TP)\nGPUs: 148,149\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert10_mlp_act [label="Expert 10 MLP GELU (TP)\nGPUs: 148,149\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert10_mlp2 [label="Expert 10 MLP Layer2 (TP)\nGPUs: 148,149\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert11_qkv [label="Expert 11 QKV Proj (TP)\nGPUs: 150,151\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert11_attn [label="Expert 11 Attention (TP)\nGPUs: 150,151\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert11_attn_out [label="Expert 11 Attn Output (TP)\nGPUs: 150,151\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert11_mlp1 [label="Expert 11 MLP Layer1 (TP)\nGPUs: 150,151\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert11_mlp_act [label="Expert 11 MLP GELU (TP)\nGPUs: 150,151\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert11_mlp2 [label="Expert 11 MLP Layer2 (TP)\nGPUs: 150,151\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert12_qkv [label="Expert 12 QKV Proj (TP)\nGPUs: 152,153\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert12_attn [label="Expert 12 Attention (TP)\nGPUs: 152,153\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert12_attn_out [label="Expert 12 Attn Output (TP)\nGPUs: 152,153\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert12_mlp1 [label="Expert 12 MLP Layer1 (TP)\nGPUs: 152,153\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert12_mlp_act [label="Expert 12 MLP GELU (TP)\nGPUs: 152,153\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert12_mlp2 [label="Expert 12 MLP Layer2 (TP)\nGPUs: 152,153\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert13_qkv [label="Expert 13 QKV Proj (TP)\nGPUs: 154,155\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert13_attn [label="Expert 13 Attention (TP)\nGPUs: 154,155\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert13_attn_out [label="Expert 13 Attn Output (TP)\nGPUs: 154,155\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert13_mlp1 [label="Expert 13 MLP Layer1 (TP)\nGPUs: 154,155\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert13_mlp_act [label="Expert 13 MLP GELU (TP)\nGPUs: 154,155\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert13_mlp2 [label="Expert 13 MLP Layer2 (TP)\nGPUs: 154,155\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert14_qkv [label="Expert 14 QKV Proj (TP)\nGPUs: 156,157\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert14_attn [label="Expert 14 Attention (TP)\nGPUs: 156,157\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert14_attn_out [label="Expert 14 Attn Output (TP)\nGPUs: 156,157\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert14_mlp1 [label="Expert 14 MLP Layer1 (TP)\nGPUs: 156,157\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert14_mlp_act [label="Expert 14 MLP GELU (TP)\nGPUs: 156,157\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert14_mlp2 [label="Expert 14 MLP Layer2 (TP)\nGPUs: 156,157\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert15_qkv [label="Expert 15 QKV Proj (TP)\nGPUs: 158,159\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert15_attn [label="Expert 15 Attention (TP)\nGPUs: 158,159\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert15_attn_out [label="Expert 15 Attn Output (TP)\nGPUs: 158,159\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert15_mlp1 [label="Expert 15 MLP Layer1 (TP)\nGPUs: 158,159\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert15_mlp_act [label="Expert 15 MLP GELU (TP)\nGPUs: 158,159\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert15_mlp2 [label="Expert 15 MLP Layer2 (TP)\nGPUs: 158,159\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert16_qkv [label="Expert 16 QKV Proj (TP)\nGPUs: 160,161\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert16_attn [label="Expert 16 Attention (TP)\nGPUs: 160,161\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert16_attn_out [label="Expert 16 Attn Output (TP)\nGPUs: 160,161\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert16_mlp1 [label="Expert 16 MLP Layer1 (TP)\nGPUs: 160,161\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert16_mlp_act [label="Expert 16 MLP GELU (TP)\nGPUs: 160,161\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert16_mlp2 [label="Expert 16 MLP Layer2 (TP)\nGPUs: 160,161\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert17_qkv [label="Expert 17 QKV Proj (TP)\nGPUs: 162,163\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert17_attn [label="Expert 17 Attention (TP)\nGPUs: 162,163\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert17_attn_out [label="Expert 17 Attn Output (TP)\nGPUs: 162,163\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert17_mlp1 [label="Expert 17 MLP Layer1 (TP)\nGPUs: 162,163\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert17_mlp_act [label="Expert 17 MLP GELU (TP)\nGPUs: 162,163\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert17_mlp2 [label="Expert 17 MLP Layer2 (TP)\nGPUs: 162,163\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert18_qkv [label="Expert 18 QKV Proj (TP)\nGPUs: 164,165\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert18_attn [label="Expert 18 Attention (TP)\nGPUs: 164,165\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert18_attn_out [label="Expert 18 Attn Output (TP)\nGPUs: 164,165\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert18_mlp1 [label="Expert 18 MLP Layer1 (TP)\nGPUs: 164,165\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert18_mlp_act [label="Expert 18 MLP GELU (TP)\nGPUs: 164,165\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert18_mlp2 [label="Expert 18 MLP Layer2 (TP)\nGPUs: 164,165\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert19_qkv [label="Expert 19 QKV Proj (TP)\nGPUs: 166,167\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert19_attn [label="Expert 19 Attention (TP)\nGPUs: 166,167\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert19_attn_out [label="Expert 19 Attn Output (TP)\nGPUs: 166,167\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert19_mlp1 [label="Expert 19 MLP Layer1 (TP)\nGPUs: 166,167\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert19_mlp_act [label="Expert 19 MLP GELU (TP)\nGPUs: 166,167\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert19_mlp2 [label="Expert 19 MLP Layer2 (TP)\nGPUs: 166,167\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert20_qkv [label="Expert 20 QKV Proj (TP)\nGPUs: 168,169\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert20_attn [label="Expert 20 Attention (TP)\nGPUs: 168,169\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert20_attn_out [label="Expert 20 Attn Output (TP)\nGPUs: 168,169\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert20_mlp1 [label="Expert 20 MLP Layer1 (TP)\nGPUs: 168,169\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert20_mlp_act [label="Expert 20 MLP GELU (TP)\nGPUs: 168,169\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert20_mlp2 [label="Expert 20 MLP Layer2 (TP)\nGPUs: 168,169\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert21_qkv [label="Expert 21 QKV Proj (TP)\nGPUs: 170,171\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert21_attn [label="Expert 21 Attention (TP)\nGPUs: 170,171\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert21_attn_out [label="Expert 21 Attn Output (TP)\nGPUs: 170,171\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert21_mlp1 [label="Expert 21 MLP Layer1 (TP)\nGPUs: 170,171\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert21_mlp_act [label="Expert 21 MLP GELU (TP)\nGPUs: 170,171\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert21_mlp2 [label="Expert 21 MLP Layer2 (TP)\nGPUs: 170,171\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert22_qkv [label="Expert 22 QKV Proj (TP)\nGPUs: 172,173\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert22_attn [label="Expert 22 Attention (TP)\nGPUs: 172,173\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert22_attn_out [label="Expert 22 Attn Output (TP)\nGPUs: 172,173\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert22_mlp1 [label="Expert 22 MLP Layer1 (TP)\nGPUs: 172,173\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert22_mlp_act [label="Expert 22 MLP GELU (TP)\nGPUs: 172,173\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert22_mlp2 [label="Expert 22 MLP Layer2 (TP)\nGPUs: 172,173\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert23_qkv [label="Expert 23 QKV Proj (TP)\nGPUs: 174,175\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert23_attn [label="Expert 23 Attention (TP)\nGPUs: 174,175\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert23_attn_out [label="Expert 23 Attn Output (TP)\nGPUs: 174,175\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert23_mlp1 [label="Expert 23 MLP Layer1 (TP)\nGPUs: 174,175\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert23_mlp_act [label="Expert 23 MLP GELU (TP)\nGPUs: 174,175\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert23_mlp2 [label="Expert 23 MLP Layer2 (TP)\nGPUs: 174,175\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert24_qkv [label="Expert 24 QKV Proj (TP)\nGPUs: 176,177\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert24_attn [label="Expert 24 Attention (TP)\nGPUs: 176,177\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert24_attn_out [label="Expert 24 Attn Output (TP)\nGPUs: 176,177\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert24_mlp1 [label="Expert 24 MLP Layer1 (TP)\nGPUs: 176,177\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert24_mlp_act [label="Expert 24 MLP GELU (TP)\nGPUs: 176,177\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert24_mlp2 [label="Expert 24 MLP Layer2 (TP)\nGPUs: 176,177\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert25_qkv [label="Expert 25 QKV Proj (TP)\nGPUs: 178,179\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert25_attn [label="Expert 25 Attention (TP)\nGPUs: 178,179\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert25_attn_out [label="Expert 25 Attn Output (TP)\nGPUs: 178,179\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert25_mlp1 [label="Expert 25 MLP Layer1 (TP)\nGPUs: 178,179\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert25_mlp_act [label="Expert 25 MLP GELU (TP)\nGPUs: 178,179\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert25_mlp2 [label="Expert 25 MLP Layer2 (TP)\nGPUs: 178,179\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert26_qkv [label="Expert 26 QKV Proj (TP)\nGPUs: 180,181\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert26_attn [label="Expert 26 Attention (TP)\nGPUs: 180,181\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert26_attn_out [label="Expert 26 Attn Output (TP)\nGPUs: 180,181\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert26_mlp1 [label="Expert 26 MLP Layer1 (TP)\nGPUs: 180,181\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert26_mlp_act [label="Expert 26 MLP GELU (TP)\nGPUs: 180,181\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert26_mlp2 [label="Expert 26 MLP Layer2 (TP)\nGPUs: 180,181\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert27_qkv [label="Expert 27 QKV Proj (TP)\nGPUs: 182,183\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert27_attn [label="Expert 27 Attention (TP)\nGPUs: 182,183\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert27_attn_out [label="Expert 27 Attn Output (TP)\nGPUs: 182,183\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert27_mlp1 [label="Expert 27 MLP Layer1 (TP)\nGPUs: 182,183\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert27_mlp_act [label="Expert 27 MLP GELU (TP)\nGPUs: 182,183\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert27_mlp2 [label="Expert 27 MLP Layer2 (TP)\nGPUs: 182,183\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert28_qkv [label="Expert 28 QKV Proj (TP)\nGPUs: 184,185\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert28_attn [label="Expert 28 Attention (TP)\nGPUs: 184,185\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert28_attn_out [label="Expert 28 Attn Output (TP)\nGPUs: 184,185\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert28_mlp1 [label="Expert 28 MLP Layer1 (TP)\nGPUs: 184,185\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert28_mlp_act [label="Expert 28 MLP GELU (TP)\nGPUs: 184,185\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert28_mlp2 [label="Expert 28 MLP Layer2 (TP)\nGPUs: 184,185\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert29_qkv [label="Expert 29 QKV Proj (TP)\nGPUs: 186,187\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert29_attn [label="Expert 29 Attention (TP)\nGPUs: 186,187\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert29_attn_out [label="Expert 29 Attn Output (TP)\nGPUs: 186,187\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert29_mlp1 [label="Expert 29 MLP Layer1 (TP)\nGPUs: 186,187\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert29_mlp_act [label="Expert 29 MLP GELU (TP)\nGPUs: 186,187\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert29_mlp2 [label="Expert 29 MLP Layer2 (TP)\nGPUs: 186,187\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert30_qkv [label="Expert 30 QKV Proj (TP)\nGPUs: 188,189\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert30_attn [label="Expert 30 Attention (TP)\nGPUs: 188,189\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert30_attn_out [label="Expert 30 Attn Output (TP)\nGPUs: 188,189\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert30_mlp1 [label="Expert 30 MLP Layer1 (TP)\nGPUs: 188,189\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert30_mlp_act [label="Expert 30 MLP GELU (TP)\nGPUs: 188,189\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert30_mlp2 [label="Expert 30 MLP Layer2 (TP)\nGPUs: 188,189\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert31_qkv [label="Expert 31 QKV Proj (TP)\nGPUs: 190,191\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert31_attn [label="Expert 31 Attention (TP)\nGPUs: 190,191\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert31_attn_out [label="Expert 31 Attn Output (TP)\nGPUs: 190,191\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert31_mlp1 [label="Expert 31 MLP Layer1 (TP)\nGPUs: 190,191\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert31_mlp_act [label="Expert 31 MLP GELU (TP)\nGPUs: 190,191\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert31_mlp2 [label="Expert 31 MLP Layer2 (TP)\nGPUs: 190,191\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert32_qkv [label="Expert 32 QKV Proj (TP)\nGPUs: 192,193\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert32_attn [label="Expert 32 Attention (TP)\nGPUs: 192,193\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert32_attn_out [label="Expert 32 Attn Output (TP)\nGPUs: 192,193\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert32_mlp1 [label="Expert 32 MLP Layer1 (TP)\nGPUs: 192,193\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert32_mlp_act [label="Expert 32 MLP GELU (TP)\nGPUs: 192,193\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert32_mlp2 [label="Expert 32 MLP Layer2 (TP)\nGPUs: 192,193\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert33_qkv [label="Expert 33 QKV Proj (TP)\nGPUs: 194,195\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert33_attn [label="Expert 33 Attention (TP)\nGPUs: 194,195\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert33_attn_out [label="Expert 33 Attn Output (TP)\nGPUs: 194,195\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert33_mlp1 [label="Expert 33 MLP Layer1 (TP)\nGPUs: 194,195\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert33_mlp_act [label="Expert 33 MLP GELU (TP)\nGPUs: 194,195\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert33_mlp2 [label="Expert 33 MLP Layer2 (TP)\nGPUs: 194,195\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert34_qkv [label="Expert 34 QKV Proj (TP)\nGPUs: 196,197\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert34_attn [label="Expert 34 Attention (TP)\nGPUs: 196,197\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert34_attn_out [label="Expert 34 Attn Output (TP)\nGPUs: 196,197\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert34_mlp1 [label="Expert 34 MLP Layer1 (TP)\nGPUs: 196,197\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert34_mlp_act [label="Expert 34 MLP GELU (TP)\nGPUs: 196,197\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert34_mlp2 [label="Expert 34 MLP Layer2 (TP)\nGPUs: 196,197\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert35_qkv [label="Expert 35 QKV Proj (TP)\nGPUs: 198,199\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert35_attn [label="Expert 35 Attention (TP)\nGPUs: 198,199\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert35_attn_out [label="Expert 35 Attn Output (TP)\nGPUs: 198,199\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert35_mlp1 [label="Expert 35 MLP Layer1 (TP)\nGPUs: 198,199\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert35_mlp_act [label="Expert 35 MLP GELU (TP)\nGPUs: 198,199\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert35_mlp2 [label="Expert 35 MLP Layer2 (TP)\nGPUs: 198,199\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert36_qkv [label="Expert 36 QKV Proj (TP)\nGPUs: 200,201\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert36_attn [label="Expert 36 Attention (TP)\nGPUs: 200,201\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert36_attn_out [label="Expert 36 Attn Output (TP)\nGPUs: 200,201\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert36_mlp1 [label="Expert 36 MLP Layer1 (TP)\nGPUs: 200,201\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert36_mlp_act [label="Expert 36 MLP GELU (TP)\nGPUs: 200,201\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert36_mlp2 [label="Expert 36 MLP Layer2 (TP)\nGPUs: 200,201\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert37_qkv [label="Expert 37 QKV Proj (TP)\nGPUs: 202,203\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert37_attn [label="Expert 37 Attention (TP)\nGPUs: 202,203\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert37_attn_out [label="Expert 37 Attn Output (TP)\nGPUs: 202,203\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert37_mlp1 [label="Expert 37 MLP Layer1 (TP)\nGPUs: 202,203\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert37_mlp_act [label="Expert 37 MLP GELU (TP)\nGPUs: 202,203\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert37_mlp2 [label="Expert 37 MLP Layer2 (TP)\nGPUs: 202,203\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert38_qkv [label="Expert 38 QKV Proj (TP)\nGPUs: 204,205\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert38_attn [label="Expert 38 Attention (TP)\nGPUs: 204,205\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert38_attn_out [label="Expert 38 Attn Output (TP)\nGPUs: 204,205\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert38_mlp1 [label="Expert 38 MLP Layer1 (TP)\nGPUs: 204,205\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert38_mlp_act [label="Expert 38 MLP GELU (TP)\nGPUs: 204,205\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert38_mlp2 [label="Expert 38 MLP Layer2 (TP)\nGPUs: 204,205\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert39_qkv [label="Expert 39 QKV Proj (TP)\nGPUs: 206,207\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert39_attn [label="Expert 39 Attention (TP)\nGPUs: 206,207\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert39_attn_out [label="Expert 39 Attn Output (TP)\nGPUs: 206,207\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert39_mlp1 [label="Expert 39 MLP Layer1 (TP)\nGPUs: 206,207\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert39_mlp_act [label="Expert 39 MLP GELU (TP)\nGPUs: 206,207\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert39_mlp2 [label="Expert 39 MLP Layer2 (TP)\nGPUs: 206,207\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert40_qkv [label="Expert 40 QKV Proj (TP)\nGPUs: 208,209\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert40_attn [label="Expert 40 Attention (TP)\nGPUs: 208,209\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert40_attn_out [label="Expert 40 Attn Output (TP)\nGPUs: 208,209\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert40_mlp1 [label="Expert 40 MLP Layer1 (TP)\nGPUs: 208,209\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert40_mlp_act [label="Expert 40 MLP GELU (TP)\nGPUs: 208,209\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert40_mlp2 [label="Expert 40 MLP Layer2 (TP)\nGPUs: 208,209\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert41_qkv [label="Expert 41 QKV Proj (TP)\nGPUs: 210,211\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert41_attn [label="Expert 41 Attention (TP)\nGPUs: 210,211\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert41_attn_out [label="Expert 41 Attn Output (TP)\nGPUs: 210,211\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert41_mlp1 [label="Expert 41 MLP Layer1 (TP)\nGPUs: 210,211\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert41_mlp_act [label="Expert 41 MLP GELU (TP)\nGPUs: 210,211\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert41_mlp2 [label="Expert 41 MLP Layer2 (TP)\nGPUs: 210,211\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert42_qkv [label="Expert 42 QKV Proj (TP)\nGPUs: 212,213\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert42_attn [label="Expert 42 Attention (TP)\nGPUs: 212,213\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert42_attn_out [label="Expert 42 Attn Output (TP)\nGPUs: 212,213\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert42_mlp1 [label="Expert 42 MLP Layer1 (TP)\nGPUs: 212,213\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert42_mlp_act [label="Expert 42 MLP GELU (TP)\nGPUs: 212,213\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert42_mlp2 [label="Expert 42 MLP Layer2 (TP)\nGPUs: 212,213\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert43_qkv [label="Expert 43 QKV Proj (TP)\nGPUs: 214,215\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert43_attn [label="Expert 43 Attention (TP)\nGPUs: 214,215\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert43_attn_out [label="Expert 43 Attn Output (TP)\nGPUs: 214,215\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert43_mlp1 [label="Expert 43 MLP Layer1 (TP)\nGPUs: 214,215\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert43_mlp_act [label="Expert 43 MLP GELU (TP)\nGPUs: 214,215\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert43_mlp2 [label="Expert 43 MLP Layer2 (TP)\nGPUs: 214,215\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert44_qkv [label="Expert 44 QKV Proj (TP)\nGPUs: 216,217\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert44_attn [label="Expert 44 Attention (TP)\nGPUs: 216,217\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert44_attn_out [label="Expert 44 Attn Output (TP)\nGPUs: 216,217\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert44_mlp1 [label="Expert 44 MLP Layer1 (TP)\nGPUs: 216,217\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert44_mlp_act [label="Expert 44 MLP GELU (TP)\nGPUs: 216,217\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert44_mlp2 [label="Expert 44 MLP Layer2 (TP)\nGPUs: 216,217\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert45_qkv [label="Expert 45 QKV Proj (TP)\nGPUs: 218,219\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert45_attn [label="Expert 45 Attention (TP)\nGPUs: 218,219\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert45_attn_out [label="Expert 45 Attn Output (TP)\nGPUs: 218,219\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert45_mlp1 [label="Expert 45 MLP Layer1 (TP)\nGPUs: 218,219\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert45_mlp_act [label="Expert 45 MLP GELU (TP)\nGPUs: 218,219\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert45_mlp2 [label="Expert 45 MLP Layer2 (TP)\nGPUs: 218,219\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert46_qkv [label="Expert 46 QKV Proj (TP)\nGPUs: 220,221\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert46_attn [label="Expert 46 Attention (TP)\nGPUs: 220,221\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert46_attn_out [label="Expert 46 Attn Output (TP)\nGPUs: 220,221\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert46_mlp1 [label="Expert 46 MLP Layer1 (TP)\nGPUs: 220,221\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert46_mlp_act [label="Expert 46 MLP GELU (TP)\nGPUs: 220,221\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert46_mlp2 [label="Expert 46 MLP Layer2 (TP)\nGPUs: 220,221\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert47_qkv [label="Expert 47 QKV Proj (TP)\nGPUs: 222,223\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert47_attn [label="Expert 47 Attention (TP)\nGPUs: 222,223\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert47_attn_out [label="Expert 47 Attn Output (TP)\nGPUs: 222,223\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert47_mlp1 [label="Expert 47 MLP Layer1 (TP)\nGPUs: 222,223\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert47_mlp_act [label="Expert 47 MLP GELU (TP)\nGPUs: 222,223\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert47_mlp2 [label="Expert 47 MLP Layer2 (TP)\nGPUs: 222,223\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert48_qkv [label="Expert 48 QKV Proj (TP)\nGPUs: 224,225\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert48_attn [label="Expert 48 Attention (TP)\nGPUs: 224,225\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert48_attn_out [label="Expert 48 Attn Output (TP)\nGPUs: 224,225\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert48_mlp1 [label="Expert 48 MLP Layer1 (TP)\nGPUs: 224,225\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert48_mlp_act [label="Expert 48 MLP GELU (TP)\nGPUs: 224,225\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert48_mlp2 [label="Expert 48 MLP Layer2 (TP)\nGPUs: 224,225\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert49_qkv [label="Expert 49 QKV Proj (TP)\nGPUs: 226,227\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert49_attn [label="Expert 49 Attention (TP)\nGPUs: 226,227\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert49_attn_out [label="Expert 49 Attn Output (TP)\nGPUs: 226,227\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert49_mlp1 [label="Expert 49 MLP Layer1 (TP)\nGPUs: 226,227\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert49_mlp_act [label="Expert 49 MLP GELU (TP)\nGPUs: 226,227\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert49_mlp2 [label="Expert 49 MLP Layer2 (TP)\nGPUs: 226,227\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert50_qkv [label="Expert 50 QKV Proj (TP)\nGPUs: 228,229\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert50_attn [label="Expert 50 Attention (TP)\nGPUs: 228,229\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert50_attn_out [label="Expert 50 Attn Output (TP)\nGPUs: 228,229\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert50_mlp1 [label="Expert 50 MLP Layer1 (TP)\nGPUs: 228,229\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert50_mlp_act [label="Expert 50 MLP GELU (TP)\nGPUs: 228,229\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert50_mlp2 [label="Expert 50 MLP Layer2 (TP)\nGPUs: 228,229\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert51_qkv [label="Expert 51 QKV Proj (TP)\nGPUs: 230,231\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert51_attn [label="Expert 51 Attention (TP)\nGPUs: 230,231\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert51_attn_out [label="Expert 51 Attn Output (TP)\nGPUs: 230,231\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert51_mlp1 [label="Expert 51 MLP Layer1 (TP)\nGPUs: 230,231\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert51_mlp_act [label="Expert 51 MLP GELU (TP)\nGPUs: 230,231\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert51_mlp2 [label="Expert 51 MLP Layer2 (TP)\nGPUs: 230,231\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert52_qkv [label="Expert 52 QKV Proj (TP)\nGPUs: 232,233\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert52_attn [label="Expert 52 Attention (TP)\nGPUs: 232,233\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert52_attn_out [label="Expert 52 Attn Output (TP)\nGPUs: 232,233\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert52_mlp1 [label="Expert 52 MLP Layer1 (TP)\nGPUs: 232,233\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert52_mlp_act [label="Expert 52 MLP GELU (TP)\nGPUs: 232,233\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert52_mlp2 [label="Expert 52 MLP Layer2 (TP)\nGPUs: 232,233\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert53_qkv [label="Expert 53 QKV Proj (TP)\nGPUs: 234,235\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert53_attn [label="Expert 53 Attention (TP)\nGPUs: 234,235\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert53_attn_out [label="Expert 53 Attn Output (TP)\nGPUs: 234,235\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert53_mlp1 [label="Expert 53 MLP Layer1 (TP)\nGPUs: 234,235\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert53_mlp_act [label="Expert 53 MLP GELU (TP)\nGPUs: 234,235\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert53_mlp2 [label="Expert 53 MLP Layer2 (TP)\nGPUs: 234,235\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert54_qkv [label="Expert 54 QKV Proj (TP)\nGPUs: 236,237\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert54_attn [label="Expert 54 Attention (TP)\nGPUs: 236,237\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert54_attn_out [label="Expert 54 Attn Output (TP)\nGPUs: 236,237\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert54_mlp1 [label="Expert 54 MLP Layer1 (TP)\nGPUs: 236,237\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert54_mlp_act [label="Expert 54 MLP GELU (TP)\nGPUs: 236,237\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert54_mlp2 [label="Expert 54 MLP Layer2 (TP)\nGPUs: 236,237\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert55_qkv [label="Expert 55 QKV Proj (TP)\nGPUs: 238,239\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert55_attn [label="Expert 55 Attention (TP)\nGPUs: 238,239\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert55_attn_out [label="Expert 55 Attn Output (TP)\nGPUs: 238,239\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert55_mlp1 [label="Expert 55 MLP Layer1 (TP)\nGPUs: 238,239\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert55_mlp_act [label="Expert 55 MLP GELU (TP)\nGPUs: 238,239\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert55_mlp2 [label="Expert 55 MLP Layer2 (TP)\nGPUs: 238,239\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert56_qkv [label="Expert 56 QKV Proj (TP)\nGPUs: 240,241\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert56_attn [label="Expert 56 Attention (TP)\nGPUs: 240,241\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert56_attn_out [label="Expert 56 Attn Output (TP)\nGPUs: 240,241\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert56_mlp1 [label="Expert 56 MLP Layer1 (TP)\nGPUs: 240,241\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert56_mlp_act [label="Expert 56 MLP GELU (TP)\nGPUs: 240,241\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert56_mlp2 [label="Expert 56 MLP Layer2 (TP)\nGPUs: 240,241\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert57_qkv [label="Expert 57 QKV Proj (TP)\nGPUs: 242,243\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert57_attn [label="Expert 57 Attention (TP)\nGPUs: 242,243\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert57_attn_out [label="Expert 57 Attn Output (TP)\nGPUs: 242,243\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert57_mlp1 [label="Expert 57 MLP Layer1 (TP)\nGPUs: 242,243\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert57_mlp_act [label="Expert 57 MLP GELU (TP)\nGPUs: 242,243\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert57_mlp2 [label="Expert 57 MLP Layer2 (TP)\nGPUs: 242,243\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert58_qkv [label="Expert 58 QKV Proj (TP)\nGPUs: 244,245\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert58_attn [label="Expert 58 Attention (TP)\nGPUs: 244,245\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert58_attn_out [label="Expert 58 Attn Output (TP)\nGPUs: 244,245\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert58_mlp1 [label="Expert 58 MLP Layer1 (TP)\nGPUs: 244,245\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert58_mlp_act [label="Expert 58 MLP GELU (TP)\nGPUs: 244,245\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert58_mlp2 [label="Expert 58 MLP Layer2 (TP)\nGPUs: 244,245\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert59_qkv [label="Expert 59 QKV Proj (TP)\nGPUs: 246,247\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert59_attn [label="Expert 59 Attention (TP)\nGPUs: 246,247\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert59_attn_out [label="Expert 59 Attn Output (TP)\nGPUs: 246,247\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert59_mlp1 [label="Expert 59 MLP Layer1 (TP)\nGPUs: 246,247\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert59_mlp_act [label="Expert 59 MLP GELU (TP)\nGPUs: 246,247\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert59_mlp2 [label="Expert 59 MLP Layer2 (TP)\nGPUs: 246,247\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert60_qkv [label="Expert 60 QKV Proj (TP)\nGPUs: 248,249\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert60_attn [label="Expert 60 Attention (TP)\nGPUs: 248,249\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert60_attn_out [label="Expert 60 Attn Output (TP)\nGPUs: 248,249\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert60_mlp1 [label="Expert 60 MLP Layer1 (TP)\nGPUs: 248,249\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert60_mlp_act [label="Expert 60 MLP GELU (TP)\nGPUs: 248,249\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert60_mlp2 [label="Expert 60 MLP Layer2 (TP)\nGPUs: 248,249\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert61_qkv [label="Expert 61 QKV Proj (TP)\nGPUs: 250,251\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert61_attn [label="Expert 61 Attention (TP)\nGPUs: 250,251\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert61_attn_out [label="Expert 61 Attn Output (TP)\nGPUs: 250,251\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert61_mlp1 [label="Expert 61 MLP Layer1 (TP)\nGPUs: 250,251\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert61_mlp_act [label="Expert 61 MLP GELU (TP)\nGPUs: 250,251\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert61_mlp2 [label="Expert 61 MLP Layer2 (TP)\nGPUs: 250,251\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert62_qkv [label="Expert 62 QKV Proj (TP)\nGPUs: 252,253\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert62_attn [label="Expert 62 Attention (TP)\nGPUs: 252,253\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert62_attn_out [label="Expert 62 Attn Output (TP)\nGPUs: 252,253\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert62_mlp1 [label="Expert 62 MLP Layer1 (TP)\nGPUs: 252,253\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert62_mlp_act [label="Expert 62 MLP GELU (TP)\nGPUs: 252,253\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert62_mlp2 [label="Expert 62 MLP Layer2 (TP)\nGPUs: 252,253\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_expert63_qkv [label="Expert 63 QKV Proj (TP)\nGPUs: 254,255\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert63_attn [label="Expert 63 Attention (TP)\nGPUs: 254,255\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage1_expert63_attn_out [label="Expert 63 Attn Output (TP)\nGPUs: 254,255\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage1_expert63_mlp1 [label="Expert 63 MLP Layer1 (TP)\nGPUs: 254,255\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert63_mlp_act [label="Expert 63 MLP GELU (TP)\nGPUs: 254,255\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage1_expert63_mlp2 [label="Expert 63 MLP Layer2 (TP)\nGPUs: 254,255\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage1_aggregate [label="Expert Output Aggregation\nGPUs: 128-255\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage1_layernorm1 [label="LayerNorm 1\nGPUs: 128-255\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
		stage1_layernorm2 [label="LayerNorm 2\nGPUs: 128-255\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
	}
	subgraph cluster_stage2 {
		bgcolor="#CCCCFF" label="Pipeline Stage 2 (Layers 4-5)\nGPUs: 256-383" style=rounded
		stage2_routing [label="Expert Routing\nAll-to-All Communication\nGPUs: 256-383\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage2_expert0_qkv [label="Expert 0 QKV Proj (TP)\nGPUs: 256,257\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage2_expert0_attn [label="Expert 0 Attention (TP)\nGPUs: 256,257\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage2_expert0_attn_out [label="Expert 0 Attn Output (TP)\nGPUs: 256,257\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage2_expert0_mlp1 [label="Expert 0 MLP Layer1 (TP)\nGPUs: 256,257\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage2_expert0_mlp_act [label="Expert 0 MLP GELU (TP)\nGPUs: 256,257\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage2_expert0_mlp2 [label="Expert 0 MLP Layer2 (TP)\nGPUs: 256,257\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage2_expert1_qkv [label="Expert 1 QKV Proj (TP)\nGPUs: 258,259\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage2_expert1_attn [label="Expert 1 Attention (TP)\nGPUs: 258,259\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage2_expert1_attn_out [label="Expert 1 Attn Output (TP)\nGPUs: 258,259\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage2_expert1_mlp1 [label="Expert 1 MLP Layer1 (TP)\nGPUs: 258,259\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage2_expert1_mlp_act [label="Expert 1 MLP GELU (TP)\nGPUs: 258,259\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage2_expert1_mlp2 [label="Expert 1 MLP Layer2 (TP)\nGPUs: 258,259\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage2_expert2_qkv [label="Expert 2 QKV Proj (TP)\nGPUs: 260,261\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage2_expert2_attn [label="Expert 2 Attention (TP)\nGPUs: 260,261\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage2_expert2_attn_out [label="Expert 2 Attn Output (TP)\nGPUs: 260,261\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage2_expert2_mlp1 [label="Expert 2 MLP Layer1 (TP)\nGPUs: 260,261\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage2_expert2_mlp_act [label="Expert 2 MLP GELU (TP)\nGPUs: 260,261\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage2_expert2_mlp2 [label="Expert 2 MLP Layer2 (TP)\nGPUs: 260,261\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage2_expert3_qkv [label="Expert 3 QKV Proj (TP)\nGPUs: 262,263\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage2_expert3_attn [label="Expert 3 Attention (TP)\nGPUs: 262,263\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage2_expert3_attn_out [label="Expert 3 Attn Output (TP)\nGPUs: 262,263\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage2_expert3_mlp1 [label="Expert 3 MLP Layer1 (TP)\nGPUs: 262,263\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage2_expert3_mlp_act [label="Expert 3 MLP GELU (TP)\nGPUs: 262,263\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage2_expert3_mlp2 [label="Expert 3 MLP Layer2 (TP)\nGPUs: 262,263\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage2_experts_ellipsis [label="... (Experts 4-63)\nSimilar operations on GPUs 264-383" fillcolor=lightgray shape=rectangle style=dashed]
		stage2_aggregate [label="Expert Output Aggregation\nGPUs: 256-383\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage2_layernorm1 [label="LayerNorm 1\nGPUs: 256-383\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
		stage2_layernorm2 [label="LayerNorm 2\nGPUs: 256-383\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
	}
	pipe_comm_2_3 [label="Pipeline Communication\nStage 2  Stage 3\nGPUs: 256-383  384-511\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=orange shape=ellipse]
	subgraph cluster_stage3 {
		bgcolor="#FFFFCC" label="Pipeline Stage 3 (Layers 6-7)\nGPUs: 384-511" style=rounded
		stage3_routing [label="Expert Routing\nAll-to-All Communication\nGPUs: 384-511\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage3_expert0_qkv [label="Expert 0 QKV Proj (TP)\nGPUs: 384,385\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage3_expert0_attn [label="Expert 0 Attention (TP)\nGPUs: 384,385\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage3_expert0_attn_out [label="Expert 0 Attn Output (TP)\nGPUs: 384,385\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage3_expert0_mlp1 [label="Expert 0 MLP Layer1 (TP)\nGPUs: 384,385\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage3_expert0_mlp_act [label="Expert 0 MLP GELU (TP)\nGPUs: 384,385\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage3_expert0_mlp2 [label="Expert 0 MLP Layer2 (TP)\nGPUs: 384,385\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage3_expert1_qkv [label="Expert 1 QKV Proj (TP)\nGPUs: 386,387\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage3_expert1_attn [label="Expert 1 Attention (TP)\nGPUs: 386,387\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage3_expert1_attn_out [label="Expert 1 Attn Output (TP)\nGPUs: 386,387\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage3_expert1_mlp1 [label="Expert 1 MLP Layer1 (TP)\nGPUs: 386,387\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage3_expert1_mlp_act [label="Expert 1 MLP GELU (TP)\nGPUs: 386,387\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage3_expert1_mlp2 [label="Expert 1 MLP Layer2 (TP)\nGPUs: 386,387\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage3_expert2_qkv [label="Expert 2 QKV Proj (TP)\nGPUs: 388,389\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage3_expert2_attn [label="Expert 2 Attention (TP)\nGPUs: 388,389\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage3_expert2_attn_out [label="Expert 2 Attn Output (TP)\nGPUs: 388,389\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage3_expert2_mlp1 [label="Expert 2 MLP Layer1 (TP)\nGPUs: 388,389\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage3_expert2_mlp_act [label="Expert 2 MLP GELU (TP)\nGPUs: 388,389\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage3_expert2_mlp2 [label="Expert 2 MLP Layer2 (TP)\nGPUs: 388,389\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage3_expert3_qkv [label="Expert 3 QKV Proj (TP)\nGPUs: 390,391\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage3_expert3_attn [label="Expert 3 Attention (TP)\nGPUs: 390,391\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage3_expert3_attn_out [label="Expert 3 Attn Output (TP)\nGPUs: 390,391\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage3_expert3_mlp1 [label="Expert 3 MLP Layer1 (TP)\nGPUs: 390,391\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage3_expert3_mlp_act [label="Expert 3 MLP GELU (TP)\nGPUs: 390,391\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage3_expert3_mlp2 [label="Expert 3 MLP Layer2 (TP)\nGPUs: 390,391\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage3_experts_ellipsis [label="... (Experts 4-63)\nSimilar operations on GPUs 392-511" fillcolor=lightgray shape=rectangle style=dashed]
		stage3_aggregate [label="Expert Output Aggregation\nGPUs: 384-511\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage3_layernorm1 [label="LayerNorm 1\nGPUs: 384-511\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
		stage3_layernorm2 [label="LayerNorm 2\nGPUs: 384-511\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
	}
	pipe_comm_3_4 [label="Pipeline Communication\nStage 3  Stage 4\nGPUs: 384-511  512-639\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=orange shape=ellipse]
	subgraph cluster_stage4 {
		bgcolor="#FFCCFF" label="Pipeline Stage 4 (Layers 8-9)\nGPUs: 512-639" style=rounded
		stage4_routing [label="Expert Routing\nAll-to-All Communication\nGPUs: 512-639\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage4_expert0_qkv [label="Expert 0 QKV Proj (TP)\nGPUs: 512,513\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage4_expert0_attn [label="Expert 0 Attention (TP)\nGPUs: 512,513\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage4_expert0_attn_out [label="Expert 0 Attn Output (TP)\nGPUs: 512,513\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage4_expert0_mlp1 [label="Expert 0 MLP Layer1 (TP)\nGPUs: 512,513\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage4_expert0_mlp_act [label="Expert 0 MLP GELU (TP)\nGPUs: 512,513\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage4_expert0_mlp2 [label="Expert 0 MLP Layer2 (TP)\nGPUs: 512,513\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage4_expert1_qkv [label="Expert 1 QKV Proj (TP)\nGPUs: 514,515\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage4_expert1_attn [label="Expert 1 Attention (TP)\nGPUs: 514,515\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage4_expert1_attn_out [label="Expert 1 Attn Output (TP)\nGPUs: 514,515\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage4_expert1_mlp1 [label="Expert 1 MLP Layer1 (TP)\nGPUs: 514,515\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage4_expert1_mlp_act [label="Expert 1 MLP GELU (TP)\nGPUs: 514,515\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage4_expert1_mlp2 [label="Expert 1 MLP Layer2 (TP)\nGPUs: 514,515\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage4_expert2_qkv [label="Expert 2 QKV Proj (TP)\nGPUs: 516,517\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage4_expert2_attn [label="Expert 2 Attention (TP)\nGPUs: 516,517\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage4_expert2_attn_out [label="Expert 2 Attn Output (TP)\nGPUs: 516,517\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage4_expert2_mlp1 [label="Expert 2 MLP Layer1 (TP)\nGPUs: 516,517\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage4_expert2_mlp_act [label="Expert 2 MLP GELU (TP)\nGPUs: 516,517\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage4_expert2_mlp2 [label="Expert 2 MLP Layer2 (TP)\nGPUs: 516,517\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage4_expert3_qkv [label="Expert 3 QKV Proj (TP)\nGPUs: 518,519\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage4_expert3_attn [label="Expert 3 Attention (TP)\nGPUs: 518,519\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage4_expert3_attn_out [label="Expert 3 Attn Output (TP)\nGPUs: 518,519\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage4_expert3_mlp1 [label="Expert 3 MLP Layer1 (TP)\nGPUs: 518,519\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage4_expert3_mlp_act [label="Expert 3 MLP GELU (TP)\nGPUs: 518,519\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage4_expert3_mlp2 [label="Expert 3 MLP Layer2 (TP)\nGPUs: 518,519\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage4_experts_ellipsis [label="... (Experts 4-63)\nSimilar operations on GPUs 520-639" fillcolor=lightgray shape=rectangle style=dashed]
		stage4_aggregate [label="Expert Output Aggregation\nGPUs: 512-639\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage4_layernorm1 [label="LayerNorm 1\nGPUs: 512-639\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
		stage4_layernorm2 [label="LayerNorm 2\nGPUs: 512-639\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
	}
	pipe_comm_4_5 [label="Pipeline Communication\nStage 4  Stage 5\nGPUs: 512-639  640-767\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=orange shape=ellipse]
	subgraph cluster_stage5 {
		bgcolor="#CCFFFF" label="Pipeline Stage 5 (Layers 10-11)\nGPUs: 640-767" style=rounded
		stage5_routing [label="Expert Routing\nAll-to-All Communication\nGPUs: 640-767\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage5_expert0_qkv [label="Expert 0 QKV Proj (TP)\nGPUs: 640,641\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage5_expert0_attn [label="Expert 0 Attention (TP)\nGPUs: 640,641\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage5_expert0_attn_out [label="Expert 0 Attn Output (TP)\nGPUs: 640,641\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage5_expert0_mlp1 [label="Expert 0 MLP Layer1 (TP)\nGPUs: 640,641\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage5_expert0_mlp_act [label="Expert 0 MLP GELU (TP)\nGPUs: 640,641\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage5_expert0_mlp2 [label="Expert 0 MLP Layer2 (TP)\nGPUs: 640,641\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage5_expert1_qkv [label="Expert 1 QKV Proj (TP)\nGPUs: 642,643\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage5_expert1_attn [label="Expert 1 Attention (TP)\nGPUs: 642,643\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage5_expert1_attn_out [label="Expert 1 Attn Output (TP)\nGPUs: 642,643\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage5_expert1_mlp1 [label="Expert 1 MLP Layer1 (TP)\nGPUs: 642,643\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage5_expert1_mlp_act [label="Expert 1 MLP GELU (TP)\nGPUs: 642,643\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage5_expert1_mlp2 [label="Expert 1 MLP Layer2 (TP)\nGPUs: 642,643\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage5_expert2_qkv [label="Expert 2 QKV Proj (TP)\nGPUs: 644,645\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage5_expert2_attn [label="Expert 2 Attention (TP)\nGPUs: 644,645\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage5_expert2_attn_out [label="Expert 2 Attn Output (TP)\nGPUs: 644,645\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage5_expert2_mlp1 [label="Expert 2 MLP Layer1 (TP)\nGPUs: 644,645\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage5_expert2_mlp_act [label="Expert 2 MLP GELU (TP)\nGPUs: 644,645\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage5_expert2_mlp2 [label="Expert 2 MLP Layer2 (TP)\nGPUs: 644,645\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage5_expert3_qkv [label="Expert 3 QKV Proj (TP)\nGPUs: 646,647\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage5_expert3_attn [label="Expert 3 Attention (TP)\nGPUs: 646,647\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage5_expert3_attn_out [label="Expert 3 Attn Output (TP)\nGPUs: 646,647\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage5_expert3_mlp1 [label="Expert 3 MLP Layer1 (TP)\nGPUs: 646,647\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage5_expert3_mlp_act [label="Expert 3 MLP GELU (TP)\nGPUs: 646,647\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage5_expert3_mlp2 [label="Expert 3 MLP Layer2 (TP)\nGPUs: 646,647\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage5_experts_ellipsis [label="... (Experts 4-63)\nSimilar operations on GPUs 648-767" fillcolor=lightgray shape=rectangle style=dashed]
		stage5_aggregate [label="Expert Output Aggregation\nGPUs: 640-767\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage5_layernorm1 [label="LayerNorm 1\nGPUs: 640-767\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
		stage5_layernorm2 [label="LayerNorm 2\nGPUs: 640-767\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
	}
	pipe_comm_5_6 [label="Pipeline Communication\nStage 5  Stage 6\nGPUs: 640-767  768-895\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=orange shape=ellipse]
	subgraph cluster_stage6 {
		bgcolor="#FFEECC" label="Pipeline Stage 6 (Layers 12-13)\nGPUs: 768-895" style=rounded
		stage6_routing [label="Expert Routing\nAll-to-All Communication\nGPUs: 768-895\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage6_expert0_qkv [label="Expert 0 QKV Proj (TP)\nGPUs: 768,769\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage6_expert0_attn [label="Expert 0 Attention (TP)\nGPUs: 768,769\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage6_expert0_attn_out [label="Expert 0 Attn Output (TP)\nGPUs: 768,769\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage6_expert0_mlp1 [label="Expert 0 MLP Layer1 (TP)\nGPUs: 768,769\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage6_expert0_mlp_act [label="Expert 0 MLP GELU (TP)\nGPUs: 768,769\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage6_expert0_mlp2 [label="Expert 0 MLP Layer2 (TP)\nGPUs: 768,769\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage6_expert1_qkv [label="Expert 1 QKV Proj (TP)\nGPUs: 770,771\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage6_expert1_attn [label="Expert 1 Attention (TP)\nGPUs: 770,771\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage6_expert1_attn_out [label="Expert 1 Attn Output (TP)\nGPUs: 770,771\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage6_expert1_mlp1 [label="Expert 1 MLP Layer1 (TP)\nGPUs: 770,771\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage6_expert1_mlp_act [label="Expert 1 MLP GELU (TP)\nGPUs: 770,771\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage6_expert1_mlp2 [label="Expert 1 MLP Layer2 (TP)\nGPUs: 770,771\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage6_expert2_qkv [label="Expert 2 QKV Proj (TP)\nGPUs: 772,773\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage6_expert2_attn [label="Expert 2 Attention (TP)\nGPUs: 772,773\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage6_expert2_attn_out [label="Expert 2 Attn Output (TP)\nGPUs: 772,773\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage6_expert2_mlp1 [label="Expert 2 MLP Layer1 (TP)\nGPUs: 772,773\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage6_expert2_mlp_act [label="Expert 2 MLP GELU (TP)\nGPUs: 772,773\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage6_expert2_mlp2 [label="Expert 2 MLP Layer2 (TP)\nGPUs: 772,773\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage6_expert3_qkv [label="Expert 3 QKV Proj (TP)\nGPUs: 774,775\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage6_expert3_attn [label="Expert 3 Attention (TP)\nGPUs: 774,775\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage6_expert3_attn_out [label="Expert 3 Attn Output (TP)\nGPUs: 774,775\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage6_expert3_mlp1 [label="Expert 3 MLP Layer1 (TP)\nGPUs: 774,775\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage6_expert3_mlp_act [label="Expert 3 MLP GELU (TP)\nGPUs: 774,775\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage6_expert3_mlp2 [label="Expert 3 MLP Layer2 (TP)\nGPUs: 774,775\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage6_experts_ellipsis [label="... (Experts 4-63)\nSimilar operations on GPUs 776-895" fillcolor=lightgray shape=rectangle style=dashed]
		stage6_aggregate [label="Expert Output Aggregation\nGPUs: 768-895\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage6_layernorm1 [label="LayerNorm 1\nGPUs: 768-895\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
		stage6_layernorm2 [label="LayerNorm 2\nGPUs: 768-895\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
	}
	pipe_comm_6_7 [label="Pipeline Communication\nStage 6  Stage 7\nGPUs: 768-895  896-1023\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=orange shape=ellipse]
	subgraph cluster_stage7 {
		bgcolor="#EECCFF" label="Pipeline Stage 7 (Layers 14-15)\nGPUs: 896-1023" style=rounded
		stage7_routing [label="Expert Routing\nAll-to-All Communication\nGPUs: 896-1023\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage7_expert0_qkv [label="Expert 0 QKV Proj (TP)\nGPUs: 896,897\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage7_expert0_attn [label="Expert 0 Attention (TP)\nGPUs: 896,897\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage7_expert0_attn_out [label="Expert 0 Attn Output (TP)\nGPUs: 896,897\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage7_expert0_mlp1 [label="Expert 0 MLP Layer1 (TP)\nGPUs: 896,897\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage7_expert0_mlp_act [label="Expert 0 MLP GELU (TP)\nGPUs: 896,897\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage7_expert0_mlp2 [label="Expert 0 MLP Layer2 (TP)\nGPUs: 896,897\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage7_expert1_qkv [label="Expert 1 QKV Proj (TP)\nGPUs: 898,899\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage7_expert1_attn [label="Expert 1 Attention (TP)\nGPUs: 898,899\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage7_expert1_attn_out [label="Expert 1 Attn Output (TP)\nGPUs: 898,899\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage7_expert1_mlp1 [label="Expert 1 MLP Layer1 (TP)\nGPUs: 898,899\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage7_expert1_mlp_act [label="Expert 1 MLP GELU (TP)\nGPUs: 898,899\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage7_expert1_mlp2 [label="Expert 1 MLP Layer2 (TP)\nGPUs: 898,899\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage7_expert2_qkv [label="Expert 2 QKV Proj (TP)\nGPUs: 900,901\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage7_expert2_attn [label="Expert 2 Attention (TP)\nGPUs: 900,901\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage7_expert2_attn_out [label="Expert 2 Attn Output (TP)\nGPUs: 900,901\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage7_expert2_mlp1 [label="Expert 2 MLP Layer1 (TP)\nGPUs: 900,901\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage7_expert2_mlp_act [label="Expert 2 MLP GELU (TP)\nGPUs: 900,901\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage7_expert2_mlp2 [label="Expert 2 MLP Layer2 (TP)\nGPUs: 900,901\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage7_expert3_qkv [label="Expert 3 QKV Proj (TP)\nGPUs: 902,903\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage7_expert3_attn [label="Expert 3 Attention (TP)\nGPUs: 902,903\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
		stage7_expert3_attn_out [label="Expert 3 Attn Output (TP)\nGPUs: 902,903\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightgreen shape=rectangle]
		stage7_expert3_mlp1 [label="Expert 3 MLP Layer1 (TP)\nGPUs: 902,903\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage7_expert3_mlp_act [label="Expert 3 MLP GELU (TP)\nGPUs: 902,903\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=16384]" fillcolor=lightcoral shape=rectangle]
		stage7_expert3_mlp2 [label="Expert 3 MLP Layer2 (TP)\nGPUs: 902,903\nInput: [batch_size=128, seq_len=1024, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightcoral shape=rectangle]
		stage7_experts_ellipsis [label="... (Experts 4-63)\nSimilar operations on GPUs 904-1023" fillcolor=lightgray shape=rectangle style=dashed]
		stage7_aggregate [label="Expert Output Aggregation\nGPUs: 896-1023\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightyellow shape=parallelogram]
		stage7_layernorm1 [label="LayerNorm 1\nGPUs: 896-1023\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
		stage7_layernorm2 [label="LayerNorm 2\nGPUs: 896-1023\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, hidden=4096]" fillcolor=lightblue shape=rectangle]
	}
	subgraph cluster_output {
		bgcolor="#F0F0F0" label="Output Layer" style=rounded
		output [label="Output Logits\nInput: [batch_size=128, seq_len=1024, hidden=4096]\nOutput: [batch_size=128, seq_len=1024, vocab_size=32000]" fillcolor=lightblue shape=ellipse]
	}
	input -> stage0_routing
	stage0_routing -> stage0_expert0_qkv
	stage0_expert0_qkv -> stage0_expert0_attn
	stage0_expert0_attn -> stage0_expert0_attn_out
	stage0_expert0_attn_out -> stage0_layernorm1
	stage0_layernorm1 -> stage0_expert0_mlp1
	stage0_expert0_mlp1 -> stage0_expert0_mlp_act
	stage0_expert0_mlp_act -> stage0_expert0_mlp2
	stage0_expert0_mlp2 -> stage0_layernorm2
	stage0_layernorm2 -> stage0_aggregate
	stage0_aggregate -> pipe_comm_0_1
	pipe_comm_0_1 -> stage1_routing
	stage1_routing -> stage1_expert0_qkv
	stage1_expert0_qkv -> stage1_expert0_attn
	stage1_expert0_attn -> stage1_expert0_attn_out
	stage1_expert0_attn_out -> stage1_layernorm1
	stage1_layernorm1 -> stage1_expert0_mlp1
	stage1_expert0_mlp1 -> stage1_expert0_mlp_act
	stage1_expert0_mlp_act -> stage1_expert0_mlp2
	stage1_expert0_mlp2 -> stage1_layernorm2
	stage1_layernorm2 -> stage1_aggregate
	stage1_aggregate -> pipe_comm_1_2
	pipe_comm_1_2 -> stage2_routing
	stage2_routing -> stage2_expert0_qkv
	stage2_expert0_qkv -> stage2_expert0_attn
	stage2_expert0_attn -> stage2_expert0_attn_out
	stage2_expert0_attn_out -> stage2_layernorm1
	stage2_layernorm1 -> stage2_expert0_mlp1
	stage2_expert0_mlp1 -> stage2_expert0_mlp_act
	stage2_expert0_mlp_act -> stage2_expert0_mlp2
	stage2_expert0_mlp2 -> stage2_layernorm2
	stage2_layernorm2 -> stage2_aggregate
	stage2_layernorm2 -> pipe_comm_2_3
	pipe_comm_2_3 -> stage3_routing
	stage3_routing -> stage3_expert0_qkv
	stage3_expert0_qkv -> stage3_expert0_attn
	stage3_expert0_attn -> stage3_expert0_attn_out
	stage3_expert0_attn_out -> stage3_layernorm1
	stage3_layernorm1 -> stage3_expert0_mlp1
	stage3_expert0_mlp1 -> stage3_expert0_mlp_act
	stage3_expert0_mlp_act -> stage3_expert0_mlp2
	stage3_expert0_mlp2 -> stage3_layernorm2
	stage3_layernorm2 -> stage3_aggregate
	stage3_layernorm2 -> pipe_comm_3_4
	pipe_comm_3_4 -> stage4_routing
	stage4_routing -> stage4_expert0_qkv
	stage4_expert0_qkv -> stage4_expert0_attn
	stage4_expert0_attn -> stage4_expert0_attn_out
	stage4_expert0_attn_out -> stage4_layernorm1
	stage4_layernorm1 -> stage4_expert0_mlp1
	stage4_expert0_mlp1 -> stage4_expert0_mlp_act
	stage4_expert0_mlp_act -> stage4_expert0_mlp2
	stage4_expert0_mlp2 -> stage4_layernorm2
	stage4_layernorm2 -> stage4_aggregate
	stage4_layernorm2 -> pipe_comm_4_5
	pipe_comm_4_5 -> stage5_routing
	stage5_routing -> stage5_expert0_qkv
	stage5_expert0_qkv -> stage5_expert0_attn
	stage5_expert0_attn -> stage5_expert0_attn_out
	stage5_expert0_attn_out -> stage5_layernorm1
	stage5_layernorm1 -> stage5_expert0_mlp1
	stage5_expert0_mlp1 -> stage5_expert0_mlp_act
	stage5_expert0_mlp_act -> stage5_expert0_mlp2
	stage5_expert0_mlp2 -> stage5_layernorm2
	stage5_layernorm2 -> stage5_aggregate
	stage5_layernorm2 -> pipe_comm_5_6
	pipe_comm_5_6 -> stage6_routing
	stage6_routing -> stage6_expert0_qkv
	stage6_expert0_qkv -> stage6_expert0_attn
	stage6_expert0_attn -> stage6_expert0_attn_out
	stage6_expert0_attn_out -> stage6_layernorm1
	stage6_layernorm1 -> stage6_expert0_mlp1
	stage6_expert0_mlp1 -> stage6_expert0_mlp_act
	stage6_expert0_mlp_act -> stage6_expert0_mlp2
	stage6_expert0_mlp2 -> stage6_layernorm2
	stage6_layernorm2 -> stage6_aggregate
	stage6_layernorm2 -> pipe_comm_6_7
	pipe_comm_6_7 -> stage7_routing
	stage7_routing -> stage7_expert0_qkv
	stage7_expert0_qkv -> stage7_expert0_attn
	stage7_expert0_attn -> stage7_expert0_attn_out
	stage7_expert0_attn_out -> stage7_layernorm1
	stage7_layernorm1 -> stage7_expert0_mlp1
	stage7_expert0_mlp1 -> stage7_expert0_mlp_act
	stage7_expert0_mlp_act -> stage7_expert0_mlp2
	stage7_expert0_mlp2 -> stage7_layernorm2
	stage7_layernorm2 -> stage7_aggregate
	stage7_layernorm2 -> output
}
