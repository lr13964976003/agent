// Complete MoE 30B Model Deployment DAG
digraph {
	bgcolor=white fontname=Arial rankdir=TB
	input [label="Input\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightblue shape=ellipse]
	stage0_input [label="Stage 0 Input Router\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	input -> stage0_input
	stage0_layer0_ln [label="Layer 0 LayerNorm\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage0_input -> stage0_layer0_ln
	stage0_layer0_attn_qkv [label="Layer 0 Attention QKV Projection\nGPU: Pipeline 0 (TP: GPU 0-127)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage0_layer0_ln -> stage0_layer0_attn_qkv
	stage0_layer0_tp_qkv_comm [label="Layer 0 TP QKV All-Gather\nGPU: Pipeline 0 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage0_layer0_attn_qkv -> stage0_layer0_tp_qkv_comm
	stage0_layer0_attn_head0 [label="Layer 0 Head 0 Attention\nGPU: Pipeline 0 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head0
	stage0_layer0_attn_head1 [label="Layer 0 Head 1 Attention\nGPU: Pipeline 0 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head1
	stage0_layer0_attn_head2 [label="Layer 0 Head 2 Attention\nGPU: Pipeline 0 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head2
	stage0_layer0_attn_head3 [label="Layer 0 Head 3 Attention\nGPU: Pipeline 0 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head3
	stage0_layer0_attn_head4 [label="Layer 0 Head 4 Attention\nGPU: Pipeline 0 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head4
	stage0_layer0_attn_head5 [label="Layer 0 Head 5 Attention\nGPU: Pipeline 0 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head5
	stage0_layer0_attn_head6 [label="Layer 0 Head 6 Attention\nGPU: Pipeline 0 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head6
	stage0_layer0_attn_head7 [label="Layer 0 Head 7 Attention\nGPU: Pipeline 0 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head7
	stage0_layer0_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_ellipsis
	stage0_layer0_attn_head24 [label="Layer 0 Head 24 Attention\nGPU: Pipeline 0 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head24
	stage0_layer0_attn_head25 [label="Layer 0 Head 25 Attention\nGPU: Pipeline 0 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head25
	stage0_layer0_attn_head26 [label="Layer 0 Head 26 Attention\nGPU: Pipeline 0 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head26
	stage0_layer0_attn_head27 [label="Layer 0 Head 27 Attention\nGPU: Pipeline 0 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head27
	stage0_layer0_attn_head28 [label="Layer 0 Head 28 Attention\nGPU: Pipeline 0 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head28
	stage0_layer0_attn_head29 [label="Layer 0 Head 29 Attention\nGPU: Pipeline 0 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head29
	stage0_layer0_attn_head30 [label="Layer 0 Head 30 Attention\nGPU: Pipeline 0 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head30
	stage0_layer0_attn_head31 [label="Layer 0 Head 31 Attention\nGPU: Pipeline 0 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_tp_qkv_comm -> stage0_layer0_attn_head31
	stage0_layer0_attn_agg [label="Layer 0 Attention Aggregate\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage0_layer0_attn_head0 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head1 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head2 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head3 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head4 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head5 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head6 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head7 -> stage0_layer0_attn_agg
	stage0_layer0_attn_ellipsis -> stage0_layer0_attn_agg
	stage0_layer0_attn_head24 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head25 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head26 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head27 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head28 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head29 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head30 -> stage0_layer0_attn_agg
	stage0_layer0_attn_head31 -> stage0_layer0_attn_agg
	stage0_layer0_attn_out [label="Layer 0 Attention Output Proj\nGPU: Pipeline 0 (TP: GPU 0-127)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage0_layer0_attn_agg -> stage0_layer0_attn_out
	stage0_layer0_tp_attn_comm [label="Layer 0 TP Attention All-Reduce\nGPU: Pipeline 0 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage0_layer0_attn_out -> stage0_layer0_tp_attn_comm
	stage0_layer0_expert_router [label="Layer 0 Expert Router (Top-k)\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage0_layer0_tp_attn_comm -> stage0_layer0_expert_router
	stage0_layer0_expert0 [label="Layer 0 Expert 0\nGPU: 0-1 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert_router -> stage0_layer0_expert0 [style=dashed]
	stage0_layer0_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 0-1 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert0 -> stage0_layer0_expert0_mlp1
	stage0_layer0_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 0-1\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert0_mlp1 -> stage0_layer0_expert0_tp1
	stage0_layer0_expert0_act [label="Expert 0 GELU Activation\nGPU: 0-1\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert0_tp1 -> stage0_layer0_expert0_act
	stage0_layer0_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 0-1 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert0_act -> stage0_layer0_expert0_mlp2
	stage0_layer0_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 0-1\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert0_mlp2 -> stage0_layer0_expert0_tp2
	stage0_layer0_expert1 [label="Layer 0 Expert 1\nGPU: 2-3 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert_router -> stage0_layer0_expert1 [style=dashed]
	stage0_layer0_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 2-3 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert1 -> stage0_layer0_expert1_mlp1
	stage0_layer0_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 2-3\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert1_mlp1 -> stage0_layer0_expert1_tp1
	stage0_layer0_expert1_act [label="Expert 1 GELU Activation\nGPU: 2-3\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert1_tp1 -> stage0_layer0_expert1_act
	stage0_layer0_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 2-3 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert1_act -> stage0_layer0_expert1_mlp2
	stage0_layer0_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 2-3\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert1_mlp2 -> stage0_layer0_expert1_tp2
	stage0_layer0_expert2 [label="Layer 0 Expert 2\nGPU: 4-5 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert_router -> stage0_layer0_expert2 [style=dashed]
	stage0_layer0_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 4-5 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert2 -> stage0_layer0_expert2_mlp1
	stage0_layer0_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 4-5\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert2_mlp1 -> stage0_layer0_expert2_tp1
	stage0_layer0_expert2_act [label="Expert 2 GELU Activation\nGPU: 4-5\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert2_tp1 -> stage0_layer0_expert2_act
	stage0_layer0_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 4-5 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert2_act -> stage0_layer0_expert2_mlp2
	stage0_layer0_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 4-5\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert2_mlp2 -> stage0_layer0_expert2_tp2
	stage0_layer0_expert3 [label="Layer 0 Expert 3\nGPU: 6-7 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert_router -> stage0_layer0_expert3 [style=dashed]
	stage0_layer0_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 6-7 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert3 -> stage0_layer0_expert3_mlp1
	stage0_layer0_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 6-7\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert3_mlp1 -> stage0_layer0_expert3_tp1
	stage0_layer0_expert3_act [label="Expert 3 GELU Activation\nGPU: 6-7\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert3_tp1 -> stage0_layer0_expert3_act
	stage0_layer0_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 6-7 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert3_act -> stage0_layer0_expert3_mlp2
	stage0_layer0_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 6-7\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert3_mlp2 -> stage0_layer0_expert3_tp2
	stage0_layer0_expert31 [label="Layer 0 Expert 31\nGPU: 62-63 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert_router -> stage0_layer0_expert31 [style=dashed]
	stage0_layer0_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 62-63 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert31 -> stage0_layer0_expert31_mlp1
	stage0_layer0_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 62-63\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert31_mlp1 -> stage0_layer0_expert31_tp1
	stage0_layer0_expert31_act [label="Expert 31 GELU Activation\nGPU: 62-63\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert31_tp1 -> stage0_layer0_expert31_act
	stage0_layer0_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 62-63 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert31_act -> stage0_layer0_expert31_mlp2
	stage0_layer0_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 62-63\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert31_mlp2 -> stage0_layer0_expert31_tp2
	stage0_layer0_expert32 [label="Layer 0 Expert 32\nGPU: 64-65 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert_router -> stage0_layer0_expert32 [style=dashed]
	stage0_layer0_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 64-65 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert32 -> stage0_layer0_expert32_mlp1
	stage0_layer0_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 64-65\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert32_mlp1 -> stage0_layer0_expert32_tp1
	stage0_layer0_expert32_act [label="Expert 32 GELU Activation\nGPU: 64-65\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert32_tp1 -> stage0_layer0_expert32_act
	stage0_layer0_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 64-65 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert32_act -> stage0_layer0_expert32_mlp2
	stage0_layer0_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 64-65\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert32_mlp2 -> stage0_layer0_expert32_tp2
	stage0_layer0_expert60 [label="Layer 0 Expert 60\nGPU: 120-121 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert_router -> stage0_layer0_expert60 [style=dashed]
	stage0_layer0_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 120-121 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert60 -> stage0_layer0_expert60_mlp1
	stage0_layer0_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 120-121\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert60_mlp1 -> stage0_layer0_expert60_tp1
	stage0_layer0_expert60_act [label="Expert 60 GELU Activation\nGPU: 120-121\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert60_tp1 -> stage0_layer0_expert60_act
	stage0_layer0_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 120-121 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert60_act -> stage0_layer0_expert60_mlp2
	stage0_layer0_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 120-121\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert60_mlp2 -> stage0_layer0_expert60_tp2
	stage0_layer0_expert61 [label="Layer 0 Expert 61\nGPU: 122-123 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert_router -> stage0_layer0_expert61 [style=dashed]
	stage0_layer0_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 122-123 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert61 -> stage0_layer0_expert61_mlp1
	stage0_layer0_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 122-123\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert61_mlp1 -> stage0_layer0_expert61_tp1
	stage0_layer0_expert61_act [label="Expert 61 GELU Activation\nGPU: 122-123\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert61_tp1 -> stage0_layer0_expert61_act
	stage0_layer0_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 122-123 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert61_act -> stage0_layer0_expert61_mlp2
	stage0_layer0_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 122-123\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert61_mlp2 -> stage0_layer0_expert61_tp2
	stage0_layer0_expert62 [label="Layer 0 Expert 62\nGPU: 124-125 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert_router -> stage0_layer0_expert62 [style=dashed]
	stage0_layer0_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 124-125 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert62 -> stage0_layer0_expert62_mlp1
	stage0_layer0_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 124-125\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert62_mlp1 -> stage0_layer0_expert62_tp1
	stage0_layer0_expert62_act [label="Expert 62 GELU Activation\nGPU: 124-125\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert62_tp1 -> stage0_layer0_expert62_act
	stage0_layer0_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 124-125 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert62_act -> stage0_layer0_expert62_mlp2
	stage0_layer0_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 124-125\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert62_mlp2 -> stage0_layer0_expert62_tp2
	stage0_layer0_expert63 [label="Layer 0 Expert 63\nGPU: 126-127 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert_router -> stage0_layer0_expert63 [style=dashed]
	stage0_layer0_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 126-127 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert63 -> stage0_layer0_expert63_mlp1
	stage0_layer0_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 126-127\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert63_mlp1 -> stage0_layer0_expert63_tp1
	stage0_layer0_expert63_act [label="Expert 63 GELU Activation\nGPU: 126-127\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert63_tp1 -> stage0_layer0_expert63_act
	stage0_layer0_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 126-127 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer0_expert63_act -> stage0_layer0_expert63_mlp2
	stage0_layer0_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 126-127\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer0_expert63_mlp2 -> stage0_layer0_expert63_tp2
	stage0_layer0_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 0 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage0_layer0_expert_router -> stage0_layer0_expert_ellipsis [style=dashed]
	stage0_layer0_expert_agg [label="Layer 0 Expert Aggregation\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage0_layer0_expert0_tp2 -> stage0_layer0_expert_agg
	stage0_layer0_expert1_tp2 -> stage0_layer0_expert_agg
	stage0_layer0_expert2_tp2 -> stage0_layer0_expert_agg
	stage0_layer0_expert3_tp2 -> stage0_layer0_expert_agg
	stage0_layer0_expert31_tp2 -> stage0_layer0_expert_agg
	stage0_layer0_expert32_tp2 -> stage0_layer0_expert_agg
	stage0_layer0_expert60_tp2 -> stage0_layer0_expert_agg
	stage0_layer0_expert61_tp2 -> stage0_layer0_expert_agg
	stage0_layer0_expert62_tp2 -> stage0_layer0_expert_agg
	stage0_layer0_expert63_tp2 -> stage0_layer0_expert_agg
	stage0_layer0_expert_ellipsis -> stage0_layer0_expert_agg
	stage0_layer0_output [label="Layer 0 Output\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage0_layer0_expert_agg -> stage0_layer0_output
	stage0_layer0_tp_attn_comm -> stage0_layer0_output [style=invis]
	stage0_layer1_ln [label="Layer 1 LayerNorm\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage0_layer0_output -> stage0_layer1_ln
	stage0_layer1_attn_qkv [label="Layer 1 Attention QKV Projection\nGPU: Pipeline 0 (TP: GPU 0-127)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage0_layer1_ln -> stage0_layer1_attn_qkv
	stage0_layer1_tp_qkv_comm [label="Layer 1 TP QKV All-Gather\nGPU: Pipeline 0 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage0_layer1_attn_qkv -> stage0_layer1_tp_qkv_comm
	stage0_layer1_attn_head0 [label="Layer 1 Head 0 Attention\nGPU: Pipeline 0 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head0
	stage0_layer1_attn_head1 [label="Layer 1 Head 1 Attention\nGPU: Pipeline 0 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head1
	stage0_layer1_attn_head2 [label="Layer 1 Head 2 Attention\nGPU: Pipeline 0 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head2
	stage0_layer1_attn_head3 [label="Layer 1 Head 3 Attention\nGPU: Pipeline 0 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head3
	stage0_layer1_attn_head4 [label="Layer 1 Head 4 Attention\nGPU: Pipeline 0 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head4
	stage0_layer1_attn_head5 [label="Layer 1 Head 5 Attention\nGPU: Pipeline 0 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head5
	stage0_layer1_attn_head6 [label="Layer 1 Head 6 Attention\nGPU: Pipeline 0 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head6
	stage0_layer1_attn_head7 [label="Layer 1 Head 7 Attention\nGPU: Pipeline 0 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head7
	stage0_layer1_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_ellipsis
	stage0_layer1_attn_head24 [label="Layer 1 Head 24 Attention\nGPU: Pipeline 0 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head24
	stage0_layer1_attn_head25 [label="Layer 1 Head 25 Attention\nGPU: Pipeline 0 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head25
	stage0_layer1_attn_head26 [label="Layer 1 Head 26 Attention\nGPU: Pipeline 0 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head26
	stage0_layer1_attn_head27 [label="Layer 1 Head 27 Attention\nGPU: Pipeline 0 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head27
	stage0_layer1_attn_head28 [label="Layer 1 Head 28 Attention\nGPU: Pipeline 0 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head28
	stage0_layer1_attn_head29 [label="Layer 1 Head 29 Attention\nGPU: Pipeline 0 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head29
	stage0_layer1_attn_head30 [label="Layer 1 Head 30 Attention\nGPU: Pipeline 0 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head30
	stage0_layer1_attn_head31 [label="Layer 1 Head 31 Attention\nGPU: Pipeline 0 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_tp_qkv_comm -> stage0_layer1_attn_head31
	stage0_layer1_attn_agg [label="Layer 1 Attention Aggregate\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage0_layer1_attn_head0 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head1 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head2 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head3 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head4 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head5 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head6 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head7 -> stage0_layer1_attn_agg
	stage0_layer1_attn_ellipsis -> stage0_layer1_attn_agg
	stage0_layer1_attn_head24 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head25 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head26 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head27 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head28 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head29 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head30 -> stage0_layer1_attn_agg
	stage0_layer1_attn_head31 -> stage0_layer1_attn_agg
	stage0_layer1_attn_out [label="Layer 1 Attention Output Proj\nGPU: Pipeline 0 (TP: GPU 0-127)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage0_layer1_attn_agg -> stage0_layer1_attn_out
	stage0_layer1_tp_attn_comm [label="Layer 1 TP Attention All-Reduce\nGPU: Pipeline 0 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage0_layer1_attn_out -> stage0_layer1_tp_attn_comm
	stage0_layer1_expert_router [label="Layer 1 Expert Router (Top-k)\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage0_layer1_tp_attn_comm -> stage0_layer1_expert_router
	stage0_layer1_expert0 [label="Layer 1 Expert 0\nGPU: 0-1 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert_router -> stage0_layer1_expert0 [style=dashed]
	stage0_layer1_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 0-1 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert0 -> stage0_layer1_expert0_mlp1
	stage0_layer1_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 0-1\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert0_mlp1 -> stage0_layer1_expert0_tp1
	stage0_layer1_expert0_act [label="Expert 0 GELU Activation\nGPU: 0-1\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert0_tp1 -> stage0_layer1_expert0_act
	stage0_layer1_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 0-1 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert0_act -> stage0_layer1_expert0_mlp2
	stage0_layer1_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 0-1\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert0_mlp2 -> stage0_layer1_expert0_tp2
	stage0_layer1_expert1 [label="Layer 1 Expert 1\nGPU: 2-3 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert_router -> stage0_layer1_expert1 [style=dashed]
	stage0_layer1_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 2-3 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert1 -> stage0_layer1_expert1_mlp1
	stage0_layer1_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 2-3\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert1_mlp1 -> stage0_layer1_expert1_tp1
	stage0_layer1_expert1_act [label="Expert 1 GELU Activation\nGPU: 2-3\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert1_tp1 -> stage0_layer1_expert1_act
	stage0_layer1_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 2-3 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert1_act -> stage0_layer1_expert1_mlp2
	stage0_layer1_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 2-3\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert1_mlp2 -> stage0_layer1_expert1_tp2
	stage0_layer1_expert2 [label="Layer 1 Expert 2\nGPU: 4-5 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert_router -> stage0_layer1_expert2 [style=dashed]
	stage0_layer1_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 4-5 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert2 -> stage0_layer1_expert2_mlp1
	stage0_layer1_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 4-5\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert2_mlp1 -> stage0_layer1_expert2_tp1
	stage0_layer1_expert2_act [label="Expert 2 GELU Activation\nGPU: 4-5\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert2_tp1 -> stage0_layer1_expert2_act
	stage0_layer1_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 4-5 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert2_act -> stage0_layer1_expert2_mlp2
	stage0_layer1_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 4-5\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert2_mlp2 -> stage0_layer1_expert2_tp2
	stage0_layer1_expert3 [label="Layer 1 Expert 3\nGPU: 6-7 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert_router -> stage0_layer1_expert3 [style=dashed]
	stage0_layer1_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 6-7 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert3 -> stage0_layer1_expert3_mlp1
	stage0_layer1_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 6-7\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert3_mlp1 -> stage0_layer1_expert3_tp1
	stage0_layer1_expert3_act [label="Expert 3 GELU Activation\nGPU: 6-7\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert3_tp1 -> stage0_layer1_expert3_act
	stage0_layer1_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 6-7 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert3_act -> stage0_layer1_expert3_mlp2
	stage0_layer1_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 6-7\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert3_mlp2 -> stage0_layer1_expert3_tp2
	stage0_layer1_expert31 [label="Layer 1 Expert 31\nGPU: 62-63 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert_router -> stage0_layer1_expert31 [style=dashed]
	stage0_layer1_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 62-63 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert31 -> stage0_layer1_expert31_mlp1
	stage0_layer1_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 62-63\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert31_mlp1 -> stage0_layer1_expert31_tp1
	stage0_layer1_expert31_act [label="Expert 31 GELU Activation\nGPU: 62-63\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert31_tp1 -> stage0_layer1_expert31_act
	stage0_layer1_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 62-63 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert31_act -> stage0_layer1_expert31_mlp2
	stage0_layer1_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 62-63\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert31_mlp2 -> stage0_layer1_expert31_tp2
	stage0_layer1_expert32 [label="Layer 1 Expert 32\nGPU: 64-65 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert_router -> stage0_layer1_expert32 [style=dashed]
	stage0_layer1_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 64-65 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert32 -> stage0_layer1_expert32_mlp1
	stage0_layer1_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 64-65\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert32_mlp1 -> stage0_layer1_expert32_tp1
	stage0_layer1_expert32_act [label="Expert 32 GELU Activation\nGPU: 64-65\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert32_tp1 -> stage0_layer1_expert32_act
	stage0_layer1_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 64-65 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert32_act -> stage0_layer1_expert32_mlp2
	stage0_layer1_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 64-65\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert32_mlp2 -> stage0_layer1_expert32_tp2
	stage0_layer1_expert60 [label="Layer 1 Expert 60\nGPU: 120-121 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert_router -> stage0_layer1_expert60 [style=dashed]
	stage0_layer1_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 120-121 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert60 -> stage0_layer1_expert60_mlp1
	stage0_layer1_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 120-121\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert60_mlp1 -> stage0_layer1_expert60_tp1
	stage0_layer1_expert60_act [label="Expert 60 GELU Activation\nGPU: 120-121\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert60_tp1 -> stage0_layer1_expert60_act
	stage0_layer1_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 120-121 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert60_act -> stage0_layer1_expert60_mlp2
	stage0_layer1_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 120-121\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert60_mlp2 -> stage0_layer1_expert60_tp2
	stage0_layer1_expert61 [label="Layer 1 Expert 61\nGPU: 122-123 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert_router -> stage0_layer1_expert61 [style=dashed]
	stage0_layer1_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 122-123 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert61 -> stage0_layer1_expert61_mlp1
	stage0_layer1_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 122-123\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert61_mlp1 -> stage0_layer1_expert61_tp1
	stage0_layer1_expert61_act [label="Expert 61 GELU Activation\nGPU: 122-123\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert61_tp1 -> stage0_layer1_expert61_act
	stage0_layer1_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 122-123 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert61_act -> stage0_layer1_expert61_mlp2
	stage0_layer1_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 122-123\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert61_mlp2 -> stage0_layer1_expert61_tp2
	stage0_layer1_expert62 [label="Layer 1 Expert 62\nGPU: 124-125 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert_router -> stage0_layer1_expert62 [style=dashed]
	stage0_layer1_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 124-125 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert62 -> stage0_layer1_expert62_mlp1
	stage0_layer1_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 124-125\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert62_mlp1 -> stage0_layer1_expert62_tp1
	stage0_layer1_expert62_act [label="Expert 62 GELU Activation\nGPU: 124-125\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert62_tp1 -> stage0_layer1_expert62_act
	stage0_layer1_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 124-125 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert62_act -> stage0_layer1_expert62_mlp2
	stage0_layer1_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 124-125\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert62_mlp2 -> stage0_layer1_expert62_tp2
	stage0_layer1_expert63 [label="Layer 1 Expert 63\nGPU: 126-127 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert_router -> stage0_layer1_expert63 [style=dashed]
	stage0_layer1_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 126-127 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert63 -> stage0_layer1_expert63_mlp1
	stage0_layer1_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 126-127\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert63_mlp1 -> stage0_layer1_expert63_tp1
	stage0_layer1_expert63_act [label="Expert 63 GELU Activation\nGPU: 126-127\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert63_tp1 -> stage0_layer1_expert63_act
	stage0_layer1_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 126-127 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage0_layer1_expert63_act -> stage0_layer1_expert63_mlp2
	stage0_layer1_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 126-127\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage0_layer1_expert63_mlp2 -> stage0_layer1_expert63_tp2
	stage0_layer1_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 0 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage0_layer1_expert_router -> stage0_layer1_expert_ellipsis [style=dashed]
	stage0_layer1_expert_agg [label="Layer 1 Expert Aggregation\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage0_layer1_expert0_tp2 -> stage0_layer1_expert_agg
	stage0_layer1_expert1_tp2 -> stage0_layer1_expert_agg
	stage0_layer1_expert2_tp2 -> stage0_layer1_expert_agg
	stage0_layer1_expert3_tp2 -> stage0_layer1_expert_agg
	stage0_layer1_expert31_tp2 -> stage0_layer1_expert_agg
	stage0_layer1_expert32_tp2 -> stage0_layer1_expert_agg
	stage0_layer1_expert60_tp2 -> stage0_layer1_expert_agg
	stage0_layer1_expert61_tp2 -> stage0_layer1_expert_agg
	stage0_layer1_expert62_tp2 -> stage0_layer1_expert_agg
	stage0_layer1_expert63_tp2 -> stage0_layer1_expert_agg
	stage0_layer1_expert_ellipsis -> stage0_layer1_expert_agg
	stage0_layer1_output [label="Layer 1 Output\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage0_layer1_expert_agg -> stage0_layer1_output
	stage0_output [label="Stage 0 Output\nGPU: Pipeline 0\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage0_layer1_output -> stage0_output
	stage1_input [label="Stage 1 Input Router\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage0_output -> stage1_input
	stage1_layer0_ln [label="Layer 2 LayerNorm\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage1_input -> stage1_layer0_ln
	stage1_layer0_attn_qkv [label="Layer 2 Attention QKV Projection\nGPU: Pipeline 1 (TP: GPU 128-255)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage1_layer0_ln -> stage1_layer0_attn_qkv
	stage1_layer0_tp_qkv_comm [label="Layer 2 TP QKV All-Gather\nGPU: Pipeline 1 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage1_layer0_attn_qkv -> stage1_layer0_tp_qkv_comm
	stage1_layer0_attn_head0 [label="Layer 2 Head 0 Attention\nGPU: Pipeline 1 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head0
	stage1_layer0_attn_head1 [label="Layer 2 Head 1 Attention\nGPU: Pipeline 1 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head1
	stage1_layer0_attn_head2 [label="Layer 2 Head 2 Attention\nGPU: Pipeline 1 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head2
	stage1_layer0_attn_head3 [label="Layer 2 Head 3 Attention\nGPU: Pipeline 1 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head3
	stage1_layer0_attn_head4 [label="Layer 2 Head 4 Attention\nGPU: Pipeline 1 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head4
	stage1_layer0_attn_head5 [label="Layer 2 Head 5 Attention\nGPU: Pipeline 1 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head5
	stage1_layer0_attn_head6 [label="Layer 2 Head 6 Attention\nGPU: Pipeline 1 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head6
	stage1_layer0_attn_head7 [label="Layer 2 Head 7 Attention\nGPU: Pipeline 1 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head7
	stage1_layer0_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_ellipsis
	stage1_layer0_attn_head24 [label="Layer 2 Head 24 Attention\nGPU: Pipeline 1 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head24
	stage1_layer0_attn_head25 [label="Layer 2 Head 25 Attention\nGPU: Pipeline 1 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head25
	stage1_layer0_attn_head26 [label="Layer 2 Head 26 Attention\nGPU: Pipeline 1 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head26
	stage1_layer0_attn_head27 [label="Layer 2 Head 27 Attention\nGPU: Pipeline 1 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head27
	stage1_layer0_attn_head28 [label="Layer 2 Head 28 Attention\nGPU: Pipeline 1 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head28
	stage1_layer0_attn_head29 [label="Layer 2 Head 29 Attention\nGPU: Pipeline 1 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head29
	stage1_layer0_attn_head30 [label="Layer 2 Head 30 Attention\nGPU: Pipeline 1 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head30
	stage1_layer0_attn_head31 [label="Layer 2 Head 31 Attention\nGPU: Pipeline 1 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_tp_qkv_comm -> stage1_layer0_attn_head31
	stage1_layer0_attn_agg [label="Layer 2 Attention Aggregate\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage1_layer0_attn_head0 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head1 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head2 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head3 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head4 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head5 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head6 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head7 -> stage1_layer0_attn_agg
	stage1_layer0_attn_ellipsis -> stage1_layer0_attn_agg
	stage1_layer0_attn_head24 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head25 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head26 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head27 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head28 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head29 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head30 -> stage1_layer0_attn_agg
	stage1_layer0_attn_head31 -> stage1_layer0_attn_agg
	stage1_layer0_attn_out [label="Layer 2 Attention Output Proj\nGPU: Pipeline 1 (TP: GPU 128-255)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage1_layer0_attn_agg -> stage1_layer0_attn_out
	stage1_layer0_tp_attn_comm [label="Layer 2 TP Attention All-Reduce\nGPU: Pipeline 1 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage1_layer0_attn_out -> stage1_layer0_tp_attn_comm
	stage1_layer0_expert_router [label="Layer 2 Expert Router (Top-k)\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage1_layer0_tp_attn_comm -> stage1_layer0_expert_router
	stage1_layer0_expert0 [label="Layer 2 Expert 0\nGPU: 128-129 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert_router -> stage1_layer0_expert0 [style=dashed]
	stage1_layer0_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 128-129 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert0 -> stage1_layer0_expert0_mlp1
	stage1_layer0_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 128-129\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert0_mlp1 -> stage1_layer0_expert0_tp1
	stage1_layer0_expert0_act [label="Expert 0 GELU Activation\nGPU: 128-129\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert0_tp1 -> stage1_layer0_expert0_act
	stage1_layer0_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 128-129 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert0_act -> stage1_layer0_expert0_mlp2
	stage1_layer0_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 128-129\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert0_mlp2 -> stage1_layer0_expert0_tp2
	stage1_layer0_expert1 [label="Layer 2 Expert 1\nGPU: 130-131 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert_router -> stage1_layer0_expert1 [style=dashed]
	stage1_layer0_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 130-131 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert1 -> stage1_layer0_expert1_mlp1
	stage1_layer0_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 130-131\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert1_mlp1 -> stage1_layer0_expert1_tp1
	stage1_layer0_expert1_act [label="Expert 1 GELU Activation\nGPU: 130-131\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert1_tp1 -> stage1_layer0_expert1_act
	stage1_layer0_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 130-131 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert1_act -> stage1_layer0_expert1_mlp2
	stage1_layer0_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 130-131\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert1_mlp2 -> stage1_layer0_expert1_tp2
	stage1_layer0_expert2 [label="Layer 2 Expert 2\nGPU: 132-133 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert_router -> stage1_layer0_expert2 [style=dashed]
	stage1_layer0_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 132-133 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert2 -> stage1_layer0_expert2_mlp1
	stage1_layer0_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 132-133\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert2_mlp1 -> stage1_layer0_expert2_tp1
	stage1_layer0_expert2_act [label="Expert 2 GELU Activation\nGPU: 132-133\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert2_tp1 -> stage1_layer0_expert2_act
	stage1_layer0_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 132-133 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert2_act -> stage1_layer0_expert2_mlp2
	stage1_layer0_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 132-133\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert2_mlp2 -> stage1_layer0_expert2_tp2
	stage1_layer0_expert3 [label="Layer 2 Expert 3\nGPU: 134-135 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert_router -> stage1_layer0_expert3 [style=dashed]
	stage1_layer0_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 134-135 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert3 -> stage1_layer0_expert3_mlp1
	stage1_layer0_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 134-135\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert3_mlp1 -> stage1_layer0_expert3_tp1
	stage1_layer0_expert3_act [label="Expert 3 GELU Activation\nGPU: 134-135\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert3_tp1 -> stage1_layer0_expert3_act
	stage1_layer0_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 134-135 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert3_act -> stage1_layer0_expert3_mlp2
	stage1_layer0_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 134-135\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert3_mlp2 -> stage1_layer0_expert3_tp2
	stage1_layer0_expert31 [label="Layer 2 Expert 31\nGPU: 190-191 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert_router -> stage1_layer0_expert31 [style=dashed]
	stage1_layer0_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 190-191 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert31 -> stage1_layer0_expert31_mlp1
	stage1_layer0_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 190-191\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert31_mlp1 -> stage1_layer0_expert31_tp1
	stage1_layer0_expert31_act [label="Expert 31 GELU Activation\nGPU: 190-191\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert31_tp1 -> stage1_layer0_expert31_act
	stage1_layer0_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 190-191 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert31_act -> stage1_layer0_expert31_mlp2
	stage1_layer0_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 190-191\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert31_mlp2 -> stage1_layer0_expert31_tp2
	stage1_layer0_expert32 [label="Layer 2 Expert 32\nGPU: 192-193 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert_router -> stage1_layer0_expert32 [style=dashed]
	stage1_layer0_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 192-193 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert32 -> stage1_layer0_expert32_mlp1
	stage1_layer0_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 192-193\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert32_mlp1 -> stage1_layer0_expert32_tp1
	stage1_layer0_expert32_act [label="Expert 32 GELU Activation\nGPU: 192-193\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert32_tp1 -> stage1_layer0_expert32_act
	stage1_layer0_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 192-193 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert32_act -> stage1_layer0_expert32_mlp2
	stage1_layer0_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 192-193\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert32_mlp2 -> stage1_layer0_expert32_tp2
	stage1_layer0_expert60 [label="Layer 2 Expert 60\nGPU: 248-249 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert_router -> stage1_layer0_expert60 [style=dashed]
	stage1_layer0_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 248-249 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert60 -> stage1_layer0_expert60_mlp1
	stage1_layer0_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 248-249\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert60_mlp1 -> stage1_layer0_expert60_tp1
	stage1_layer0_expert60_act [label="Expert 60 GELU Activation\nGPU: 248-249\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert60_tp1 -> stage1_layer0_expert60_act
	stage1_layer0_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 248-249 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert60_act -> stage1_layer0_expert60_mlp2
	stage1_layer0_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 248-249\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert60_mlp2 -> stage1_layer0_expert60_tp2
	stage1_layer0_expert61 [label="Layer 2 Expert 61\nGPU: 250-251 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert_router -> stage1_layer0_expert61 [style=dashed]
	stage1_layer0_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 250-251 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert61 -> stage1_layer0_expert61_mlp1
	stage1_layer0_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 250-251\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert61_mlp1 -> stage1_layer0_expert61_tp1
	stage1_layer0_expert61_act [label="Expert 61 GELU Activation\nGPU: 250-251\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert61_tp1 -> stage1_layer0_expert61_act
	stage1_layer0_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 250-251 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert61_act -> stage1_layer0_expert61_mlp2
	stage1_layer0_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 250-251\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert61_mlp2 -> stage1_layer0_expert61_tp2
	stage1_layer0_expert62 [label="Layer 2 Expert 62\nGPU: 252-253 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert_router -> stage1_layer0_expert62 [style=dashed]
	stage1_layer0_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 252-253 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert62 -> stage1_layer0_expert62_mlp1
	stage1_layer0_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 252-253\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert62_mlp1 -> stage1_layer0_expert62_tp1
	stage1_layer0_expert62_act [label="Expert 62 GELU Activation\nGPU: 252-253\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert62_tp1 -> stage1_layer0_expert62_act
	stage1_layer0_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 252-253 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert62_act -> stage1_layer0_expert62_mlp2
	stage1_layer0_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 252-253\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert62_mlp2 -> stage1_layer0_expert62_tp2
	stage1_layer0_expert63 [label="Layer 2 Expert 63\nGPU: 254-255 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert_router -> stage1_layer0_expert63 [style=dashed]
	stage1_layer0_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 254-255 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert63 -> stage1_layer0_expert63_mlp1
	stage1_layer0_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 254-255\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert63_mlp1 -> stage1_layer0_expert63_tp1
	stage1_layer0_expert63_act [label="Expert 63 GELU Activation\nGPU: 254-255\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert63_tp1 -> stage1_layer0_expert63_act
	stage1_layer0_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 254-255 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer0_expert63_act -> stage1_layer0_expert63_mlp2
	stage1_layer0_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 254-255\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer0_expert63_mlp2 -> stage1_layer0_expert63_tp2
	stage1_layer0_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 1 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage1_layer0_expert_router -> stage1_layer0_expert_ellipsis [style=dashed]
	stage1_layer0_expert_agg [label="Layer 2 Expert Aggregation\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage1_layer0_expert0_tp2 -> stage1_layer0_expert_agg
	stage1_layer0_expert1_tp2 -> stage1_layer0_expert_agg
	stage1_layer0_expert2_tp2 -> stage1_layer0_expert_agg
	stage1_layer0_expert3_tp2 -> stage1_layer0_expert_agg
	stage1_layer0_expert31_tp2 -> stage1_layer0_expert_agg
	stage1_layer0_expert32_tp2 -> stage1_layer0_expert_agg
	stage1_layer0_expert60_tp2 -> stage1_layer0_expert_agg
	stage1_layer0_expert61_tp2 -> stage1_layer0_expert_agg
	stage1_layer0_expert62_tp2 -> stage1_layer0_expert_agg
	stage1_layer0_expert63_tp2 -> stage1_layer0_expert_agg
	stage1_layer0_expert_ellipsis -> stage1_layer0_expert_agg
	stage1_layer0_output [label="Layer 2 Output\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage1_layer0_expert_agg -> stage1_layer0_output
	stage1_layer0_tp_attn_comm -> stage1_layer0_output [style=invis]
	stage1_layer1_ln [label="Layer 3 LayerNorm\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage1_layer0_output -> stage1_layer1_ln
	stage1_layer1_attn_qkv [label="Layer 3 Attention QKV Projection\nGPU: Pipeline 1 (TP: GPU 128-255)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage1_layer1_ln -> stage1_layer1_attn_qkv
	stage1_layer1_tp_qkv_comm [label="Layer 3 TP QKV All-Gather\nGPU: Pipeline 1 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage1_layer1_attn_qkv -> stage1_layer1_tp_qkv_comm
	stage1_layer1_attn_head0 [label="Layer 3 Head 0 Attention\nGPU: Pipeline 1 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head0
	stage1_layer1_attn_head1 [label="Layer 3 Head 1 Attention\nGPU: Pipeline 1 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head1
	stage1_layer1_attn_head2 [label="Layer 3 Head 2 Attention\nGPU: Pipeline 1 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head2
	stage1_layer1_attn_head3 [label="Layer 3 Head 3 Attention\nGPU: Pipeline 1 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head3
	stage1_layer1_attn_head4 [label="Layer 3 Head 4 Attention\nGPU: Pipeline 1 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head4
	stage1_layer1_attn_head5 [label="Layer 3 Head 5 Attention\nGPU: Pipeline 1 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head5
	stage1_layer1_attn_head6 [label="Layer 3 Head 6 Attention\nGPU: Pipeline 1 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head6
	stage1_layer1_attn_head7 [label="Layer 3 Head 7 Attention\nGPU: Pipeline 1 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head7
	stage1_layer1_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_ellipsis
	stage1_layer1_attn_head24 [label="Layer 3 Head 24 Attention\nGPU: Pipeline 1 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head24
	stage1_layer1_attn_head25 [label="Layer 3 Head 25 Attention\nGPU: Pipeline 1 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head25
	stage1_layer1_attn_head26 [label="Layer 3 Head 26 Attention\nGPU: Pipeline 1 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head26
	stage1_layer1_attn_head27 [label="Layer 3 Head 27 Attention\nGPU: Pipeline 1 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head27
	stage1_layer1_attn_head28 [label="Layer 3 Head 28 Attention\nGPU: Pipeline 1 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head28
	stage1_layer1_attn_head29 [label="Layer 3 Head 29 Attention\nGPU: Pipeline 1 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head29
	stage1_layer1_attn_head30 [label="Layer 3 Head 30 Attention\nGPU: Pipeline 1 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head30
	stage1_layer1_attn_head31 [label="Layer 3 Head 31 Attention\nGPU: Pipeline 1 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_tp_qkv_comm -> stage1_layer1_attn_head31
	stage1_layer1_attn_agg [label="Layer 3 Attention Aggregate\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage1_layer1_attn_head0 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head1 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head2 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head3 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head4 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head5 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head6 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head7 -> stage1_layer1_attn_agg
	stage1_layer1_attn_ellipsis -> stage1_layer1_attn_agg
	stage1_layer1_attn_head24 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head25 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head26 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head27 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head28 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head29 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head30 -> stage1_layer1_attn_agg
	stage1_layer1_attn_head31 -> stage1_layer1_attn_agg
	stage1_layer1_attn_out [label="Layer 3 Attention Output Proj\nGPU: Pipeline 1 (TP: GPU 128-255)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage1_layer1_attn_agg -> stage1_layer1_attn_out
	stage1_layer1_tp_attn_comm [label="Layer 3 TP Attention All-Reduce\nGPU: Pipeline 1 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage1_layer1_attn_out -> stage1_layer1_tp_attn_comm
	stage1_layer1_expert_router [label="Layer 3 Expert Router (Top-k)\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage1_layer1_tp_attn_comm -> stage1_layer1_expert_router
	stage1_layer1_expert0 [label="Layer 3 Expert 0\nGPU: 128-129 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert_router -> stage1_layer1_expert0 [style=dashed]
	stage1_layer1_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 128-129 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert0 -> stage1_layer1_expert0_mlp1
	stage1_layer1_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 128-129\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert0_mlp1 -> stage1_layer1_expert0_tp1
	stage1_layer1_expert0_act [label="Expert 0 GELU Activation\nGPU: 128-129\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert0_tp1 -> stage1_layer1_expert0_act
	stage1_layer1_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 128-129 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert0_act -> stage1_layer1_expert0_mlp2
	stage1_layer1_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 128-129\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert0_mlp2 -> stage1_layer1_expert0_tp2
	stage1_layer1_expert1 [label="Layer 3 Expert 1\nGPU: 130-131 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert_router -> stage1_layer1_expert1 [style=dashed]
	stage1_layer1_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 130-131 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert1 -> stage1_layer1_expert1_mlp1
	stage1_layer1_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 130-131\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert1_mlp1 -> stage1_layer1_expert1_tp1
	stage1_layer1_expert1_act [label="Expert 1 GELU Activation\nGPU: 130-131\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert1_tp1 -> stage1_layer1_expert1_act
	stage1_layer1_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 130-131 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert1_act -> stage1_layer1_expert1_mlp2
	stage1_layer1_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 130-131\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert1_mlp2 -> stage1_layer1_expert1_tp2
	stage1_layer1_expert2 [label="Layer 3 Expert 2\nGPU: 132-133 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert_router -> stage1_layer1_expert2 [style=dashed]
	stage1_layer1_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 132-133 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert2 -> stage1_layer1_expert2_mlp1
	stage1_layer1_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 132-133\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert2_mlp1 -> stage1_layer1_expert2_tp1
	stage1_layer1_expert2_act [label="Expert 2 GELU Activation\nGPU: 132-133\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert2_tp1 -> stage1_layer1_expert2_act
	stage1_layer1_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 132-133 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert2_act -> stage1_layer1_expert2_mlp2
	stage1_layer1_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 132-133\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert2_mlp2 -> stage1_layer1_expert2_tp2
	stage1_layer1_expert3 [label="Layer 3 Expert 3\nGPU: 134-135 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert_router -> stage1_layer1_expert3 [style=dashed]
	stage1_layer1_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 134-135 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert3 -> stage1_layer1_expert3_mlp1
	stage1_layer1_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 134-135\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert3_mlp1 -> stage1_layer1_expert3_tp1
	stage1_layer1_expert3_act [label="Expert 3 GELU Activation\nGPU: 134-135\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert3_tp1 -> stage1_layer1_expert3_act
	stage1_layer1_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 134-135 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert3_act -> stage1_layer1_expert3_mlp2
	stage1_layer1_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 134-135\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert3_mlp2 -> stage1_layer1_expert3_tp2
	stage1_layer1_expert31 [label="Layer 3 Expert 31\nGPU: 190-191 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert_router -> stage1_layer1_expert31 [style=dashed]
	stage1_layer1_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 190-191 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert31 -> stage1_layer1_expert31_mlp1
	stage1_layer1_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 190-191\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert31_mlp1 -> stage1_layer1_expert31_tp1
	stage1_layer1_expert31_act [label="Expert 31 GELU Activation\nGPU: 190-191\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert31_tp1 -> stage1_layer1_expert31_act
	stage1_layer1_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 190-191 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert31_act -> stage1_layer1_expert31_mlp2
	stage1_layer1_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 190-191\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert31_mlp2 -> stage1_layer1_expert31_tp2
	stage1_layer1_expert32 [label="Layer 3 Expert 32\nGPU: 192-193 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert_router -> stage1_layer1_expert32 [style=dashed]
	stage1_layer1_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 192-193 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert32 -> stage1_layer1_expert32_mlp1
	stage1_layer1_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 192-193\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert32_mlp1 -> stage1_layer1_expert32_tp1
	stage1_layer1_expert32_act [label="Expert 32 GELU Activation\nGPU: 192-193\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert32_tp1 -> stage1_layer1_expert32_act
	stage1_layer1_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 192-193 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert32_act -> stage1_layer1_expert32_mlp2
	stage1_layer1_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 192-193\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert32_mlp2 -> stage1_layer1_expert32_tp2
	stage1_layer1_expert60 [label="Layer 3 Expert 60\nGPU: 248-249 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert_router -> stage1_layer1_expert60 [style=dashed]
	stage1_layer1_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 248-249 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert60 -> stage1_layer1_expert60_mlp1
	stage1_layer1_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 248-249\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert60_mlp1 -> stage1_layer1_expert60_tp1
	stage1_layer1_expert60_act [label="Expert 60 GELU Activation\nGPU: 248-249\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert60_tp1 -> stage1_layer1_expert60_act
	stage1_layer1_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 248-249 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert60_act -> stage1_layer1_expert60_mlp2
	stage1_layer1_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 248-249\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert60_mlp2 -> stage1_layer1_expert60_tp2
	stage1_layer1_expert61 [label="Layer 3 Expert 61\nGPU: 250-251 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert_router -> stage1_layer1_expert61 [style=dashed]
	stage1_layer1_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 250-251 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert61 -> stage1_layer1_expert61_mlp1
	stage1_layer1_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 250-251\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert61_mlp1 -> stage1_layer1_expert61_tp1
	stage1_layer1_expert61_act [label="Expert 61 GELU Activation\nGPU: 250-251\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert61_tp1 -> stage1_layer1_expert61_act
	stage1_layer1_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 250-251 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert61_act -> stage1_layer1_expert61_mlp2
	stage1_layer1_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 250-251\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert61_mlp2 -> stage1_layer1_expert61_tp2
	stage1_layer1_expert62 [label="Layer 3 Expert 62\nGPU: 252-253 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert_router -> stage1_layer1_expert62 [style=dashed]
	stage1_layer1_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 252-253 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert62 -> stage1_layer1_expert62_mlp1
	stage1_layer1_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 252-253\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert62_mlp1 -> stage1_layer1_expert62_tp1
	stage1_layer1_expert62_act [label="Expert 62 GELU Activation\nGPU: 252-253\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert62_tp1 -> stage1_layer1_expert62_act
	stage1_layer1_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 252-253 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert62_act -> stage1_layer1_expert62_mlp2
	stage1_layer1_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 252-253\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert62_mlp2 -> stage1_layer1_expert62_tp2
	stage1_layer1_expert63 [label="Layer 3 Expert 63\nGPU: 254-255 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert_router -> stage1_layer1_expert63 [style=dashed]
	stage1_layer1_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 254-255 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert63 -> stage1_layer1_expert63_mlp1
	stage1_layer1_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 254-255\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert63_mlp1 -> stage1_layer1_expert63_tp1
	stage1_layer1_expert63_act [label="Expert 63 GELU Activation\nGPU: 254-255\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert63_tp1 -> stage1_layer1_expert63_act
	stage1_layer1_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 254-255 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage1_layer1_expert63_act -> stage1_layer1_expert63_mlp2
	stage1_layer1_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 254-255\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage1_layer1_expert63_mlp2 -> stage1_layer1_expert63_tp2
	stage1_layer1_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 1 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage1_layer1_expert_router -> stage1_layer1_expert_ellipsis [style=dashed]
	stage1_layer1_expert_agg [label="Layer 3 Expert Aggregation\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage1_layer1_expert0_tp2 -> stage1_layer1_expert_agg
	stage1_layer1_expert1_tp2 -> stage1_layer1_expert_agg
	stage1_layer1_expert2_tp2 -> stage1_layer1_expert_agg
	stage1_layer1_expert3_tp2 -> stage1_layer1_expert_agg
	stage1_layer1_expert31_tp2 -> stage1_layer1_expert_agg
	stage1_layer1_expert32_tp2 -> stage1_layer1_expert_agg
	stage1_layer1_expert60_tp2 -> stage1_layer1_expert_agg
	stage1_layer1_expert61_tp2 -> stage1_layer1_expert_agg
	stage1_layer1_expert62_tp2 -> stage1_layer1_expert_agg
	stage1_layer1_expert63_tp2 -> stage1_layer1_expert_agg
	stage1_layer1_expert_ellipsis -> stage1_layer1_expert_agg
	stage1_layer1_output [label="Layer 3 Output\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage1_layer1_expert_agg -> stage1_layer1_output
	stage1_output [label="Stage 1 Output\nGPU: Pipeline 1\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage1_layer1_output -> stage1_output
	stage2_input [label="Stage 2 Input Router\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage1_output -> stage2_input
	stage2_layer0_ln [label="Layer 4 LayerNorm\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage2_input -> stage2_layer0_ln
	stage2_layer0_attn_qkv [label="Layer 4 Attention QKV Projection\nGPU: Pipeline 2 (TP: GPU 256-383)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage2_layer0_ln -> stage2_layer0_attn_qkv
	stage2_layer0_tp_qkv_comm [label="Layer 4 TP QKV All-Gather\nGPU: Pipeline 2 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage2_layer0_attn_qkv -> stage2_layer0_tp_qkv_comm
	stage2_layer0_attn_head0 [label="Layer 4 Head 0 Attention\nGPU: Pipeline 2 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head0
	stage2_layer0_attn_head1 [label="Layer 4 Head 1 Attention\nGPU: Pipeline 2 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head1
	stage2_layer0_attn_head2 [label="Layer 4 Head 2 Attention\nGPU: Pipeline 2 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head2
	stage2_layer0_attn_head3 [label="Layer 4 Head 3 Attention\nGPU: Pipeline 2 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head3
	stage2_layer0_attn_head4 [label="Layer 4 Head 4 Attention\nGPU: Pipeline 2 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head4
	stage2_layer0_attn_head5 [label="Layer 4 Head 5 Attention\nGPU: Pipeline 2 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head5
	stage2_layer0_attn_head6 [label="Layer 4 Head 6 Attention\nGPU: Pipeline 2 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head6
	stage2_layer0_attn_head7 [label="Layer 4 Head 7 Attention\nGPU: Pipeline 2 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head7
	stage2_layer0_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_ellipsis
	stage2_layer0_attn_head24 [label="Layer 4 Head 24 Attention\nGPU: Pipeline 2 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head24
	stage2_layer0_attn_head25 [label="Layer 4 Head 25 Attention\nGPU: Pipeline 2 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head25
	stage2_layer0_attn_head26 [label="Layer 4 Head 26 Attention\nGPU: Pipeline 2 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head26
	stage2_layer0_attn_head27 [label="Layer 4 Head 27 Attention\nGPU: Pipeline 2 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head27
	stage2_layer0_attn_head28 [label="Layer 4 Head 28 Attention\nGPU: Pipeline 2 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head28
	stage2_layer0_attn_head29 [label="Layer 4 Head 29 Attention\nGPU: Pipeline 2 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head29
	stage2_layer0_attn_head30 [label="Layer 4 Head 30 Attention\nGPU: Pipeline 2 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head30
	stage2_layer0_attn_head31 [label="Layer 4 Head 31 Attention\nGPU: Pipeline 2 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_tp_qkv_comm -> stage2_layer0_attn_head31
	stage2_layer0_attn_agg [label="Layer 4 Attention Aggregate\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage2_layer0_attn_head0 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head1 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head2 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head3 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head4 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head5 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head6 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head7 -> stage2_layer0_attn_agg
	stage2_layer0_attn_ellipsis -> stage2_layer0_attn_agg
	stage2_layer0_attn_head24 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head25 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head26 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head27 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head28 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head29 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head30 -> stage2_layer0_attn_agg
	stage2_layer0_attn_head31 -> stage2_layer0_attn_agg
	stage2_layer0_attn_out [label="Layer 4 Attention Output Proj\nGPU: Pipeline 2 (TP: GPU 256-383)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage2_layer0_attn_agg -> stage2_layer0_attn_out
	stage2_layer0_tp_attn_comm [label="Layer 4 TP Attention All-Reduce\nGPU: Pipeline 2 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage2_layer0_attn_out -> stage2_layer0_tp_attn_comm
	stage2_layer0_expert_router [label="Layer 4 Expert Router (Top-k)\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage2_layer0_tp_attn_comm -> stage2_layer0_expert_router
	stage2_layer0_expert0 [label="Layer 4 Expert 0\nGPU: 256-257 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert_router -> stage2_layer0_expert0 [style=dashed]
	stage2_layer0_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 256-257 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert0 -> stage2_layer0_expert0_mlp1
	stage2_layer0_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 256-257\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert0_mlp1 -> stage2_layer0_expert0_tp1
	stage2_layer0_expert0_act [label="Expert 0 GELU Activation\nGPU: 256-257\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert0_tp1 -> stage2_layer0_expert0_act
	stage2_layer0_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 256-257 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert0_act -> stage2_layer0_expert0_mlp2
	stage2_layer0_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 256-257\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert0_mlp2 -> stage2_layer0_expert0_tp2
	stage2_layer0_expert1 [label="Layer 4 Expert 1\nGPU: 258-259 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert_router -> stage2_layer0_expert1 [style=dashed]
	stage2_layer0_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 258-259 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert1 -> stage2_layer0_expert1_mlp1
	stage2_layer0_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 258-259\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert1_mlp1 -> stage2_layer0_expert1_tp1
	stage2_layer0_expert1_act [label="Expert 1 GELU Activation\nGPU: 258-259\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert1_tp1 -> stage2_layer0_expert1_act
	stage2_layer0_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 258-259 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert1_act -> stage2_layer0_expert1_mlp2
	stage2_layer0_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 258-259\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert1_mlp2 -> stage2_layer0_expert1_tp2
	stage2_layer0_expert2 [label="Layer 4 Expert 2\nGPU: 260-261 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert_router -> stage2_layer0_expert2 [style=dashed]
	stage2_layer0_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 260-261 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert2 -> stage2_layer0_expert2_mlp1
	stage2_layer0_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 260-261\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert2_mlp1 -> stage2_layer0_expert2_tp1
	stage2_layer0_expert2_act [label="Expert 2 GELU Activation\nGPU: 260-261\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert2_tp1 -> stage2_layer0_expert2_act
	stage2_layer0_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 260-261 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert2_act -> stage2_layer0_expert2_mlp2
	stage2_layer0_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 260-261\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert2_mlp2 -> stage2_layer0_expert2_tp2
	stage2_layer0_expert3 [label="Layer 4 Expert 3\nGPU: 262-263 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert_router -> stage2_layer0_expert3 [style=dashed]
	stage2_layer0_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 262-263 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert3 -> stage2_layer0_expert3_mlp1
	stage2_layer0_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 262-263\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert3_mlp1 -> stage2_layer0_expert3_tp1
	stage2_layer0_expert3_act [label="Expert 3 GELU Activation\nGPU: 262-263\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert3_tp1 -> stage2_layer0_expert3_act
	stage2_layer0_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 262-263 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert3_act -> stage2_layer0_expert3_mlp2
	stage2_layer0_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 262-263\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert3_mlp2 -> stage2_layer0_expert3_tp2
	stage2_layer0_expert31 [label="Layer 4 Expert 31\nGPU: 318-319 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert_router -> stage2_layer0_expert31 [style=dashed]
	stage2_layer0_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 318-319 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert31 -> stage2_layer0_expert31_mlp1
	stage2_layer0_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 318-319\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert31_mlp1 -> stage2_layer0_expert31_tp1
	stage2_layer0_expert31_act [label="Expert 31 GELU Activation\nGPU: 318-319\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert31_tp1 -> stage2_layer0_expert31_act
	stage2_layer0_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 318-319 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert31_act -> stage2_layer0_expert31_mlp2
	stage2_layer0_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 318-319\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert31_mlp2 -> stage2_layer0_expert31_tp2
	stage2_layer0_expert32 [label="Layer 4 Expert 32\nGPU: 320-321 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert_router -> stage2_layer0_expert32 [style=dashed]
	stage2_layer0_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 320-321 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert32 -> stage2_layer0_expert32_mlp1
	stage2_layer0_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 320-321\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert32_mlp1 -> stage2_layer0_expert32_tp1
	stage2_layer0_expert32_act [label="Expert 32 GELU Activation\nGPU: 320-321\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert32_tp1 -> stage2_layer0_expert32_act
	stage2_layer0_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 320-321 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert32_act -> stage2_layer0_expert32_mlp2
	stage2_layer0_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 320-321\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert32_mlp2 -> stage2_layer0_expert32_tp2
	stage2_layer0_expert60 [label="Layer 4 Expert 60\nGPU: 376-377 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert_router -> stage2_layer0_expert60 [style=dashed]
	stage2_layer0_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 376-377 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert60 -> stage2_layer0_expert60_mlp1
	stage2_layer0_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 376-377\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert60_mlp1 -> stage2_layer0_expert60_tp1
	stage2_layer0_expert60_act [label="Expert 60 GELU Activation\nGPU: 376-377\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert60_tp1 -> stage2_layer0_expert60_act
	stage2_layer0_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 376-377 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert60_act -> stage2_layer0_expert60_mlp2
	stage2_layer0_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 376-377\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert60_mlp2 -> stage2_layer0_expert60_tp2
	stage2_layer0_expert61 [label="Layer 4 Expert 61\nGPU: 378-379 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert_router -> stage2_layer0_expert61 [style=dashed]
	stage2_layer0_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 378-379 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert61 -> stage2_layer0_expert61_mlp1
	stage2_layer0_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 378-379\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert61_mlp1 -> stage2_layer0_expert61_tp1
	stage2_layer0_expert61_act [label="Expert 61 GELU Activation\nGPU: 378-379\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert61_tp1 -> stage2_layer0_expert61_act
	stage2_layer0_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 378-379 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert61_act -> stage2_layer0_expert61_mlp2
	stage2_layer0_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 378-379\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert61_mlp2 -> stage2_layer0_expert61_tp2
	stage2_layer0_expert62 [label="Layer 4 Expert 62\nGPU: 380-381 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert_router -> stage2_layer0_expert62 [style=dashed]
	stage2_layer0_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 380-381 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert62 -> stage2_layer0_expert62_mlp1
	stage2_layer0_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 380-381\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert62_mlp1 -> stage2_layer0_expert62_tp1
	stage2_layer0_expert62_act [label="Expert 62 GELU Activation\nGPU: 380-381\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert62_tp1 -> stage2_layer0_expert62_act
	stage2_layer0_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 380-381 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert62_act -> stage2_layer0_expert62_mlp2
	stage2_layer0_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 380-381\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert62_mlp2 -> stage2_layer0_expert62_tp2
	stage2_layer0_expert63 [label="Layer 4 Expert 63\nGPU: 382-383 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert_router -> stage2_layer0_expert63 [style=dashed]
	stage2_layer0_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 382-383 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert63 -> stage2_layer0_expert63_mlp1
	stage2_layer0_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 382-383\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert63_mlp1 -> stage2_layer0_expert63_tp1
	stage2_layer0_expert63_act [label="Expert 63 GELU Activation\nGPU: 382-383\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert63_tp1 -> stage2_layer0_expert63_act
	stage2_layer0_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 382-383 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer0_expert63_act -> stage2_layer0_expert63_mlp2
	stage2_layer0_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 382-383\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer0_expert63_mlp2 -> stage2_layer0_expert63_tp2
	stage2_layer0_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 2 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage2_layer0_expert_router -> stage2_layer0_expert_ellipsis [style=dashed]
	stage2_layer0_expert_agg [label="Layer 4 Expert Aggregation\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage2_layer0_expert0_tp2 -> stage2_layer0_expert_agg
	stage2_layer0_expert1_tp2 -> stage2_layer0_expert_agg
	stage2_layer0_expert2_tp2 -> stage2_layer0_expert_agg
	stage2_layer0_expert3_tp2 -> stage2_layer0_expert_agg
	stage2_layer0_expert31_tp2 -> stage2_layer0_expert_agg
	stage2_layer0_expert32_tp2 -> stage2_layer0_expert_agg
	stage2_layer0_expert60_tp2 -> stage2_layer0_expert_agg
	stage2_layer0_expert61_tp2 -> stage2_layer0_expert_agg
	stage2_layer0_expert62_tp2 -> stage2_layer0_expert_agg
	stage2_layer0_expert63_tp2 -> stage2_layer0_expert_agg
	stage2_layer0_expert_ellipsis -> stage2_layer0_expert_agg
	stage2_layer0_output [label="Layer 4 Output\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage2_layer0_expert_agg -> stage2_layer0_output
	stage2_layer0_tp_attn_comm -> stage2_layer0_output [style=invis]
	stage2_layer1_ln [label="Layer 5 LayerNorm\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage2_layer0_output -> stage2_layer1_ln
	stage2_layer1_attn_qkv [label="Layer 5 Attention QKV Projection\nGPU: Pipeline 2 (TP: GPU 256-383)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage2_layer1_ln -> stage2_layer1_attn_qkv
	stage2_layer1_tp_qkv_comm [label="Layer 5 TP QKV All-Gather\nGPU: Pipeline 2 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage2_layer1_attn_qkv -> stage2_layer1_tp_qkv_comm
	stage2_layer1_attn_head0 [label="Layer 5 Head 0 Attention\nGPU: Pipeline 2 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head0
	stage2_layer1_attn_head1 [label="Layer 5 Head 1 Attention\nGPU: Pipeline 2 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head1
	stage2_layer1_attn_head2 [label="Layer 5 Head 2 Attention\nGPU: Pipeline 2 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head2
	stage2_layer1_attn_head3 [label="Layer 5 Head 3 Attention\nGPU: Pipeline 2 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head3
	stage2_layer1_attn_head4 [label="Layer 5 Head 4 Attention\nGPU: Pipeline 2 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head4
	stage2_layer1_attn_head5 [label="Layer 5 Head 5 Attention\nGPU: Pipeline 2 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head5
	stage2_layer1_attn_head6 [label="Layer 5 Head 6 Attention\nGPU: Pipeline 2 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head6
	stage2_layer1_attn_head7 [label="Layer 5 Head 7 Attention\nGPU: Pipeline 2 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head7
	stage2_layer1_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_ellipsis
	stage2_layer1_attn_head24 [label="Layer 5 Head 24 Attention\nGPU: Pipeline 2 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head24
	stage2_layer1_attn_head25 [label="Layer 5 Head 25 Attention\nGPU: Pipeline 2 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head25
	stage2_layer1_attn_head26 [label="Layer 5 Head 26 Attention\nGPU: Pipeline 2 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head26
	stage2_layer1_attn_head27 [label="Layer 5 Head 27 Attention\nGPU: Pipeline 2 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head27
	stage2_layer1_attn_head28 [label="Layer 5 Head 28 Attention\nGPU: Pipeline 2 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head28
	stage2_layer1_attn_head29 [label="Layer 5 Head 29 Attention\nGPU: Pipeline 2 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head29
	stage2_layer1_attn_head30 [label="Layer 5 Head 30 Attention\nGPU: Pipeline 2 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head30
	stage2_layer1_attn_head31 [label="Layer 5 Head 31 Attention\nGPU: Pipeline 2 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_tp_qkv_comm -> stage2_layer1_attn_head31
	stage2_layer1_attn_agg [label="Layer 5 Attention Aggregate\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage2_layer1_attn_head0 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head1 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head2 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head3 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head4 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head5 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head6 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head7 -> stage2_layer1_attn_agg
	stage2_layer1_attn_ellipsis -> stage2_layer1_attn_agg
	stage2_layer1_attn_head24 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head25 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head26 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head27 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head28 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head29 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head30 -> stage2_layer1_attn_agg
	stage2_layer1_attn_head31 -> stage2_layer1_attn_agg
	stage2_layer1_attn_out [label="Layer 5 Attention Output Proj\nGPU: Pipeline 2 (TP: GPU 256-383)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage2_layer1_attn_agg -> stage2_layer1_attn_out
	stage2_layer1_tp_attn_comm [label="Layer 5 TP Attention All-Reduce\nGPU: Pipeline 2 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage2_layer1_attn_out -> stage2_layer1_tp_attn_comm
	stage2_layer1_expert_router [label="Layer 5 Expert Router (Top-k)\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage2_layer1_tp_attn_comm -> stage2_layer1_expert_router
	stage2_layer1_expert0 [label="Layer 5 Expert 0\nGPU: 256-257 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert_router -> stage2_layer1_expert0 [style=dashed]
	stage2_layer1_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 256-257 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert0 -> stage2_layer1_expert0_mlp1
	stage2_layer1_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 256-257\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert0_mlp1 -> stage2_layer1_expert0_tp1
	stage2_layer1_expert0_act [label="Expert 0 GELU Activation\nGPU: 256-257\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert0_tp1 -> stage2_layer1_expert0_act
	stage2_layer1_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 256-257 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert0_act -> stage2_layer1_expert0_mlp2
	stage2_layer1_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 256-257\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert0_mlp2 -> stage2_layer1_expert0_tp2
	stage2_layer1_expert1 [label="Layer 5 Expert 1\nGPU: 258-259 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert_router -> stage2_layer1_expert1 [style=dashed]
	stage2_layer1_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 258-259 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert1 -> stage2_layer1_expert1_mlp1
	stage2_layer1_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 258-259\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert1_mlp1 -> stage2_layer1_expert1_tp1
	stage2_layer1_expert1_act [label="Expert 1 GELU Activation\nGPU: 258-259\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert1_tp1 -> stage2_layer1_expert1_act
	stage2_layer1_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 258-259 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert1_act -> stage2_layer1_expert1_mlp2
	stage2_layer1_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 258-259\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert1_mlp2 -> stage2_layer1_expert1_tp2
	stage2_layer1_expert2 [label="Layer 5 Expert 2\nGPU: 260-261 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert_router -> stage2_layer1_expert2 [style=dashed]
	stage2_layer1_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 260-261 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert2 -> stage2_layer1_expert2_mlp1
	stage2_layer1_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 260-261\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert2_mlp1 -> stage2_layer1_expert2_tp1
	stage2_layer1_expert2_act [label="Expert 2 GELU Activation\nGPU: 260-261\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert2_tp1 -> stage2_layer1_expert2_act
	stage2_layer1_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 260-261 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert2_act -> stage2_layer1_expert2_mlp2
	stage2_layer1_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 260-261\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert2_mlp2 -> stage2_layer1_expert2_tp2
	stage2_layer1_expert3 [label="Layer 5 Expert 3\nGPU: 262-263 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert_router -> stage2_layer1_expert3 [style=dashed]
	stage2_layer1_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 262-263 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert3 -> stage2_layer1_expert3_mlp1
	stage2_layer1_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 262-263\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert3_mlp1 -> stage2_layer1_expert3_tp1
	stage2_layer1_expert3_act [label="Expert 3 GELU Activation\nGPU: 262-263\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert3_tp1 -> stage2_layer1_expert3_act
	stage2_layer1_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 262-263 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert3_act -> stage2_layer1_expert3_mlp2
	stage2_layer1_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 262-263\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert3_mlp2 -> stage2_layer1_expert3_tp2
	stage2_layer1_expert31 [label="Layer 5 Expert 31\nGPU: 318-319 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert_router -> stage2_layer1_expert31 [style=dashed]
	stage2_layer1_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 318-319 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert31 -> stage2_layer1_expert31_mlp1
	stage2_layer1_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 318-319\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert31_mlp1 -> stage2_layer1_expert31_tp1
	stage2_layer1_expert31_act [label="Expert 31 GELU Activation\nGPU: 318-319\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert31_tp1 -> stage2_layer1_expert31_act
	stage2_layer1_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 318-319 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert31_act -> stage2_layer1_expert31_mlp2
	stage2_layer1_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 318-319\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert31_mlp2 -> stage2_layer1_expert31_tp2
	stage2_layer1_expert32 [label="Layer 5 Expert 32\nGPU: 320-321 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert_router -> stage2_layer1_expert32 [style=dashed]
	stage2_layer1_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 320-321 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert32 -> stage2_layer1_expert32_mlp1
	stage2_layer1_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 320-321\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert32_mlp1 -> stage2_layer1_expert32_tp1
	stage2_layer1_expert32_act [label="Expert 32 GELU Activation\nGPU: 320-321\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert32_tp1 -> stage2_layer1_expert32_act
	stage2_layer1_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 320-321 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert32_act -> stage2_layer1_expert32_mlp2
	stage2_layer1_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 320-321\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert32_mlp2 -> stage2_layer1_expert32_tp2
	stage2_layer1_expert60 [label="Layer 5 Expert 60\nGPU: 376-377 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert_router -> stage2_layer1_expert60 [style=dashed]
	stage2_layer1_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 376-377 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert60 -> stage2_layer1_expert60_mlp1
	stage2_layer1_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 376-377\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert60_mlp1 -> stage2_layer1_expert60_tp1
	stage2_layer1_expert60_act [label="Expert 60 GELU Activation\nGPU: 376-377\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert60_tp1 -> stage2_layer1_expert60_act
	stage2_layer1_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 376-377 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert60_act -> stage2_layer1_expert60_mlp2
	stage2_layer1_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 376-377\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert60_mlp2 -> stage2_layer1_expert60_tp2
	stage2_layer1_expert61 [label="Layer 5 Expert 61\nGPU: 378-379 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert_router -> stage2_layer1_expert61 [style=dashed]
	stage2_layer1_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 378-379 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert61 -> stage2_layer1_expert61_mlp1
	stage2_layer1_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 378-379\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert61_mlp1 -> stage2_layer1_expert61_tp1
	stage2_layer1_expert61_act [label="Expert 61 GELU Activation\nGPU: 378-379\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert61_tp1 -> stage2_layer1_expert61_act
	stage2_layer1_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 378-379 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert61_act -> stage2_layer1_expert61_mlp2
	stage2_layer1_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 378-379\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert61_mlp2 -> stage2_layer1_expert61_tp2
	stage2_layer1_expert62 [label="Layer 5 Expert 62\nGPU: 380-381 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert_router -> stage2_layer1_expert62 [style=dashed]
	stage2_layer1_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 380-381 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert62 -> stage2_layer1_expert62_mlp1
	stage2_layer1_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 380-381\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert62_mlp1 -> stage2_layer1_expert62_tp1
	stage2_layer1_expert62_act [label="Expert 62 GELU Activation\nGPU: 380-381\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert62_tp1 -> stage2_layer1_expert62_act
	stage2_layer1_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 380-381 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert62_act -> stage2_layer1_expert62_mlp2
	stage2_layer1_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 380-381\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert62_mlp2 -> stage2_layer1_expert62_tp2
	stage2_layer1_expert63 [label="Layer 5 Expert 63\nGPU: 382-383 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert_router -> stage2_layer1_expert63 [style=dashed]
	stage2_layer1_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 382-383 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert63 -> stage2_layer1_expert63_mlp1
	stage2_layer1_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 382-383\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert63_mlp1 -> stage2_layer1_expert63_tp1
	stage2_layer1_expert63_act [label="Expert 63 GELU Activation\nGPU: 382-383\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert63_tp1 -> stage2_layer1_expert63_act
	stage2_layer1_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 382-383 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage2_layer1_expert63_act -> stage2_layer1_expert63_mlp2
	stage2_layer1_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 382-383\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage2_layer1_expert63_mlp2 -> stage2_layer1_expert63_tp2
	stage2_layer1_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 2 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage2_layer1_expert_router -> stage2_layer1_expert_ellipsis [style=dashed]
	stage2_layer1_expert_agg [label="Layer 5 Expert Aggregation\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage2_layer1_expert0_tp2 -> stage2_layer1_expert_agg
	stage2_layer1_expert1_tp2 -> stage2_layer1_expert_agg
	stage2_layer1_expert2_tp2 -> stage2_layer1_expert_agg
	stage2_layer1_expert3_tp2 -> stage2_layer1_expert_agg
	stage2_layer1_expert31_tp2 -> stage2_layer1_expert_agg
	stage2_layer1_expert32_tp2 -> stage2_layer1_expert_agg
	stage2_layer1_expert60_tp2 -> stage2_layer1_expert_agg
	stage2_layer1_expert61_tp2 -> stage2_layer1_expert_agg
	stage2_layer1_expert62_tp2 -> stage2_layer1_expert_agg
	stage2_layer1_expert63_tp2 -> stage2_layer1_expert_agg
	stage2_layer1_expert_ellipsis -> stage2_layer1_expert_agg
	stage2_layer1_output [label="Layer 5 Output\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage2_layer1_expert_agg -> stage2_layer1_output
	stage2_output [label="Stage 2 Output\nGPU: Pipeline 2\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage2_layer1_output -> stage2_output
	stage3_input [label="Stage 3 Input Router\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage2_output -> stage3_input
	stage3_layer0_ln [label="Layer 6 LayerNorm\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage3_input -> stage3_layer0_ln
	stage3_layer0_attn_qkv [label="Layer 6 Attention QKV Projection\nGPU: Pipeline 3 (TP: GPU 384-511)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage3_layer0_ln -> stage3_layer0_attn_qkv
	stage3_layer0_tp_qkv_comm [label="Layer 6 TP QKV All-Gather\nGPU: Pipeline 3 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage3_layer0_attn_qkv -> stage3_layer0_tp_qkv_comm
	stage3_layer0_attn_head0 [label="Layer 6 Head 0 Attention\nGPU: Pipeline 3 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head0
	stage3_layer0_attn_head1 [label="Layer 6 Head 1 Attention\nGPU: Pipeline 3 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head1
	stage3_layer0_attn_head2 [label="Layer 6 Head 2 Attention\nGPU: Pipeline 3 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head2
	stage3_layer0_attn_head3 [label="Layer 6 Head 3 Attention\nGPU: Pipeline 3 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head3
	stage3_layer0_attn_head4 [label="Layer 6 Head 4 Attention\nGPU: Pipeline 3 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head4
	stage3_layer0_attn_head5 [label="Layer 6 Head 5 Attention\nGPU: Pipeline 3 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head5
	stage3_layer0_attn_head6 [label="Layer 6 Head 6 Attention\nGPU: Pipeline 3 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head6
	stage3_layer0_attn_head7 [label="Layer 6 Head 7 Attention\nGPU: Pipeline 3 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head7
	stage3_layer0_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_ellipsis
	stage3_layer0_attn_head24 [label="Layer 6 Head 24 Attention\nGPU: Pipeline 3 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head24
	stage3_layer0_attn_head25 [label="Layer 6 Head 25 Attention\nGPU: Pipeline 3 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head25
	stage3_layer0_attn_head26 [label="Layer 6 Head 26 Attention\nGPU: Pipeline 3 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head26
	stage3_layer0_attn_head27 [label="Layer 6 Head 27 Attention\nGPU: Pipeline 3 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head27
	stage3_layer0_attn_head28 [label="Layer 6 Head 28 Attention\nGPU: Pipeline 3 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head28
	stage3_layer0_attn_head29 [label="Layer 6 Head 29 Attention\nGPU: Pipeline 3 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head29
	stage3_layer0_attn_head30 [label="Layer 6 Head 30 Attention\nGPU: Pipeline 3 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head30
	stage3_layer0_attn_head31 [label="Layer 6 Head 31 Attention\nGPU: Pipeline 3 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_tp_qkv_comm -> stage3_layer0_attn_head31
	stage3_layer0_attn_agg [label="Layer 6 Attention Aggregate\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage3_layer0_attn_head0 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head1 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head2 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head3 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head4 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head5 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head6 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head7 -> stage3_layer0_attn_agg
	stage3_layer0_attn_ellipsis -> stage3_layer0_attn_agg
	stage3_layer0_attn_head24 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head25 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head26 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head27 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head28 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head29 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head30 -> stage3_layer0_attn_agg
	stage3_layer0_attn_head31 -> stage3_layer0_attn_agg
	stage3_layer0_attn_out [label="Layer 6 Attention Output Proj\nGPU: Pipeline 3 (TP: GPU 384-511)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage3_layer0_attn_agg -> stage3_layer0_attn_out
	stage3_layer0_tp_attn_comm [label="Layer 6 TP Attention All-Reduce\nGPU: Pipeline 3 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage3_layer0_attn_out -> stage3_layer0_tp_attn_comm
	stage3_layer0_expert_router [label="Layer 6 Expert Router (Top-k)\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage3_layer0_tp_attn_comm -> stage3_layer0_expert_router
	stage3_layer0_expert0 [label="Layer 6 Expert 0\nGPU: 384-385 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert_router -> stage3_layer0_expert0 [style=dashed]
	stage3_layer0_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 384-385 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert0 -> stage3_layer0_expert0_mlp1
	stage3_layer0_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 384-385\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert0_mlp1 -> stage3_layer0_expert0_tp1
	stage3_layer0_expert0_act [label="Expert 0 GELU Activation\nGPU: 384-385\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert0_tp1 -> stage3_layer0_expert0_act
	stage3_layer0_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 384-385 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert0_act -> stage3_layer0_expert0_mlp2
	stage3_layer0_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 384-385\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert0_mlp2 -> stage3_layer0_expert0_tp2
	stage3_layer0_expert1 [label="Layer 6 Expert 1\nGPU: 386-387 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert_router -> stage3_layer0_expert1 [style=dashed]
	stage3_layer0_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 386-387 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert1 -> stage3_layer0_expert1_mlp1
	stage3_layer0_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 386-387\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert1_mlp1 -> stage3_layer0_expert1_tp1
	stage3_layer0_expert1_act [label="Expert 1 GELU Activation\nGPU: 386-387\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert1_tp1 -> stage3_layer0_expert1_act
	stage3_layer0_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 386-387 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert1_act -> stage3_layer0_expert1_mlp2
	stage3_layer0_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 386-387\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert1_mlp2 -> stage3_layer0_expert1_tp2
	stage3_layer0_expert2 [label="Layer 6 Expert 2\nGPU: 388-389 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert_router -> stage3_layer0_expert2 [style=dashed]
	stage3_layer0_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 388-389 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert2 -> stage3_layer0_expert2_mlp1
	stage3_layer0_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 388-389\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert2_mlp1 -> stage3_layer0_expert2_tp1
	stage3_layer0_expert2_act [label="Expert 2 GELU Activation\nGPU: 388-389\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert2_tp1 -> stage3_layer0_expert2_act
	stage3_layer0_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 388-389 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert2_act -> stage3_layer0_expert2_mlp2
	stage3_layer0_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 388-389\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert2_mlp2 -> stage3_layer0_expert2_tp2
	stage3_layer0_expert3 [label="Layer 6 Expert 3\nGPU: 390-391 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert_router -> stage3_layer0_expert3 [style=dashed]
	stage3_layer0_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 390-391 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert3 -> stage3_layer0_expert3_mlp1
	stage3_layer0_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 390-391\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert3_mlp1 -> stage3_layer0_expert3_tp1
	stage3_layer0_expert3_act [label="Expert 3 GELU Activation\nGPU: 390-391\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert3_tp1 -> stage3_layer0_expert3_act
	stage3_layer0_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 390-391 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert3_act -> stage3_layer0_expert3_mlp2
	stage3_layer0_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 390-391\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert3_mlp2 -> stage3_layer0_expert3_tp2
	stage3_layer0_expert31 [label="Layer 6 Expert 31\nGPU: 446-447 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert_router -> stage3_layer0_expert31 [style=dashed]
	stage3_layer0_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 446-447 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert31 -> stage3_layer0_expert31_mlp1
	stage3_layer0_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 446-447\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert31_mlp1 -> stage3_layer0_expert31_tp1
	stage3_layer0_expert31_act [label="Expert 31 GELU Activation\nGPU: 446-447\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert31_tp1 -> stage3_layer0_expert31_act
	stage3_layer0_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 446-447 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert31_act -> stage3_layer0_expert31_mlp2
	stage3_layer0_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 446-447\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert31_mlp2 -> stage3_layer0_expert31_tp2
	stage3_layer0_expert32 [label="Layer 6 Expert 32\nGPU: 448-449 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert_router -> stage3_layer0_expert32 [style=dashed]
	stage3_layer0_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 448-449 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert32 -> stage3_layer0_expert32_mlp1
	stage3_layer0_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 448-449\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert32_mlp1 -> stage3_layer0_expert32_tp1
	stage3_layer0_expert32_act [label="Expert 32 GELU Activation\nGPU: 448-449\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert32_tp1 -> stage3_layer0_expert32_act
	stage3_layer0_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 448-449 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert32_act -> stage3_layer0_expert32_mlp2
	stage3_layer0_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 448-449\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert32_mlp2 -> stage3_layer0_expert32_tp2
	stage3_layer0_expert60 [label="Layer 6 Expert 60\nGPU: 504-505 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert_router -> stage3_layer0_expert60 [style=dashed]
	stage3_layer0_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 504-505 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert60 -> stage3_layer0_expert60_mlp1
	stage3_layer0_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 504-505\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert60_mlp1 -> stage3_layer0_expert60_tp1
	stage3_layer0_expert60_act [label="Expert 60 GELU Activation\nGPU: 504-505\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert60_tp1 -> stage3_layer0_expert60_act
	stage3_layer0_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 504-505 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert60_act -> stage3_layer0_expert60_mlp2
	stage3_layer0_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 504-505\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert60_mlp2 -> stage3_layer0_expert60_tp2
	stage3_layer0_expert61 [label="Layer 6 Expert 61\nGPU: 506-507 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert_router -> stage3_layer0_expert61 [style=dashed]
	stage3_layer0_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 506-507 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert61 -> stage3_layer0_expert61_mlp1
	stage3_layer0_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 506-507\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert61_mlp1 -> stage3_layer0_expert61_tp1
	stage3_layer0_expert61_act [label="Expert 61 GELU Activation\nGPU: 506-507\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert61_tp1 -> stage3_layer0_expert61_act
	stage3_layer0_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 506-507 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert61_act -> stage3_layer0_expert61_mlp2
	stage3_layer0_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 506-507\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert61_mlp2 -> stage3_layer0_expert61_tp2
	stage3_layer0_expert62 [label="Layer 6 Expert 62\nGPU: 508-509 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert_router -> stage3_layer0_expert62 [style=dashed]
	stage3_layer0_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 508-509 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert62 -> stage3_layer0_expert62_mlp1
	stage3_layer0_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 508-509\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert62_mlp1 -> stage3_layer0_expert62_tp1
	stage3_layer0_expert62_act [label="Expert 62 GELU Activation\nGPU: 508-509\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert62_tp1 -> stage3_layer0_expert62_act
	stage3_layer0_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 508-509 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert62_act -> stage3_layer0_expert62_mlp2
	stage3_layer0_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 508-509\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert62_mlp2 -> stage3_layer0_expert62_tp2
	stage3_layer0_expert63 [label="Layer 6 Expert 63\nGPU: 510-511 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert_router -> stage3_layer0_expert63 [style=dashed]
	stage3_layer0_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 510-511 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert63 -> stage3_layer0_expert63_mlp1
	stage3_layer0_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 510-511\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert63_mlp1 -> stage3_layer0_expert63_tp1
	stage3_layer0_expert63_act [label="Expert 63 GELU Activation\nGPU: 510-511\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert63_tp1 -> stage3_layer0_expert63_act
	stage3_layer0_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 510-511 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer0_expert63_act -> stage3_layer0_expert63_mlp2
	stage3_layer0_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 510-511\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer0_expert63_mlp2 -> stage3_layer0_expert63_tp2
	stage3_layer0_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 3 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage3_layer0_expert_router -> stage3_layer0_expert_ellipsis [style=dashed]
	stage3_layer0_expert_agg [label="Layer 6 Expert Aggregation\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage3_layer0_expert0_tp2 -> stage3_layer0_expert_agg
	stage3_layer0_expert1_tp2 -> stage3_layer0_expert_agg
	stage3_layer0_expert2_tp2 -> stage3_layer0_expert_agg
	stage3_layer0_expert3_tp2 -> stage3_layer0_expert_agg
	stage3_layer0_expert31_tp2 -> stage3_layer0_expert_agg
	stage3_layer0_expert32_tp2 -> stage3_layer0_expert_agg
	stage3_layer0_expert60_tp2 -> stage3_layer0_expert_agg
	stage3_layer0_expert61_tp2 -> stage3_layer0_expert_agg
	stage3_layer0_expert62_tp2 -> stage3_layer0_expert_agg
	stage3_layer0_expert63_tp2 -> stage3_layer0_expert_agg
	stage3_layer0_expert_ellipsis -> stage3_layer0_expert_agg
	stage3_layer0_output [label="Layer 6 Output\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage3_layer0_expert_agg -> stage3_layer0_output
	stage3_layer0_tp_attn_comm -> stage3_layer0_output [style=invis]
	stage3_layer1_ln [label="Layer 7 LayerNorm\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage3_layer0_output -> stage3_layer1_ln
	stage3_layer1_attn_qkv [label="Layer 7 Attention QKV Projection\nGPU: Pipeline 3 (TP: GPU 384-511)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage3_layer1_ln -> stage3_layer1_attn_qkv
	stage3_layer1_tp_qkv_comm [label="Layer 7 TP QKV All-Gather\nGPU: Pipeline 3 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage3_layer1_attn_qkv -> stage3_layer1_tp_qkv_comm
	stage3_layer1_attn_head0 [label="Layer 7 Head 0 Attention\nGPU: Pipeline 3 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head0
	stage3_layer1_attn_head1 [label="Layer 7 Head 1 Attention\nGPU: Pipeline 3 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head1
	stage3_layer1_attn_head2 [label="Layer 7 Head 2 Attention\nGPU: Pipeline 3 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head2
	stage3_layer1_attn_head3 [label="Layer 7 Head 3 Attention\nGPU: Pipeline 3 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head3
	stage3_layer1_attn_head4 [label="Layer 7 Head 4 Attention\nGPU: Pipeline 3 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head4
	stage3_layer1_attn_head5 [label="Layer 7 Head 5 Attention\nGPU: Pipeline 3 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head5
	stage3_layer1_attn_head6 [label="Layer 7 Head 6 Attention\nGPU: Pipeline 3 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head6
	stage3_layer1_attn_head7 [label="Layer 7 Head 7 Attention\nGPU: Pipeline 3 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head7
	stage3_layer1_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_ellipsis
	stage3_layer1_attn_head24 [label="Layer 7 Head 24 Attention\nGPU: Pipeline 3 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head24
	stage3_layer1_attn_head25 [label="Layer 7 Head 25 Attention\nGPU: Pipeline 3 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head25
	stage3_layer1_attn_head26 [label="Layer 7 Head 26 Attention\nGPU: Pipeline 3 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head26
	stage3_layer1_attn_head27 [label="Layer 7 Head 27 Attention\nGPU: Pipeline 3 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head27
	stage3_layer1_attn_head28 [label="Layer 7 Head 28 Attention\nGPU: Pipeline 3 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head28
	stage3_layer1_attn_head29 [label="Layer 7 Head 29 Attention\nGPU: Pipeline 3 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head29
	stage3_layer1_attn_head30 [label="Layer 7 Head 30 Attention\nGPU: Pipeline 3 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head30
	stage3_layer1_attn_head31 [label="Layer 7 Head 31 Attention\nGPU: Pipeline 3 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_tp_qkv_comm -> stage3_layer1_attn_head31
	stage3_layer1_attn_agg [label="Layer 7 Attention Aggregate\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage3_layer1_attn_head0 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head1 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head2 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head3 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head4 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head5 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head6 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head7 -> stage3_layer1_attn_agg
	stage3_layer1_attn_ellipsis -> stage3_layer1_attn_agg
	stage3_layer1_attn_head24 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head25 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head26 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head27 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head28 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head29 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head30 -> stage3_layer1_attn_agg
	stage3_layer1_attn_head31 -> stage3_layer1_attn_agg
	stage3_layer1_attn_out [label="Layer 7 Attention Output Proj\nGPU: Pipeline 3 (TP: GPU 384-511)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage3_layer1_attn_agg -> stage3_layer1_attn_out
	stage3_layer1_tp_attn_comm [label="Layer 7 TP Attention All-Reduce\nGPU: Pipeline 3 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage3_layer1_attn_out -> stage3_layer1_tp_attn_comm
	stage3_layer1_expert_router [label="Layer 7 Expert Router (Top-k)\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage3_layer1_tp_attn_comm -> stage3_layer1_expert_router
	stage3_layer1_expert0 [label="Layer 7 Expert 0\nGPU: 384-385 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert_router -> stage3_layer1_expert0 [style=dashed]
	stage3_layer1_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 384-385 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert0 -> stage3_layer1_expert0_mlp1
	stage3_layer1_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 384-385\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert0_mlp1 -> stage3_layer1_expert0_tp1
	stage3_layer1_expert0_act [label="Expert 0 GELU Activation\nGPU: 384-385\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert0_tp1 -> stage3_layer1_expert0_act
	stage3_layer1_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 384-385 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert0_act -> stage3_layer1_expert0_mlp2
	stage3_layer1_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 384-385\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert0_mlp2 -> stage3_layer1_expert0_tp2
	stage3_layer1_expert1 [label="Layer 7 Expert 1\nGPU: 386-387 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert_router -> stage3_layer1_expert1 [style=dashed]
	stage3_layer1_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 386-387 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert1 -> stage3_layer1_expert1_mlp1
	stage3_layer1_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 386-387\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert1_mlp1 -> stage3_layer1_expert1_tp1
	stage3_layer1_expert1_act [label="Expert 1 GELU Activation\nGPU: 386-387\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert1_tp1 -> stage3_layer1_expert1_act
	stage3_layer1_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 386-387 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert1_act -> stage3_layer1_expert1_mlp2
	stage3_layer1_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 386-387\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert1_mlp2 -> stage3_layer1_expert1_tp2
	stage3_layer1_expert2 [label="Layer 7 Expert 2\nGPU: 388-389 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert_router -> stage3_layer1_expert2 [style=dashed]
	stage3_layer1_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 388-389 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert2 -> stage3_layer1_expert2_mlp1
	stage3_layer1_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 388-389\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert2_mlp1 -> stage3_layer1_expert2_tp1
	stage3_layer1_expert2_act [label="Expert 2 GELU Activation\nGPU: 388-389\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert2_tp1 -> stage3_layer1_expert2_act
	stage3_layer1_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 388-389 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert2_act -> stage3_layer1_expert2_mlp2
	stage3_layer1_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 388-389\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert2_mlp2 -> stage3_layer1_expert2_tp2
	stage3_layer1_expert3 [label="Layer 7 Expert 3\nGPU: 390-391 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert_router -> stage3_layer1_expert3 [style=dashed]
	stage3_layer1_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 390-391 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert3 -> stage3_layer1_expert3_mlp1
	stage3_layer1_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 390-391\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert3_mlp1 -> stage3_layer1_expert3_tp1
	stage3_layer1_expert3_act [label="Expert 3 GELU Activation\nGPU: 390-391\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert3_tp1 -> stage3_layer1_expert3_act
	stage3_layer1_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 390-391 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert3_act -> stage3_layer1_expert3_mlp2
	stage3_layer1_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 390-391\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert3_mlp2 -> stage3_layer1_expert3_tp2
	stage3_layer1_expert31 [label="Layer 7 Expert 31\nGPU: 446-447 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert_router -> stage3_layer1_expert31 [style=dashed]
	stage3_layer1_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 446-447 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert31 -> stage3_layer1_expert31_mlp1
	stage3_layer1_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 446-447\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert31_mlp1 -> stage3_layer1_expert31_tp1
	stage3_layer1_expert31_act [label="Expert 31 GELU Activation\nGPU: 446-447\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert31_tp1 -> stage3_layer1_expert31_act
	stage3_layer1_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 446-447 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert31_act -> stage3_layer1_expert31_mlp2
	stage3_layer1_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 446-447\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert31_mlp2 -> stage3_layer1_expert31_tp2
	stage3_layer1_expert32 [label="Layer 7 Expert 32\nGPU: 448-449 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert_router -> stage3_layer1_expert32 [style=dashed]
	stage3_layer1_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 448-449 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert32 -> stage3_layer1_expert32_mlp1
	stage3_layer1_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 448-449\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert32_mlp1 -> stage3_layer1_expert32_tp1
	stage3_layer1_expert32_act [label="Expert 32 GELU Activation\nGPU: 448-449\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert32_tp1 -> stage3_layer1_expert32_act
	stage3_layer1_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 448-449 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert32_act -> stage3_layer1_expert32_mlp2
	stage3_layer1_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 448-449\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert32_mlp2 -> stage3_layer1_expert32_tp2
	stage3_layer1_expert60 [label="Layer 7 Expert 60\nGPU: 504-505 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert_router -> stage3_layer1_expert60 [style=dashed]
	stage3_layer1_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 504-505 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert60 -> stage3_layer1_expert60_mlp1
	stage3_layer1_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 504-505\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert60_mlp1 -> stage3_layer1_expert60_tp1
	stage3_layer1_expert60_act [label="Expert 60 GELU Activation\nGPU: 504-505\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert60_tp1 -> stage3_layer1_expert60_act
	stage3_layer1_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 504-505 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert60_act -> stage3_layer1_expert60_mlp2
	stage3_layer1_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 504-505\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert60_mlp2 -> stage3_layer1_expert60_tp2
	stage3_layer1_expert61 [label="Layer 7 Expert 61\nGPU: 506-507 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert_router -> stage3_layer1_expert61 [style=dashed]
	stage3_layer1_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 506-507 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert61 -> stage3_layer1_expert61_mlp1
	stage3_layer1_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 506-507\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert61_mlp1 -> stage3_layer1_expert61_tp1
	stage3_layer1_expert61_act [label="Expert 61 GELU Activation\nGPU: 506-507\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert61_tp1 -> stage3_layer1_expert61_act
	stage3_layer1_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 506-507 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert61_act -> stage3_layer1_expert61_mlp2
	stage3_layer1_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 506-507\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert61_mlp2 -> stage3_layer1_expert61_tp2
	stage3_layer1_expert62 [label="Layer 7 Expert 62\nGPU: 508-509 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert_router -> stage3_layer1_expert62 [style=dashed]
	stage3_layer1_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 508-509 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert62 -> stage3_layer1_expert62_mlp1
	stage3_layer1_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 508-509\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert62_mlp1 -> stage3_layer1_expert62_tp1
	stage3_layer1_expert62_act [label="Expert 62 GELU Activation\nGPU: 508-509\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert62_tp1 -> stage3_layer1_expert62_act
	stage3_layer1_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 508-509 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert62_act -> stage3_layer1_expert62_mlp2
	stage3_layer1_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 508-509\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert62_mlp2 -> stage3_layer1_expert62_tp2
	stage3_layer1_expert63 [label="Layer 7 Expert 63\nGPU: 510-511 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert_router -> stage3_layer1_expert63 [style=dashed]
	stage3_layer1_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 510-511 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert63 -> stage3_layer1_expert63_mlp1
	stage3_layer1_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 510-511\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert63_mlp1 -> stage3_layer1_expert63_tp1
	stage3_layer1_expert63_act [label="Expert 63 GELU Activation\nGPU: 510-511\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert63_tp1 -> stage3_layer1_expert63_act
	stage3_layer1_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 510-511 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage3_layer1_expert63_act -> stage3_layer1_expert63_mlp2
	stage3_layer1_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 510-511\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage3_layer1_expert63_mlp2 -> stage3_layer1_expert63_tp2
	stage3_layer1_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 3 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage3_layer1_expert_router -> stage3_layer1_expert_ellipsis [style=dashed]
	stage3_layer1_expert_agg [label="Layer 7 Expert Aggregation\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage3_layer1_expert0_tp2 -> stage3_layer1_expert_agg
	stage3_layer1_expert1_tp2 -> stage3_layer1_expert_agg
	stage3_layer1_expert2_tp2 -> stage3_layer1_expert_agg
	stage3_layer1_expert3_tp2 -> stage3_layer1_expert_agg
	stage3_layer1_expert31_tp2 -> stage3_layer1_expert_agg
	stage3_layer1_expert32_tp2 -> stage3_layer1_expert_agg
	stage3_layer1_expert60_tp2 -> stage3_layer1_expert_agg
	stage3_layer1_expert61_tp2 -> stage3_layer1_expert_agg
	stage3_layer1_expert62_tp2 -> stage3_layer1_expert_agg
	stage3_layer1_expert63_tp2 -> stage3_layer1_expert_agg
	stage3_layer1_expert_ellipsis -> stage3_layer1_expert_agg
	stage3_layer1_output [label="Layer 7 Output\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage3_layer1_expert_agg -> stage3_layer1_output
	stage3_output [label="Stage 3 Output\nGPU: Pipeline 3\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage3_layer1_output -> stage3_output
	stage4_input [label="Stage 4 Input Router\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage3_output -> stage4_input
	stage4_layer0_ln [label="Layer 8 LayerNorm\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage4_input -> stage4_layer0_ln
	stage4_layer0_attn_qkv [label="Layer 8 Attention QKV Projection\nGPU: Pipeline 4 (TP: GPU 512-639)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage4_layer0_ln -> stage4_layer0_attn_qkv
	stage4_layer0_tp_qkv_comm [label="Layer 8 TP QKV All-Gather\nGPU: Pipeline 4 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage4_layer0_attn_qkv -> stage4_layer0_tp_qkv_comm
	stage4_layer0_attn_head0 [label="Layer 8 Head 0 Attention\nGPU: Pipeline 4 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head0
	stage4_layer0_attn_head1 [label="Layer 8 Head 1 Attention\nGPU: Pipeline 4 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head1
	stage4_layer0_attn_head2 [label="Layer 8 Head 2 Attention\nGPU: Pipeline 4 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head2
	stage4_layer0_attn_head3 [label="Layer 8 Head 3 Attention\nGPU: Pipeline 4 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head3
	stage4_layer0_attn_head4 [label="Layer 8 Head 4 Attention\nGPU: Pipeline 4 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head4
	stage4_layer0_attn_head5 [label="Layer 8 Head 5 Attention\nGPU: Pipeline 4 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head5
	stage4_layer0_attn_head6 [label="Layer 8 Head 6 Attention\nGPU: Pipeline 4 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head6
	stage4_layer0_attn_head7 [label="Layer 8 Head 7 Attention\nGPU: Pipeline 4 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head7
	stage4_layer0_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_ellipsis
	stage4_layer0_attn_head24 [label="Layer 8 Head 24 Attention\nGPU: Pipeline 4 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head24
	stage4_layer0_attn_head25 [label="Layer 8 Head 25 Attention\nGPU: Pipeline 4 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head25
	stage4_layer0_attn_head26 [label="Layer 8 Head 26 Attention\nGPU: Pipeline 4 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head26
	stage4_layer0_attn_head27 [label="Layer 8 Head 27 Attention\nGPU: Pipeline 4 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head27
	stage4_layer0_attn_head28 [label="Layer 8 Head 28 Attention\nGPU: Pipeline 4 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head28
	stage4_layer0_attn_head29 [label="Layer 8 Head 29 Attention\nGPU: Pipeline 4 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head29
	stage4_layer0_attn_head30 [label="Layer 8 Head 30 Attention\nGPU: Pipeline 4 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head30
	stage4_layer0_attn_head31 [label="Layer 8 Head 31 Attention\nGPU: Pipeline 4 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_tp_qkv_comm -> stage4_layer0_attn_head31
	stage4_layer0_attn_agg [label="Layer 8 Attention Aggregate\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage4_layer0_attn_head0 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head1 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head2 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head3 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head4 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head5 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head6 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head7 -> stage4_layer0_attn_agg
	stage4_layer0_attn_ellipsis -> stage4_layer0_attn_agg
	stage4_layer0_attn_head24 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head25 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head26 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head27 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head28 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head29 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head30 -> stage4_layer0_attn_agg
	stage4_layer0_attn_head31 -> stage4_layer0_attn_agg
	stage4_layer0_attn_out [label="Layer 8 Attention Output Proj\nGPU: Pipeline 4 (TP: GPU 512-639)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage4_layer0_attn_agg -> stage4_layer0_attn_out
	stage4_layer0_tp_attn_comm [label="Layer 8 TP Attention All-Reduce\nGPU: Pipeline 4 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage4_layer0_attn_out -> stage4_layer0_tp_attn_comm
	stage4_layer0_expert_router [label="Layer 8 Expert Router (Top-k)\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage4_layer0_tp_attn_comm -> stage4_layer0_expert_router
	stage4_layer0_expert0 [label="Layer 8 Expert 0\nGPU: 512-513 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert_router -> stage4_layer0_expert0 [style=dashed]
	stage4_layer0_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 512-513 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert0 -> stage4_layer0_expert0_mlp1
	stage4_layer0_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 512-513\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert0_mlp1 -> stage4_layer0_expert0_tp1
	stage4_layer0_expert0_act [label="Expert 0 GELU Activation\nGPU: 512-513\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert0_tp1 -> stage4_layer0_expert0_act
	stage4_layer0_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 512-513 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert0_act -> stage4_layer0_expert0_mlp2
	stage4_layer0_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 512-513\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert0_mlp2 -> stage4_layer0_expert0_tp2
	stage4_layer0_expert1 [label="Layer 8 Expert 1\nGPU: 514-515 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert_router -> stage4_layer0_expert1 [style=dashed]
	stage4_layer0_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 514-515 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert1 -> stage4_layer0_expert1_mlp1
	stage4_layer0_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 514-515\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert1_mlp1 -> stage4_layer0_expert1_tp1
	stage4_layer0_expert1_act [label="Expert 1 GELU Activation\nGPU: 514-515\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert1_tp1 -> stage4_layer0_expert1_act
	stage4_layer0_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 514-515 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert1_act -> stage4_layer0_expert1_mlp2
	stage4_layer0_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 514-515\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert1_mlp2 -> stage4_layer0_expert1_tp2
	stage4_layer0_expert2 [label="Layer 8 Expert 2\nGPU: 516-517 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert_router -> stage4_layer0_expert2 [style=dashed]
	stage4_layer0_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 516-517 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert2 -> stage4_layer0_expert2_mlp1
	stage4_layer0_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 516-517\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert2_mlp1 -> stage4_layer0_expert2_tp1
	stage4_layer0_expert2_act [label="Expert 2 GELU Activation\nGPU: 516-517\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert2_tp1 -> stage4_layer0_expert2_act
	stage4_layer0_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 516-517 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert2_act -> stage4_layer0_expert2_mlp2
	stage4_layer0_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 516-517\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert2_mlp2 -> stage4_layer0_expert2_tp2
	stage4_layer0_expert3 [label="Layer 8 Expert 3\nGPU: 518-519 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert_router -> stage4_layer0_expert3 [style=dashed]
	stage4_layer0_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 518-519 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert3 -> stage4_layer0_expert3_mlp1
	stage4_layer0_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 518-519\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert3_mlp1 -> stage4_layer0_expert3_tp1
	stage4_layer0_expert3_act [label="Expert 3 GELU Activation\nGPU: 518-519\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert3_tp1 -> stage4_layer0_expert3_act
	stage4_layer0_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 518-519 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert3_act -> stage4_layer0_expert3_mlp2
	stage4_layer0_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 518-519\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert3_mlp2 -> stage4_layer0_expert3_tp2
	stage4_layer0_expert31 [label="Layer 8 Expert 31\nGPU: 574-575 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert_router -> stage4_layer0_expert31 [style=dashed]
	stage4_layer0_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 574-575 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert31 -> stage4_layer0_expert31_mlp1
	stage4_layer0_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 574-575\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert31_mlp1 -> stage4_layer0_expert31_tp1
	stage4_layer0_expert31_act [label="Expert 31 GELU Activation\nGPU: 574-575\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert31_tp1 -> stage4_layer0_expert31_act
	stage4_layer0_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 574-575 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert31_act -> stage4_layer0_expert31_mlp2
	stage4_layer0_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 574-575\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert31_mlp2 -> stage4_layer0_expert31_tp2
	stage4_layer0_expert32 [label="Layer 8 Expert 32\nGPU: 576-577 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert_router -> stage4_layer0_expert32 [style=dashed]
	stage4_layer0_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 576-577 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert32 -> stage4_layer0_expert32_mlp1
	stage4_layer0_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 576-577\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert32_mlp1 -> stage4_layer0_expert32_tp1
	stage4_layer0_expert32_act [label="Expert 32 GELU Activation\nGPU: 576-577\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert32_tp1 -> stage4_layer0_expert32_act
	stage4_layer0_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 576-577 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert32_act -> stage4_layer0_expert32_mlp2
	stage4_layer0_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 576-577\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert32_mlp2 -> stage4_layer0_expert32_tp2
	stage4_layer0_expert60 [label="Layer 8 Expert 60\nGPU: 632-633 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert_router -> stage4_layer0_expert60 [style=dashed]
	stage4_layer0_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 632-633 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert60 -> stage4_layer0_expert60_mlp1
	stage4_layer0_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 632-633\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert60_mlp1 -> stage4_layer0_expert60_tp1
	stage4_layer0_expert60_act [label="Expert 60 GELU Activation\nGPU: 632-633\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert60_tp1 -> stage4_layer0_expert60_act
	stage4_layer0_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 632-633 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert60_act -> stage4_layer0_expert60_mlp2
	stage4_layer0_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 632-633\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert60_mlp2 -> stage4_layer0_expert60_tp2
	stage4_layer0_expert61 [label="Layer 8 Expert 61\nGPU: 634-635 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert_router -> stage4_layer0_expert61 [style=dashed]
	stage4_layer0_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 634-635 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert61 -> stage4_layer0_expert61_mlp1
	stage4_layer0_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 634-635\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert61_mlp1 -> stage4_layer0_expert61_tp1
	stage4_layer0_expert61_act [label="Expert 61 GELU Activation\nGPU: 634-635\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert61_tp1 -> stage4_layer0_expert61_act
	stage4_layer0_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 634-635 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert61_act -> stage4_layer0_expert61_mlp2
	stage4_layer0_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 634-635\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert61_mlp2 -> stage4_layer0_expert61_tp2
	stage4_layer0_expert62 [label="Layer 8 Expert 62\nGPU: 636-637 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert_router -> stage4_layer0_expert62 [style=dashed]
	stage4_layer0_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 636-637 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert62 -> stage4_layer0_expert62_mlp1
	stage4_layer0_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 636-637\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert62_mlp1 -> stage4_layer0_expert62_tp1
	stage4_layer0_expert62_act [label="Expert 62 GELU Activation\nGPU: 636-637\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert62_tp1 -> stage4_layer0_expert62_act
	stage4_layer0_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 636-637 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert62_act -> stage4_layer0_expert62_mlp2
	stage4_layer0_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 636-637\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert62_mlp2 -> stage4_layer0_expert62_tp2
	stage4_layer0_expert63 [label="Layer 8 Expert 63\nGPU: 638-639 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert_router -> stage4_layer0_expert63 [style=dashed]
	stage4_layer0_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 638-639 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert63 -> stage4_layer0_expert63_mlp1
	stage4_layer0_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 638-639\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert63_mlp1 -> stage4_layer0_expert63_tp1
	stage4_layer0_expert63_act [label="Expert 63 GELU Activation\nGPU: 638-639\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert63_tp1 -> stage4_layer0_expert63_act
	stage4_layer0_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 638-639 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer0_expert63_act -> stage4_layer0_expert63_mlp2
	stage4_layer0_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 638-639\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer0_expert63_mlp2 -> stage4_layer0_expert63_tp2
	stage4_layer0_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 4 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage4_layer0_expert_router -> stage4_layer0_expert_ellipsis [style=dashed]
	stage4_layer0_expert_agg [label="Layer 8 Expert Aggregation\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage4_layer0_expert0_tp2 -> stage4_layer0_expert_agg
	stage4_layer0_expert1_tp2 -> stage4_layer0_expert_agg
	stage4_layer0_expert2_tp2 -> stage4_layer0_expert_agg
	stage4_layer0_expert3_tp2 -> stage4_layer0_expert_agg
	stage4_layer0_expert31_tp2 -> stage4_layer0_expert_agg
	stage4_layer0_expert32_tp2 -> stage4_layer0_expert_agg
	stage4_layer0_expert60_tp2 -> stage4_layer0_expert_agg
	stage4_layer0_expert61_tp2 -> stage4_layer0_expert_agg
	stage4_layer0_expert62_tp2 -> stage4_layer0_expert_agg
	stage4_layer0_expert63_tp2 -> stage4_layer0_expert_agg
	stage4_layer0_expert_ellipsis -> stage4_layer0_expert_agg
	stage4_layer0_output [label="Layer 8 Output\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage4_layer0_expert_agg -> stage4_layer0_output
	stage4_layer0_tp_attn_comm -> stage4_layer0_output [style=invis]
	stage4_layer1_ln [label="Layer 9 LayerNorm\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage4_layer0_output -> stage4_layer1_ln
	stage4_layer1_attn_qkv [label="Layer 9 Attention QKV Projection\nGPU: Pipeline 4 (TP: GPU 512-639)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage4_layer1_ln -> stage4_layer1_attn_qkv
	stage4_layer1_tp_qkv_comm [label="Layer 9 TP QKV All-Gather\nGPU: Pipeline 4 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage4_layer1_attn_qkv -> stage4_layer1_tp_qkv_comm
	stage4_layer1_attn_head0 [label="Layer 9 Head 0 Attention\nGPU: Pipeline 4 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head0
	stage4_layer1_attn_head1 [label="Layer 9 Head 1 Attention\nGPU: Pipeline 4 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head1
	stage4_layer1_attn_head2 [label="Layer 9 Head 2 Attention\nGPU: Pipeline 4 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head2
	stage4_layer1_attn_head3 [label="Layer 9 Head 3 Attention\nGPU: Pipeline 4 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head3
	stage4_layer1_attn_head4 [label="Layer 9 Head 4 Attention\nGPU: Pipeline 4 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head4
	stage4_layer1_attn_head5 [label="Layer 9 Head 5 Attention\nGPU: Pipeline 4 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head5
	stage4_layer1_attn_head6 [label="Layer 9 Head 6 Attention\nGPU: Pipeline 4 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head6
	stage4_layer1_attn_head7 [label="Layer 9 Head 7 Attention\nGPU: Pipeline 4 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head7
	stage4_layer1_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_ellipsis
	stage4_layer1_attn_head24 [label="Layer 9 Head 24 Attention\nGPU: Pipeline 4 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head24
	stage4_layer1_attn_head25 [label="Layer 9 Head 25 Attention\nGPU: Pipeline 4 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head25
	stage4_layer1_attn_head26 [label="Layer 9 Head 26 Attention\nGPU: Pipeline 4 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head26
	stage4_layer1_attn_head27 [label="Layer 9 Head 27 Attention\nGPU: Pipeline 4 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head27
	stage4_layer1_attn_head28 [label="Layer 9 Head 28 Attention\nGPU: Pipeline 4 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head28
	stage4_layer1_attn_head29 [label="Layer 9 Head 29 Attention\nGPU: Pipeline 4 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head29
	stage4_layer1_attn_head30 [label="Layer 9 Head 30 Attention\nGPU: Pipeline 4 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head30
	stage4_layer1_attn_head31 [label="Layer 9 Head 31 Attention\nGPU: Pipeline 4 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_tp_qkv_comm -> stage4_layer1_attn_head31
	stage4_layer1_attn_agg [label="Layer 9 Attention Aggregate\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage4_layer1_attn_head0 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head1 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head2 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head3 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head4 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head5 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head6 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head7 -> stage4_layer1_attn_agg
	stage4_layer1_attn_ellipsis -> stage4_layer1_attn_agg
	stage4_layer1_attn_head24 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head25 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head26 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head27 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head28 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head29 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head30 -> stage4_layer1_attn_agg
	stage4_layer1_attn_head31 -> stage4_layer1_attn_agg
	stage4_layer1_attn_out [label="Layer 9 Attention Output Proj\nGPU: Pipeline 4 (TP: GPU 512-639)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage4_layer1_attn_agg -> stage4_layer1_attn_out
	stage4_layer1_tp_attn_comm [label="Layer 9 TP Attention All-Reduce\nGPU: Pipeline 4 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage4_layer1_attn_out -> stage4_layer1_tp_attn_comm
	stage4_layer1_expert_router [label="Layer 9 Expert Router (Top-k)\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage4_layer1_tp_attn_comm -> stage4_layer1_expert_router
	stage4_layer1_expert0 [label="Layer 9 Expert 0\nGPU: 512-513 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert_router -> stage4_layer1_expert0 [style=dashed]
	stage4_layer1_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 512-513 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert0 -> stage4_layer1_expert0_mlp1
	stage4_layer1_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 512-513\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert0_mlp1 -> stage4_layer1_expert0_tp1
	stage4_layer1_expert0_act [label="Expert 0 GELU Activation\nGPU: 512-513\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert0_tp1 -> stage4_layer1_expert0_act
	stage4_layer1_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 512-513 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert0_act -> stage4_layer1_expert0_mlp2
	stage4_layer1_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 512-513\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert0_mlp2 -> stage4_layer1_expert0_tp2
	stage4_layer1_expert1 [label="Layer 9 Expert 1\nGPU: 514-515 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert_router -> stage4_layer1_expert1 [style=dashed]
	stage4_layer1_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 514-515 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert1 -> stage4_layer1_expert1_mlp1
	stage4_layer1_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 514-515\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert1_mlp1 -> stage4_layer1_expert1_tp1
	stage4_layer1_expert1_act [label="Expert 1 GELU Activation\nGPU: 514-515\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert1_tp1 -> stage4_layer1_expert1_act
	stage4_layer1_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 514-515 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert1_act -> stage4_layer1_expert1_mlp2
	stage4_layer1_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 514-515\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert1_mlp2 -> stage4_layer1_expert1_tp2
	stage4_layer1_expert2 [label="Layer 9 Expert 2\nGPU: 516-517 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert_router -> stage4_layer1_expert2 [style=dashed]
	stage4_layer1_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 516-517 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert2 -> stage4_layer1_expert2_mlp1
	stage4_layer1_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 516-517\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert2_mlp1 -> stage4_layer1_expert2_tp1
	stage4_layer1_expert2_act [label="Expert 2 GELU Activation\nGPU: 516-517\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert2_tp1 -> stage4_layer1_expert2_act
	stage4_layer1_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 516-517 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert2_act -> stage4_layer1_expert2_mlp2
	stage4_layer1_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 516-517\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert2_mlp2 -> stage4_layer1_expert2_tp2
	stage4_layer1_expert3 [label="Layer 9 Expert 3\nGPU: 518-519 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert_router -> stage4_layer1_expert3 [style=dashed]
	stage4_layer1_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 518-519 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert3 -> stage4_layer1_expert3_mlp1
	stage4_layer1_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 518-519\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert3_mlp1 -> stage4_layer1_expert3_tp1
	stage4_layer1_expert3_act [label="Expert 3 GELU Activation\nGPU: 518-519\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert3_tp1 -> stage4_layer1_expert3_act
	stage4_layer1_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 518-519 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert3_act -> stage4_layer1_expert3_mlp2
	stage4_layer1_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 518-519\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert3_mlp2 -> stage4_layer1_expert3_tp2
	stage4_layer1_expert31 [label="Layer 9 Expert 31\nGPU: 574-575 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert_router -> stage4_layer1_expert31 [style=dashed]
	stage4_layer1_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 574-575 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert31 -> stage4_layer1_expert31_mlp1
	stage4_layer1_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 574-575\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert31_mlp1 -> stage4_layer1_expert31_tp1
	stage4_layer1_expert31_act [label="Expert 31 GELU Activation\nGPU: 574-575\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert31_tp1 -> stage4_layer1_expert31_act
	stage4_layer1_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 574-575 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert31_act -> stage4_layer1_expert31_mlp2
	stage4_layer1_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 574-575\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert31_mlp2 -> stage4_layer1_expert31_tp2
	stage4_layer1_expert32 [label="Layer 9 Expert 32\nGPU: 576-577 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert_router -> stage4_layer1_expert32 [style=dashed]
	stage4_layer1_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 576-577 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert32 -> stage4_layer1_expert32_mlp1
	stage4_layer1_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 576-577\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert32_mlp1 -> stage4_layer1_expert32_tp1
	stage4_layer1_expert32_act [label="Expert 32 GELU Activation\nGPU: 576-577\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert32_tp1 -> stage4_layer1_expert32_act
	stage4_layer1_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 576-577 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert32_act -> stage4_layer1_expert32_mlp2
	stage4_layer1_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 576-577\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert32_mlp2 -> stage4_layer1_expert32_tp2
	stage4_layer1_expert60 [label="Layer 9 Expert 60\nGPU: 632-633 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert_router -> stage4_layer1_expert60 [style=dashed]
	stage4_layer1_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 632-633 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert60 -> stage4_layer1_expert60_mlp1
	stage4_layer1_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 632-633\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert60_mlp1 -> stage4_layer1_expert60_tp1
	stage4_layer1_expert60_act [label="Expert 60 GELU Activation\nGPU: 632-633\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert60_tp1 -> stage4_layer1_expert60_act
	stage4_layer1_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 632-633 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert60_act -> stage4_layer1_expert60_mlp2
	stage4_layer1_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 632-633\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert60_mlp2 -> stage4_layer1_expert60_tp2
	stage4_layer1_expert61 [label="Layer 9 Expert 61\nGPU: 634-635 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert_router -> stage4_layer1_expert61 [style=dashed]
	stage4_layer1_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 634-635 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert61 -> stage4_layer1_expert61_mlp1
	stage4_layer1_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 634-635\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert61_mlp1 -> stage4_layer1_expert61_tp1
	stage4_layer1_expert61_act [label="Expert 61 GELU Activation\nGPU: 634-635\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert61_tp1 -> stage4_layer1_expert61_act
	stage4_layer1_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 634-635 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert61_act -> stage4_layer1_expert61_mlp2
	stage4_layer1_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 634-635\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert61_mlp2 -> stage4_layer1_expert61_tp2
	stage4_layer1_expert62 [label="Layer 9 Expert 62\nGPU: 636-637 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert_router -> stage4_layer1_expert62 [style=dashed]
	stage4_layer1_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 636-637 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert62 -> stage4_layer1_expert62_mlp1
	stage4_layer1_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 636-637\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert62_mlp1 -> stage4_layer1_expert62_tp1
	stage4_layer1_expert62_act [label="Expert 62 GELU Activation\nGPU: 636-637\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert62_tp1 -> stage4_layer1_expert62_act
	stage4_layer1_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 636-637 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert62_act -> stage4_layer1_expert62_mlp2
	stage4_layer1_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 636-637\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert62_mlp2 -> stage4_layer1_expert62_tp2
	stage4_layer1_expert63 [label="Layer 9 Expert 63\nGPU: 638-639 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert_router -> stage4_layer1_expert63 [style=dashed]
	stage4_layer1_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 638-639 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert63 -> stage4_layer1_expert63_mlp1
	stage4_layer1_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 638-639\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert63_mlp1 -> stage4_layer1_expert63_tp1
	stage4_layer1_expert63_act [label="Expert 63 GELU Activation\nGPU: 638-639\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert63_tp1 -> stage4_layer1_expert63_act
	stage4_layer1_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 638-639 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage4_layer1_expert63_act -> stage4_layer1_expert63_mlp2
	stage4_layer1_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 638-639\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage4_layer1_expert63_mlp2 -> stage4_layer1_expert63_tp2
	stage4_layer1_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 4 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage4_layer1_expert_router -> stage4_layer1_expert_ellipsis [style=dashed]
	stage4_layer1_expert_agg [label="Layer 9 Expert Aggregation\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage4_layer1_expert0_tp2 -> stage4_layer1_expert_agg
	stage4_layer1_expert1_tp2 -> stage4_layer1_expert_agg
	stage4_layer1_expert2_tp2 -> stage4_layer1_expert_agg
	stage4_layer1_expert3_tp2 -> stage4_layer1_expert_agg
	stage4_layer1_expert31_tp2 -> stage4_layer1_expert_agg
	stage4_layer1_expert32_tp2 -> stage4_layer1_expert_agg
	stage4_layer1_expert60_tp2 -> stage4_layer1_expert_agg
	stage4_layer1_expert61_tp2 -> stage4_layer1_expert_agg
	stage4_layer1_expert62_tp2 -> stage4_layer1_expert_agg
	stage4_layer1_expert63_tp2 -> stage4_layer1_expert_agg
	stage4_layer1_expert_ellipsis -> stage4_layer1_expert_agg
	stage4_layer1_output [label="Layer 9 Output\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage4_layer1_expert_agg -> stage4_layer1_output
	stage4_output [label="Stage 4 Output\nGPU: Pipeline 4\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage4_layer1_output -> stage4_output
	stage5_input [label="Stage 5 Input Router\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage4_output -> stage5_input
	stage5_layer0_ln [label="Layer 10 LayerNorm\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage5_input -> stage5_layer0_ln
	stage5_layer0_attn_qkv [label="Layer 10 Attention QKV Projection\nGPU: Pipeline 5 (TP: GPU 640-767)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage5_layer0_ln -> stage5_layer0_attn_qkv
	stage5_layer0_tp_qkv_comm [label="Layer 10 TP QKV All-Gather\nGPU: Pipeline 5 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage5_layer0_attn_qkv -> stage5_layer0_tp_qkv_comm
	stage5_layer0_attn_head0 [label="Layer 10 Head 0 Attention\nGPU: Pipeline 5 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head0
	stage5_layer0_attn_head1 [label="Layer 10 Head 1 Attention\nGPU: Pipeline 5 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head1
	stage5_layer0_attn_head2 [label="Layer 10 Head 2 Attention\nGPU: Pipeline 5 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head2
	stage5_layer0_attn_head3 [label="Layer 10 Head 3 Attention\nGPU: Pipeline 5 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head3
	stage5_layer0_attn_head4 [label="Layer 10 Head 4 Attention\nGPU: Pipeline 5 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head4
	stage5_layer0_attn_head5 [label="Layer 10 Head 5 Attention\nGPU: Pipeline 5 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head5
	stage5_layer0_attn_head6 [label="Layer 10 Head 6 Attention\nGPU: Pipeline 5 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head6
	stage5_layer0_attn_head7 [label="Layer 10 Head 7 Attention\nGPU: Pipeline 5 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head7
	stage5_layer0_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_ellipsis
	stage5_layer0_attn_head24 [label="Layer 10 Head 24 Attention\nGPU: Pipeline 5 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head24
	stage5_layer0_attn_head25 [label="Layer 10 Head 25 Attention\nGPU: Pipeline 5 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head25
	stage5_layer0_attn_head26 [label="Layer 10 Head 26 Attention\nGPU: Pipeline 5 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head26
	stage5_layer0_attn_head27 [label="Layer 10 Head 27 Attention\nGPU: Pipeline 5 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head27
	stage5_layer0_attn_head28 [label="Layer 10 Head 28 Attention\nGPU: Pipeline 5 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head28
	stage5_layer0_attn_head29 [label="Layer 10 Head 29 Attention\nGPU: Pipeline 5 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head29
	stage5_layer0_attn_head30 [label="Layer 10 Head 30 Attention\nGPU: Pipeline 5 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head30
	stage5_layer0_attn_head31 [label="Layer 10 Head 31 Attention\nGPU: Pipeline 5 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_tp_qkv_comm -> stage5_layer0_attn_head31
	stage5_layer0_attn_agg [label="Layer 10 Attention Aggregate\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage5_layer0_attn_head0 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head1 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head2 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head3 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head4 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head5 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head6 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head7 -> stage5_layer0_attn_agg
	stage5_layer0_attn_ellipsis -> stage5_layer0_attn_agg
	stage5_layer0_attn_head24 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head25 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head26 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head27 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head28 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head29 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head30 -> stage5_layer0_attn_agg
	stage5_layer0_attn_head31 -> stage5_layer0_attn_agg
	stage5_layer0_attn_out [label="Layer 10 Attention Output Proj\nGPU: Pipeline 5 (TP: GPU 640-767)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage5_layer0_attn_agg -> stage5_layer0_attn_out
	stage5_layer0_tp_attn_comm [label="Layer 10 TP Attention All-Reduce\nGPU: Pipeline 5 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage5_layer0_attn_out -> stage5_layer0_tp_attn_comm
	stage5_layer0_expert_router [label="Layer 10 Expert Router (Top-k)\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage5_layer0_tp_attn_comm -> stage5_layer0_expert_router
	stage5_layer0_expert0 [label="Layer 10 Expert 0\nGPU: 640-641 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert_router -> stage5_layer0_expert0 [style=dashed]
	stage5_layer0_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 640-641 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert0 -> stage5_layer0_expert0_mlp1
	stage5_layer0_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 640-641\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert0_mlp1 -> stage5_layer0_expert0_tp1
	stage5_layer0_expert0_act [label="Expert 0 GELU Activation\nGPU: 640-641\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert0_tp1 -> stage5_layer0_expert0_act
	stage5_layer0_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 640-641 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert0_act -> stage5_layer0_expert0_mlp2
	stage5_layer0_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 640-641\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert0_mlp2 -> stage5_layer0_expert0_tp2
	stage5_layer0_expert1 [label="Layer 10 Expert 1\nGPU: 642-643 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert_router -> stage5_layer0_expert1 [style=dashed]
	stage5_layer0_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 642-643 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert1 -> stage5_layer0_expert1_mlp1
	stage5_layer0_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 642-643\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert1_mlp1 -> stage5_layer0_expert1_tp1
	stage5_layer0_expert1_act [label="Expert 1 GELU Activation\nGPU: 642-643\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert1_tp1 -> stage5_layer0_expert1_act
	stage5_layer0_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 642-643 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert1_act -> stage5_layer0_expert1_mlp2
	stage5_layer0_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 642-643\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert1_mlp2 -> stage5_layer0_expert1_tp2
	stage5_layer0_expert2 [label="Layer 10 Expert 2\nGPU: 644-645 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert_router -> stage5_layer0_expert2 [style=dashed]
	stage5_layer0_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 644-645 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert2 -> stage5_layer0_expert2_mlp1
	stage5_layer0_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 644-645\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert2_mlp1 -> stage5_layer0_expert2_tp1
	stage5_layer0_expert2_act [label="Expert 2 GELU Activation\nGPU: 644-645\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert2_tp1 -> stage5_layer0_expert2_act
	stage5_layer0_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 644-645 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert2_act -> stage5_layer0_expert2_mlp2
	stage5_layer0_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 644-645\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert2_mlp2 -> stage5_layer0_expert2_tp2
	stage5_layer0_expert3 [label="Layer 10 Expert 3\nGPU: 646-647 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert_router -> stage5_layer0_expert3 [style=dashed]
	stage5_layer0_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 646-647 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert3 -> stage5_layer0_expert3_mlp1
	stage5_layer0_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 646-647\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert3_mlp1 -> stage5_layer0_expert3_tp1
	stage5_layer0_expert3_act [label="Expert 3 GELU Activation\nGPU: 646-647\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert3_tp1 -> stage5_layer0_expert3_act
	stage5_layer0_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 646-647 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert3_act -> stage5_layer0_expert3_mlp2
	stage5_layer0_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 646-647\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert3_mlp2 -> stage5_layer0_expert3_tp2
	stage5_layer0_expert31 [label="Layer 10 Expert 31\nGPU: 702-703 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert_router -> stage5_layer0_expert31 [style=dashed]
	stage5_layer0_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 702-703 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert31 -> stage5_layer0_expert31_mlp1
	stage5_layer0_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 702-703\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert31_mlp1 -> stage5_layer0_expert31_tp1
	stage5_layer0_expert31_act [label="Expert 31 GELU Activation\nGPU: 702-703\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert31_tp1 -> stage5_layer0_expert31_act
	stage5_layer0_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 702-703 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert31_act -> stage5_layer0_expert31_mlp2
	stage5_layer0_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 702-703\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert31_mlp2 -> stage5_layer0_expert31_tp2
	stage5_layer0_expert32 [label="Layer 10 Expert 32\nGPU: 704-705 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert_router -> stage5_layer0_expert32 [style=dashed]
	stage5_layer0_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 704-705 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert32 -> stage5_layer0_expert32_mlp1
	stage5_layer0_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 704-705\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert32_mlp1 -> stage5_layer0_expert32_tp1
	stage5_layer0_expert32_act [label="Expert 32 GELU Activation\nGPU: 704-705\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert32_tp1 -> stage5_layer0_expert32_act
	stage5_layer0_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 704-705 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert32_act -> stage5_layer0_expert32_mlp2
	stage5_layer0_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 704-705\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert32_mlp2 -> stage5_layer0_expert32_tp2
	stage5_layer0_expert60 [label="Layer 10 Expert 60\nGPU: 760-761 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert_router -> stage5_layer0_expert60 [style=dashed]
	stage5_layer0_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 760-761 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert60 -> stage5_layer0_expert60_mlp1
	stage5_layer0_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 760-761\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert60_mlp1 -> stage5_layer0_expert60_tp1
	stage5_layer0_expert60_act [label="Expert 60 GELU Activation\nGPU: 760-761\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert60_tp1 -> stage5_layer0_expert60_act
	stage5_layer0_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 760-761 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert60_act -> stage5_layer0_expert60_mlp2
	stage5_layer0_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 760-761\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert60_mlp2 -> stage5_layer0_expert60_tp2
	stage5_layer0_expert61 [label="Layer 10 Expert 61\nGPU: 762-763 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert_router -> stage5_layer0_expert61 [style=dashed]
	stage5_layer0_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 762-763 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert61 -> stage5_layer0_expert61_mlp1
	stage5_layer0_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 762-763\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert61_mlp1 -> stage5_layer0_expert61_tp1
	stage5_layer0_expert61_act [label="Expert 61 GELU Activation\nGPU: 762-763\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert61_tp1 -> stage5_layer0_expert61_act
	stage5_layer0_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 762-763 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert61_act -> stage5_layer0_expert61_mlp2
	stage5_layer0_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 762-763\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert61_mlp2 -> stage5_layer0_expert61_tp2
	stage5_layer0_expert62 [label="Layer 10 Expert 62\nGPU: 764-765 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert_router -> stage5_layer0_expert62 [style=dashed]
	stage5_layer0_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 764-765 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert62 -> stage5_layer0_expert62_mlp1
	stage5_layer0_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 764-765\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert62_mlp1 -> stage5_layer0_expert62_tp1
	stage5_layer0_expert62_act [label="Expert 62 GELU Activation\nGPU: 764-765\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert62_tp1 -> stage5_layer0_expert62_act
	stage5_layer0_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 764-765 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert62_act -> stage5_layer0_expert62_mlp2
	stage5_layer0_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 764-765\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert62_mlp2 -> stage5_layer0_expert62_tp2
	stage5_layer0_expert63 [label="Layer 10 Expert 63\nGPU: 766-767 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert_router -> stage5_layer0_expert63 [style=dashed]
	stage5_layer0_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 766-767 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert63 -> stage5_layer0_expert63_mlp1
	stage5_layer0_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 766-767\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert63_mlp1 -> stage5_layer0_expert63_tp1
	stage5_layer0_expert63_act [label="Expert 63 GELU Activation\nGPU: 766-767\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert63_tp1 -> stage5_layer0_expert63_act
	stage5_layer0_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 766-767 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer0_expert63_act -> stage5_layer0_expert63_mlp2
	stage5_layer0_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 766-767\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer0_expert63_mlp2 -> stage5_layer0_expert63_tp2
	stage5_layer0_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 5 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage5_layer0_expert_router -> stage5_layer0_expert_ellipsis [style=dashed]
	stage5_layer0_expert_agg [label="Layer 10 Expert Aggregation\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage5_layer0_expert0_tp2 -> stage5_layer0_expert_agg
	stage5_layer0_expert1_tp2 -> stage5_layer0_expert_agg
	stage5_layer0_expert2_tp2 -> stage5_layer0_expert_agg
	stage5_layer0_expert3_tp2 -> stage5_layer0_expert_agg
	stage5_layer0_expert31_tp2 -> stage5_layer0_expert_agg
	stage5_layer0_expert32_tp2 -> stage5_layer0_expert_agg
	stage5_layer0_expert60_tp2 -> stage5_layer0_expert_agg
	stage5_layer0_expert61_tp2 -> stage5_layer0_expert_agg
	stage5_layer0_expert62_tp2 -> stage5_layer0_expert_agg
	stage5_layer0_expert63_tp2 -> stage5_layer0_expert_agg
	stage5_layer0_expert_ellipsis -> stage5_layer0_expert_agg
	stage5_layer0_output [label="Layer 10 Output\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage5_layer0_expert_agg -> stage5_layer0_output
	stage5_layer0_tp_attn_comm -> stage5_layer0_output [style=invis]
	stage5_layer1_ln [label="Layer 11 LayerNorm\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage5_layer0_output -> stage5_layer1_ln
	stage5_layer1_attn_qkv [label="Layer 11 Attention QKV Projection\nGPU: Pipeline 5 (TP: GPU 640-767)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage5_layer1_ln -> stage5_layer1_attn_qkv
	stage5_layer1_tp_qkv_comm [label="Layer 11 TP QKV All-Gather\nGPU: Pipeline 5 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage5_layer1_attn_qkv -> stage5_layer1_tp_qkv_comm
	stage5_layer1_attn_head0 [label="Layer 11 Head 0 Attention\nGPU: Pipeline 5 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head0
	stage5_layer1_attn_head1 [label="Layer 11 Head 1 Attention\nGPU: Pipeline 5 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head1
	stage5_layer1_attn_head2 [label="Layer 11 Head 2 Attention\nGPU: Pipeline 5 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head2
	stage5_layer1_attn_head3 [label="Layer 11 Head 3 Attention\nGPU: Pipeline 5 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head3
	stage5_layer1_attn_head4 [label="Layer 11 Head 4 Attention\nGPU: Pipeline 5 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head4
	stage5_layer1_attn_head5 [label="Layer 11 Head 5 Attention\nGPU: Pipeline 5 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head5
	stage5_layer1_attn_head6 [label="Layer 11 Head 6 Attention\nGPU: Pipeline 5 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head6
	stage5_layer1_attn_head7 [label="Layer 11 Head 7 Attention\nGPU: Pipeline 5 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head7
	stage5_layer1_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_ellipsis
	stage5_layer1_attn_head24 [label="Layer 11 Head 24 Attention\nGPU: Pipeline 5 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head24
	stage5_layer1_attn_head25 [label="Layer 11 Head 25 Attention\nGPU: Pipeline 5 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head25
	stage5_layer1_attn_head26 [label="Layer 11 Head 26 Attention\nGPU: Pipeline 5 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head26
	stage5_layer1_attn_head27 [label="Layer 11 Head 27 Attention\nGPU: Pipeline 5 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head27
	stage5_layer1_attn_head28 [label="Layer 11 Head 28 Attention\nGPU: Pipeline 5 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head28
	stage5_layer1_attn_head29 [label="Layer 11 Head 29 Attention\nGPU: Pipeline 5 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head29
	stage5_layer1_attn_head30 [label="Layer 11 Head 30 Attention\nGPU: Pipeline 5 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head30
	stage5_layer1_attn_head31 [label="Layer 11 Head 31 Attention\nGPU: Pipeline 5 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_tp_qkv_comm -> stage5_layer1_attn_head31
	stage5_layer1_attn_agg [label="Layer 11 Attention Aggregate\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage5_layer1_attn_head0 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head1 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head2 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head3 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head4 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head5 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head6 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head7 -> stage5_layer1_attn_agg
	stage5_layer1_attn_ellipsis -> stage5_layer1_attn_agg
	stage5_layer1_attn_head24 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head25 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head26 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head27 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head28 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head29 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head30 -> stage5_layer1_attn_agg
	stage5_layer1_attn_head31 -> stage5_layer1_attn_agg
	stage5_layer1_attn_out [label="Layer 11 Attention Output Proj\nGPU: Pipeline 5 (TP: GPU 640-767)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage5_layer1_attn_agg -> stage5_layer1_attn_out
	stage5_layer1_tp_attn_comm [label="Layer 11 TP Attention All-Reduce\nGPU: Pipeline 5 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage5_layer1_attn_out -> stage5_layer1_tp_attn_comm
	stage5_layer1_expert_router [label="Layer 11 Expert Router (Top-k)\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage5_layer1_tp_attn_comm -> stage5_layer1_expert_router
	stage5_layer1_expert0 [label="Layer 11 Expert 0\nGPU: 640-641 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert_router -> stage5_layer1_expert0 [style=dashed]
	stage5_layer1_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 640-641 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert0 -> stage5_layer1_expert0_mlp1
	stage5_layer1_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 640-641\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert0_mlp1 -> stage5_layer1_expert0_tp1
	stage5_layer1_expert0_act [label="Expert 0 GELU Activation\nGPU: 640-641\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert0_tp1 -> stage5_layer1_expert0_act
	stage5_layer1_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 640-641 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert0_act -> stage5_layer1_expert0_mlp2
	stage5_layer1_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 640-641\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert0_mlp2 -> stage5_layer1_expert0_tp2
	stage5_layer1_expert1 [label="Layer 11 Expert 1\nGPU: 642-643 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert_router -> stage5_layer1_expert1 [style=dashed]
	stage5_layer1_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 642-643 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert1 -> stage5_layer1_expert1_mlp1
	stage5_layer1_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 642-643\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert1_mlp1 -> stage5_layer1_expert1_tp1
	stage5_layer1_expert1_act [label="Expert 1 GELU Activation\nGPU: 642-643\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert1_tp1 -> stage5_layer1_expert1_act
	stage5_layer1_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 642-643 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert1_act -> stage5_layer1_expert1_mlp2
	stage5_layer1_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 642-643\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert1_mlp2 -> stage5_layer1_expert1_tp2
	stage5_layer1_expert2 [label="Layer 11 Expert 2\nGPU: 644-645 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert_router -> stage5_layer1_expert2 [style=dashed]
	stage5_layer1_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 644-645 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert2 -> stage5_layer1_expert2_mlp1
	stage5_layer1_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 644-645\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert2_mlp1 -> stage5_layer1_expert2_tp1
	stage5_layer1_expert2_act [label="Expert 2 GELU Activation\nGPU: 644-645\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert2_tp1 -> stage5_layer1_expert2_act
	stage5_layer1_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 644-645 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert2_act -> stage5_layer1_expert2_mlp2
	stage5_layer1_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 644-645\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert2_mlp2 -> stage5_layer1_expert2_tp2
	stage5_layer1_expert3 [label="Layer 11 Expert 3\nGPU: 646-647 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert_router -> stage5_layer1_expert3 [style=dashed]
	stage5_layer1_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 646-647 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert3 -> stage5_layer1_expert3_mlp1
	stage5_layer1_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 646-647\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert3_mlp1 -> stage5_layer1_expert3_tp1
	stage5_layer1_expert3_act [label="Expert 3 GELU Activation\nGPU: 646-647\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert3_tp1 -> stage5_layer1_expert3_act
	stage5_layer1_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 646-647 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert3_act -> stage5_layer1_expert3_mlp2
	stage5_layer1_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 646-647\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert3_mlp2 -> stage5_layer1_expert3_tp2
	stage5_layer1_expert31 [label="Layer 11 Expert 31\nGPU: 702-703 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert_router -> stage5_layer1_expert31 [style=dashed]
	stage5_layer1_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 702-703 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert31 -> stage5_layer1_expert31_mlp1
	stage5_layer1_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 702-703\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert31_mlp1 -> stage5_layer1_expert31_tp1
	stage5_layer1_expert31_act [label="Expert 31 GELU Activation\nGPU: 702-703\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert31_tp1 -> stage5_layer1_expert31_act
	stage5_layer1_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 702-703 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert31_act -> stage5_layer1_expert31_mlp2
	stage5_layer1_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 702-703\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert31_mlp2 -> stage5_layer1_expert31_tp2
	stage5_layer1_expert32 [label="Layer 11 Expert 32\nGPU: 704-705 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert_router -> stage5_layer1_expert32 [style=dashed]
	stage5_layer1_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 704-705 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert32 -> stage5_layer1_expert32_mlp1
	stage5_layer1_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 704-705\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert32_mlp1 -> stage5_layer1_expert32_tp1
	stage5_layer1_expert32_act [label="Expert 32 GELU Activation\nGPU: 704-705\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert32_tp1 -> stage5_layer1_expert32_act
	stage5_layer1_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 704-705 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert32_act -> stage5_layer1_expert32_mlp2
	stage5_layer1_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 704-705\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert32_mlp2 -> stage5_layer1_expert32_tp2
	stage5_layer1_expert60 [label="Layer 11 Expert 60\nGPU: 760-761 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert_router -> stage5_layer1_expert60 [style=dashed]
	stage5_layer1_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 760-761 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert60 -> stage5_layer1_expert60_mlp1
	stage5_layer1_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 760-761\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert60_mlp1 -> stage5_layer1_expert60_tp1
	stage5_layer1_expert60_act [label="Expert 60 GELU Activation\nGPU: 760-761\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert60_tp1 -> stage5_layer1_expert60_act
	stage5_layer1_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 760-761 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert60_act -> stage5_layer1_expert60_mlp2
	stage5_layer1_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 760-761\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert60_mlp2 -> stage5_layer1_expert60_tp2
	stage5_layer1_expert61 [label="Layer 11 Expert 61\nGPU: 762-763 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert_router -> stage5_layer1_expert61 [style=dashed]
	stage5_layer1_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 762-763 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert61 -> stage5_layer1_expert61_mlp1
	stage5_layer1_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 762-763\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert61_mlp1 -> stage5_layer1_expert61_tp1
	stage5_layer1_expert61_act [label="Expert 61 GELU Activation\nGPU: 762-763\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert61_tp1 -> stage5_layer1_expert61_act
	stage5_layer1_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 762-763 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert61_act -> stage5_layer1_expert61_mlp2
	stage5_layer1_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 762-763\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert61_mlp2 -> stage5_layer1_expert61_tp2
	stage5_layer1_expert62 [label="Layer 11 Expert 62\nGPU: 764-765 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert_router -> stage5_layer1_expert62 [style=dashed]
	stage5_layer1_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 764-765 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert62 -> stage5_layer1_expert62_mlp1
	stage5_layer1_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 764-765\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert62_mlp1 -> stage5_layer1_expert62_tp1
	stage5_layer1_expert62_act [label="Expert 62 GELU Activation\nGPU: 764-765\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert62_tp1 -> stage5_layer1_expert62_act
	stage5_layer1_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 764-765 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert62_act -> stage5_layer1_expert62_mlp2
	stage5_layer1_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 764-765\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert62_mlp2 -> stage5_layer1_expert62_tp2
	stage5_layer1_expert63 [label="Layer 11 Expert 63\nGPU: 766-767 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert_router -> stage5_layer1_expert63 [style=dashed]
	stage5_layer1_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 766-767 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert63 -> stage5_layer1_expert63_mlp1
	stage5_layer1_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 766-767\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert63_mlp1 -> stage5_layer1_expert63_tp1
	stage5_layer1_expert63_act [label="Expert 63 GELU Activation\nGPU: 766-767\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert63_tp1 -> stage5_layer1_expert63_act
	stage5_layer1_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 766-767 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage5_layer1_expert63_act -> stage5_layer1_expert63_mlp2
	stage5_layer1_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 766-767\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage5_layer1_expert63_mlp2 -> stage5_layer1_expert63_tp2
	stage5_layer1_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 5 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage5_layer1_expert_router -> stage5_layer1_expert_ellipsis [style=dashed]
	stage5_layer1_expert_agg [label="Layer 11 Expert Aggregation\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage5_layer1_expert0_tp2 -> stage5_layer1_expert_agg
	stage5_layer1_expert1_tp2 -> stage5_layer1_expert_agg
	stage5_layer1_expert2_tp2 -> stage5_layer1_expert_agg
	stage5_layer1_expert3_tp2 -> stage5_layer1_expert_agg
	stage5_layer1_expert31_tp2 -> stage5_layer1_expert_agg
	stage5_layer1_expert32_tp2 -> stage5_layer1_expert_agg
	stage5_layer1_expert60_tp2 -> stage5_layer1_expert_agg
	stage5_layer1_expert61_tp2 -> stage5_layer1_expert_agg
	stage5_layer1_expert62_tp2 -> stage5_layer1_expert_agg
	stage5_layer1_expert63_tp2 -> stage5_layer1_expert_agg
	stage5_layer1_expert_ellipsis -> stage5_layer1_expert_agg
	stage5_layer1_output [label="Layer 11 Output\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage5_layer1_expert_agg -> stage5_layer1_output
	stage5_output [label="Stage 5 Output\nGPU: Pipeline 5\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage5_layer1_output -> stage5_output
	stage6_input [label="Stage 6 Input Router\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage5_output -> stage6_input
	stage6_layer0_ln [label="Layer 12 LayerNorm\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage6_input -> stage6_layer0_ln
	stage6_layer0_attn_qkv [label="Layer 12 Attention QKV Projection\nGPU: Pipeline 6 (TP: GPU 768-895)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage6_layer0_ln -> stage6_layer0_attn_qkv
	stage6_layer0_tp_qkv_comm [label="Layer 12 TP QKV All-Gather\nGPU: Pipeline 6 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage6_layer0_attn_qkv -> stage6_layer0_tp_qkv_comm
	stage6_layer0_attn_head0 [label="Layer 12 Head 0 Attention\nGPU: Pipeline 6 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head0
	stage6_layer0_attn_head1 [label="Layer 12 Head 1 Attention\nGPU: Pipeline 6 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head1
	stage6_layer0_attn_head2 [label="Layer 12 Head 2 Attention\nGPU: Pipeline 6 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head2
	stage6_layer0_attn_head3 [label="Layer 12 Head 3 Attention\nGPU: Pipeline 6 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head3
	stage6_layer0_attn_head4 [label="Layer 12 Head 4 Attention\nGPU: Pipeline 6 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head4
	stage6_layer0_attn_head5 [label="Layer 12 Head 5 Attention\nGPU: Pipeline 6 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head5
	stage6_layer0_attn_head6 [label="Layer 12 Head 6 Attention\nGPU: Pipeline 6 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head6
	stage6_layer0_attn_head7 [label="Layer 12 Head 7 Attention\nGPU: Pipeline 6 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head7
	stage6_layer0_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_ellipsis
	stage6_layer0_attn_head24 [label="Layer 12 Head 24 Attention\nGPU: Pipeline 6 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head24
	stage6_layer0_attn_head25 [label="Layer 12 Head 25 Attention\nGPU: Pipeline 6 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head25
	stage6_layer0_attn_head26 [label="Layer 12 Head 26 Attention\nGPU: Pipeline 6 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head26
	stage6_layer0_attn_head27 [label="Layer 12 Head 27 Attention\nGPU: Pipeline 6 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head27
	stage6_layer0_attn_head28 [label="Layer 12 Head 28 Attention\nGPU: Pipeline 6 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head28
	stage6_layer0_attn_head29 [label="Layer 12 Head 29 Attention\nGPU: Pipeline 6 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head29
	stage6_layer0_attn_head30 [label="Layer 12 Head 30 Attention\nGPU: Pipeline 6 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head30
	stage6_layer0_attn_head31 [label="Layer 12 Head 31 Attention\nGPU: Pipeline 6 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_tp_qkv_comm -> stage6_layer0_attn_head31
	stage6_layer0_attn_agg [label="Layer 12 Attention Aggregate\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage6_layer0_attn_head0 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head1 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head2 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head3 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head4 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head5 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head6 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head7 -> stage6_layer0_attn_agg
	stage6_layer0_attn_ellipsis -> stage6_layer0_attn_agg
	stage6_layer0_attn_head24 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head25 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head26 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head27 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head28 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head29 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head30 -> stage6_layer0_attn_agg
	stage6_layer0_attn_head31 -> stage6_layer0_attn_agg
	stage6_layer0_attn_out [label="Layer 12 Attention Output Proj\nGPU: Pipeline 6 (TP: GPU 768-895)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage6_layer0_attn_agg -> stage6_layer0_attn_out
	stage6_layer0_tp_attn_comm [label="Layer 12 TP Attention All-Reduce\nGPU: Pipeline 6 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage6_layer0_attn_out -> stage6_layer0_tp_attn_comm
	stage6_layer0_expert_router [label="Layer 12 Expert Router (Top-k)\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage6_layer0_tp_attn_comm -> stage6_layer0_expert_router
	stage6_layer0_expert0 [label="Layer 12 Expert 0\nGPU: 768-769 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert_router -> stage6_layer0_expert0 [style=dashed]
	stage6_layer0_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 768-769 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert0 -> stage6_layer0_expert0_mlp1
	stage6_layer0_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 768-769\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert0_mlp1 -> stage6_layer0_expert0_tp1
	stage6_layer0_expert0_act [label="Expert 0 GELU Activation\nGPU: 768-769\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert0_tp1 -> stage6_layer0_expert0_act
	stage6_layer0_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 768-769 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert0_act -> stage6_layer0_expert0_mlp2
	stage6_layer0_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 768-769\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert0_mlp2 -> stage6_layer0_expert0_tp2
	stage6_layer0_expert1 [label="Layer 12 Expert 1\nGPU: 770-771 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert_router -> stage6_layer0_expert1 [style=dashed]
	stage6_layer0_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 770-771 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert1 -> stage6_layer0_expert1_mlp1
	stage6_layer0_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 770-771\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert1_mlp1 -> stage6_layer0_expert1_tp1
	stage6_layer0_expert1_act [label="Expert 1 GELU Activation\nGPU: 770-771\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert1_tp1 -> stage6_layer0_expert1_act
	stage6_layer0_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 770-771 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert1_act -> stage6_layer0_expert1_mlp2
	stage6_layer0_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 770-771\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert1_mlp2 -> stage6_layer0_expert1_tp2
	stage6_layer0_expert2 [label="Layer 12 Expert 2\nGPU: 772-773 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert_router -> stage6_layer0_expert2 [style=dashed]
	stage6_layer0_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 772-773 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert2 -> stage6_layer0_expert2_mlp1
	stage6_layer0_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 772-773\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert2_mlp1 -> stage6_layer0_expert2_tp1
	stage6_layer0_expert2_act [label="Expert 2 GELU Activation\nGPU: 772-773\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert2_tp1 -> stage6_layer0_expert2_act
	stage6_layer0_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 772-773 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert2_act -> stage6_layer0_expert2_mlp2
	stage6_layer0_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 772-773\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert2_mlp2 -> stage6_layer0_expert2_tp2
	stage6_layer0_expert3 [label="Layer 12 Expert 3\nGPU: 774-775 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert_router -> stage6_layer0_expert3 [style=dashed]
	stage6_layer0_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 774-775 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert3 -> stage6_layer0_expert3_mlp1
	stage6_layer0_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 774-775\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert3_mlp1 -> stage6_layer0_expert3_tp1
	stage6_layer0_expert3_act [label="Expert 3 GELU Activation\nGPU: 774-775\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert3_tp1 -> stage6_layer0_expert3_act
	stage6_layer0_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 774-775 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert3_act -> stage6_layer0_expert3_mlp2
	stage6_layer0_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 774-775\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert3_mlp2 -> stage6_layer0_expert3_tp2
	stage6_layer0_expert31 [label="Layer 12 Expert 31\nGPU: 830-831 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert_router -> stage6_layer0_expert31 [style=dashed]
	stage6_layer0_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 830-831 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert31 -> stage6_layer0_expert31_mlp1
	stage6_layer0_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 830-831\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert31_mlp1 -> stage6_layer0_expert31_tp1
	stage6_layer0_expert31_act [label="Expert 31 GELU Activation\nGPU: 830-831\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert31_tp1 -> stage6_layer0_expert31_act
	stage6_layer0_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 830-831 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert31_act -> stage6_layer0_expert31_mlp2
	stage6_layer0_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 830-831\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert31_mlp2 -> stage6_layer0_expert31_tp2
	stage6_layer0_expert32 [label="Layer 12 Expert 32\nGPU: 832-833 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert_router -> stage6_layer0_expert32 [style=dashed]
	stage6_layer0_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 832-833 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert32 -> stage6_layer0_expert32_mlp1
	stage6_layer0_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 832-833\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert32_mlp1 -> stage6_layer0_expert32_tp1
	stage6_layer0_expert32_act [label="Expert 32 GELU Activation\nGPU: 832-833\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert32_tp1 -> stage6_layer0_expert32_act
	stage6_layer0_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 832-833 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert32_act -> stage6_layer0_expert32_mlp2
	stage6_layer0_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 832-833\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert32_mlp2 -> stage6_layer0_expert32_tp2
	stage6_layer0_expert60 [label="Layer 12 Expert 60\nGPU: 888-889 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert_router -> stage6_layer0_expert60 [style=dashed]
	stage6_layer0_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 888-889 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert60 -> stage6_layer0_expert60_mlp1
	stage6_layer0_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 888-889\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert60_mlp1 -> stage6_layer0_expert60_tp1
	stage6_layer0_expert60_act [label="Expert 60 GELU Activation\nGPU: 888-889\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert60_tp1 -> stage6_layer0_expert60_act
	stage6_layer0_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 888-889 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert60_act -> stage6_layer0_expert60_mlp2
	stage6_layer0_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 888-889\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert60_mlp2 -> stage6_layer0_expert60_tp2
	stage6_layer0_expert61 [label="Layer 12 Expert 61\nGPU: 890-891 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert_router -> stage6_layer0_expert61 [style=dashed]
	stage6_layer0_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 890-891 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert61 -> stage6_layer0_expert61_mlp1
	stage6_layer0_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 890-891\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert61_mlp1 -> stage6_layer0_expert61_tp1
	stage6_layer0_expert61_act [label="Expert 61 GELU Activation\nGPU: 890-891\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert61_tp1 -> stage6_layer0_expert61_act
	stage6_layer0_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 890-891 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert61_act -> stage6_layer0_expert61_mlp2
	stage6_layer0_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 890-891\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert61_mlp2 -> stage6_layer0_expert61_tp2
	stage6_layer0_expert62 [label="Layer 12 Expert 62\nGPU: 892-893 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert_router -> stage6_layer0_expert62 [style=dashed]
	stage6_layer0_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 892-893 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert62 -> stage6_layer0_expert62_mlp1
	stage6_layer0_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 892-893\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert62_mlp1 -> stage6_layer0_expert62_tp1
	stage6_layer0_expert62_act [label="Expert 62 GELU Activation\nGPU: 892-893\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert62_tp1 -> stage6_layer0_expert62_act
	stage6_layer0_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 892-893 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert62_act -> stage6_layer0_expert62_mlp2
	stage6_layer0_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 892-893\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert62_mlp2 -> stage6_layer0_expert62_tp2
	stage6_layer0_expert63 [label="Layer 12 Expert 63\nGPU: 894-895 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert_router -> stage6_layer0_expert63 [style=dashed]
	stage6_layer0_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 894-895 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert63 -> stage6_layer0_expert63_mlp1
	stage6_layer0_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 894-895\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert63_mlp1 -> stage6_layer0_expert63_tp1
	stage6_layer0_expert63_act [label="Expert 63 GELU Activation\nGPU: 894-895\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert63_tp1 -> stage6_layer0_expert63_act
	stage6_layer0_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 894-895 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer0_expert63_act -> stage6_layer0_expert63_mlp2
	stage6_layer0_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 894-895\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer0_expert63_mlp2 -> stage6_layer0_expert63_tp2
	stage6_layer0_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 6 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage6_layer0_expert_router -> stage6_layer0_expert_ellipsis [style=dashed]
	stage6_layer0_expert_agg [label="Layer 12 Expert Aggregation\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage6_layer0_expert0_tp2 -> stage6_layer0_expert_agg
	stage6_layer0_expert1_tp2 -> stage6_layer0_expert_agg
	stage6_layer0_expert2_tp2 -> stage6_layer0_expert_agg
	stage6_layer0_expert3_tp2 -> stage6_layer0_expert_agg
	stage6_layer0_expert31_tp2 -> stage6_layer0_expert_agg
	stage6_layer0_expert32_tp2 -> stage6_layer0_expert_agg
	stage6_layer0_expert60_tp2 -> stage6_layer0_expert_agg
	stage6_layer0_expert61_tp2 -> stage6_layer0_expert_agg
	stage6_layer0_expert62_tp2 -> stage6_layer0_expert_agg
	stage6_layer0_expert63_tp2 -> stage6_layer0_expert_agg
	stage6_layer0_expert_ellipsis -> stage6_layer0_expert_agg
	stage6_layer0_output [label="Layer 12 Output\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage6_layer0_expert_agg -> stage6_layer0_output
	stage6_layer0_tp_attn_comm -> stage6_layer0_output [style=invis]
	stage6_layer1_ln [label="Layer 13 LayerNorm\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage6_layer0_output -> stage6_layer1_ln
	stage6_layer1_attn_qkv [label="Layer 13 Attention QKV Projection\nGPU: Pipeline 6 (TP: GPU 768-895)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage6_layer1_ln -> stage6_layer1_attn_qkv
	stage6_layer1_tp_qkv_comm [label="Layer 13 TP QKV All-Gather\nGPU: Pipeline 6 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage6_layer1_attn_qkv -> stage6_layer1_tp_qkv_comm
	stage6_layer1_attn_head0 [label="Layer 13 Head 0 Attention\nGPU: Pipeline 6 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head0
	stage6_layer1_attn_head1 [label="Layer 13 Head 1 Attention\nGPU: Pipeline 6 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head1
	stage6_layer1_attn_head2 [label="Layer 13 Head 2 Attention\nGPU: Pipeline 6 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head2
	stage6_layer1_attn_head3 [label="Layer 13 Head 3 Attention\nGPU: Pipeline 6 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head3
	stage6_layer1_attn_head4 [label="Layer 13 Head 4 Attention\nGPU: Pipeline 6 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head4
	stage6_layer1_attn_head5 [label="Layer 13 Head 5 Attention\nGPU: Pipeline 6 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head5
	stage6_layer1_attn_head6 [label="Layer 13 Head 6 Attention\nGPU: Pipeline 6 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head6
	stage6_layer1_attn_head7 [label="Layer 13 Head 7 Attention\nGPU: Pipeline 6 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head7
	stage6_layer1_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_ellipsis
	stage6_layer1_attn_head24 [label="Layer 13 Head 24 Attention\nGPU: Pipeline 6 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head24
	stage6_layer1_attn_head25 [label="Layer 13 Head 25 Attention\nGPU: Pipeline 6 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head25
	stage6_layer1_attn_head26 [label="Layer 13 Head 26 Attention\nGPU: Pipeline 6 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head26
	stage6_layer1_attn_head27 [label="Layer 13 Head 27 Attention\nGPU: Pipeline 6 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head27
	stage6_layer1_attn_head28 [label="Layer 13 Head 28 Attention\nGPU: Pipeline 6 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head28
	stage6_layer1_attn_head29 [label="Layer 13 Head 29 Attention\nGPU: Pipeline 6 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head29
	stage6_layer1_attn_head30 [label="Layer 13 Head 30 Attention\nGPU: Pipeline 6 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head30
	stage6_layer1_attn_head31 [label="Layer 13 Head 31 Attention\nGPU: Pipeline 6 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_tp_qkv_comm -> stage6_layer1_attn_head31
	stage6_layer1_attn_agg [label="Layer 13 Attention Aggregate\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage6_layer1_attn_head0 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head1 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head2 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head3 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head4 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head5 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head6 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head7 -> stage6_layer1_attn_agg
	stage6_layer1_attn_ellipsis -> stage6_layer1_attn_agg
	stage6_layer1_attn_head24 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head25 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head26 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head27 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head28 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head29 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head30 -> stage6_layer1_attn_agg
	stage6_layer1_attn_head31 -> stage6_layer1_attn_agg
	stage6_layer1_attn_out [label="Layer 13 Attention Output Proj\nGPU: Pipeline 6 (TP: GPU 768-895)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage6_layer1_attn_agg -> stage6_layer1_attn_out
	stage6_layer1_tp_attn_comm [label="Layer 13 TP Attention All-Reduce\nGPU: Pipeline 6 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage6_layer1_attn_out -> stage6_layer1_tp_attn_comm
	stage6_layer1_expert_router [label="Layer 13 Expert Router (Top-k)\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage6_layer1_tp_attn_comm -> stage6_layer1_expert_router
	stage6_layer1_expert0 [label="Layer 13 Expert 0\nGPU: 768-769 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert_router -> stage6_layer1_expert0 [style=dashed]
	stage6_layer1_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 768-769 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert0 -> stage6_layer1_expert0_mlp1
	stage6_layer1_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 768-769\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert0_mlp1 -> stage6_layer1_expert0_tp1
	stage6_layer1_expert0_act [label="Expert 0 GELU Activation\nGPU: 768-769\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert0_tp1 -> stage6_layer1_expert0_act
	stage6_layer1_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 768-769 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert0_act -> stage6_layer1_expert0_mlp2
	stage6_layer1_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 768-769\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert0_mlp2 -> stage6_layer1_expert0_tp2
	stage6_layer1_expert1 [label="Layer 13 Expert 1\nGPU: 770-771 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert_router -> stage6_layer1_expert1 [style=dashed]
	stage6_layer1_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 770-771 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert1 -> stage6_layer1_expert1_mlp1
	stage6_layer1_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 770-771\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert1_mlp1 -> stage6_layer1_expert1_tp1
	stage6_layer1_expert1_act [label="Expert 1 GELU Activation\nGPU: 770-771\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert1_tp1 -> stage6_layer1_expert1_act
	stage6_layer1_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 770-771 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert1_act -> stage6_layer1_expert1_mlp2
	stage6_layer1_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 770-771\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert1_mlp2 -> stage6_layer1_expert1_tp2
	stage6_layer1_expert2 [label="Layer 13 Expert 2\nGPU: 772-773 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert_router -> stage6_layer1_expert2 [style=dashed]
	stage6_layer1_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 772-773 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert2 -> stage6_layer1_expert2_mlp1
	stage6_layer1_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 772-773\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert2_mlp1 -> stage6_layer1_expert2_tp1
	stage6_layer1_expert2_act [label="Expert 2 GELU Activation\nGPU: 772-773\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert2_tp1 -> stage6_layer1_expert2_act
	stage6_layer1_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 772-773 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert2_act -> stage6_layer1_expert2_mlp2
	stage6_layer1_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 772-773\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert2_mlp2 -> stage6_layer1_expert2_tp2
	stage6_layer1_expert3 [label="Layer 13 Expert 3\nGPU: 774-775 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert_router -> stage6_layer1_expert3 [style=dashed]
	stage6_layer1_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 774-775 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert3 -> stage6_layer1_expert3_mlp1
	stage6_layer1_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 774-775\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert3_mlp1 -> stage6_layer1_expert3_tp1
	stage6_layer1_expert3_act [label="Expert 3 GELU Activation\nGPU: 774-775\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert3_tp1 -> stage6_layer1_expert3_act
	stage6_layer1_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 774-775 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert3_act -> stage6_layer1_expert3_mlp2
	stage6_layer1_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 774-775\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert3_mlp2 -> stage6_layer1_expert3_tp2
	stage6_layer1_expert31 [label="Layer 13 Expert 31\nGPU: 830-831 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert_router -> stage6_layer1_expert31 [style=dashed]
	stage6_layer1_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 830-831 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert31 -> stage6_layer1_expert31_mlp1
	stage6_layer1_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 830-831\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert31_mlp1 -> stage6_layer1_expert31_tp1
	stage6_layer1_expert31_act [label="Expert 31 GELU Activation\nGPU: 830-831\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert31_tp1 -> stage6_layer1_expert31_act
	stage6_layer1_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 830-831 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert31_act -> stage6_layer1_expert31_mlp2
	stage6_layer1_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 830-831\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert31_mlp2 -> stage6_layer1_expert31_tp2
	stage6_layer1_expert32 [label="Layer 13 Expert 32\nGPU: 832-833 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert_router -> stage6_layer1_expert32 [style=dashed]
	stage6_layer1_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 832-833 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert32 -> stage6_layer1_expert32_mlp1
	stage6_layer1_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 832-833\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert32_mlp1 -> stage6_layer1_expert32_tp1
	stage6_layer1_expert32_act [label="Expert 32 GELU Activation\nGPU: 832-833\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert32_tp1 -> stage6_layer1_expert32_act
	stage6_layer1_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 832-833 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert32_act -> stage6_layer1_expert32_mlp2
	stage6_layer1_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 832-833\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert32_mlp2 -> stage6_layer1_expert32_tp2
	stage6_layer1_expert60 [label="Layer 13 Expert 60\nGPU: 888-889 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert_router -> stage6_layer1_expert60 [style=dashed]
	stage6_layer1_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 888-889 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert60 -> stage6_layer1_expert60_mlp1
	stage6_layer1_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 888-889\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert60_mlp1 -> stage6_layer1_expert60_tp1
	stage6_layer1_expert60_act [label="Expert 60 GELU Activation\nGPU: 888-889\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert60_tp1 -> stage6_layer1_expert60_act
	stage6_layer1_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 888-889 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert60_act -> stage6_layer1_expert60_mlp2
	stage6_layer1_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 888-889\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert60_mlp2 -> stage6_layer1_expert60_tp2
	stage6_layer1_expert61 [label="Layer 13 Expert 61\nGPU: 890-891 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert_router -> stage6_layer1_expert61 [style=dashed]
	stage6_layer1_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 890-891 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert61 -> stage6_layer1_expert61_mlp1
	stage6_layer1_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 890-891\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert61_mlp1 -> stage6_layer1_expert61_tp1
	stage6_layer1_expert61_act [label="Expert 61 GELU Activation\nGPU: 890-891\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert61_tp1 -> stage6_layer1_expert61_act
	stage6_layer1_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 890-891 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert61_act -> stage6_layer1_expert61_mlp2
	stage6_layer1_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 890-891\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert61_mlp2 -> stage6_layer1_expert61_tp2
	stage6_layer1_expert62 [label="Layer 13 Expert 62\nGPU: 892-893 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert_router -> stage6_layer1_expert62 [style=dashed]
	stage6_layer1_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 892-893 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert62 -> stage6_layer1_expert62_mlp1
	stage6_layer1_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 892-893\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert62_mlp1 -> stage6_layer1_expert62_tp1
	stage6_layer1_expert62_act [label="Expert 62 GELU Activation\nGPU: 892-893\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert62_tp1 -> stage6_layer1_expert62_act
	stage6_layer1_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 892-893 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert62_act -> stage6_layer1_expert62_mlp2
	stage6_layer1_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 892-893\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert62_mlp2 -> stage6_layer1_expert62_tp2
	stage6_layer1_expert63 [label="Layer 13 Expert 63\nGPU: 894-895 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert_router -> stage6_layer1_expert63 [style=dashed]
	stage6_layer1_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 894-895 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert63 -> stage6_layer1_expert63_mlp1
	stage6_layer1_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 894-895\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert63_mlp1 -> stage6_layer1_expert63_tp1
	stage6_layer1_expert63_act [label="Expert 63 GELU Activation\nGPU: 894-895\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert63_tp1 -> stage6_layer1_expert63_act
	stage6_layer1_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 894-895 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage6_layer1_expert63_act -> stage6_layer1_expert63_mlp2
	stage6_layer1_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 894-895\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage6_layer1_expert63_mlp2 -> stage6_layer1_expert63_tp2
	stage6_layer1_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 6 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage6_layer1_expert_router -> stage6_layer1_expert_ellipsis [style=dashed]
	stage6_layer1_expert_agg [label="Layer 13 Expert Aggregation\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage6_layer1_expert0_tp2 -> stage6_layer1_expert_agg
	stage6_layer1_expert1_tp2 -> stage6_layer1_expert_agg
	stage6_layer1_expert2_tp2 -> stage6_layer1_expert_agg
	stage6_layer1_expert3_tp2 -> stage6_layer1_expert_agg
	stage6_layer1_expert31_tp2 -> stage6_layer1_expert_agg
	stage6_layer1_expert32_tp2 -> stage6_layer1_expert_agg
	stage6_layer1_expert60_tp2 -> stage6_layer1_expert_agg
	stage6_layer1_expert61_tp2 -> stage6_layer1_expert_agg
	stage6_layer1_expert62_tp2 -> stage6_layer1_expert_agg
	stage6_layer1_expert63_tp2 -> stage6_layer1_expert_agg
	stage6_layer1_expert_ellipsis -> stage6_layer1_expert_agg
	stage6_layer1_output [label="Layer 13 Output\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage6_layer1_expert_agg -> stage6_layer1_output
	stage6_output [label="Stage 6 Output\nGPU: Pipeline 6\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage6_layer1_output -> stage6_output
	stage7_input [label="Stage 7 Input Router\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage6_output -> stage7_input
	stage7_layer0_ln [label="Layer 14 LayerNorm\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage7_input -> stage7_layer0_ln
	stage7_layer0_attn_qkv [label="Layer 14 Attention QKV Projection\nGPU: Pipeline 7 (TP: GPU 896-1023)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage7_layer0_ln -> stage7_layer0_attn_qkv
	stage7_layer0_tp_qkv_comm [label="Layer 14 TP QKV All-Gather\nGPU: Pipeline 7 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage7_layer0_attn_qkv -> stage7_layer0_tp_qkv_comm
	stage7_layer0_attn_head0 [label="Layer 14 Head 0 Attention\nGPU: Pipeline 7 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head0
	stage7_layer0_attn_head1 [label="Layer 14 Head 1 Attention\nGPU: Pipeline 7 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head1
	stage7_layer0_attn_head2 [label="Layer 14 Head 2 Attention\nGPU: Pipeline 7 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head2
	stage7_layer0_attn_head3 [label="Layer 14 Head 3 Attention\nGPU: Pipeline 7 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head3
	stage7_layer0_attn_head4 [label="Layer 14 Head 4 Attention\nGPU: Pipeline 7 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head4
	stage7_layer0_attn_head5 [label="Layer 14 Head 5 Attention\nGPU: Pipeline 7 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head5
	stage7_layer0_attn_head6 [label="Layer 14 Head 6 Attention\nGPU: Pipeline 7 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head6
	stage7_layer0_attn_head7 [label="Layer 14 Head 7 Attention\nGPU: Pipeline 7 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head7
	stage7_layer0_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_ellipsis
	stage7_layer0_attn_head24 [label="Layer 14 Head 24 Attention\nGPU: Pipeline 7 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head24
	stage7_layer0_attn_head25 [label="Layer 14 Head 25 Attention\nGPU: Pipeline 7 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head25
	stage7_layer0_attn_head26 [label="Layer 14 Head 26 Attention\nGPU: Pipeline 7 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head26
	stage7_layer0_attn_head27 [label="Layer 14 Head 27 Attention\nGPU: Pipeline 7 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head27
	stage7_layer0_attn_head28 [label="Layer 14 Head 28 Attention\nGPU: Pipeline 7 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head28
	stage7_layer0_attn_head29 [label="Layer 14 Head 29 Attention\nGPU: Pipeline 7 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head29
	stage7_layer0_attn_head30 [label="Layer 14 Head 30 Attention\nGPU: Pipeline 7 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head30
	stage7_layer0_attn_head31 [label="Layer 14 Head 31 Attention\nGPU: Pipeline 7 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_tp_qkv_comm -> stage7_layer0_attn_head31
	stage7_layer0_attn_agg [label="Layer 14 Attention Aggregate\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage7_layer0_attn_head0 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head1 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head2 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head3 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head4 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head5 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head6 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head7 -> stage7_layer0_attn_agg
	stage7_layer0_attn_ellipsis -> stage7_layer0_attn_agg
	stage7_layer0_attn_head24 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head25 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head26 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head27 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head28 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head29 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head30 -> stage7_layer0_attn_agg
	stage7_layer0_attn_head31 -> stage7_layer0_attn_agg
	stage7_layer0_attn_out [label="Layer 14 Attention Output Proj\nGPU: Pipeline 7 (TP: GPU 896-1023)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage7_layer0_attn_agg -> stage7_layer0_attn_out
	stage7_layer0_tp_attn_comm [label="Layer 14 TP Attention All-Reduce\nGPU: Pipeline 7 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage7_layer0_attn_out -> stage7_layer0_tp_attn_comm
	stage7_layer0_expert_router [label="Layer 14 Expert Router (Top-k)\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage7_layer0_tp_attn_comm -> stage7_layer0_expert_router
	stage7_layer0_expert0 [label="Layer 14 Expert 0\nGPU: 896-897 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert_router -> stage7_layer0_expert0 [style=dashed]
	stage7_layer0_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 896-897 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert0 -> stage7_layer0_expert0_mlp1
	stage7_layer0_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 896-897\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert0_mlp1 -> stage7_layer0_expert0_tp1
	stage7_layer0_expert0_act [label="Expert 0 GELU Activation\nGPU: 896-897\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert0_tp1 -> stage7_layer0_expert0_act
	stage7_layer0_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 896-897 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert0_act -> stage7_layer0_expert0_mlp2
	stage7_layer0_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 896-897\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert0_mlp2 -> stage7_layer0_expert0_tp2
	stage7_layer0_expert1 [label="Layer 14 Expert 1\nGPU: 898-899 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert_router -> stage7_layer0_expert1 [style=dashed]
	stage7_layer0_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 898-899 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert1 -> stage7_layer0_expert1_mlp1
	stage7_layer0_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 898-899\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert1_mlp1 -> stage7_layer0_expert1_tp1
	stage7_layer0_expert1_act [label="Expert 1 GELU Activation\nGPU: 898-899\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert1_tp1 -> stage7_layer0_expert1_act
	stage7_layer0_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 898-899 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert1_act -> stage7_layer0_expert1_mlp2
	stage7_layer0_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 898-899\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert1_mlp2 -> stage7_layer0_expert1_tp2
	stage7_layer0_expert2 [label="Layer 14 Expert 2\nGPU: 900-901 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert_router -> stage7_layer0_expert2 [style=dashed]
	stage7_layer0_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 900-901 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert2 -> stage7_layer0_expert2_mlp1
	stage7_layer0_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 900-901\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert2_mlp1 -> stage7_layer0_expert2_tp1
	stage7_layer0_expert2_act [label="Expert 2 GELU Activation\nGPU: 900-901\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert2_tp1 -> stage7_layer0_expert2_act
	stage7_layer0_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 900-901 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert2_act -> stage7_layer0_expert2_mlp2
	stage7_layer0_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 900-901\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert2_mlp2 -> stage7_layer0_expert2_tp2
	stage7_layer0_expert3 [label="Layer 14 Expert 3\nGPU: 902-903 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert_router -> stage7_layer0_expert3 [style=dashed]
	stage7_layer0_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 902-903 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert3 -> stage7_layer0_expert3_mlp1
	stage7_layer0_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 902-903\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert3_mlp1 -> stage7_layer0_expert3_tp1
	stage7_layer0_expert3_act [label="Expert 3 GELU Activation\nGPU: 902-903\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert3_tp1 -> stage7_layer0_expert3_act
	stage7_layer0_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 902-903 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert3_act -> stage7_layer0_expert3_mlp2
	stage7_layer0_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 902-903\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert3_mlp2 -> stage7_layer0_expert3_tp2
	stage7_layer0_expert31 [label="Layer 14 Expert 31\nGPU: 958-959 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert_router -> stage7_layer0_expert31 [style=dashed]
	stage7_layer0_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 958-959 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert31 -> stage7_layer0_expert31_mlp1
	stage7_layer0_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 958-959\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert31_mlp1 -> stage7_layer0_expert31_tp1
	stage7_layer0_expert31_act [label="Expert 31 GELU Activation\nGPU: 958-959\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert31_tp1 -> stage7_layer0_expert31_act
	stage7_layer0_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 958-959 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert31_act -> stage7_layer0_expert31_mlp2
	stage7_layer0_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 958-959\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert31_mlp2 -> stage7_layer0_expert31_tp2
	stage7_layer0_expert32 [label="Layer 14 Expert 32\nGPU: 960-961 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert_router -> stage7_layer0_expert32 [style=dashed]
	stage7_layer0_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 960-961 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert32 -> stage7_layer0_expert32_mlp1
	stage7_layer0_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 960-961\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert32_mlp1 -> stage7_layer0_expert32_tp1
	stage7_layer0_expert32_act [label="Expert 32 GELU Activation\nGPU: 960-961\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert32_tp1 -> stage7_layer0_expert32_act
	stage7_layer0_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 960-961 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert32_act -> stage7_layer0_expert32_mlp2
	stage7_layer0_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 960-961\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert32_mlp2 -> stage7_layer0_expert32_tp2
	stage7_layer0_expert60 [label="Layer 14 Expert 60\nGPU: 1016-1017 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert_router -> stage7_layer0_expert60 [style=dashed]
	stage7_layer0_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 1016-1017 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert60 -> stage7_layer0_expert60_mlp1
	stage7_layer0_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 1016-1017\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert60_mlp1 -> stage7_layer0_expert60_tp1
	stage7_layer0_expert60_act [label="Expert 60 GELU Activation\nGPU: 1016-1017\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert60_tp1 -> stage7_layer0_expert60_act
	stage7_layer0_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 1016-1017 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert60_act -> stage7_layer0_expert60_mlp2
	stage7_layer0_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 1016-1017\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert60_mlp2 -> stage7_layer0_expert60_tp2
	stage7_layer0_expert61 [label="Layer 14 Expert 61\nGPU: 1018-1019 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert_router -> stage7_layer0_expert61 [style=dashed]
	stage7_layer0_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 1018-1019 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert61 -> stage7_layer0_expert61_mlp1
	stage7_layer0_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 1018-1019\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert61_mlp1 -> stage7_layer0_expert61_tp1
	stage7_layer0_expert61_act [label="Expert 61 GELU Activation\nGPU: 1018-1019\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert61_tp1 -> stage7_layer0_expert61_act
	stage7_layer0_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 1018-1019 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert61_act -> stage7_layer0_expert61_mlp2
	stage7_layer0_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 1018-1019\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert61_mlp2 -> stage7_layer0_expert61_tp2
	stage7_layer0_expert62 [label="Layer 14 Expert 62\nGPU: 1020-1021 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert_router -> stage7_layer0_expert62 [style=dashed]
	stage7_layer0_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 1020-1021 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert62 -> stage7_layer0_expert62_mlp1
	stage7_layer0_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 1020-1021\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert62_mlp1 -> stage7_layer0_expert62_tp1
	stage7_layer0_expert62_act [label="Expert 62 GELU Activation\nGPU: 1020-1021\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert62_tp1 -> stage7_layer0_expert62_act
	stage7_layer0_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 1020-1021 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert62_act -> stage7_layer0_expert62_mlp2
	stage7_layer0_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 1020-1021\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert62_mlp2 -> stage7_layer0_expert62_tp2
	stage7_layer0_expert63 [label="Layer 14 Expert 63\nGPU: 1022-1023 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert_router -> stage7_layer0_expert63 [style=dashed]
	stage7_layer0_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 1022-1023 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert63 -> stage7_layer0_expert63_mlp1
	stage7_layer0_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 1022-1023\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert63_mlp1 -> stage7_layer0_expert63_tp1
	stage7_layer0_expert63_act [label="Expert 63 GELU Activation\nGPU: 1022-1023\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert63_tp1 -> stage7_layer0_expert63_act
	stage7_layer0_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 1022-1023 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer0_expert63_act -> stage7_layer0_expert63_mlp2
	stage7_layer0_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 1022-1023\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer0_expert63_mlp2 -> stage7_layer0_expert63_tp2
	stage7_layer0_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 7 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage7_layer0_expert_router -> stage7_layer0_expert_ellipsis [style=dashed]
	stage7_layer0_expert_agg [label="Layer 14 Expert Aggregation\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage7_layer0_expert0_tp2 -> stage7_layer0_expert_agg
	stage7_layer0_expert1_tp2 -> stage7_layer0_expert_agg
	stage7_layer0_expert2_tp2 -> stage7_layer0_expert_agg
	stage7_layer0_expert3_tp2 -> stage7_layer0_expert_agg
	stage7_layer0_expert31_tp2 -> stage7_layer0_expert_agg
	stage7_layer0_expert32_tp2 -> stage7_layer0_expert_agg
	stage7_layer0_expert60_tp2 -> stage7_layer0_expert_agg
	stage7_layer0_expert61_tp2 -> stage7_layer0_expert_agg
	stage7_layer0_expert62_tp2 -> stage7_layer0_expert_agg
	stage7_layer0_expert63_tp2 -> stage7_layer0_expert_agg
	stage7_layer0_expert_ellipsis -> stage7_layer0_expert_agg
	stage7_layer0_output [label="Layer 14 Output\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage7_layer0_expert_agg -> stage7_layer0_output
	stage7_layer0_tp_attn_comm -> stage7_layer0_output [style=invis]
	stage7_layer1_ln [label="Layer 15 LayerNorm\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage7_layer0_output -> stage7_layer1_ln
	stage7_layer1_attn_qkv [label="Layer 15 Attention QKV Projection\nGPU: Pipeline 7 (TP: GPU 896-1023)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=lightgreen shape=rectangle]
	stage7_layer1_ln -> stage7_layer1_attn_qkv
	stage7_layer1_tp_qkv_comm [label="Layer 15 TP QKV All-Gather\nGPU: Pipeline 7 (TP pairs)\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, heads=32, d_k=128]" fillcolor=orange shape=ellipse]
	stage7_layer1_attn_qkv -> stage7_layer1_tp_qkv_comm
	stage7_layer1_attn_head0 [label="Layer 15 Head 0 Attention\nGPU: Pipeline 7 (Head 0)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head0
	stage7_layer1_attn_head1 [label="Layer 15 Head 1 Attention\nGPU: Pipeline 7 (Head 1)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head1
	stage7_layer1_attn_head2 [label="Layer 15 Head 2 Attention\nGPU: Pipeline 7 (Head 2)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head2
	stage7_layer1_attn_head3 [label="Layer 15 Head 3 Attention\nGPU: Pipeline 7 (Head 3)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head3
	stage7_layer1_attn_head4 [label="Layer 15 Head 4 Attention\nGPU: Pipeline 7 (Head 4)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head4
	stage7_layer1_attn_head5 [label="Layer 15 Head 5 Attention\nGPU: Pipeline 7 (Head 5)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head5
	stage7_layer1_attn_head6 [label="Layer 15 Head 6 Attention\nGPU: Pipeline 7 (Head 6)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head6
	stage7_layer1_attn_head7 [label="Layer 15 Head 7 Attention\nGPU: Pipeline 7 (Head 7)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head7
	stage7_layer1_attn_ellipsis [label="... 16 more heads ...\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_ellipsis
	stage7_layer1_attn_head24 [label="Layer 15 Head 24 Attention\nGPU: Pipeline 7 (Head 24)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head24
	stage7_layer1_attn_head25 [label="Layer 15 Head 25 Attention\nGPU: Pipeline 7 (Head 25)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head25
	stage7_layer1_attn_head26 [label="Layer 15 Head 26 Attention\nGPU: Pipeline 7 (Head 26)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head26
	stage7_layer1_attn_head27 [label="Layer 15 Head 27 Attention\nGPU: Pipeline 7 (Head 27)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head27
	stage7_layer1_attn_head28 [label="Layer 15 Head 28 Attention\nGPU: Pipeline 7 (Head 28)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head28
	stage7_layer1_attn_head29 [label="Layer 15 Head 29 Attention\nGPU: Pipeline 7 (Head 29)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head29
	stage7_layer1_attn_head30 [label="Layer 15 Head 30 Attention\nGPU: Pipeline 7 (Head 30)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head30
	stage7_layer1_attn_head31 [label="Layer 15 Head 31 Attention\nGPU: Pipeline 7 (Head 31)\nInput: [batch_size=128, seq_len=1024, d_k=128]\nOutput: [batch_size=128, seq_len=1024, d_k=128]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_tp_qkv_comm -> stage7_layer1_attn_head31
	stage7_layer1_attn_agg [label="Layer 15 Attention Aggregate\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage7_layer1_attn_head0 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head1 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head2 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head3 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head4 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head5 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head6 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head7 -> stage7_layer1_attn_agg
	stage7_layer1_attn_ellipsis -> stage7_layer1_attn_agg
	stage7_layer1_attn_head24 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head25 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head26 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head27 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head28 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head29 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head30 -> stage7_layer1_attn_agg
	stage7_layer1_attn_head31 -> stage7_layer1_attn_agg
	stage7_layer1_attn_out [label="Layer 15 Attention Output Proj\nGPU: Pipeline 7 (TP: GPU 896-1023)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage7_layer1_attn_agg -> stage7_layer1_attn_out
	stage7_layer1_tp_attn_comm [label="Layer 15 TP Attention All-Reduce\nGPU: Pipeline 7 (TP pairs)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	stage7_layer1_attn_out -> stage7_layer1_tp_attn_comm
	stage7_layer1_expert_router [label="Layer 15 Expert Router (Top-k)\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, experts=64]" fillcolor=yellow shape=parallelogram]
	stage7_layer1_tp_attn_comm -> stage7_layer1_expert_router
	stage7_layer1_expert0 [label="Layer 15 Expert 0\nGPU: 896-897 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert_router -> stage7_layer1_expert0 [style=dashed]
	stage7_layer1_expert0_mlp1 [label="Expert 0 MLP Layer 1\nGPU: 896-897 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert0 -> stage7_layer1_expert0_mlp1
	stage7_layer1_expert0_tp1 [label="Expert 0 TP All-Gather\nGPU: 896-897\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert0_mlp1 -> stage7_layer1_expert0_tp1
	stage7_layer1_expert0_act [label="Expert 0 GELU Activation\nGPU: 896-897\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert0_tp1 -> stage7_layer1_expert0_act
	stage7_layer1_expert0_mlp2 [label="Expert 0 MLP Layer 2\nGPU: 896-897 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert0_act -> stage7_layer1_expert0_mlp2
	stage7_layer1_expert0_tp2 [label="Expert 0 TP All-Reduce\nGPU: 896-897\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert0_mlp2 -> stage7_layer1_expert0_tp2
	stage7_layer1_expert1 [label="Layer 15 Expert 1\nGPU: 898-899 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert_router -> stage7_layer1_expert1 [style=dashed]
	stage7_layer1_expert1_mlp1 [label="Expert 1 MLP Layer 1\nGPU: 898-899 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert1 -> stage7_layer1_expert1_mlp1
	stage7_layer1_expert1_tp1 [label="Expert 1 TP All-Gather\nGPU: 898-899\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert1_mlp1 -> stage7_layer1_expert1_tp1
	stage7_layer1_expert1_act [label="Expert 1 GELU Activation\nGPU: 898-899\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert1_tp1 -> stage7_layer1_expert1_act
	stage7_layer1_expert1_mlp2 [label="Expert 1 MLP Layer 2\nGPU: 898-899 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert1_act -> stage7_layer1_expert1_mlp2
	stage7_layer1_expert1_tp2 [label="Expert 1 TP All-Reduce\nGPU: 898-899\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert1_mlp2 -> stage7_layer1_expert1_tp2
	stage7_layer1_expert2 [label="Layer 15 Expert 2\nGPU: 900-901 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert_router -> stage7_layer1_expert2 [style=dashed]
	stage7_layer1_expert2_mlp1 [label="Expert 2 MLP Layer 1\nGPU: 900-901 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert2 -> stage7_layer1_expert2_mlp1
	stage7_layer1_expert2_tp1 [label="Expert 2 TP All-Gather\nGPU: 900-901\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert2_mlp1 -> stage7_layer1_expert2_tp1
	stage7_layer1_expert2_act [label="Expert 2 GELU Activation\nGPU: 900-901\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert2_tp1 -> stage7_layer1_expert2_act
	stage7_layer1_expert2_mlp2 [label="Expert 2 MLP Layer 2\nGPU: 900-901 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert2_act -> stage7_layer1_expert2_mlp2
	stage7_layer1_expert2_tp2 [label="Expert 2 TP All-Reduce\nGPU: 900-901\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert2_mlp2 -> stage7_layer1_expert2_tp2
	stage7_layer1_expert3 [label="Layer 15 Expert 3\nGPU: 902-903 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert_router -> stage7_layer1_expert3 [style=dashed]
	stage7_layer1_expert3_mlp1 [label="Expert 3 MLP Layer 1\nGPU: 902-903 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert3 -> stage7_layer1_expert3_mlp1
	stage7_layer1_expert3_tp1 [label="Expert 3 TP All-Gather\nGPU: 902-903\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert3_mlp1 -> stage7_layer1_expert3_tp1
	stage7_layer1_expert3_act [label="Expert 3 GELU Activation\nGPU: 902-903\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert3_tp1 -> stage7_layer1_expert3_act
	stage7_layer1_expert3_mlp2 [label="Expert 3 MLP Layer 2\nGPU: 902-903 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert3_act -> stage7_layer1_expert3_mlp2
	stage7_layer1_expert3_tp2 [label="Expert 3 TP All-Reduce\nGPU: 902-903\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert3_mlp2 -> stage7_layer1_expert3_tp2
	stage7_layer1_expert31 [label="Layer 15 Expert 31\nGPU: 958-959 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert_router -> stage7_layer1_expert31 [style=dashed]
	stage7_layer1_expert31_mlp1 [label="Expert 31 MLP Layer 1\nGPU: 958-959 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert31 -> stage7_layer1_expert31_mlp1
	stage7_layer1_expert31_tp1 [label="Expert 31 TP All-Gather\nGPU: 958-959\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert31_mlp1 -> stage7_layer1_expert31_tp1
	stage7_layer1_expert31_act [label="Expert 31 GELU Activation\nGPU: 958-959\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert31_tp1 -> stage7_layer1_expert31_act
	stage7_layer1_expert31_mlp2 [label="Expert 31 MLP Layer 2\nGPU: 958-959 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert31_act -> stage7_layer1_expert31_mlp2
	stage7_layer1_expert31_tp2 [label="Expert 31 TP All-Reduce\nGPU: 958-959\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert31_mlp2 -> stage7_layer1_expert31_tp2
	stage7_layer1_expert32 [label="Layer 15 Expert 32\nGPU: 960-961 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert_router -> stage7_layer1_expert32 [style=dashed]
	stage7_layer1_expert32_mlp1 [label="Expert 32 MLP Layer 1\nGPU: 960-961 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert32 -> stage7_layer1_expert32_mlp1
	stage7_layer1_expert32_tp1 [label="Expert 32 TP All-Gather\nGPU: 960-961\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert32_mlp1 -> stage7_layer1_expert32_tp1
	stage7_layer1_expert32_act [label="Expert 32 GELU Activation\nGPU: 960-961\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert32_tp1 -> stage7_layer1_expert32_act
	stage7_layer1_expert32_mlp2 [label="Expert 32 MLP Layer 2\nGPU: 960-961 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert32_act -> stage7_layer1_expert32_mlp2
	stage7_layer1_expert32_tp2 [label="Expert 32 TP All-Reduce\nGPU: 960-961\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert32_mlp2 -> stage7_layer1_expert32_tp2
	stage7_layer1_expert60 [label="Layer 15 Expert 60\nGPU: 1016-1017 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert_router -> stage7_layer1_expert60 [style=dashed]
	stage7_layer1_expert60_mlp1 [label="Expert 60 MLP Layer 1\nGPU: 1016-1017 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert60 -> stage7_layer1_expert60_mlp1
	stage7_layer1_expert60_tp1 [label="Expert 60 TP All-Gather\nGPU: 1016-1017\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert60_mlp1 -> stage7_layer1_expert60_tp1
	stage7_layer1_expert60_act [label="Expert 60 GELU Activation\nGPU: 1016-1017\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert60_tp1 -> stage7_layer1_expert60_act
	stage7_layer1_expert60_mlp2 [label="Expert 60 MLP Layer 2\nGPU: 1016-1017 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert60_act -> stage7_layer1_expert60_mlp2
	stage7_layer1_expert60_tp2 [label="Expert 60 TP All-Reduce\nGPU: 1016-1017\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert60_mlp2 -> stage7_layer1_expert60_tp2
	stage7_layer1_expert61 [label="Layer 15 Expert 61\nGPU: 1018-1019 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert_router -> stage7_layer1_expert61 [style=dashed]
	stage7_layer1_expert61_mlp1 [label="Expert 61 MLP Layer 1\nGPU: 1018-1019 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert61 -> stage7_layer1_expert61_mlp1
	stage7_layer1_expert61_tp1 [label="Expert 61 TP All-Gather\nGPU: 1018-1019\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert61_mlp1 -> stage7_layer1_expert61_tp1
	stage7_layer1_expert61_act [label="Expert 61 GELU Activation\nGPU: 1018-1019\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert61_tp1 -> stage7_layer1_expert61_act
	stage7_layer1_expert61_mlp2 [label="Expert 61 MLP Layer 2\nGPU: 1018-1019 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert61_act -> stage7_layer1_expert61_mlp2
	stage7_layer1_expert61_tp2 [label="Expert 61 TP All-Reduce\nGPU: 1018-1019\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert61_mlp2 -> stage7_layer1_expert61_tp2
	stage7_layer1_expert62 [label="Layer 15 Expert 62\nGPU: 1020-1021 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert_router -> stage7_layer1_expert62 [style=dashed]
	stage7_layer1_expert62_mlp1 [label="Expert 62 MLP Layer 1\nGPU: 1020-1021 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert62 -> stage7_layer1_expert62_mlp1
	stage7_layer1_expert62_tp1 [label="Expert 62 TP All-Gather\nGPU: 1020-1021\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert62_mlp1 -> stage7_layer1_expert62_tp1
	stage7_layer1_expert62_act [label="Expert 62 GELU Activation\nGPU: 1020-1021\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert62_tp1 -> stage7_layer1_expert62_act
	stage7_layer1_expert62_mlp2 [label="Expert 62 MLP Layer 2\nGPU: 1020-1021 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert62_act -> stage7_layer1_expert62_mlp2
	stage7_layer1_expert62_tp2 [label="Expert 62 TP All-Reduce\nGPU: 1020-1021\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert62_mlp2 -> stage7_layer1_expert62_tp2
	stage7_layer1_expert63 [label="Layer 15 Expert 63\nGPU: 1022-1023 (TP pair)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert_router -> stage7_layer1_expert63 [style=dashed]
	stage7_layer1_expert63_mlp1 [label="Expert 63 MLP Layer 1\nGPU: 1022-1023 (TP: Column Parallel)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, ffn=8192]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert63 -> stage7_layer1_expert63_mlp1
	stage7_layer1_expert63_tp1 [label="Expert 63 TP All-Gather\nGPU: 1022-1023\nInput: [batch_size=128, seq_len=1024, ffn=8192]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert63_mlp1 -> stage7_layer1_expert63_tp1
	stage7_layer1_expert63_act [label="Expert 63 GELU Activation\nGPU: 1022-1023\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, ffn=16384]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert63_tp1 -> stage7_layer1_expert63_act
	stage7_layer1_expert63_mlp2 [label="Expert 63 MLP Layer 2\nGPU: 1022-1023 (TP: Row Parallel)\nInput: [batch_size=128, seq_len=1024, ffn=16384]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle]
	stage7_layer1_expert63_act -> stage7_layer1_expert63_mlp2
	stage7_layer1_expert63_tp2 [label="Expert 63 TP All-Reduce\nGPU: 1022-1023\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=orange fontsize=10 shape=ellipse]
	stage7_layer1_expert63_mlp2 -> stage7_layer1_expert63_tp2
	stage7_layer1_expert_ellipsis [label="... 54 more experts ...\nGPU: Pipeline 7 (Experts 4-59)\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen fontsize=10 shape=rectangle style=dashed]
	stage7_layer1_expert_router -> stage7_layer1_expert_ellipsis [style=dashed]
	stage7_layer1_expert_agg [label="Layer 15 Expert Aggregation\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, experts=64, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage7_layer1_expert0_tp2 -> stage7_layer1_expert_agg
	stage7_layer1_expert1_tp2 -> stage7_layer1_expert_agg
	stage7_layer1_expert2_tp2 -> stage7_layer1_expert_agg
	stage7_layer1_expert3_tp2 -> stage7_layer1_expert_agg
	stage7_layer1_expert31_tp2 -> stage7_layer1_expert_agg
	stage7_layer1_expert32_tp2 -> stage7_layer1_expert_agg
	stage7_layer1_expert60_tp2 -> stage7_layer1_expert_agg
	stage7_layer1_expert61_tp2 -> stage7_layer1_expert_agg
	stage7_layer1_expert62_tp2 -> stage7_layer1_expert_agg
	stage7_layer1_expert63_tp2 -> stage7_layer1_expert_agg
	stage7_layer1_expert_ellipsis -> stage7_layer1_expert_agg
	stage7_layer1_output [label="Layer 15 Output\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	stage7_layer1_expert_agg -> stage7_layer1_output
	stage7_output [label="Stage 7 Output\nGPU: Pipeline 7\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=yellow shape=parallelogram]
	stage7_layer1_output -> stage7_output
	output [label="Output\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, hidden_size=4096]" fillcolor=lightblue shape=ellipse]
	stage7_output -> output
}
