// Baseline MoE (TP=8, PP=2)
digraph baseline_moe_tp8_pp2 {
	compound=true rankdir=TB size="30,40"
	node [fontname=Arial fontsize=10]
	subgraph cluster_stage0 {
		bgcolor=lightcyan color=black label="Pipeline Stage 0 (GPUs 0-7)" style=rounded
		stage0_input [label="Stage 0 Input
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0-7" fillcolor=lightblue shape=ellipse style=filled]
		subgraph cluster_layer0_s0 {
			color=blue label="Layer 0 (Stage 0)" style=rounded
			input_split_l0_tp0 [label="Input Split TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 0" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l0_tp0 [label="MHA QKV TP0
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l0_tp0 [label="MHA Attention TP0
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l0_tp0 [label="MHA Output TP0
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l0_tp0 [label="Attention All-Reduce TP0
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l0_tp0 [label="Residual Add TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l0_tp0 [label="Layer Norm TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l0_tp1 [label="Input Split TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 1" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l0_tp1 [label="MHA QKV TP1
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l0_tp1 [label="MHA Attention TP1
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l0_tp1 [label="MHA Output TP1
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l0_tp1 [label="Attention All-Reduce TP1
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 1" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l0_tp1 [label="Residual Add TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l0_tp1 [label="Layer Norm TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l0_tp2 [label="Input Split TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 2" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l0_tp2 [label="MHA QKV TP2
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l0_tp2 [label="MHA Attention TP2
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l0_tp2 [label="MHA Output TP2
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l0_tp2 [label="Attention All-Reduce TP2
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 2" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l0_tp2 [label="Residual Add TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l0_tp2 [label="Layer Norm TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l0_tp3 [label="Input Split TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 3" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l0_tp3 [label="MHA QKV TP3
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l0_tp3 [label="MHA Attention TP3
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l0_tp3 [label="MHA Output TP3
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l0_tp3 [label="Attention All-Reduce TP3
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 3" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l0_tp3 [label="Residual Add TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l0_tp3 [label="Layer Norm TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l0_tp4 [label="Input Split TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 4" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l0_tp4 [label="MHA QKV TP4
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l0_tp4 [label="MHA Attention TP4
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l0_tp4 [label="MHA Output TP4
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l0_tp4 [label="Attention All-Reduce TP4
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 4" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l0_tp4 [label="Residual Add TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l0_tp4 [label="Layer Norm TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l0_tp5 [label="Input Split TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 5" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l0_tp5 [label="MHA QKV TP5
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l0_tp5 [label="MHA Attention TP5
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l0_tp5 [label="MHA Output TP5
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l0_tp5 [label="Attention All-Reduce TP5
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 5" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l0_tp5 [label="Residual Add TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l0_tp5 [label="Layer Norm TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l0_tp6 [label="Input Split TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 6" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l0_tp6 [label="MHA QKV TP6
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l0_tp6 [label="MHA Attention TP6
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l0_tp6 [label="MHA Output TP6
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l0_tp6 [label="Attention All-Reduce TP6
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 6" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l0_tp6 [label="Residual Add TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l0_tp6 [label="Layer Norm TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l0_tp7 [label="Input Split TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 7" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l0_tp7 [label="MHA QKV TP7
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l0_tp7 [label="MHA Attention TP7
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l0_tp7 [label="MHA Output TP7
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l0_tp7 [label="Attention All-Reduce TP7
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 7" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l0_tp7 [label="Residual Add TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l0_tp7 [label="Layer Norm TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l0_tp0 [label="Gate Network TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 0" fillcolor=lightyellow shape=parallelogram style=filled]
			gate_l0_tp1 [label="Gate Network TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 1" fillcolor=lightyellow shape=parallelogram style=filled]
			gate_l0_tp2 [label="Gate Network TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 2" fillcolor=lightyellow shape=parallelogram style=filled]
			gate_l0_tp3 [label="Gate Network TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 3" fillcolor=lightyellow shape=parallelogram style=filled]
			gate_l0_tp4 [label="Gate Network TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 4" fillcolor=lightyellow shape=parallelogram style=filled]
			gate_l0_tp5 [label="Gate Network TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 5" fillcolor=lightyellow shape=parallelogram style=filled]
			gate_l0_tp6 [label="Gate Network TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 6" fillcolor=lightyellow shape=parallelogram style=filled]
			gate_l0_tp7 [label="Gate Network TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 7" fillcolor=lightyellow shape=parallelogram style=filled]
			expert0_l0_tp0 [label="Expert 0
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert1_l0_tp0 [label="Expert 1
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert2_l0_tp0 [label="Expert 2
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert3_l0_tp0 [label="Expert 3
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert4_l0_tp0 [label="Expert 4
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert5_l0_tp0 [label="Expert 5
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert6_l0_tp0 [label="Expert 6
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert7_l0_tp0 [label="Expert 7
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert8_l0_tp1 [label="Expert 8
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert9_l0_tp1 [label="Expert 9
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert10_l0_tp1 [label="Expert 10
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert11_l0_tp1 [label="Expert 11
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert12_l0_tp1 [label="Expert 12
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert13_l0_tp1 [label="Expert 13
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert14_l0_tp1 [label="Expert 14
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert15_l0_tp1 [label="Expert 15
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert16_l0_tp2 [label="Expert 16
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert17_l0_tp2 [label="Expert 17
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert18_l0_tp2 [label="Expert 18
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert19_l0_tp2 [label="Expert 19
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert20_l0_tp2 [label="Expert 20
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert21_l0_tp2 [label="Expert 21
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert22_l0_tp2 [label="Expert 22
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert23_l0_tp2 [label="Expert 23
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert24_l0_tp3 [label="Expert 24
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert25_l0_tp3 [label="Expert 25
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert26_l0_tp3 [label="Expert 26
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert27_l0_tp3 [label="Expert 27
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert28_l0_tp3 [label="Expert 28
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert29_l0_tp3 [label="Expert 29
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert30_l0_tp3 [label="Expert 30
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert31_l0_tp3 [label="Expert 31
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert32_l0_tp4 [label="Expert 32
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert33_l0_tp4 [label="Expert 33
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert34_l0_tp4 [label="Expert 34
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert35_l0_tp4 [label="Expert 35
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert36_l0_tp4 [label="Expert 36
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert37_l0_tp4 [label="Expert 37
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert38_l0_tp4 [label="Expert 38
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert39_l0_tp4 [label="Expert 39
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert40_l0_tp5 [label="Expert 40
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert41_l0_tp5 [label="Expert 41
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert42_l0_tp5 [label="Expert 42
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert43_l0_tp5 [label="Expert 43
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert44_l0_tp5 [label="Expert 44
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert45_l0_tp5 [label="Expert 45
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert46_l0_tp5 [label="Expert 46
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert47_l0_tp5 [label="Expert 47
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert48_l0_tp6 [label="Expert 48
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert49_l0_tp6 [label="Expert 49
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert50_l0_tp6 [label="Expert 50
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert51_l0_tp6 [label="Expert 51
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert52_l0_tp6 [label="Expert 52
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert53_l0_tp6 [label="Expert 53
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert54_l0_tp6 [label="Expert 54
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert55_l0_tp6 [label="Expert 55
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert56_l0_tp7 [label="Expert 56
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert57_l0_tp7 [label="Expert 57
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert58_l0_tp7 [label="Expert 58
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert59_l0_tp7 [label="Expert 59
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert60_l0_tp7 [label="Expert 60
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert61_l0_tp7 [label="Expert 61
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert62_l0_tp7 [label="Expert 62
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert63_l0_tp7 [label="Expert 63
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l0_tp0 [label="Expert Aggregation TP0
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l0_tp0 [label="Residual Add 2 TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l0_tp0 [label="Layer Norm 2 TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			expert_agg_l0_tp1 [label="Expert Aggregation TP1
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 1" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l0_tp1 [label="Residual Add 2 TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l0_tp1 [label="Layer Norm 2 TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			expert_agg_l0_tp2 [label="Expert Aggregation TP2
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 2" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l0_tp2 [label="Residual Add 2 TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l0_tp2 [label="Layer Norm 2 TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			expert_agg_l0_tp3 [label="Expert Aggregation TP3
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 3" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l0_tp3 [label="Residual Add 2 TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l0_tp3 [label="Layer Norm 2 TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			expert_agg_l0_tp4 [label="Expert Aggregation TP4
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 4" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l0_tp4 [label="Residual Add 2 TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l0_tp4 [label="Layer Norm 2 TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			expert_agg_l0_tp5 [label="Expert Aggregation TP5
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 5" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l0_tp5 [label="Residual Add 2 TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l0_tp5 [label="Layer Norm 2 TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			expert_agg_l0_tp6 [label="Expert Aggregation TP6
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 6" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l0_tp6 [label="Residual Add 2 TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l0_tp6 [label="Layer Norm 2 TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			expert_agg_l0_tp7 [label="Expert Aggregation TP7
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 7" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l0_tp7 [label="Residual Add 2 TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l0_tp7 [label="Layer Norm 2 TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
		}
		subgraph cluster_layer1_s0 {
			color=green label="Layer 1 (Stage 0)" style=rounded
			input_split_l1_tp0 [label="Input Split TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 0" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l1_tp0 [label="MHA QKV TP0
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l1_tp0 [label="MHA Attention TP0
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l1_tp0 [label="MHA Output TP0
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l1_tp0 [label="Attention All-Reduce TP0
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l1_tp0 [label="Residual Add TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l1_tp0 [label="Layer Norm TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l1_tp0 [label="Gate Network TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 0" fillcolor=lightyellow shape=parallelogram style=filled]
			expert0_l1_tp0 [label="Expert 0
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert1_l1_tp0 [label="Expert 1
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert2_l1_tp0 [label="Expert 2
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert3_l1_tp0 [label="Expert 3
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert4_l1_tp0 [label="Expert 4
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert5_l1_tp0 [label="Expert 5
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert6_l1_tp0 [label="Expert 6
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert7_l1_tp0 [label="Expert 7
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l1_tp0 [label="Expert Aggregation TP0
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l1_tp0 [label="Residual Add 2 TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l1_tp0 [label="Layer Norm 2 TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l1_tp1 [label="Input Split TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 1" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l1_tp1 [label="MHA QKV TP1
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l1_tp1 [label="MHA Attention TP1
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l1_tp1 [label="MHA Output TP1
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l1_tp1 [label="Attention All-Reduce TP1
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 1" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l1_tp1 [label="Residual Add TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l1_tp1 [label="Layer Norm TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l1_tp1 [label="Gate Network TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 1" fillcolor=lightyellow shape=parallelogram style=filled]
			expert8_l1_tp1 [label="Expert 8
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert9_l1_tp1 [label="Expert 9
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert10_l1_tp1 [label="Expert 10
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert11_l1_tp1 [label="Expert 11
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert12_l1_tp1 [label="Expert 12
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert13_l1_tp1 [label="Expert 13
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert14_l1_tp1 [label="Expert 14
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert15_l1_tp1 [label="Expert 15
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l1_tp1 [label="Expert Aggregation TP1
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 1" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l1_tp1 [label="Residual Add 2 TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l1_tp1 [label="Layer Norm 2 TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 1" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l1_tp2 [label="Input Split TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 2" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l1_tp2 [label="MHA QKV TP2
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l1_tp2 [label="MHA Attention TP2
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l1_tp2 [label="MHA Output TP2
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l1_tp2 [label="Attention All-Reduce TP2
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 2" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l1_tp2 [label="Residual Add TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l1_tp2 [label="Layer Norm TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l1_tp2 [label="Gate Network TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 2" fillcolor=lightyellow shape=parallelogram style=filled]
			expert16_l1_tp2 [label="Expert 16
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert17_l1_tp2 [label="Expert 17
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert18_l1_tp2 [label="Expert 18
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert19_l1_tp2 [label="Expert 19
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert20_l1_tp2 [label="Expert 20
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert21_l1_tp2 [label="Expert 21
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert22_l1_tp2 [label="Expert 22
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert23_l1_tp2 [label="Expert 23
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l1_tp2 [label="Expert Aggregation TP2
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 2" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l1_tp2 [label="Residual Add 2 TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l1_tp2 [label="Layer Norm 2 TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 2" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l1_tp3 [label="Input Split TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 3" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l1_tp3 [label="MHA QKV TP3
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l1_tp3 [label="MHA Attention TP3
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l1_tp3 [label="MHA Output TP3
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l1_tp3 [label="Attention All-Reduce TP3
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 3" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l1_tp3 [label="Residual Add TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l1_tp3 [label="Layer Norm TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l1_tp3 [label="Gate Network TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 3" fillcolor=lightyellow shape=parallelogram style=filled]
			expert24_l1_tp3 [label="Expert 24
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert25_l1_tp3 [label="Expert 25
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert26_l1_tp3 [label="Expert 26
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert27_l1_tp3 [label="Expert 27
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert28_l1_tp3 [label="Expert 28
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert29_l1_tp3 [label="Expert 29
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert30_l1_tp3 [label="Expert 30
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert31_l1_tp3 [label="Expert 31
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l1_tp3 [label="Expert Aggregation TP3
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 3" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l1_tp3 [label="Residual Add 2 TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l1_tp3 [label="Layer Norm 2 TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 3" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l1_tp4 [label="Input Split TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 4" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l1_tp4 [label="MHA QKV TP4
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l1_tp4 [label="MHA Attention TP4
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l1_tp4 [label="MHA Output TP4
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l1_tp4 [label="Attention All-Reduce TP4
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 4" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l1_tp4 [label="Residual Add TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l1_tp4 [label="Layer Norm TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l1_tp4 [label="Gate Network TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 4" fillcolor=lightyellow shape=parallelogram style=filled]
			expert32_l1_tp4 [label="Expert 32
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert33_l1_tp4 [label="Expert 33
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert34_l1_tp4 [label="Expert 34
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert35_l1_tp4 [label="Expert 35
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert36_l1_tp4 [label="Expert 36
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert37_l1_tp4 [label="Expert 37
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert38_l1_tp4 [label="Expert 38
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert39_l1_tp4 [label="Expert 39
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l1_tp4 [label="Expert Aggregation TP4
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 4" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l1_tp4 [label="Residual Add 2 TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l1_tp4 [label="Layer Norm 2 TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 4" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l1_tp5 [label="Input Split TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 5" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l1_tp5 [label="MHA QKV TP5
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l1_tp5 [label="MHA Attention TP5
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l1_tp5 [label="MHA Output TP5
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l1_tp5 [label="Attention All-Reduce TP5
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 5" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l1_tp5 [label="Residual Add TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l1_tp5 [label="Layer Norm TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l1_tp5 [label="Gate Network TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 5" fillcolor=lightyellow shape=parallelogram style=filled]
			expert40_l1_tp5 [label="Expert 40
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert41_l1_tp5 [label="Expert 41
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert42_l1_tp5 [label="Expert 42
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert43_l1_tp5 [label="Expert 43
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert44_l1_tp5 [label="Expert 44
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert45_l1_tp5 [label="Expert 45
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert46_l1_tp5 [label="Expert 46
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert47_l1_tp5 [label="Expert 47
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l1_tp5 [label="Expert Aggregation TP5
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 5" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l1_tp5 [label="Residual Add 2 TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l1_tp5 [label="Layer Norm 2 TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 5" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l1_tp6 [label="Input Split TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 6" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l1_tp6 [label="MHA QKV TP6
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l1_tp6 [label="MHA Attention TP6
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l1_tp6 [label="MHA Output TP6
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l1_tp6 [label="Attention All-Reduce TP6
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 6" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l1_tp6 [label="Residual Add TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l1_tp6 [label="Layer Norm TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l1_tp6 [label="Gate Network TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 6" fillcolor=lightyellow shape=parallelogram style=filled]
			expert48_l1_tp6 [label="Expert 48
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert49_l1_tp6 [label="Expert 49
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert50_l1_tp6 [label="Expert 50
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert51_l1_tp6 [label="Expert 51
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert52_l1_tp6 [label="Expert 52
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert53_l1_tp6 [label="Expert 53
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert54_l1_tp6 [label="Expert 54
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert55_l1_tp6 [label="Expert 55
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l1_tp6 [label="Expert Aggregation TP6
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 6" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l1_tp6 [label="Residual Add 2 TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l1_tp6 [label="Layer Norm 2 TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 6" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l1_tp7 [label="Input Split TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 7" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l1_tp7 [label="MHA QKV TP7
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l1_tp7 [label="MHA Attention TP7
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l1_tp7 [label="MHA Output TP7
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l1_tp7 [label="Attention All-Reduce TP7
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 7" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l1_tp7 [label="Residual Add TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l1_tp7 [label="Layer Norm TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l1_tp7 [label="Gate Network TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 7" fillcolor=lightyellow shape=parallelogram style=filled]
			expert56_l1_tp7 [label="Expert 56
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert57_l1_tp7 [label="Expert 57
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert58_l1_tp7 [label="Expert 58
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert59_l1_tp7 [label="Expert 59
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert60_l1_tp7 [label="Expert 60
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert61_l1_tp7 [label="Expert 61
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert62_l1_tp7 [label="Expert 62
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert63_l1_tp7 [label="Expert 63
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l1_tp7 [label="Expert Aggregation TP7
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 7" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l1_tp7 [label="Residual Add 2 TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l1_tp7 [label="Layer Norm 2 TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 7" fillcolor=lightgreen shape=rectangle style=filled]
		}
	}
	subgraph cluster_stage1 {
		bgcolor=lightyellow color=black label="Pipeline Stage 1 (GPUs 8-15)" style=rounded
		pipeline_comm_01 [label="Pipeline Communication S0S1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 0-7  8-15" fillcolor=lightsteelblue shape=ellipse style=filled]
		subgraph cluster_layer2_s1 {
			color=red label="Layer 2 (Stage 1)" style=rounded
			input_split_l2_tp0 [label="Input Split TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 8" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l2_tp0 [label="MHA QKV TP0
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l2_tp0 [label="MHA Attention TP0
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l2_tp0 [label="MHA Output TP0
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l2_tp0 [label="Attention All-Reduce TP0
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 8" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l2_tp0 [label="Residual Add TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l2_tp0 [label="Layer Norm TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l2_tp0 [label="Gate Network TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 8" fillcolor=lightyellow shape=parallelogram style=filled]
			expert8_l2_tp0 [label="Expert 8
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert9_l2_tp0 [label="Expert 9
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert10_l2_tp0 [label="Expert 10
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert11_l2_tp0 [label="Expert 11
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert12_l2_tp0 [label="Expert 12
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert13_l2_tp0 [label="Expert 13
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert14_l2_tp0 [label="Expert 14
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert15_l2_tp0 [label="Expert 15
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l2_tp0 [label="Expert Aggregation TP0
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 8" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l2_tp0 [label="Residual Add 2 TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l2_tp0 [label="Layer Norm 2 TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l2_tp1 [label="Input Split TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 9" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l2_tp1 [label="MHA QKV TP1
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l2_tp1 [label="MHA Attention TP1
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l2_tp1 [label="MHA Output TP1
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l2_tp1 [label="Attention All-Reduce TP1
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 9" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l2_tp1 [label="Residual Add TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l2_tp1 [label="Layer Norm TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l2_tp1 [label="Gate Network TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 9" fillcolor=lightyellow shape=parallelogram style=filled]
			expert16_l2_tp1 [label="Expert 16
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert17_l2_tp1 [label="Expert 17
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert18_l2_tp1 [label="Expert 18
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert19_l2_tp1 [label="Expert 19
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert20_l2_tp1 [label="Expert 20
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert21_l2_tp1 [label="Expert 21
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert22_l2_tp1 [label="Expert 22
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert23_l2_tp1 [label="Expert 23
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l2_tp1 [label="Expert Aggregation TP1
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 9" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l2_tp1 [label="Residual Add 2 TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l2_tp1 [label="Layer Norm 2 TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l2_tp2 [label="Input Split TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 10" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l2_tp2 [label="MHA QKV TP2
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l2_tp2 [label="MHA Attention TP2
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l2_tp2 [label="MHA Output TP2
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l2_tp2 [label="Attention All-Reduce TP2
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 10" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l2_tp2 [label="Residual Add TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l2_tp2 [label="Layer Norm TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l2_tp2 [label="Gate Network TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 10" fillcolor=lightyellow shape=parallelogram style=filled]
			expert24_l2_tp2 [label="Expert 24
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert25_l2_tp2 [label="Expert 25
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert26_l2_tp2 [label="Expert 26
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert27_l2_tp2 [label="Expert 27
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert28_l2_tp2 [label="Expert 28
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert29_l2_tp2 [label="Expert 29
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert30_l2_tp2 [label="Expert 30
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert31_l2_tp2 [label="Expert 31
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l2_tp2 [label="Expert Aggregation TP2
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 10" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l2_tp2 [label="Residual Add 2 TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l2_tp2 [label="Layer Norm 2 TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l2_tp3 [label="Input Split TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 11" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l2_tp3 [label="MHA QKV TP3
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l2_tp3 [label="MHA Attention TP3
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l2_tp3 [label="MHA Output TP3
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l2_tp3 [label="Attention All-Reduce TP3
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 11" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l2_tp3 [label="Residual Add TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l2_tp3 [label="Layer Norm TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l2_tp3 [label="Gate Network TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 11" fillcolor=lightyellow shape=parallelogram style=filled]
			expert32_l2_tp3 [label="Expert 32
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert33_l2_tp3 [label="Expert 33
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert34_l2_tp3 [label="Expert 34
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert35_l2_tp3 [label="Expert 35
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert36_l2_tp3 [label="Expert 36
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert37_l2_tp3 [label="Expert 37
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert38_l2_tp3 [label="Expert 38
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert39_l2_tp3 [label="Expert 39
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l2_tp3 [label="Expert Aggregation TP3
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 11" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l2_tp3 [label="Residual Add 2 TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l2_tp3 [label="Layer Norm 2 TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l2_tp4 [label="Input Split TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 12" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l2_tp4 [label="MHA QKV TP4
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l2_tp4 [label="MHA Attention TP4
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l2_tp4 [label="MHA Output TP4
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l2_tp4 [label="Attention All-Reduce TP4
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 12" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l2_tp4 [label="Residual Add TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l2_tp4 [label="Layer Norm TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l2_tp4 [label="Gate Network TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 12" fillcolor=lightyellow shape=parallelogram style=filled]
			expert40_l2_tp4 [label="Expert 40
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert41_l2_tp4 [label="Expert 41
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert42_l2_tp4 [label="Expert 42
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert43_l2_tp4 [label="Expert 43
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert44_l2_tp4 [label="Expert 44
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert45_l2_tp4 [label="Expert 45
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert46_l2_tp4 [label="Expert 46
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert47_l2_tp4 [label="Expert 47
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l2_tp4 [label="Expert Aggregation TP4
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 12" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l2_tp4 [label="Residual Add 2 TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l2_tp4 [label="Layer Norm 2 TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l2_tp5 [label="Input Split TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 13" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l2_tp5 [label="MHA QKV TP5
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l2_tp5 [label="MHA Attention TP5
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l2_tp5 [label="MHA Output TP5
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l2_tp5 [label="Attention All-Reduce TP5
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 13" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l2_tp5 [label="Residual Add TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l2_tp5 [label="Layer Norm TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l2_tp5 [label="Gate Network TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 13" fillcolor=lightyellow shape=parallelogram style=filled]
			expert48_l2_tp5 [label="Expert 48
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert49_l2_tp5 [label="Expert 49
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert50_l2_tp5 [label="Expert 50
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert51_l2_tp5 [label="Expert 51
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert52_l2_tp5 [label="Expert 52
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert53_l2_tp5 [label="Expert 53
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert54_l2_tp5 [label="Expert 54
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert55_l2_tp5 [label="Expert 55
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l2_tp5 [label="Expert Aggregation TP5
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 13" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l2_tp5 [label="Residual Add 2 TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l2_tp5 [label="Layer Norm 2 TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l2_tp6 [label="Input Split TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 14" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l2_tp6 [label="MHA QKV TP6
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l2_tp6 [label="MHA Attention TP6
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l2_tp6 [label="MHA Output TP6
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l2_tp6 [label="Attention All-Reduce TP6
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 14" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l2_tp6 [label="Residual Add TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l2_tp6 [label="Layer Norm TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l2_tp6 [label="Gate Network TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 14" fillcolor=lightyellow shape=parallelogram style=filled]
			expert56_l2_tp6 [label="Expert 56
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert57_l2_tp6 [label="Expert 57
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert58_l2_tp6 [label="Expert 58
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert59_l2_tp6 [label="Expert 59
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert60_l2_tp6 [label="Expert 60
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert61_l2_tp6 [label="Expert 61
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert62_l2_tp6 [label="Expert 62
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert63_l2_tp6 [label="Expert 63
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l2_tp6 [label="Expert Aggregation TP6
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 14" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l2_tp6 [label="Residual Add 2 TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l2_tp6 [label="Layer Norm 2 TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l2_tp7 [label="Input Split TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 15" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l2_tp7 [label="MHA QKV TP7
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l2_tp7 [label="MHA Attention TP7
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l2_tp7 [label="MHA Output TP7
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l2_tp7 [label="Attention All-Reduce TP7
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 15" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l2_tp7 [label="Residual Add TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l2_tp7 [label="Layer Norm TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l2_tp7 [label="Gate Network TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 15" fillcolor=lightyellow shape=parallelogram style=filled]
			expert64_l2_tp7 [label="Expert 64
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert65_l2_tp7 [label="Expert 65
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert66_l2_tp7 [label="Expert 66
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert67_l2_tp7 [label="Expert 67
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert68_l2_tp7 [label="Expert 68
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert69_l2_tp7 [label="Expert 69
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert70_l2_tp7 [label="Expert 70
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert71_l2_tp7 [label="Expert 71
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l2_tp7 [label="Expert Aggregation TP7
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 15" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l2_tp7 [label="Residual Add 2 TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l2_tp7 [label="Layer Norm 2 TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
		}
		final_output [label="Final Output
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 8-15" fillcolor=lightblue shape=ellipse style=filled]
		subgraph cluster_layer3_s1 {
			color=purple label="Layer 3 (Stage 1)" style=rounded
			input_split_l3_tp0 [label="Input Split TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 8" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l3_tp0 [label="MHA QKV TP0
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l3_tp0 [label="MHA Attention TP0
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l3_tp0 [label="MHA Output TP0
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l3_tp0 [label="Attention All-Reduce TP0
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 8" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l3_tp0 [label="Residual Add TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l3_tp0 [label="Layer Norm TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l3_tp0 [label="Gate Network TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 8" fillcolor=lightyellow shape=parallelogram style=filled]
			expert8_l3_tp0 [label="Expert 8
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert9_l3_tp0 [label="Expert 9
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert10_l3_tp0 [label="Expert 10
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert11_l3_tp0 [label="Expert 11
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert12_l3_tp0 [label="Expert 12
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert13_l3_tp0 [label="Expert 13
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert14_l3_tp0 [label="Expert 14
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert15_l3_tp0 [label="Expert 15
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l3_tp0 [label="Expert Aggregation TP0
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 8" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l3_tp0 [label="Residual Add 2 TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l3_tp0 [label="Layer Norm 2 TP0
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 8" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l3_tp1 [label="Input Split TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 9" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l3_tp1 [label="MHA QKV TP1
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l3_tp1 [label="MHA Attention TP1
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l3_tp1 [label="MHA Output TP1
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l3_tp1 [label="Attention All-Reduce TP1
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 9" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l3_tp1 [label="Residual Add TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l3_tp1 [label="Layer Norm TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l3_tp1 [label="Gate Network TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 9" fillcolor=lightyellow shape=parallelogram style=filled]
			expert16_l3_tp1 [label="Expert 16
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert17_l3_tp1 [label="Expert 17
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert18_l3_tp1 [label="Expert 18
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert19_l3_tp1 [label="Expert 19
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert20_l3_tp1 [label="Expert 20
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert21_l3_tp1 [label="Expert 21
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert22_l3_tp1 [label="Expert 22
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert23_l3_tp1 [label="Expert 23
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l3_tp1 [label="Expert Aggregation TP1
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 9" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l3_tp1 [label="Residual Add 2 TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l3_tp1 [label="Layer Norm 2 TP1
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 9" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l3_tp2 [label="Input Split TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 10" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l3_tp2 [label="MHA QKV TP2
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l3_tp2 [label="MHA Attention TP2
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l3_tp2 [label="MHA Output TP2
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l3_tp2 [label="Attention All-Reduce TP2
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 10" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l3_tp2 [label="Residual Add TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l3_tp2 [label="Layer Norm TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l3_tp2 [label="Gate Network TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 10" fillcolor=lightyellow shape=parallelogram style=filled]
			expert24_l3_tp2 [label="Expert 24
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert25_l3_tp2 [label="Expert 25
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert26_l3_tp2 [label="Expert 26
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert27_l3_tp2 [label="Expert 27
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert28_l3_tp2 [label="Expert 28
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert29_l3_tp2 [label="Expert 29
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert30_l3_tp2 [label="Expert 30
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert31_l3_tp2 [label="Expert 31
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l3_tp2 [label="Expert Aggregation TP2
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 10" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l3_tp2 [label="Residual Add 2 TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l3_tp2 [label="Layer Norm 2 TP2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 10" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l3_tp3 [label="Input Split TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 11" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l3_tp3 [label="MHA QKV TP3
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l3_tp3 [label="MHA Attention TP3
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l3_tp3 [label="MHA Output TP3
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l3_tp3 [label="Attention All-Reduce TP3
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 11" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l3_tp3 [label="Residual Add TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l3_tp3 [label="Layer Norm TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l3_tp3 [label="Gate Network TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 11" fillcolor=lightyellow shape=parallelogram style=filled]
			expert32_l3_tp3 [label="Expert 32
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert33_l3_tp3 [label="Expert 33
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert34_l3_tp3 [label="Expert 34
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert35_l3_tp3 [label="Expert 35
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert36_l3_tp3 [label="Expert 36
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert37_l3_tp3 [label="Expert 37
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert38_l3_tp3 [label="Expert 38
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert39_l3_tp3 [label="Expert 39
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l3_tp3 [label="Expert Aggregation TP3
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 11" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l3_tp3 [label="Residual Add 2 TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l3_tp3 [label="Layer Norm 2 TP3
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 11" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l3_tp4 [label="Input Split TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 12" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l3_tp4 [label="MHA QKV TP4
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l3_tp4 [label="MHA Attention TP4
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l3_tp4 [label="MHA Output TP4
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l3_tp4 [label="Attention All-Reduce TP4
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 12" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l3_tp4 [label="Residual Add TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l3_tp4 [label="Layer Norm TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l3_tp4 [label="Gate Network TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 12" fillcolor=lightyellow shape=parallelogram style=filled]
			expert40_l3_tp4 [label="Expert 40
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert41_l3_tp4 [label="Expert 41
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert42_l3_tp4 [label="Expert 42
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert43_l3_tp4 [label="Expert 43
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert44_l3_tp4 [label="Expert 44
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert45_l3_tp4 [label="Expert 45
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert46_l3_tp4 [label="Expert 46
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert47_l3_tp4 [label="Expert 47
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l3_tp4 [label="Expert Aggregation TP4
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 12" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l3_tp4 [label="Residual Add 2 TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l3_tp4 [label="Layer Norm 2 TP4
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 12" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l3_tp5 [label="Input Split TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 13" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l3_tp5 [label="MHA QKV TP5
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l3_tp5 [label="MHA Attention TP5
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l3_tp5 [label="MHA Output TP5
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l3_tp5 [label="Attention All-Reduce TP5
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 13" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l3_tp5 [label="Residual Add TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l3_tp5 [label="Layer Norm TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l3_tp5 [label="Gate Network TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 13" fillcolor=lightyellow shape=parallelogram style=filled]
			expert48_l3_tp5 [label="Expert 48
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert49_l3_tp5 [label="Expert 49
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert50_l3_tp5 [label="Expert 50
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert51_l3_tp5 [label="Expert 51
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert52_l3_tp5 [label="Expert 52
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert53_l3_tp5 [label="Expert 53
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert54_l3_tp5 [label="Expert 54
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert55_l3_tp5 [label="Expert 55
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l3_tp5 [label="Expert Aggregation TP5
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 13" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l3_tp5 [label="Residual Add 2 TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l3_tp5 [label="Layer Norm 2 TP5
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 13" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l3_tp6 [label="Input Split TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 14" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l3_tp6 [label="MHA QKV TP6
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l3_tp6 [label="MHA Attention TP6
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l3_tp6 [label="MHA Output TP6
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l3_tp6 [label="Attention All-Reduce TP6
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 14" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l3_tp6 [label="Residual Add TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l3_tp6 [label="Layer Norm TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l3_tp6 [label="Gate Network TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 14" fillcolor=lightyellow shape=parallelogram style=filled]
			expert56_l3_tp6 [label="Expert 56
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert57_l3_tp6 [label="Expert 57
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert58_l3_tp6 [label="Expert 58
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert59_l3_tp6 [label="Expert 59
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert60_l3_tp6 [label="Expert 60
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert61_l3_tp6 [label="Expert 61
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert62_l3_tp6 [label="Expert 62
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert63_l3_tp6 [label="Expert 63
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l3_tp6 [label="Expert Aggregation TP6
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 14" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l3_tp6 [label="Residual Add 2 TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l3_tp6 [label="Layer Norm 2 TP6
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 14" fillcolor=lightgreen shape=rectangle style=filled]
			input_split_l3_tp7 [label="Input Split TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 15" fillcolor=lightgray shape=ellipse style=filled]
			mha_qkv_l3_tp7 [label="MHA QKV TP7
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
			mha_attn_l3_tp7 [label="MHA Attention TP7
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
			mha_out_l3_tp7 [label="MHA Output TP7
Input: [batch_size=1024, seq_len=10000, heads=2, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=1024]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
			attn_allreduce_l3_tp7 [label="Attention All-Reduce TP7
Input: [batch_size=1024, seq_len=10000, hidden=1024]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 15" fillcolor=lightgray shape=ellipse style=filled]
			res_add_l3_tp7 [label="Residual Add TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
			norm_l3_tp7 [label="Layer Norm TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
			gate_l3_tp7 [label="Gate Network TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: 15" fillcolor=lightyellow shape=parallelogram style=filled]
			expert64_l3_tp7 [label="Expert 64
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert65_l3_tp7 [label="Expert 65
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert66_l3_tp7 [label="Expert 66
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert67_l3_tp7 [label="Expert 67
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert68_l3_tp7 [label="Expert 68
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert69_l3_tp7 [label="Expert 69
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert70_l3_tp7 [label="Expert 70
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert71_l3_tp7 [label="Expert 71
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
			expert_agg_l3_tp7 [label="Expert Aggregation TP7
Input: [processed_tokens from 8 experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 15" fillcolor=lightpink shape=parallelogram style=filled]
			res_add2_l3_tp7 [label="Residual Add 2 TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
			norm2_l3_tp7 [label="Layer Norm 2 TP7
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: 15" fillcolor=lightgreen shape=rectangle style=filled]
		}
	}
	stage0_input -> input_split_l0_tp0
	input_split_l0_tp0 -> mha_qkv_l0_tp0
	mha_qkv_l0_tp0 -> mha_attn_l0_tp0
	mha_attn_l0_tp0 -> mha_out_l0_tp0
	mha_out_l0_tp0 -> attn_allreduce_l0_tp0
	stage0_input -> res_add_l0_tp0
	attn_allreduce_l0_tp0 -> res_add_l0_tp0
	res_add_l0_tp0 -> norm_l0_tp0
	norm_l0_tp0 -> gate_l0_tp0
	norm_l0_tp0 -> expert0_l0_tp0
	expert0_l0_tp0 -> expert_agg_l0_tp0
	norm_l0_tp0 -> expert1_l0_tp0
	expert1_l0_tp0 -> expert_agg_l0_tp0
	norm_l0_tp0 -> expert2_l0_tp0
	expert2_l0_tp0 -> expert_agg_l0_tp0
	norm_l0_tp0 -> expert3_l0_tp0
	expert3_l0_tp0 -> expert_agg_l0_tp0
	norm_l0_tp0 -> expert4_l0_tp0
	expert4_l0_tp0 -> expert_agg_l0_tp0
	norm_l0_tp0 -> expert5_l0_tp0
	expert5_l0_tp0 -> expert_agg_l0_tp0
	norm_l0_tp0 -> expert6_l0_tp0
	expert6_l0_tp0 -> expert_agg_l0_tp0
	norm_l0_tp0 -> expert7_l0_tp0
	expert7_l0_tp0 -> expert_agg_l0_tp0
	expert_agg_l0_tp0 -> res_add2_l0_tp0
	norm_l0_tp0 -> res_add2_l0_tp0
	res_add2_l0_tp0 -> norm2_l0_tp0
	stage0_input -> input_split_l0_tp1
	input_split_l0_tp1 -> mha_qkv_l0_tp1
	mha_qkv_l0_tp1 -> mha_attn_l0_tp1
	mha_attn_l0_tp1 -> mha_out_l0_tp1
	mha_out_l0_tp1 -> attn_allreduce_l0_tp1
	stage0_input -> res_add_l0_tp1
	attn_allreduce_l0_tp1 -> res_add_l0_tp1
	res_add_l0_tp1 -> norm_l0_tp1
	norm_l0_tp1 -> gate_l0_tp1
	norm_l0_tp1 -> expert8_l0_tp1
	expert8_l0_tp1 -> expert_agg_l0_tp1
	norm_l0_tp1 -> expert9_l0_tp1
	expert9_l0_tp1 -> expert_agg_l0_tp1
	norm_l0_tp1 -> expert10_l0_tp1
	expert10_l0_tp1 -> expert_agg_l0_tp1
	norm_l0_tp1 -> expert11_l0_tp1
	expert11_l0_tp1 -> expert_agg_l0_tp1
	norm_l0_tp1 -> expert12_l0_tp1
	expert12_l0_tp1 -> expert_agg_l0_tp1
	norm_l0_tp1 -> expert13_l0_tp1
	expert13_l0_tp1 -> expert_agg_l0_tp1
	norm_l0_tp1 -> expert14_l0_tp1
	expert14_l0_tp1 -> expert_agg_l0_tp1
	norm_l0_tp1 -> expert15_l0_tp1
	expert15_l0_tp1 -> expert_agg_l0_tp1
	expert_agg_l0_tp1 -> res_add2_l0_tp1
	norm_l0_tp1 -> res_add2_l0_tp1
	res_add2_l0_tp1 -> norm2_l0_tp1
	stage0_input -> input_split_l0_tp2
	input_split_l0_tp2 -> mha_qkv_l0_tp2
	mha_qkv_l0_tp2 -> mha_attn_l0_tp2
	mha_attn_l0_tp2 -> mha_out_l0_tp2
	mha_out_l0_tp2 -> attn_allreduce_l0_tp2
	stage0_input -> res_add_l0_tp2
	attn_allreduce_l0_tp2 -> res_add_l0_tp2
	res_add_l0_tp2 -> norm_l0_tp2
	norm_l0_tp2 -> gate_l0_tp2
	norm_l0_tp2 -> expert16_l0_tp2
	expert16_l0_tp2 -> expert_agg_l0_tp2
	norm_l0_tp2 -> expert17_l0_tp2
	expert17_l0_tp2 -> expert_agg_l0_tp2
	norm_l0_tp2 -> expert18_l0_tp2
	expert18_l0_tp2 -> expert_agg_l0_tp2
	norm_l0_tp2 -> expert19_l0_tp2
	expert19_l0_tp2 -> expert_agg_l0_tp2
	norm_l0_tp2 -> expert20_l0_tp2
	expert20_l0_tp2 -> expert_agg_l0_tp2
	norm_l0_tp2 -> expert21_l0_tp2
	expert21_l0_tp2 -> expert_agg_l0_tp2
	norm_l0_tp2 -> expert22_l0_tp2
	expert22_l0_tp2 -> expert_agg_l0_tp2
	norm_l0_tp2 -> expert23_l0_tp2
	expert23_l0_tp2 -> expert_agg_l0_tp2
	expert_agg_l0_tp2 -> res_add2_l0_tp2
	norm_l0_tp2 -> res_add2_l0_tp2
	res_add2_l0_tp2 -> norm2_l0_tp2
	stage0_input -> input_split_l0_tp3
	input_split_l0_tp3 -> mha_qkv_l0_tp3
	mha_qkv_l0_tp3 -> mha_attn_l0_tp3
	mha_attn_l0_tp3 -> mha_out_l0_tp3
	mha_out_l0_tp3 -> attn_allreduce_l0_tp3
	stage0_input -> res_add_l0_tp3
	attn_allreduce_l0_tp3 -> res_add_l0_tp3
	res_add_l0_tp3 -> norm_l0_tp3
	norm_l0_tp3 -> gate_l0_tp3
	norm_l0_tp3 -> expert24_l0_tp3
	expert24_l0_tp3 -> expert_agg_l0_tp3
	norm_l0_tp3 -> expert25_l0_tp3
	expert25_l0_tp3 -> expert_agg_l0_tp3
	norm_l0_tp3 -> expert26_l0_tp3
	expert26_l0_tp3 -> expert_agg_l0_tp3
	norm_l0_tp3 -> expert27_l0_tp3
	expert27_l0_tp3 -> expert_agg_l0_tp3
	norm_l0_tp3 -> expert28_l0_tp3
	expert28_l0_tp3 -> expert_agg_l0_tp3
	norm_l0_tp3 -> expert29_l0_tp3
	expert29_l0_tp3 -> expert_agg_l0_tp3
	norm_l0_tp3 -> expert30_l0_tp3
	expert30_l0_tp3 -> expert_agg_l0_tp3
	norm_l0_tp3 -> expert31_l0_tp3
	expert31_l0_tp3 -> expert_agg_l0_tp3
	expert_agg_l0_tp3 -> res_add2_l0_tp3
	norm_l0_tp3 -> res_add2_l0_tp3
	res_add2_l0_tp3 -> norm2_l0_tp3
	stage0_input -> input_split_l0_tp4
	input_split_l0_tp4 -> mha_qkv_l0_tp4
	mha_qkv_l0_tp4 -> mha_attn_l0_tp4
	mha_attn_l0_tp4 -> mha_out_l0_tp4
	mha_out_l0_tp4 -> attn_allreduce_l0_tp4
	stage0_input -> res_add_l0_tp4
	attn_allreduce_l0_tp4 -> res_add_l0_tp4
	res_add_l0_tp4 -> norm_l0_tp4
	norm_l0_tp4 -> gate_l0_tp4
	norm_l0_tp4 -> expert32_l0_tp4
	expert32_l0_tp4 -> expert_agg_l0_tp4
	norm_l0_tp4 -> expert33_l0_tp4
	expert33_l0_tp4 -> expert_agg_l0_tp4
	norm_l0_tp4 -> expert34_l0_tp4
	expert34_l0_tp4 -> expert_agg_l0_tp4
	norm_l0_tp4 -> expert35_l0_tp4
	expert35_l0_tp4 -> expert_agg_l0_tp4
	norm_l0_tp4 -> expert36_l0_tp4
	expert36_l0_tp4 -> expert_agg_l0_tp4
	norm_l0_tp4 -> expert37_l0_tp4
	expert37_l0_tp4 -> expert_agg_l0_tp4
	norm_l0_tp4 -> expert38_l0_tp4
	expert38_l0_tp4 -> expert_agg_l0_tp4
	norm_l0_tp4 -> expert39_l0_tp4
	expert39_l0_tp4 -> expert_agg_l0_tp4
	expert_agg_l0_tp4 -> res_add2_l0_tp4
	norm_l0_tp4 -> res_add2_l0_tp4
	res_add2_l0_tp4 -> norm2_l0_tp4
	stage0_input -> input_split_l0_tp5
	input_split_l0_tp5 -> mha_qkv_l0_tp5
	mha_qkv_l0_tp5 -> mha_attn_l0_tp5
	mha_attn_l0_tp5 -> mha_out_l0_tp5
	mha_out_l0_tp5 -> attn_allreduce_l0_tp5
	stage0_input -> res_add_l0_tp5
	attn_allreduce_l0_tp5 -> res_add_l0_tp5
	res_add_l0_tp5 -> norm_l0_tp5
	norm_l0_tp5 -> gate_l0_tp5
	norm_l0_tp5 -> expert40_l0_tp5
	expert40_l0_tp5 -> expert_agg_l0_tp5
	norm_l0_tp5 -> expert41_l0_tp5
	expert41_l0_tp5 -> expert_agg_l0_tp5
	norm_l0_tp5 -> expert42_l0_tp5
	expert42_l0_tp5 -> expert_agg_l0_tp5
	norm_l0_tp5 -> expert43_l0_tp5
	expert43_l0_tp5 -> expert_agg_l0_tp5
	norm_l0_tp5 -> expert44_l0_tp5
	expert44_l0_tp5 -> expert_agg_l0_tp5
	norm_l0_tp5 -> expert45_l0_tp5
	expert45_l0_tp5 -> expert_agg_l0_tp5
	norm_l0_tp5 -> expert46_l0_tp5
	expert46_l0_tp5 -> expert_agg_l0_tp5
	norm_l0_tp5 -> expert47_l0_tp5
	expert47_l0_tp5 -> expert_agg_l0_tp5
	expert_agg_l0_tp5 -> res_add2_l0_tp5
	norm_l0_tp5 -> res_add2_l0_tp5
	res_add2_l0_tp5 -> norm2_l0_tp5
	stage0_input -> input_split_l0_tp6
	input_split_l0_tp6 -> mha_qkv_l0_tp6
	mha_qkv_l0_tp6 -> mha_attn_l0_tp6
	mha_attn_l0_tp6 -> mha_out_l0_tp6
	mha_out_l0_tp6 -> attn_allreduce_l0_tp6
	stage0_input -> res_add_l0_tp6
	attn_allreduce_l0_tp6 -> res_add_l0_tp6
	res_add_l0_tp6 -> norm_l0_tp6
	norm_l0_tp6 -> gate_l0_tp6
	norm_l0_tp6 -> expert48_l0_tp6
	expert48_l0_tp6 -> expert_agg_l0_tp6
	norm_l0_tp6 -> expert49_l0_tp6
	expert49_l0_tp6 -> expert_agg_l0_tp6
	norm_l0_tp6 -> expert50_l0_tp6
	expert50_l0_tp6 -> expert_agg_l0_tp6
	norm_l0_tp6 -> expert51_l0_tp6
	expert51_l0_tp6 -> expert_agg_l0_tp6
	norm_l0_tp6 -> expert52_l0_tp6
	expert52_l0_tp6 -> expert_agg_l0_tp6
	norm_l0_tp6 -> expert53_l0_tp6
	expert53_l0_tp6 -> expert_agg_l0_tp6
	norm_l0_tp6 -> expert54_l0_tp6
	expert54_l0_tp6 -> expert_agg_l0_tp6
	norm_l0_tp6 -> expert55_l0_tp6
	expert55_l0_tp6 -> expert_agg_l0_tp6
	expert_agg_l0_tp6 -> res_add2_l0_tp6
	norm_l0_tp6 -> res_add2_l0_tp6
	res_add2_l0_tp6 -> norm2_l0_tp6
	stage0_input -> input_split_l0_tp7
	input_split_l0_tp7 -> mha_qkv_l0_tp7
	mha_qkv_l0_tp7 -> mha_attn_l0_tp7
	mha_attn_l0_tp7 -> mha_out_l0_tp7
	mha_out_l0_tp7 -> attn_allreduce_l0_tp7
	stage0_input -> res_add_l0_tp7
	attn_allreduce_l0_tp7 -> res_add_l0_tp7
	res_add_l0_tp7 -> norm_l0_tp7
	norm_l0_tp7 -> gate_l0_tp7
	norm_l0_tp7 -> expert56_l0_tp7
	expert56_l0_tp7 -> expert_agg_l0_tp7
	norm_l0_tp7 -> expert57_l0_tp7
	expert57_l0_tp7 -> expert_agg_l0_tp7
	norm_l0_tp7 -> expert58_l0_tp7
	expert58_l0_tp7 -> expert_agg_l0_tp7
	norm_l0_tp7 -> expert59_l0_tp7
	expert59_l0_tp7 -> expert_agg_l0_tp7
	norm_l0_tp7 -> expert60_l0_tp7
	expert60_l0_tp7 -> expert_agg_l0_tp7
	norm_l0_tp7 -> expert61_l0_tp7
	expert61_l0_tp7 -> expert_agg_l0_tp7
	norm_l0_tp7 -> expert62_l0_tp7
	expert62_l0_tp7 -> expert_agg_l0_tp7
	norm_l0_tp7 -> expert63_l0_tp7
	expert63_l0_tp7 -> expert_agg_l0_tp7
	expert_agg_l0_tp7 -> res_add2_l0_tp7
	norm_l0_tp7 -> res_add2_l0_tp7
	res_add2_l0_tp7 -> norm2_l0_tp7
	norm2_l0_tp0 -> input_split_l1_tp0
	input_split_l1_tp0 -> mha_qkv_l1_tp0
	mha_qkv_l1_tp0 -> mha_attn_l1_tp0
	mha_attn_l1_tp0 -> mha_out_l1_tp0
	mha_out_l1_tp0 -> attn_allreduce_l1_tp0
	norm2_l0_tp0 -> res_add_l1_tp0
	attn_allreduce_l1_tp0 -> res_add_l1_tp0
	res_add_l1_tp0 -> norm_l1_tp0
	norm_l1_tp0 -> gate_l1_tp0
	norm_l1_tp0 -> expert0_l1_tp0
	expert0_l1_tp0 -> expert_agg_l1_tp0
	norm_l1_tp0 -> expert1_l1_tp0
	expert1_l1_tp0 -> expert_agg_l1_tp0
	norm_l1_tp0 -> expert2_l1_tp0
	expert2_l1_tp0 -> expert_agg_l1_tp0
	norm_l1_tp0 -> expert3_l1_tp0
	expert3_l1_tp0 -> expert_agg_l1_tp0
	norm_l1_tp0 -> expert4_l1_tp0
	expert4_l1_tp0 -> expert_agg_l1_tp0
	norm_l1_tp0 -> expert5_l1_tp0
	expert5_l1_tp0 -> expert_agg_l1_tp0
	norm_l1_tp0 -> expert6_l1_tp0
	expert6_l1_tp0 -> expert_agg_l1_tp0
	norm_l1_tp0 -> expert7_l1_tp0
	expert7_l1_tp0 -> expert_agg_l1_tp0
	expert_agg_l1_tp0 -> res_add2_l1_tp0
	norm_l1_tp0 -> res_add2_l1_tp0
	res_add2_l1_tp0 -> norm2_l1_tp0
	norm2_l0_tp1 -> input_split_l1_tp1
	input_split_l1_tp1 -> mha_qkv_l1_tp1
	mha_qkv_l1_tp1 -> mha_attn_l1_tp1
	mha_attn_l1_tp1 -> mha_out_l1_tp1
	mha_out_l1_tp1 -> attn_allreduce_l1_tp1
	norm2_l0_tp1 -> res_add_l1_tp1
	attn_allreduce_l1_tp1 -> res_add_l1_tp1
	res_add_l1_tp1 -> norm_l1_tp1
	norm_l1_tp1 -> gate_l1_tp1
	norm_l1_tp1 -> expert8_l1_tp1
	expert8_l1_tp1 -> expert_agg_l1_tp1
	norm_l1_tp1 -> expert9_l1_tp1
	expert9_l1_tp1 -> expert_agg_l1_tp1
	norm_l1_tp1 -> expert10_l1_tp1
	expert10_l1_tp1 -> expert_agg_l1_tp1
	norm_l1_tp1 -> expert11_l1_tp1
	expert11_l1_tp1 -> expert_agg_l1_tp1
	norm_l1_tp1 -> expert12_l1_tp1
	expert12_l1_tp1 -> expert_agg_l1_tp1
	norm_l1_tp1 -> expert13_l1_tp1
	expert13_l1_tp1 -> expert_agg_l1_tp1
	norm_l1_tp1 -> expert14_l1_tp1
	expert14_l1_tp1 -> expert_agg_l1_tp1
	norm_l1_tp1 -> expert15_l1_tp1
	expert15_l1_tp1 -> expert_agg_l1_tp1
	expert_agg_l1_tp1 -> res_add2_l1_tp1
	norm_l1_tp1 -> res_add2_l1_tp1
	res_add2_l1_tp1 -> norm2_l1_tp1
	norm2_l0_tp2 -> input_split_l1_tp2
	input_split_l1_tp2 -> mha_qkv_l1_tp2
	mha_qkv_l1_tp2 -> mha_attn_l1_tp2
	mha_attn_l1_tp2 -> mha_out_l1_tp2
	mha_out_l1_tp2 -> attn_allreduce_l1_tp2
	norm2_l0_tp2 -> res_add_l1_tp2
	attn_allreduce_l1_tp2 -> res_add_l1_tp2
	res_add_l1_tp2 -> norm_l1_tp2
	norm_l1_tp2 -> gate_l1_tp2
	norm_l1_tp2 -> expert16_l1_tp2
	expert16_l1_tp2 -> expert_agg_l1_tp2
	norm_l1_tp2 -> expert17_l1_tp2
	expert17_l1_tp2 -> expert_agg_l1_tp2
	norm_l1_tp2 -> expert18_l1_tp2
	expert18_l1_tp2 -> expert_agg_l1_tp2
	norm_l1_tp2 -> expert19_l1_tp2
	expert19_l1_tp2 -> expert_agg_l1_tp2
	norm_l1_tp2 -> expert20_l1_tp2
	expert20_l1_tp2 -> expert_agg_l1_tp2
	norm_l1_tp2 -> expert21_l1_tp2
	expert21_l1_tp2 -> expert_agg_l1_tp2
	norm_l1_tp2 -> expert22_l1_tp2
	expert22_l1_tp2 -> expert_agg_l1_tp2
	norm_l1_tp2 -> expert23_l1_tp2
	expert23_l1_tp2 -> expert_agg_l1_tp2
	expert_agg_l1_tp2 -> res_add2_l1_tp2
	norm_l1_tp2 -> res_add2_l1_tp2
	res_add2_l1_tp2 -> norm2_l1_tp2
	norm2_l0_tp3 -> input_split_l1_tp3
	input_split_l1_tp3 -> mha_qkv_l1_tp3
	mha_qkv_l1_tp3 -> mha_attn_l1_tp3
	mha_attn_l1_tp3 -> mha_out_l1_tp3
	mha_out_l1_tp3 -> attn_allreduce_l1_tp3
	norm2_l0_tp3 -> res_add_l1_tp3
	attn_allreduce_l1_tp3 -> res_add_l1_tp3
	res_add_l1_tp3 -> norm_l1_tp3
	norm_l1_tp3 -> gate_l1_tp3
	norm_l1_tp3 -> expert24_l1_tp3
	expert24_l1_tp3 -> expert_agg_l1_tp3
	norm_l1_tp3 -> expert25_l1_tp3
	expert25_l1_tp3 -> expert_agg_l1_tp3
	norm_l1_tp3 -> expert26_l1_tp3
	expert26_l1_tp3 -> expert_agg_l1_tp3
	norm_l1_tp3 -> expert27_l1_tp3
	expert27_l1_tp3 -> expert_agg_l1_tp3
	norm_l1_tp3 -> expert28_l1_tp3
	expert28_l1_tp3 -> expert_agg_l1_tp3
	norm_l1_tp3 -> expert29_l1_tp3
	expert29_l1_tp3 -> expert_agg_l1_tp3
	norm_l1_tp3 -> expert30_l1_tp3
	expert30_l1_tp3 -> expert_agg_l1_tp3
	norm_l1_tp3 -> expert31_l1_tp3
	expert31_l1_tp3 -> expert_agg_l1_tp3
	expert_agg_l1_tp3 -> res_add2_l1_tp3
	norm_l1_tp3 -> res_add2_l1_tp3
	res_add2_l1_tp3 -> norm2_l1_tp3
	norm2_l0_tp4 -> input_split_l1_tp4
	input_split_l1_tp4 -> mha_qkv_l1_tp4
	mha_qkv_l1_tp4 -> mha_attn_l1_tp4
	mha_attn_l1_tp4 -> mha_out_l1_tp4
	mha_out_l1_tp4 -> attn_allreduce_l1_tp4
	norm2_l0_tp4 -> res_add_l1_tp4
	attn_allreduce_l1_tp4 -> res_add_l1_tp4
	res_add_l1_tp4 -> norm_l1_tp4
	norm_l1_tp4 -> gate_l1_tp4
	norm_l1_tp4 -> expert32_l1_tp4
	expert32_l1_tp4 -> expert_agg_l1_tp4
	norm_l1_tp4 -> expert33_l1_tp4
	expert33_l1_tp4 -> expert_agg_l1_tp4
	norm_l1_tp4 -> expert34_l1_tp4
	expert34_l1_tp4 -> expert_agg_l1_tp4
	norm_l1_tp4 -> expert35_l1_tp4
	expert35_l1_tp4 -> expert_agg_l1_tp4
	norm_l1_tp4 -> expert36_l1_tp4
	expert36_l1_tp4 -> expert_agg_l1_tp4
	norm_l1_tp4 -> expert37_l1_tp4
	expert37_l1_tp4 -> expert_agg_l1_tp4
	norm_l1_tp4 -> expert38_l1_tp4
	expert38_l1_tp4 -> expert_agg_l1_tp4
	norm_l1_tp4 -> expert39_l1_tp4
	expert39_l1_tp4 -> expert_agg_l1_tp4
	expert_agg_l1_tp4 -> res_add2_l1_tp4
	norm_l1_tp4 -> res_add2_l1_tp4
	res_add2_l1_tp4 -> norm2_l1_tp4
	norm2_l0_tp5 -> input_split_l1_tp5
	input_split_l1_tp5 -> mha_qkv_l1_tp5
	mha_qkv_l1_tp5 -> mha_attn_l1_tp5
	mha_attn_l1_tp5 -> mha_out_l1_tp5
	mha_out_l1_tp5 -> attn_allreduce_l1_tp5
	norm2_l0_tp5 -> res_add_l1_tp5
	attn_allreduce_l1_tp5 -> res_add_l1_tp5
	res_add_l1_tp5 -> norm_l1_tp5
	norm_l1_tp5 -> gate_l1_tp5
	norm_l1_tp5 -> expert40_l1_tp5
	expert40_l1_tp5 -> expert_agg_l1_tp5
	norm_l1_tp5 -> expert41_l1_tp5
	expert41_l1_tp5 -> expert_agg_l1_tp5
	norm_l1_tp5 -> expert42_l1_tp5
	expert42_l1_tp5 -> expert_agg_l1_tp5
	norm_l1_tp5 -> expert43_l1_tp5
	expert43_l1_tp5 -> expert_agg_l1_tp5
	norm_l1_tp5 -> expert44_l1_tp5
	expert44_l1_tp5 -> expert_agg_l1_tp5
	norm_l1_tp5 -> expert45_l1_tp5
	expert45_l1_tp5 -> expert_agg_l1_tp5
	norm_l1_tp5 -> expert46_l1_tp5
	expert46_l1_tp5 -> expert_agg_l1_tp5
	norm_l1_tp5 -> expert47_l1_tp5
	expert47_l1_tp5 -> expert_agg_l1_tp5
	expert_agg_l1_tp5 -> res_add2_l1_tp5
	norm_l1_tp5 -> res_add2_l1_tp5
	res_add2_l1_tp5 -> norm2_l1_tp5
	norm2_l0_tp6 -> input_split_l1_tp6
	input_split_l1_tp6 -> mha_qkv_l1_tp6
	mha_qkv_l1_tp6 -> mha_attn_l1_tp6
	mha_attn_l1_tp6 -> mha_out_l1_tp6
	mha_out_l1_tp6 -> attn_allreduce_l1_tp6
	norm2_l0_tp6 -> res_add_l1_tp6
	attn_allreduce_l1_tp6 -> res_add_l1_tp6
	res_add_l1_tp6 -> norm_l1_tp6
	norm_l1_tp6 -> gate_l1_tp6
	norm_l1_tp6 -> expert48_l1_tp6
	expert48_l1_tp6 -> expert_agg_l1_tp6
	norm_l1_tp6 -> expert49_l1_tp6
	expert49_l1_tp6 -> expert_agg_l1_tp6
	norm_l1_tp6 -> expert50_l1_tp6
	expert50_l1_tp6 -> expert_agg_l1_tp6
	norm_l1_tp6 -> expert51_l1_tp6
	expert51_l1_tp6 -> expert_agg_l1_tp6
	norm_l1_tp6 -> expert52_l1_tp6
	expert52_l1_tp6 -> expert_agg_l1_tp6
	norm_l1_tp6 -> expert53_l1_tp6
	expert53_l1_tp6 -> expert_agg_l1_tp6
	norm_l1_tp6 -> expert54_l1_tp6
	expert54_l1_tp6 -> expert_agg_l1_tp6
	norm_l1_tp6 -> expert55_l1_tp6
	expert55_l1_tp6 -> expert_agg_l1_tp6
	expert_agg_l1_tp6 -> res_add2_l1_tp6
	norm_l1_tp6 -> res_add2_l1_tp6
	res_add2_l1_tp6 -> norm2_l1_tp6
	norm2_l0_tp7 -> input_split_l1_tp7
	input_split_l1_tp7 -> mha_qkv_l1_tp7
	mha_qkv_l1_tp7 -> mha_attn_l1_tp7
	mha_attn_l1_tp7 -> mha_out_l1_tp7
	mha_out_l1_tp7 -> attn_allreduce_l1_tp7
	norm2_l0_tp7 -> res_add_l1_tp7
	attn_allreduce_l1_tp7 -> res_add_l1_tp7
	res_add_l1_tp7 -> norm_l1_tp7
	norm_l1_tp7 -> gate_l1_tp7
	norm_l1_tp7 -> expert56_l1_tp7
	expert56_l1_tp7 -> expert_agg_l1_tp7
	norm_l1_tp7 -> expert57_l1_tp7
	expert57_l1_tp7 -> expert_agg_l1_tp7
	norm_l1_tp7 -> expert58_l1_tp7
	expert58_l1_tp7 -> expert_agg_l1_tp7
	norm_l1_tp7 -> expert59_l1_tp7
	expert59_l1_tp7 -> expert_agg_l1_tp7
	norm_l1_tp7 -> expert60_l1_tp7
	expert60_l1_tp7 -> expert_agg_l1_tp7
	norm_l1_tp7 -> expert61_l1_tp7
	expert61_l1_tp7 -> expert_agg_l1_tp7
	norm_l1_tp7 -> expert62_l1_tp7
	expert62_l1_tp7 -> expert_agg_l1_tp7
	norm_l1_tp7 -> expert63_l1_tp7
	expert63_l1_tp7 -> expert_agg_l1_tp7
	expert_agg_l1_tp7 -> res_add2_l1_tp7
	norm_l1_tp7 -> res_add2_l1_tp7
	res_add2_l1_tp7 -> norm2_l1_tp7
	norm2_l1_tp0 -> pipeline_comm_01
	pipeline_comm_01 -> input_split_l2_tp0
	norm2_l1_tp1 -> pipeline_comm_01
	pipeline_comm_01 -> input_split_l2_tp1
	norm2_l1_tp2 -> pipeline_comm_01
	pipeline_comm_01 -> input_split_l2_tp2
	norm2_l1_tp3 -> pipeline_comm_01
	pipeline_comm_01 -> input_split_l2_tp3
	norm2_l1_tp4 -> pipeline_comm_01
	pipeline_comm_01 -> input_split_l2_tp4
	norm2_l1_tp5 -> pipeline_comm_01
	pipeline_comm_01 -> input_split_l2_tp5
	norm2_l1_tp6 -> pipeline_comm_01
	pipeline_comm_01 -> input_split_l2_tp6
	norm2_l1_tp7 -> pipeline_comm_01
	pipeline_comm_01 -> input_split_l2_tp7
	input_split_l2_tp0 -> mha_qkv_l2_tp0
	mha_qkv_l2_tp0 -> mha_attn_l2_tp0
	mha_attn_l2_tp0 -> mha_out_l2_tp0
	mha_out_l2_tp0 -> attn_allreduce_l2_tp0
	input_split_l2_tp0 -> res_add_l2_tp0
	attn_allreduce_l2_tp0 -> res_add_l2_tp0
	res_add_l2_tp0 -> norm_l2_tp0
	norm_l2_tp0 -> gate_l2_tp0
	norm_l2_tp0 -> expert8_l2_tp0
	expert8_l2_tp0 -> expert_agg_l2_tp0
	norm_l2_tp0 -> expert9_l2_tp0
	expert9_l2_tp0 -> expert_agg_l2_tp0
	norm_l2_tp0 -> expert10_l2_tp0
	expert10_l2_tp0 -> expert_agg_l2_tp0
	norm_l2_tp0 -> expert11_l2_tp0
	expert11_l2_tp0 -> expert_agg_l2_tp0
	norm_l2_tp0 -> expert12_l2_tp0
	expert12_l2_tp0 -> expert_agg_l2_tp0
	norm_l2_tp0 -> expert13_l2_tp0
	expert13_l2_tp0 -> expert_agg_l2_tp0
	norm_l2_tp0 -> expert14_l2_tp0
	expert14_l2_tp0 -> expert_agg_l2_tp0
	norm_l2_tp0 -> expert15_l2_tp0
	expert15_l2_tp0 -> expert_agg_l2_tp0
	expert_agg_l2_tp0 -> res_add2_l2_tp0
	norm_l2_tp0 -> res_add2_l2_tp0
	res_add2_l2_tp0 -> norm2_l2_tp0
	input_split_l2_tp1 -> mha_qkv_l2_tp1
	mha_qkv_l2_tp1 -> mha_attn_l2_tp1
	mha_attn_l2_tp1 -> mha_out_l2_tp1
	mha_out_l2_tp1 -> attn_allreduce_l2_tp1
	input_split_l2_tp1 -> res_add_l2_tp1
	attn_allreduce_l2_tp1 -> res_add_l2_tp1
	res_add_l2_tp1 -> norm_l2_tp1
	norm_l2_tp1 -> gate_l2_tp1
	norm_l2_tp1 -> expert16_l2_tp1
	expert16_l2_tp1 -> expert_agg_l2_tp1
	norm_l2_tp1 -> expert17_l2_tp1
	expert17_l2_tp1 -> expert_agg_l2_tp1
	norm_l2_tp1 -> expert18_l2_tp1
	expert18_l2_tp1 -> expert_agg_l2_tp1
	norm_l2_tp1 -> expert19_l2_tp1
	expert19_l2_tp1 -> expert_agg_l2_tp1
	norm_l2_tp1 -> expert20_l2_tp1
	expert20_l2_tp1 -> expert_agg_l2_tp1
	norm_l2_tp1 -> expert21_l2_tp1
	expert21_l2_tp1 -> expert_agg_l2_tp1
	norm_l2_tp1 -> expert22_l2_tp1
	expert22_l2_tp1 -> expert_agg_l2_tp1
	norm_l2_tp1 -> expert23_l2_tp1
	expert23_l2_tp1 -> expert_agg_l2_tp1
	expert_agg_l2_tp1 -> res_add2_l2_tp1
	norm_l2_tp1 -> res_add2_l2_tp1
	res_add2_l2_tp1 -> norm2_l2_tp1
	input_split_l2_tp2 -> mha_qkv_l2_tp2
	mha_qkv_l2_tp2 -> mha_attn_l2_tp2
	mha_attn_l2_tp2 -> mha_out_l2_tp2
	mha_out_l2_tp2 -> attn_allreduce_l2_tp2
	input_split_l2_tp2 -> res_add_l2_tp2
	attn_allreduce_l2_tp2 -> res_add_l2_tp2
	res_add_l2_tp2 -> norm_l2_tp2
	norm_l2_tp2 -> gate_l2_tp2
	norm_l2_tp2 -> expert24_l2_tp2
	expert24_l2_tp2 -> expert_agg_l2_tp2
	norm_l2_tp2 -> expert25_l2_tp2
	expert25_l2_tp2 -> expert_agg_l2_tp2
	norm_l2_tp2 -> expert26_l2_tp2
	expert26_l2_tp2 -> expert_agg_l2_tp2
	norm_l2_tp2 -> expert27_l2_tp2
	expert27_l2_tp2 -> expert_agg_l2_tp2
	norm_l2_tp2 -> expert28_l2_tp2
	expert28_l2_tp2 -> expert_agg_l2_tp2
	norm_l2_tp2 -> expert29_l2_tp2
	expert29_l2_tp2 -> expert_agg_l2_tp2
	norm_l2_tp2 -> expert30_l2_tp2
	expert30_l2_tp2 -> expert_agg_l2_tp2
	norm_l2_tp2 -> expert31_l2_tp2
	expert31_l2_tp2 -> expert_agg_l2_tp2
	expert_agg_l2_tp2 -> res_add2_l2_tp2
	norm_l2_tp2 -> res_add2_l2_tp2
	res_add2_l2_tp2 -> norm2_l2_tp2
	input_split_l2_tp3 -> mha_qkv_l2_tp3
	mha_qkv_l2_tp3 -> mha_attn_l2_tp3
	mha_attn_l2_tp3 -> mha_out_l2_tp3
	mha_out_l2_tp3 -> attn_allreduce_l2_tp3
	input_split_l2_tp3 -> res_add_l2_tp3
	attn_allreduce_l2_tp3 -> res_add_l2_tp3
	res_add_l2_tp3 -> norm_l2_tp3
	norm_l2_tp3 -> gate_l2_tp3
	norm_l2_tp3 -> expert32_l2_tp3
	expert32_l2_tp3 -> expert_agg_l2_tp3
	norm_l2_tp3 -> expert33_l2_tp3
	expert33_l2_tp3 -> expert_agg_l2_tp3
	norm_l2_tp3 -> expert34_l2_tp3
	expert34_l2_tp3 -> expert_agg_l2_tp3
	norm_l2_tp3 -> expert35_l2_tp3
	expert35_l2_tp3 -> expert_agg_l2_tp3
	norm_l2_tp3 -> expert36_l2_tp3
	expert36_l2_tp3 -> expert_agg_l2_tp3
	norm_l2_tp3 -> expert37_l2_tp3
	expert37_l2_tp3 -> expert_agg_l2_tp3
	norm_l2_tp3 -> expert38_l2_tp3
	expert38_l2_tp3 -> expert_agg_l2_tp3
	norm_l2_tp3 -> expert39_l2_tp3
	expert39_l2_tp3 -> expert_agg_l2_tp3
	expert_agg_l2_tp3 -> res_add2_l2_tp3
	norm_l2_tp3 -> res_add2_l2_tp3
	res_add2_l2_tp3 -> norm2_l2_tp3
	input_split_l2_tp4 -> mha_qkv_l2_tp4
	mha_qkv_l2_tp4 -> mha_attn_l2_tp4
	mha_attn_l2_tp4 -> mha_out_l2_tp4
	mha_out_l2_tp4 -> attn_allreduce_l2_tp4
	input_split_l2_tp4 -> res_add_l2_tp4
	attn_allreduce_l2_tp4 -> res_add_l2_tp4
	res_add_l2_tp4 -> norm_l2_tp4
	norm_l2_tp4 -> gate_l2_tp4
	norm_l2_tp4 -> expert40_l2_tp4
	expert40_l2_tp4 -> expert_agg_l2_tp4
	norm_l2_tp4 -> expert41_l2_tp4
	expert41_l2_tp4 -> expert_agg_l2_tp4
	norm_l2_tp4 -> expert42_l2_tp4
	expert42_l2_tp4 -> expert_agg_l2_tp4
	norm_l2_tp4 -> expert43_l2_tp4
	expert43_l2_tp4 -> expert_agg_l2_tp4
	norm_l2_tp4 -> expert44_l2_tp4
	expert44_l2_tp4 -> expert_agg_l2_tp4
	norm_l2_tp4 -> expert45_l2_tp4
	expert45_l2_tp4 -> expert_agg_l2_tp4
	norm_l2_tp4 -> expert46_l2_tp4
	expert46_l2_tp4 -> expert_agg_l2_tp4
	norm_l2_tp4 -> expert47_l2_tp4
	expert47_l2_tp4 -> expert_agg_l2_tp4
	expert_agg_l2_tp4 -> res_add2_l2_tp4
	norm_l2_tp4 -> res_add2_l2_tp4
	res_add2_l2_tp4 -> norm2_l2_tp4
	input_split_l2_tp5 -> mha_qkv_l2_tp5
	mha_qkv_l2_tp5 -> mha_attn_l2_tp5
	mha_attn_l2_tp5 -> mha_out_l2_tp5
	mha_out_l2_tp5 -> attn_allreduce_l2_tp5
	input_split_l2_tp5 -> res_add_l2_tp5
	attn_allreduce_l2_tp5 -> res_add_l2_tp5
	res_add_l2_tp5 -> norm_l2_tp5
	norm_l2_tp5 -> gate_l2_tp5
	norm_l2_tp5 -> expert48_l2_tp5
	expert48_l2_tp5 -> expert_agg_l2_tp5
	norm_l2_tp5 -> expert49_l2_tp5
	expert49_l2_tp5 -> expert_agg_l2_tp5
	norm_l2_tp5 -> expert50_l2_tp5
	expert50_l2_tp5 -> expert_agg_l2_tp5
	norm_l2_tp5 -> expert51_l2_tp5
	expert51_l2_tp5 -> expert_agg_l2_tp5
	norm_l2_tp5 -> expert52_l2_tp5
	expert52_l2_tp5 -> expert_agg_l2_tp5
	norm_l2_tp5 -> expert53_l2_tp5
	expert53_l2_tp5 -> expert_agg_l2_tp5
	norm_l2_tp5 -> expert54_l2_tp5
	expert54_l2_tp5 -> expert_agg_l2_tp5
	norm_l2_tp5 -> expert55_l2_tp5
	expert55_l2_tp5 -> expert_agg_l2_tp5
	expert_agg_l2_tp5 -> res_add2_l2_tp5
	norm_l2_tp5 -> res_add2_l2_tp5
	res_add2_l2_tp5 -> norm2_l2_tp5
	input_split_l2_tp6 -> mha_qkv_l2_tp6
	mha_qkv_l2_tp6 -> mha_attn_l2_tp6
	mha_attn_l2_tp6 -> mha_out_l2_tp6
	mha_out_l2_tp6 -> attn_allreduce_l2_tp6
	input_split_l2_tp6 -> res_add_l2_tp6
	attn_allreduce_l2_tp6 -> res_add_l2_tp6
	res_add_l2_tp6 -> norm_l2_tp6
	norm_l2_tp6 -> gate_l2_tp6
	norm_l2_tp6 -> expert56_l2_tp6
	expert56_l2_tp6 -> expert_agg_l2_tp6
	norm_l2_tp6 -> expert57_l2_tp6
	expert57_l2_tp6 -> expert_agg_l2_tp6
	norm_l2_tp6 -> expert58_l2_tp6
	expert58_l2_tp6 -> expert_agg_l2_tp6
	norm_l2_tp6 -> expert59_l2_tp6
	expert59_l2_tp6 -> expert_agg_l2_tp6
	norm_l2_tp6 -> expert60_l2_tp6
	expert60_l2_tp6 -> expert_agg_l2_tp6
	norm_l2_tp6 -> expert61_l2_tp6
	expert61_l2_tp6 -> expert_agg_l2_tp6
	norm_l2_tp6 -> expert62_l2_tp6
	expert62_l2_tp6 -> expert_agg_l2_tp6
	norm_l2_tp6 -> expert63_l2_tp6
	expert63_l2_tp6 -> expert_agg_l2_tp6
	expert_agg_l2_tp6 -> res_add2_l2_tp6
	norm_l2_tp6 -> res_add2_l2_tp6
	res_add2_l2_tp6 -> norm2_l2_tp6
	input_split_l2_tp7 -> mha_qkv_l2_tp7
	mha_qkv_l2_tp7 -> mha_attn_l2_tp7
	mha_attn_l2_tp7 -> mha_out_l2_tp7
	mha_out_l2_tp7 -> attn_allreduce_l2_tp7
	input_split_l2_tp7 -> res_add_l2_tp7
	attn_allreduce_l2_tp7 -> res_add_l2_tp7
	res_add_l2_tp7 -> norm_l2_tp7
	norm_l2_tp7 -> gate_l2_tp7
	norm_l2_tp7 -> expert64_l2_tp7
	expert64_l2_tp7 -> expert_agg_l2_tp7
	norm_l2_tp7 -> expert65_l2_tp7
	expert65_l2_tp7 -> expert_agg_l2_tp7
	norm_l2_tp7 -> expert66_l2_tp7
	expert66_l2_tp7 -> expert_agg_l2_tp7
	norm_l2_tp7 -> expert67_l2_tp7
	expert67_l2_tp7 -> expert_agg_l2_tp7
	norm_l2_tp7 -> expert68_l2_tp7
	expert68_l2_tp7 -> expert_agg_l2_tp7
	norm_l2_tp7 -> expert69_l2_tp7
	expert69_l2_tp7 -> expert_agg_l2_tp7
	norm_l2_tp7 -> expert70_l2_tp7
	expert70_l2_tp7 -> expert_agg_l2_tp7
	norm_l2_tp7 -> expert71_l2_tp7
	expert71_l2_tp7 -> expert_agg_l2_tp7
	expert_agg_l2_tp7 -> res_add2_l2_tp7
	norm_l2_tp7 -> res_add2_l2_tp7
	res_add2_l2_tp7 -> norm2_l2_tp7
	norm2_l2_tp0 -> input_split_l3_tp0
	input_split_l3_tp0 -> mha_qkv_l3_tp0
	mha_qkv_l3_tp0 -> mha_attn_l3_tp0
	mha_attn_l3_tp0 -> mha_out_l3_tp0
	mha_out_l3_tp0 -> attn_allreduce_l3_tp0
	input_split_l3_tp0 -> res_add_l3_tp0
	attn_allreduce_l3_tp0 -> res_add_l3_tp0
	res_add_l3_tp0 -> norm_l3_tp0
	norm_l3_tp0 -> gate_l3_tp0
	norm_l3_tp0 -> expert8_l3_tp0
	expert8_l3_tp0 -> expert_agg_l3_tp0
	norm_l3_tp0 -> expert9_l3_tp0
	expert9_l3_tp0 -> expert_agg_l3_tp0
	norm_l3_tp0 -> expert10_l3_tp0
	expert10_l3_tp0 -> expert_agg_l3_tp0
	norm_l3_tp0 -> expert11_l3_tp0
	expert11_l3_tp0 -> expert_agg_l3_tp0
	norm_l3_tp0 -> expert12_l3_tp0
	expert12_l3_tp0 -> expert_agg_l3_tp0
	norm_l3_tp0 -> expert13_l3_tp0
	expert13_l3_tp0 -> expert_agg_l3_tp0
	norm_l3_tp0 -> expert14_l3_tp0
	expert14_l3_tp0 -> expert_agg_l3_tp0
	norm_l3_tp0 -> expert15_l3_tp0
	expert15_l3_tp0 -> expert_agg_l3_tp0
	expert_agg_l3_tp0 -> res_add2_l3_tp0
	norm_l3_tp0 -> res_add2_l3_tp0
	res_add2_l3_tp0 -> norm2_l3_tp0
	norm2_l3_tp0 -> final_output
	norm2_l2_tp1 -> input_split_l3_tp1
	input_split_l3_tp1 -> mha_qkv_l3_tp1
	mha_qkv_l3_tp1 -> mha_attn_l3_tp1
	mha_attn_l3_tp1 -> mha_out_l3_tp1
	mha_out_l3_tp1 -> attn_allreduce_l3_tp1
	input_split_l3_tp1 -> res_add_l3_tp1
	attn_allreduce_l3_tp1 -> res_add_l3_tp1
	res_add_l3_tp1 -> norm_l3_tp1
	norm_l3_tp1 -> gate_l3_tp1
	norm_l3_tp1 -> expert16_l3_tp1
	expert16_l3_tp1 -> expert_agg_l3_tp1
	norm_l3_tp1 -> expert17_l3_tp1
	expert17_l3_tp1 -> expert_agg_l3_tp1
	norm_l3_tp1 -> expert18_l3_tp1
	expert18_l3_tp1 -> expert_agg_l3_tp1
	norm_l3_tp1 -> expert19_l3_tp1
	expert19_l3_tp1 -> expert_agg_l3_tp1
	norm_l3_tp1 -> expert20_l3_tp1
	expert20_l3_tp1 -> expert_agg_l3_tp1
	norm_l3_tp1 -> expert21_l3_tp1
	expert21_l3_tp1 -> expert_agg_l3_tp1
	norm_l3_tp1 -> expert22_l3_tp1
	expert22_l3_tp1 -> expert_agg_l3_tp1
	norm_l3_tp1 -> expert23_l3_tp1
	expert23_l3_tp1 -> expert_agg_l3_tp1
	expert_agg_l3_tp1 -> res_add2_l3_tp1
	norm_l3_tp1 -> res_add2_l3_tp1
	res_add2_l3_tp1 -> norm2_l3_tp1
	norm2_l3_tp1 -> final_output
	norm2_l2_tp2 -> input_split_l3_tp2
	input_split_l3_tp2 -> mha_qkv_l3_tp2
	mha_qkv_l3_tp2 -> mha_attn_l3_tp2
	mha_attn_l3_tp2 -> mha_out_l3_tp2
	mha_out_l3_tp2 -> attn_allreduce_l3_tp2
	input_split_l3_tp2 -> res_add_l3_tp2
	attn_allreduce_l3_tp2 -> res_add_l3_tp2
	res_add_l3_tp2 -> norm_l3_tp2
	norm_l3_tp2 -> gate_l3_tp2
	norm_l3_tp2 -> expert24_l3_tp2
	expert24_l3_tp2 -> expert_agg_l3_tp2
	norm_l3_tp2 -> expert25_l3_tp2
	expert25_l3_tp2 -> expert_agg_l3_tp2
	norm_l3_tp2 -> expert26_l3_tp2
	expert26_l3_tp2 -> expert_agg_l3_tp2
	norm_l3_tp2 -> expert27_l3_tp2
	expert27_l3_tp2 -> expert_agg_l3_tp2
	norm_l3_tp2 -> expert28_l3_tp2
	expert28_l3_tp2 -> expert_agg_l3_tp2
	norm_l3_tp2 -> expert29_l3_tp2
	expert29_l3_tp2 -> expert_agg_l3_tp2
	norm_l3_tp2 -> expert30_l3_tp2
	expert30_l3_tp2 -> expert_agg_l3_tp2
	norm_l3_tp2 -> expert31_l3_tp2
	expert31_l3_tp2 -> expert_agg_l3_tp2
	expert_agg_l3_tp2 -> res_add2_l3_tp2
	norm_l3_tp2 -> res_add2_l3_tp2
	res_add2_l3_tp2 -> norm2_l3_tp2
	norm2_l3_tp2 -> final_output
	norm2_l2_tp3 -> input_split_l3_tp3
	input_split_l3_tp3 -> mha_qkv_l3_tp3
	mha_qkv_l3_tp3 -> mha_attn_l3_tp3
	mha_attn_l3_tp3 -> mha_out_l3_tp3
	mha_out_l3_tp3 -> attn_allreduce_l3_tp3
	input_split_l3_tp3 -> res_add_l3_tp3
	attn_allreduce_l3_tp3 -> res_add_l3_tp3
	res_add_l3_tp3 -> norm_l3_tp3
	norm_l3_tp3 -> gate_l3_tp3
	norm_l3_tp3 -> expert32_l3_tp3
	expert32_l3_tp3 -> expert_agg_l3_tp3
	norm_l3_tp3 -> expert33_l3_tp3
	expert33_l3_tp3 -> expert_agg_l3_tp3
	norm_l3_tp3 -> expert34_l3_tp3
	expert34_l3_tp3 -> expert_agg_l3_tp3
	norm_l3_tp3 -> expert35_l3_tp3
	expert35_l3_tp3 -> expert_agg_l3_tp3
	norm_l3_tp3 -> expert36_l3_tp3
	expert36_l3_tp3 -> expert_agg_l3_tp3
	norm_l3_tp3 -> expert37_l3_tp3
	expert37_l3_tp3 -> expert_agg_l3_tp3
	norm_l3_tp3 -> expert38_l3_tp3
	expert38_l3_tp3 -> expert_agg_l3_tp3
	norm_l3_tp3 -> expert39_l3_tp3
	expert39_l3_tp3 -> expert_agg_l3_tp3
	expert_agg_l3_tp3 -> res_add2_l3_tp3
	norm_l3_tp3 -> res_add2_l3_tp3
	res_add2_l3_tp3 -> norm2_l3_tp3
	norm2_l3_tp3 -> final_output
	norm2_l2_tp4 -> input_split_l3_tp4
	input_split_l3_tp4 -> mha_qkv_l3_tp4
	mha_qkv_l3_tp4 -> mha_attn_l3_tp4
	mha_attn_l3_tp4 -> mha_out_l3_tp4
	mha_out_l3_tp4 -> attn_allreduce_l3_tp4
	input_split_l3_tp4 -> res_add_l3_tp4
	attn_allreduce_l3_tp4 -> res_add_l3_tp4
	res_add_l3_tp4 -> norm_l3_tp4
	norm_l3_tp4 -> gate_l3_tp4
	norm_l3_tp4 -> expert40_l3_tp4
	expert40_l3_tp4 -> expert_agg_l3_tp4
	norm_l3_tp4 -> expert41_l3_tp4
	expert41_l3_tp4 -> expert_agg_l3_tp4
	norm_l3_tp4 -> expert42_l3_tp4
	expert42_l3_tp4 -> expert_agg_l3_tp4
	norm_l3_tp4 -> expert43_l3_tp4
	expert43_l3_tp4 -> expert_agg_l3_tp4
	norm_l3_tp4 -> expert44_l3_tp4
	expert44_l3_tp4 -> expert_agg_l3_tp4
	norm_l3_tp4 -> expert45_l3_tp4
	expert45_l3_tp4 -> expert_agg_l3_tp4
	norm_l3_tp4 -> expert46_l3_tp4
	expert46_l3_tp4 -> expert_agg_l3_tp4
	norm_l3_tp4 -> expert47_l3_tp4
	expert47_l3_tp4 -> expert_agg_l3_tp4
	expert_agg_l3_tp4 -> res_add2_l3_tp4
	norm_l3_tp4 -> res_add2_l3_tp4
	res_add2_l3_tp4 -> norm2_l3_tp4
	norm2_l3_tp4 -> final_output
	norm2_l2_tp5 -> input_split_l3_tp5
	input_split_l3_tp5 -> mha_qkv_l3_tp5
	mha_qkv_l3_tp5 -> mha_attn_l3_tp5
	mha_attn_l3_tp5 -> mha_out_l3_tp5
	mha_out_l3_tp5 -> attn_allreduce_l3_tp5
	input_split_l3_tp5 -> res_add_l3_tp5
	attn_allreduce_l3_tp5 -> res_add_l3_tp5
	res_add_l3_tp5 -> norm_l3_tp5
	norm_l3_tp5 -> gate_l3_tp5
	norm_l3_tp5 -> expert48_l3_tp5
	expert48_l3_tp5 -> expert_agg_l3_tp5
	norm_l3_tp5 -> expert49_l3_tp5
	expert49_l3_tp5 -> expert_agg_l3_tp5
	norm_l3_tp5 -> expert50_l3_tp5
	expert50_l3_tp5 -> expert_agg_l3_tp5
	norm_l3_tp5 -> expert51_l3_tp5
	expert51_l3_tp5 -> expert_agg_l3_tp5
	norm_l3_tp5 -> expert52_l3_tp5
	expert52_l3_tp5 -> expert_agg_l3_tp5
	norm_l3_tp5 -> expert53_l3_tp5
	expert53_l3_tp5 -> expert_agg_l3_tp5
	norm_l3_tp5 -> expert54_l3_tp5
	expert54_l3_tp5 -> expert_agg_l3_tp5
	norm_l3_tp5 -> expert55_l3_tp5
	expert55_l3_tp5 -> expert_agg_l3_tp5
	expert_agg_l3_tp5 -> res_add2_l3_tp5
	norm_l3_tp5 -> res_add2_l3_tp5
	res_add2_l3_tp5 -> norm2_l3_tp5
	norm2_l3_tp5 -> final_output
	norm2_l2_tp6 -> input_split_l3_tp6
	input_split_l3_tp6 -> mha_qkv_l3_tp6
	mha_qkv_l3_tp6 -> mha_attn_l3_tp6
	mha_attn_l3_tp6 -> mha_out_l3_tp6
	mha_out_l3_tp6 -> attn_allreduce_l3_tp6
	input_split_l3_tp6 -> res_add_l3_tp6
	attn_allreduce_l3_tp6 -> res_add_l3_tp6
	res_add_l3_tp6 -> norm_l3_tp6
	norm_l3_tp6 -> gate_l3_tp6
	norm_l3_tp6 -> expert56_l3_tp6
	expert56_l3_tp6 -> expert_agg_l3_tp6
	norm_l3_tp6 -> expert57_l3_tp6
	expert57_l3_tp6 -> expert_agg_l3_tp6
	norm_l3_tp6 -> expert58_l3_tp6
	expert58_l3_tp6 -> expert_agg_l3_tp6
	norm_l3_tp6 -> expert59_l3_tp6
	expert59_l3_tp6 -> expert_agg_l3_tp6
	norm_l3_tp6 -> expert60_l3_tp6
	expert60_l3_tp6 -> expert_agg_l3_tp6
	norm_l3_tp6 -> expert61_l3_tp6
	expert61_l3_tp6 -> expert_agg_l3_tp6
	norm_l3_tp6 -> expert62_l3_tp6
	expert62_l3_tp6 -> expert_agg_l3_tp6
	norm_l3_tp6 -> expert63_l3_tp6
	expert63_l3_tp6 -> expert_agg_l3_tp6
	expert_agg_l3_tp6 -> res_add2_l3_tp6
	norm_l3_tp6 -> res_add2_l3_tp6
	res_add2_l3_tp6 -> norm2_l3_tp6
	norm2_l3_tp6 -> final_output
	norm2_l2_tp7 -> input_split_l3_tp7
	input_split_l3_tp7 -> mha_qkv_l3_tp7
	mha_qkv_l3_tp7 -> mha_attn_l3_tp7
	mha_attn_l3_tp7 -> mha_out_l3_tp7
	mha_out_l3_tp7 -> attn_allreduce_l3_tp7
	input_split_l3_tp7 -> res_add_l3_tp7
	attn_allreduce_l3_tp7 -> res_add_l3_tp7
	res_add_l3_tp7 -> norm_l3_tp7
	norm_l3_tp7 -> gate_l3_tp7
	norm_l3_tp7 -> expert64_l3_tp7
	expert64_l3_tp7 -> expert_agg_l3_tp7
	norm_l3_tp7 -> expert65_l3_tp7
	expert65_l3_tp7 -> expert_agg_l3_tp7
	norm_l3_tp7 -> expert66_l3_tp7
	expert66_l3_tp7 -> expert_agg_l3_tp7
	norm_l3_tp7 -> expert67_l3_tp7
	expert67_l3_tp7 -> expert_agg_l3_tp7
	norm_l3_tp7 -> expert68_l3_tp7
	expert68_l3_tp7 -> expert_agg_l3_tp7
	norm_l3_tp7 -> expert69_l3_tp7
	expert69_l3_tp7 -> expert_agg_l3_tp7
	norm_l3_tp7 -> expert70_l3_tp7
	expert70_l3_tp7 -> expert_agg_l3_tp7
	norm_l3_tp7 -> expert71_l3_tp7
	expert71_l3_tp7 -> expert_agg_l3_tp7
	expert_agg_l3_tp7 -> res_add2_l3_tp7
	norm_l3_tp7 -> res_add2_l3_tp7
	res_add2_l3_tp7 -> norm2_l3_tp7
	norm2_l3_tp7 -> final_output
}
