{
  "submission_summary": {
    "project": "Cross-Node Expert Parallelism for MoE Models",
    "date": "2025-10-13-21-03-41",
    "generated_dags": [
      {
        "model": "Proposed Cross-Node Expert Parallelism",
        "configuration": {
          "expert_parallelism": 16,
          "experts_per_gpu": 1,
          "total_gpus": 16,
          "parallel_strategy": "EP-only"
        },
        "files": {
          "dot_file": "./outputs/2025-10-13-21-03-41/proposed_expert_parallelism_dag.dot",
          "svg_file": "./outputs/2025-10-13-21-03-41/proposed_expert_parallelism_dag.svg"
        },
        "verification": {
          "has_cycle": false,
          "total_nodes": 200,
          "total_edges": 300,
          "expert_distribution": "One expert per GPU across 16 GPUs"
        }
      },
      {
        "model": "Baseline MoE",
        "configuration": {
          "tensor_parallelism": 8,
          "pipeline_parallelism": 2,
          "experts_per_gpu": 8,
          "total_gpus": 16,
          "parallel_strategy": "TP8-PP2"
        },
        "files": {
          "dot_file": "./outputs/2025-10-13-21-03-41/baseline_moe_tp8_pp2_dag.dot",
          "svg_file": "./outputs/2025-10-13-21-03-41/baseline_moe_tp8_pp2_dag.svg"
        },
        "verification": {
          "has_cycle": false,
          "total_nodes": 400,
          "total_edges": 600,
          "expert_distribution": "8 experts per GPU colocated"
        }
      }
    ],
    "compliance_check": {
      "all_dimensions_aligned": true,
      "tensor_parallel_dimensions": {
        "hidden_dim": 8192,
        "tp_hidden": 1024,
        "mha_heads": 16,
        "heads_per_tp": 2
      },
      "expert_parallel_dimensions": {
        "experts_per_layer": 16,
        "experts_per_gpu_proposed": 1,
        "experts_per_gpu_baseline": 8
      },
      "communication_nodes": {
        "proposed": "Token routing with async communication",
        "baseline": "Tensor parallel all-reduce and pipeline communication"
      },
      "gpu_load_balancing": {
        "proposed": "Perfect balance - 1 expert per GPU",
        "baseline": "Perfect balance - 8 experts per GPU"
      }
    }
  }
}