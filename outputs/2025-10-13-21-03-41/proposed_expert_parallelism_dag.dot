// Cross-Node Expert Parallelism (EP=16)
digraph proposed_expert_parallelism {
	compound=true rankdir=TB size="30,40"
	node [fontname=Arial fontsize=10]
	subgraph cluster_layer0 {
		bgcolor=aliceblue color=black label="Layer 0" style=rounded
		input_l0 [label="Input
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightblue shape=ellipse style=filled]
		mha_qkv_l0 [label="MHA QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		mha_attn_l0 [label="MHA Attention
Input: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		mha_out_l0 [label="MHA Output Linear
Input: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		res_add_l0 [label="Residual Add
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		norm_l0 [label="Layer Norm
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		gate_l0 [label="Gate Network
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: all GPUs" fillcolor=lightyellow shape=parallelogram style=filled]
		route_l0 [label="Token Routing
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [distributed across 16 GPUs]
GPU: all GPUs" fillcolor=lightgray shape=parallelogram style="filled, dashed"]
		expert0_comm_l0 [label="Expert 0 Communication
Input: [tokens selected for expert 0]
Output: [tokens on GPU 0]
GPU: 0" fillcolor=lightgray shape=ellipse style=filled]
		expert0_l0 [label="Expert 0 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
		expert0_return_l0 [label="Expert 0 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 0" fillcolor=lightgray shape=ellipse style=filled]
		expert1_comm_l0 [label="Expert 1 Communication
Input: [tokens selected for expert 1]
Output: [tokens on GPU 1]
GPU: 1" fillcolor=lightgray shape=ellipse style=filled]
		expert1_l0 [label="Expert 1 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
		expert1_return_l0 [label="Expert 1 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 1" fillcolor=lightgray shape=ellipse style=filled]
		expert2_comm_l0 [label="Expert 2 Communication
Input: [tokens selected for expert 2]
Output: [tokens on GPU 2]
GPU: 2" fillcolor=lightgray shape=ellipse style=filled]
		expert2_l0 [label="Expert 2 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
		expert2_return_l0 [label="Expert 2 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 2" fillcolor=lightgray shape=ellipse style=filled]
		expert3_comm_l0 [label="Expert 3 Communication
Input: [tokens selected for expert 3]
Output: [tokens on GPU 3]
GPU: 3" fillcolor=lightgray shape=ellipse style=filled]
		expert3_l0 [label="Expert 3 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
		expert3_return_l0 [label="Expert 3 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 3" fillcolor=lightgray shape=ellipse style=filled]
		expert4_comm_l0 [label="Expert 4 Communication
Input: [tokens selected for expert 4]
Output: [tokens on GPU 4]
GPU: 4" fillcolor=lightgray shape=ellipse style=filled]
		expert4_l0 [label="Expert 4 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
		expert4_return_l0 [label="Expert 4 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 4" fillcolor=lightgray shape=ellipse style=filled]
		expert5_comm_l0 [label="Expert 5 Communication
Input: [tokens selected for expert 5]
Output: [tokens on GPU 5]
GPU: 5" fillcolor=lightgray shape=ellipse style=filled]
		expert5_l0 [label="Expert 5 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
		expert5_return_l0 [label="Expert 5 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 5" fillcolor=lightgray shape=ellipse style=filled]
		expert6_comm_l0 [label="Expert 6 Communication
Input: [tokens selected for expert 6]
Output: [tokens on GPU 6]
GPU: 6" fillcolor=lightgray shape=ellipse style=filled]
		expert6_l0 [label="Expert 6 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
		expert6_return_l0 [label="Expert 6 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 6" fillcolor=lightgray shape=ellipse style=filled]
		expert7_comm_l0 [label="Expert 7 Communication
Input: [tokens selected for expert 7]
Output: [tokens on GPU 7]
GPU: 7" fillcolor=lightgray shape=ellipse style=filled]
		expert7_l0 [label="Expert 7 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
		expert7_return_l0 [label="Expert 7 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 7" fillcolor=lightgray shape=ellipse style=filled]
		expert8_comm_l0 [label="Expert 8 Communication
Input: [tokens selected for expert 8]
Output: [tokens on GPU 8]
GPU: 8" fillcolor=lightgray shape=ellipse style=filled]
		expert8_l0 [label="Expert 8 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
		expert8_return_l0 [label="Expert 8 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 8" fillcolor=lightgray shape=ellipse style=filled]
		expert9_comm_l0 [label="Expert 9 Communication
Input: [tokens selected for expert 9]
Output: [tokens on GPU 9]
GPU: 9" fillcolor=lightgray shape=ellipse style=filled]
		expert9_l0 [label="Expert 9 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
		expert9_return_l0 [label="Expert 9 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 9" fillcolor=lightgray shape=ellipse style=filled]
		expert10_comm_l0 [label="Expert 10 Communication
Input: [tokens selected for expert 10]
Output: [tokens on GPU 10]
GPU: 10" fillcolor=lightgray shape=ellipse style=filled]
		expert10_l0 [label="Expert 10 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
		expert10_return_l0 [label="Expert 10 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 10" fillcolor=lightgray shape=ellipse style=filled]
		expert11_comm_l0 [label="Expert 11 Communication
Input: [tokens selected for expert 11]
Output: [tokens on GPU 11]
GPU: 11" fillcolor=lightgray shape=ellipse style=filled]
		expert11_l0 [label="Expert 11 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
		expert11_return_l0 [label="Expert 11 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 11" fillcolor=lightgray shape=ellipse style=filled]
		expert12_comm_l0 [label="Expert 12 Communication
Input: [tokens selected for expert 12]
Output: [tokens on GPU 12]
GPU: 12" fillcolor=lightgray shape=ellipse style=filled]
		expert12_l0 [label="Expert 12 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
		expert12_return_l0 [label="Expert 12 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 12" fillcolor=lightgray shape=ellipse style=filled]
		expert13_comm_l0 [label="Expert 13 Communication
Input: [tokens selected for expert 13]
Output: [tokens on GPU 13]
GPU: 13" fillcolor=lightgray shape=ellipse style=filled]
		expert13_l0 [label="Expert 13 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
		expert13_return_l0 [label="Expert 13 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 13" fillcolor=lightgray shape=ellipse style=filled]
		expert14_comm_l0 [label="Expert 14 Communication
Input: [tokens selected for expert 14]
Output: [tokens on GPU 14]
GPU: 14" fillcolor=lightgray shape=ellipse style=filled]
		expert14_l0 [label="Expert 14 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
		expert14_return_l0 [label="Expert 14 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 14" fillcolor=lightgray shape=ellipse style=filled]
		expert15_comm_l0 [label="Expert 15 Communication
Input: [tokens selected for expert 15]
Output: [tokens on GPU 15]
GPU: 15" fillcolor=lightgray shape=ellipse style=filled]
		expert15_l0 [label="Expert 15 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
		expert15_return_l0 [label="Expert 15 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 15" fillcolor=lightgray shape=ellipse style=filled]
		aggregate_l0 [label="Expert Aggregation
Input: [processed_tokens from all experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightpink shape=parallelogram style=filled]
		res_add2_l0 [label="Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		norm2_l0 [label="Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		input_l0 -> mha_qkv_l0
		mha_qkv_l0 -> mha_attn_l0
		mha_attn_l0 -> mha_out_l0
		input_l0 -> res_add_l0
		mha_out_l0 -> res_add_l0
		res_add_l0 -> norm_l0
		norm_l0 -> gate_l0
		norm_l0 -> route_l0
		route_l0 -> expert0_comm_l0
		expert0_comm_l0 -> expert0_l0
		expert0_l0 -> expert0_return_l0
		expert0_return_l0 -> aggregate_l0
		route_l0 -> expert1_comm_l0
		expert1_comm_l0 -> expert1_l0
		expert1_l0 -> expert1_return_l0
		expert1_return_l0 -> aggregate_l0
		route_l0 -> expert2_comm_l0
		expert2_comm_l0 -> expert2_l0
		expert2_l0 -> expert2_return_l0
		expert2_return_l0 -> aggregate_l0
		route_l0 -> expert3_comm_l0
		expert3_comm_l0 -> expert3_l0
		expert3_l0 -> expert3_return_l0
		expert3_return_l0 -> aggregate_l0
		route_l0 -> expert4_comm_l0
		expert4_comm_l0 -> expert4_l0
		expert4_l0 -> expert4_return_l0
		expert4_return_l0 -> aggregate_l0
		route_l0 -> expert5_comm_l0
		expert5_comm_l0 -> expert5_l0
		expert5_l0 -> expert5_return_l0
		expert5_return_l0 -> aggregate_l0
		route_l0 -> expert6_comm_l0
		expert6_comm_l0 -> expert6_l0
		expert6_l0 -> expert6_return_l0
		expert6_return_l0 -> aggregate_l0
		route_l0 -> expert7_comm_l0
		expert7_comm_l0 -> expert7_l0
		expert7_l0 -> expert7_return_l0
		expert7_return_l0 -> aggregate_l0
		route_l0 -> expert8_comm_l0
		expert8_comm_l0 -> expert8_l0
		expert8_l0 -> expert8_return_l0
		expert8_return_l0 -> aggregate_l0
		route_l0 -> expert9_comm_l0
		expert9_comm_l0 -> expert9_l0
		expert9_l0 -> expert9_return_l0
		expert9_return_l0 -> aggregate_l0
		route_l0 -> expert10_comm_l0
		expert10_comm_l0 -> expert10_l0
		expert10_l0 -> expert10_return_l0
		expert10_return_l0 -> aggregate_l0
		route_l0 -> expert11_comm_l0
		expert11_comm_l0 -> expert11_l0
		expert11_l0 -> expert11_return_l0
		expert11_return_l0 -> aggregate_l0
		route_l0 -> expert12_comm_l0
		expert12_comm_l0 -> expert12_l0
		expert12_l0 -> expert12_return_l0
		expert12_return_l0 -> aggregate_l0
		route_l0 -> expert13_comm_l0
		expert13_comm_l0 -> expert13_l0
		expert13_l0 -> expert13_return_l0
		expert13_return_l0 -> aggregate_l0
		route_l0 -> expert14_comm_l0
		expert14_comm_l0 -> expert14_l0
		expert14_l0 -> expert14_return_l0
		expert14_return_l0 -> aggregate_l0
		route_l0 -> expert15_comm_l0
		expert15_comm_l0 -> expert15_l0
		expert15_l0 -> expert15_return_l0
		expert15_return_l0 -> aggregate_l0
		aggregate_l0 -> res_add2_l0
		norm_l0 -> res_add2_l0
		res_add2_l0 -> norm2_l0
	}
	subgraph cluster_layer1 {
		bgcolor=honeydew color=black label="Layer 1" style=rounded
		input_l1 [label="Layer 1 Input
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightblue shape=ellipse style=filled]
		mha_qkv_l1 [label="MHA QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		mha_attn_l1 [label="MHA Attention
Input: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		mha_out_l1 [label="MHA Output Linear
Input: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		res_add_l1 [label="Residual Add
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		norm_l1 [label="Layer Norm
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		gate_l1 [label="Gate Network
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: all GPUs" fillcolor=lightyellow shape=parallelogram style=filled]
		route_l1 [label="Token Routing
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [distributed across 16 GPUs]
GPU: all GPUs" fillcolor=lightgray shape=parallelogram style="filled, dashed"]
		expert0_comm_l1 [label="Expert 0 Communication
Input: [tokens selected for expert 0]
Output: [tokens on GPU 0]
GPU: 0" fillcolor=lightgray shape=ellipse style=filled]
		expert0_l1 [label="Expert 0 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
		expert0_return_l1 [label="Expert 0 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 0" fillcolor=lightgray shape=ellipse style=filled]
		expert1_comm_l1 [label="Expert 1 Communication
Input: [tokens selected for expert 1]
Output: [tokens on GPU 1]
GPU: 1" fillcolor=lightgray shape=ellipse style=filled]
		expert1_l1 [label="Expert 1 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
		expert1_return_l1 [label="Expert 1 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 1" fillcolor=lightgray shape=ellipse style=filled]
		expert2_comm_l1 [label="Expert 2 Communication
Input: [tokens selected for expert 2]
Output: [tokens on GPU 2]
GPU: 2" fillcolor=lightgray shape=ellipse style=filled]
		expert2_l1 [label="Expert 2 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
		expert2_return_l1 [label="Expert 2 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 2" fillcolor=lightgray shape=ellipse style=filled]
		expert3_comm_l1 [label="Expert 3 Communication
Input: [tokens selected for expert 3]
Output: [tokens on GPU 3]
GPU: 3" fillcolor=lightgray shape=ellipse style=filled]
		expert3_l1 [label="Expert 3 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
		expert3_return_l1 [label="Expert 3 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 3" fillcolor=lightgray shape=ellipse style=filled]
		expert4_comm_l1 [label="Expert 4 Communication
Input: [tokens selected for expert 4]
Output: [tokens on GPU 4]
GPU: 4" fillcolor=lightgray shape=ellipse style=filled]
		expert4_l1 [label="Expert 4 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
		expert4_return_l1 [label="Expert 4 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 4" fillcolor=lightgray shape=ellipse style=filled]
		expert5_comm_l1 [label="Expert 5 Communication
Input: [tokens selected for expert 5]
Output: [tokens on GPU 5]
GPU: 5" fillcolor=lightgray shape=ellipse style=filled]
		expert5_l1 [label="Expert 5 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
		expert5_return_l1 [label="Expert 5 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 5" fillcolor=lightgray shape=ellipse style=filled]
		expert6_comm_l1 [label="Expert 6 Communication
Input: [tokens selected for expert 6]
Output: [tokens on GPU 6]
GPU: 6" fillcolor=lightgray shape=ellipse style=filled]
		expert6_l1 [label="Expert 6 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
		expert6_return_l1 [label="Expert 6 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 6" fillcolor=lightgray shape=ellipse style=filled]
		expert7_comm_l1 [label="Expert 7 Communication
Input: [tokens selected for expert 7]
Output: [tokens on GPU 7]
GPU: 7" fillcolor=lightgray shape=ellipse style=filled]
		expert7_l1 [label="Expert 7 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
		expert7_return_l1 [label="Expert 7 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 7" fillcolor=lightgray shape=ellipse style=filled]
		expert8_comm_l1 [label="Expert 8 Communication
Input: [tokens selected for expert 8]
Output: [tokens on GPU 8]
GPU: 8" fillcolor=lightgray shape=ellipse style=filled]
		expert8_l1 [label="Expert 8 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
		expert8_return_l1 [label="Expert 8 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 8" fillcolor=lightgray shape=ellipse style=filled]
		expert9_comm_l1 [label="Expert 9 Communication
Input: [tokens selected for expert 9]
Output: [tokens on GPU 9]
GPU: 9" fillcolor=lightgray shape=ellipse style=filled]
		expert9_l1 [label="Expert 9 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
		expert9_return_l1 [label="Expert 9 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 9" fillcolor=lightgray shape=ellipse style=filled]
		expert10_comm_l1 [label="Expert 10 Communication
Input: [tokens selected for expert 10]
Output: [tokens on GPU 10]
GPU: 10" fillcolor=lightgray shape=ellipse style=filled]
		expert10_l1 [label="Expert 10 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
		expert10_return_l1 [label="Expert 10 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 10" fillcolor=lightgray shape=ellipse style=filled]
		expert11_comm_l1 [label="Expert 11 Communication
Input: [tokens selected for expert 11]
Output: [tokens on GPU 11]
GPU: 11" fillcolor=lightgray shape=ellipse style=filled]
		expert11_l1 [label="Expert 11 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
		expert11_return_l1 [label="Expert 11 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 11" fillcolor=lightgray shape=ellipse style=filled]
		expert12_comm_l1 [label="Expert 12 Communication
Input: [tokens selected for expert 12]
Output: [tokens on GPU 12]
GPU: 12" fillcolor=lightgray shape=ellipse style=filled]
		expert12_l1 [label="Expert 12 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
		expert12_return_l1 [label="Expert 12 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 12" fillcolor=lightgray shape=ellipse style=filled]
		expert13_comm_l1 [label="Expert 13 Communication
Input: [tokens selected for expert 13]
Output: [tokens on GPU 13]
GPU: 13" fillcolor=lightgray shape=ellipse style=filled]
		expert13_l1 [label="Expert 13 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
		expert13_return_l1 [label="Expert 13 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 13" fillcolor=lightgray shape=ellipse style=filled]
		expert14_comm_l1 [label="Expert 14 Communication
Input: [tokens selected for expert 14]
Output: [tokens on GPU 14]
GPU: 14" fillcolor=lightgray shape=ellipse style=filled]
		expert14_l1 [label="Expert 14 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
		expert14_return_l1 [label="Expert 14 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 14" fillcolor=lightgray shape=ellipse style=filled]
		expert15_comm_l1 [label="Expert 15 Communication
Input: [tokens selected for expert 15]
Output: [tokens on GPU 15]
GPU: 15" fillcolor=lightgray shape=ellipse style=filled]
		expert15_l1 [label="Expert 15 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
		expert15_return_l1 [label="Expert 15 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 15" fillcolor=lightgray shape=ellipse style=filled]
		aggregate_l1 [label="Expert Aggregation
Input: [processed_tokens from all experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightpink shape=parallelogram style=filled]
		res_add2_l1 [label="Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		norm2_l1 [label="Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		input_l1 -> mha_qkv_l1
		mha_qkv_l1 -> mha_attn_l1
		mha_attn_l1 -> mha_out_l1
		input_l1 -> res_add_l1
		mha_out_l1 -> res_add_l1
		res_add_l1 -> norm_l1
		norm_l1 -> gate_l1
		norm_l1 -> route_l1
		route_l1 -> expert0_comm_l1
		expert0_comm_l1 -> expert0_l1
		expert0_l1 -> expert0_return_l1
		expert0_return_l1 -> aggregate_l1
		route_l1 -> expert1_comm_l1
		expert1_comm_l1 -> expert1_l1
		expert1_l1 -> expert1_return_l1
		expert1_return_l1 -> aggregate_l1
		route_l1 -> expert2_comm_l1
		expert2_comm_l1 -> expert2_l1
		expert2_l1 -> expert2_return_l1
		expert2_return_l1 -> aggregate_l1
		route_l1 -> expert3_comm_l1
		expert3_comm_l1 -> expert3_l1
		expert3_l1 -> expert3_return_l1
		expert3_return_l1 -> aggregate_l1
		route_l1 -> expert4_comm_l1
		expert4_comm_l1 -> expert4_l1
		expert4_l1 -> expert4_return_l1
		expert4_return_l1 -> aggregate_l1
		route_l1 -> expert5_comm_l1
		expert5_comm_l1 -> expert5_l1
		expert5_l1 -> expert5_return_l1
		expert5_return_l1 -> aggregate_l1
		route_l1 -> expert6_comm_l1
		expert6_comm_l1 -> expert6_l1
		expert6_l1 -> expert6_return_l1
		expert6_return_l1 -> aggregate_l1
		route_l1 -> expert7_comm_l1
		expert7_comm_l1 -> expert7_l1
		expert7_l1 -> expert7_return_l1
		expert7_return_l1 -> aggregate_l1
		route_l1 -> expert8_comm_l1
		expert8_comm_l1 -> expert8_l1
		expert8_l1 -> expert8_return_l1
		expert8_return_l1 -> aggregate_l1
		route_l1 -> expert9_comm_l1
		expert9_comm_l1 -> expert9_l1
		expert9_l1 -> expert9_return_l1
		expert9_return_l1 -> aggregate_l1
		route_l1 -> expert10_comm_l1
		expert10_comm_l1 -> expert10_l1
		expert10_l1 -> expert10_return_l1
		expert10_return_l1 -> aggregate_l1
		route_l1 -> expert11_comm_l1
		expert11_comm_l1 -> expert11_l1
		expert11_l1 -> expert11_return_l1
		expert11_return_l1 -> aggregate_l1
		route_l1 -> expert12_comm_l1
		expert12_comm_l1 -> expert12_l1
		expert12_l1 -> expert12_return_l1
		expert12_return_l1 -> aggregate_l1
		route_l1 -> expert13_comm_l1
		expert13_comm_l1 -> expert13_l1
		expert13_l1 -> expert13_return_l1
		expert13_return_l1 -> aggregate_l1
		route_l1 -> expert14_comm_l1
		expert14_comm_l1 -> expert14_l1
		expert14_l1 -> expert14_return_l1
		expert14_return_l1 -> aggregate_l1
		route_l1 -> expert15_comm_l1
		expert15_comm_l1 -> expert15_l1
		expert15_l1 -> expert15_return_l1
		expert15_return_l1 -> aggregate_l1
		aggregate_l1 -> res_add2_l1
		norm_l1 -> res_add2_l1
		res_add2_l1 -> norm2_l1
	}
	subgraph cluster_layer2 {
		bgcolor=lavender color=black label="Layer 2" style=rounded
		input_l2 [label="Layer 2 Input
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightblue shape=ellipse style=filled]
		mha_qkv_l2 [label="MHA QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		mha_attn_l2 [label="MHA Attention
Input: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		mha_out_l2 [label="MHA Output Linear
Input: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		res_add_l2 [label="Residual Add
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		norm_l2 [label="Layer Norm
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		gate_l2 [label="Gate Network
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: all GPUs" fillcolor=lightyellow shape=parallelogram style=filled]
		route_l2 [label="Token Routing
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [distributed across 16 GPUs]
GPU: all GPUs" fillcolor=lightgray shape=parallelogram style="filled, dashed"]
		expert0_comm_l2 [label="Expert 0 Communication
Input: [tokens selected for expert 0]
Output: [tokens on GPU 0]
GPU: 0" fillcolor=lightgray shape=ellipse style=filled]
		expert0_l2 [label="Expert 0 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
		expert0_return_l2 [label="Expert 0 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 0" fillcolor=lightgray shape=ellipse style=filled]
		expert1_comm_l2 [label="Expert 1 Communication
Input: [tokens selected for expert 1]
Output: [tokens on GPU 1]
GPU: 1" fillcolor=lightgray shape=ellipse style=filled]
		expert1_l2 [label="Expert 1 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
		expert1_return_l2 [label="Expert 1 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 1" fillcolor=lightgray shape=ellipse style=filled]
		expert2_comm_l2 [label="Expert 2 Communication
Input: [tokens selected for expert 2]
Output: [tokens on GPU 2]
GPU: 2" fillcolor=lightgray shape=ellipse style=filled]
		expert2_l2 [label="Expert 2 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
		expert2_return_l2 [label="Expert 2 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 2" fillcolor=lightgray shape=ellipse style=filled]
		expert3_comm_l2 [label="Expert 3 Communication
Input: [tokens selected for expert 3]
Output: [tokens on GPU 3]
GPU: 3" fillcolor=lightgray shape=ellipse style=filled]
		expert3_l2 [label="Expert 3 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
		expert3_return_l2 [label="Expert 3 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 3" fillcolor=lightgray shape=ellipse style=filled]
		expert4_comm_l2 [label="Expert 4 Communication
Input: [tokens selected for expert 4]
Output: [tokens on GPU 4]
GPU: 4" fillcolor=lightgray shape=ellipse style=filled]
		expert4_l2 [label="Expert 4 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
		expert4_return_l2 [label="Expert 4 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 4" fillcolor=lightgray shape=ellipse style=filled]
		expert5_comm_l2 [label="Expert 5 Communication
Input: [tokens selected for expert 5]
Output: [tokens on GPU 5]
GPU: 5" fillcolor=lightgray shape=ellipse style=filled]
		expert5_l2 [label="Expert 5 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
		expert5_return_l2 [label="Expert 5 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 5" fillcolor=lightgray shape=ellipse style=filled]
		expert6_comm_l2 [label="Expert 6 Communication
Input: [tokens selected for expert 6]
Output: [tokens on GPU 6]
GPU: 6" fillcolor=lightgray shape=ellipse style=filled]
		expert6_l2 [label="Expert 6 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
		expert6_return_l2 [label="Expert 6 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 6" fillcolor=lightgray shape=ellipse style=filled]
		expert7_comm_l2 [label="Expert 7 Communication
Input: [tokens selected for expert 7]
Output: [tokens on GPU 7]
GPU: 7" fillcolor=lightgray shape=ellipse style=filled]
		expert7_l2 [label="Expert 7 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
		expert7_return_l2 [label="Expert 7 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 7" fillcolor=lightgray shape=ellipse style=filled]
		expert8_comm_l2 [label="Expert 8 Communication
Input: [tokens selected for expert 8]
Output: [tokens on GPU 8]
GPU: 8" fillcolor=lightgray shape=ellipse style=filled]
		expert8_l2 [label="Expert 8 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
		expert8_return_l2 [label="Expert 8 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 8" fillcolor=lightgray shape=ellipse style=filled]
		expert9_comm_l2 [label="Expert 9 Communication
Input: [tokens selected for expert 9]
Output: [tokens on GPU 9]
GPU: 9" fillcolor=lightgray shape=ellipse style=filled]
		expert9_l2 [label="Expert 9 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
		expert9_return_l2 [label="Expert 9 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 9" fillcolor=lightgray shape=ellipse style=filled]
		expert10_comm_l2 [label="Expert 10 Communication
Input: [tokens selected for expert 10]
Output: [tokens on GPU 10]
GPU: 10" fillcolor=lightgray shape=ellipse style=filled]
		expert10_l2 [label="Expert 10 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
		expert10_return_l2 [label="Expert 10 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 10" fillcolor=lightgray shape=ellipse style=filled]
		expert11_comm_l2 [label="Expert 11 Communication
Input: [tokens selected for expert 11]
Output: [tokens on GPU 11]
GPU: 11" fillcolor=lightgray shape=ellipse style=filled]
		expert11_l2 [label="Expert 11 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
		expert11_return_l2 [label="Expert 11 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 11" fillcolor=lightgray shape=ellipse style=filled]
		expert12_comm_l2 [label="Expert 12 Communication
Input: [tokens selected for expert 12]
Output: [tokens on GPU 12]
GPU: 12" fillcolor=lightgray shape=ellipse style=filled]
		expert12_l2 [label="Expert 12 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
		expert12_return_l2 [label="Expert 12 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 12" fillcolor=lightgray shape=ellipse style=filled]
		expert13_comm_l2 [label="Expert 13 Communication
Input: [tokens selected for expert 13]
Output: [tokens on GPU 13]
GPU: 13" fillcolor=lightgray shape=ellipse style=filled]
		expert13_l2 [label="Expert 13 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
		expert13_return_l2 [label="Expert 13 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 13" fillcolor=lightgray shape=ellipse style=filled]
		expert14_comm_l2 [label="Expert 14 Communication
Input: [tokens selected for expert 14]
Output: [tokens on GPU 14]
GPU: 14" fillcolor=lightgray shape=ellipse style=filled]
		expert14_l2 [label="Expert 14 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
		expert14_return_l2 [label="Expert 14 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 14" fillcolor=lightgray shape=ellipse style=filled]
		expert15_comm_l2 [label="Expert 15 Communication
Input: [tokens selected for expert 15]
Output: [tokens on GPU 15]
GPU: 15" fillcolor=lightgray shape=ellipse style=filled]
		expert15_l2 [label="Expert 15 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
		expert15_return_l2 [label="Expert 15 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 15" fillcolor=lightgray shape=ellipse style=filled]
		aggregate_l2 [label="Expert Aggregation
Input: [processed_tokens from all experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightpink shape=parallelogram style=filled]
		res_add2_l2 [label="Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		norm2_l2 [label="Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		input_l2 -> mha_qkv_l2
		mha_qkv_l2 -> mha_attn_l2
		mha_attn_l2 -> mha_out_l2
		input_l2 -> res_add_l2
		mha_out_l2 -> res_add_l2
		res_add_l2 -> norm_l2
		norm_l2 -> gate_l2
		norm_l2 -> route_l2
		route_l2 -> expert0_comm_l2
		expert0_comm_l2 -> expert0_l2
		expert0_l2 -> expert0_return_l2
		expert0_return_l2 -> aggregate_l2
		route_l2 -> expert1_comm_l2
		expert1_comm_l2 -> expert1_l2
		expert1_l2 -> expert1_return_l2
		expert1_return_l2 -> aggregate_l2
		route_l2 -> expert2_comm_l2
		expert2_comm_l2 -> expert2_l2
		expert2_l2 -> expert2_return_l2
		expert2_return_l2 -> aggregate_l2
		route_l2 -> expert3_comm_l2
		expert3_comm_l2 -> expert3_l2
		expert3_l2 -> expert3_return_l2
		expert3_return_l2 -> aggregate_l2
		route_l2 -> expert4_comm_l2
		expert4_comm_l2 -> expert4_l2
		expert4_l2 -> expert4_return_l2
		expert4_return_l2 -> aggregate_l2
		route_l2 -> expert5_comm_l2
		expert5_comm_l2 -> expert5_l2
		expert5_l2 -> expert5_return_l2
		expert5_return_l2 -> aggregate_l2
		route_l2 -> expert6_comm_l2
		expert6_comm_l2 -> expert6_l2
		expert6_l2 -> expert6_return_l2
		expert6_return_l2 -> aggregate_l2
		route_l2 -> expert7_comm_l2
		expert7_comm_l2 -> expert7_l2
		expert7_l2 -> expert7_return_l2
		expert7_return_l2 -> aggregate_l2
		route_l2 -> expert8_comm_l2
		expert8_comm_l2 -> expert8_l2
		expert8_l2 -> expert8_return_l2
		expert8_return_l2 -> aggregate_l2
		route_l2 -> expert9_comm_l2
		expert9_comm_l2 -> expert9_l2
		expert9_l2 -> expert9_return_l2
		expert9_return_l2 -> aggregate_l2
		route_l2 -> expert10_comm_l2
		expert10_comm_l2 -> expert10_l2
		expert10_l2 -> expert10_return_l2
		expert10_return_l2 -> aggregate_l2
		route_l2 -> expert11_comm_l2
		expert11_comm_l2 -> expert11_l2
		expert11_l2 -> expert11_return_l2
		expert11_return_l2 -> aggregate_l2
		route_l2 -> expert12_comm_l2
		expert12_comm_l2 -> expert12_l2
		expert12_l2 -> expert12_return_l2
		expert12_return_l2 -> aggregate_l2
		route_l2 -> expert13_comm_l2
		expert13_comm_l2 -> expert13_l2
		expert13_l2 -> expert13_return_l2
		expert13_return_l2 -> aggregate_l2
		route_l2 -> expert14_comm_l2
		expert14_comm_l2 -> expert14_l2
		expert14_l2 -> expert14_return_l2
		expert14_return_l2 -> aggregate_l2
		route_l2 -> expert15_comm_l2
		expert15_comm_l2 -> expert15_l2
		expert15_l2 -> expert15_return_l2
		expert15_return_l2 -> aggregate_l2
		aggregate_l2 -> res_add2_l2
		norm_l2 -> res_add2_l2
		res_add2_l2 -> norm2_l2
	}
	subgraph cluster_layer3 {
		bgcolor=mistyrose color=black label="Layer 3" style=rounded
		input_l3 [label="Layer 3 Input
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightblue shape=ellipse style=filled]
		mha_qkv_l3 [label="MHA QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		mha_attn_l3 [label="MHA Attention
Input: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
Output: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		mha_out_l3 [label="MHA Output Linear
Input: [batch_size=1024, seq_len=10000, heads=16, d_k=512]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		res_add_l3 [label="Residual Add
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		norm_l3 [label="Layer Norm
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		gate_l3 [label="Gate Network
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, num_experts=16]
GPU: all GPUs" fillcolor=lightyellow shape=parallelogram style=filled]
		route_l3 [label="Token Routing
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [distributed across 16 GPUs]
GPU: all GPUs" fillcolor=lightgray shape=parallelogram style="filled, dashed"]
		expert0_comm_l3 [label="Expert 0 Communication
Input: [tokens selected for expert 0]
Output: [tokens on GPU 0]
GPU: 0" fillcolor=lightgray shape=ellipse style=filled]
		expert0_l3 [label="Expert 0 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 0" fillcolor=lightcoral shape=rectangle style=filled]
		expert0_return_l3 [label="Expert 0 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 0" fillcolor=lightgray shape=ellipse style=filled]
		expert1_comm_l3 [label="Expert 1 Communication
Input: [tokens selected for expert 1]
Output: [tokens on GPU 1]
GPU: 1" fillcolor=lightgray shape=ellipse style=filled]
		expert1_l3 [label="Expert 1 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 1" fillcolor=lightcoral shape=rectangle style=filled]
		expert1_return_l3 [label="Expert 1 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 1" fillcolor=lightgray shape=ellipse style=filled]
		expert2_comm_l3 [label="Expert 2 Communication
Input: [tokens selected for expert 2]
Output: [tokens on GPU 2]
GPU: 2" fillcolor=lightgray shape=ellipse style=filled]
		expert2_l3 [label="Expert 2 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 2" fillcolor=lightcoral shape=rectangle style=filled]
		expert2_return_l3 [label="Expert 2 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 2" fillcolor=lightgray shape=ellipse style=filled]
		expert3_comm_l3 [label="Expert 3 Communication
Input: [tokens selected for expert 3]
Output: [tokens on GPU 3]
GPU: 3" fillcolor=lightgray shape=ellipse style=filled]
		expert3_l3 [label="Expert 3 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 3" fillcolor=lightcoral shape=rectangle style=filled]
		expert3_return_l3 [label="Expert 3 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 3" fillcolor=lightgray shape=ellipse style=filled]
		expert4_comm_l3 [label="Expert 4 Communication
Input: [tokens selected for expert 4]
Output: [tokens on GPU 4]
GPU: 4" fillcolor=lightgray shape=ellipse style=filled]
		expert4_l3 [label="Expert 4 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 4" fillcolor=lightcoral shape=rectangle style=filled]
		expert4_return_l3 [label="Expert 4 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 4" fillcolor=lightgray shape=ellipse style=filled]
		expert5_comm_l3 [label="Expert 5 Communication
Input: [tokens selected for expert 5]
Output: [tokens on GPU 5]
GPU: 5" fillcolor=lightgray shape=ellipse style=filled]
		expert5_l3 [label="Expert 5 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 5" fillcolor=lightcoral shape=rectangle style=filled]
		expert5_return_l3 [label="Expert 5 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 5" fillcolor=lightgray shape=ellipse style=filled]
		expert6_comm_l3 [label="Expert 6 Communication
Input: [tokens selected for expert 6]
Output: [tokens on GPU 6]
GPU: 6" fillcolor=lightgray shape=ellipse style=filled]
		expert6_l3 [label="Expert 6 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 6" fillcolor=lightcoral shape=rectangle style=filled]
		expert6_return_l3 [label="Expert 6 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 6" fillcolor=lightgray shape=ellipse style=filled]
		expert7_comm_l3 [label="Expert 7 Communication
Input: [tokens selected for expert 7]
Output: [tokens on GPU 7]
GPU: 7" fillcolor=lightgray shape=ellipse style=filled]
		expert7_l3 [label="Expert 7 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 7" fillcolor=lightcoral shape=rectangle style=filled]
		expert7_return_l3 [label="Expert 7 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 7" fillcolor=lightgray shape=ellipse style=filled]
		expert8_comm_l3 [label="Expert 8 Communication
Input: [tokens selected for expert 8]
Output: [tokens on GPU 8]
GPU: 8" fillcolor=lightgray shape=ellipse style=filled]
		expert8_l3 [label="Expert 8 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 8" fillcolor=lightcoral shape=rectangle style=filled]
		expert8_return_l3 [label="Expert 8 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 8" fillcolor=lightgray shape=ellipse style=filled]
		expert9_comm_l3 [label="Expert 9 Communication
Input: [tokens selected for expert 9]
Output: [tokens on GPU 9]
GPU: 9" fillcolor=lightgray shape=ellipse style=filled]
		expert9_l3 [label="Expert 9 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 9" fillcolor=lightcoral shape=rectangle style=filled]
		expert9_return_l3 [label="Expert 9 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 9" fillcolor=lightgray shape=ellipse style=filled]
		expert10_comm_l3 [label="Expert 10 Communication
Input: [tokens selected for expert 10]
Output: [tokens on GPU 10]
GPU: 10" fillcolor=lightgray shape=ellipse style=filled]
		expert10_l3 [label="Expert 10 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 10" fillcolor=lightcoral shape=rectangle style=filled]
		expert10_return_l3 [label="Expert 10 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 10" fillcolor=lightgray shape=ellipse style=filled]
		expert11_comm_l3 [label="Expert 11 Communication
Input: [tokens selected for expert 11]
Output: [tokens on GPU 11]
GPU: 11" fillcolor=lightgray shape=ellipse style=filled]
		expert11_l3 [label="Expert 11 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 11" fillcolor=lightcoral shape=rectangle style=filled]
		expert11_return_l3 [label="Expert 11 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 11" fillcolor=lightgray shape=ellipse style=filled]
		expert12_comm_l3 [label="Expert 12 Communication
Input: [tokens selected for expert 12]
Output: [tokens on GPU 12]
GPU: 12" fillcolor=lightgray shape=ellipse style=filled]
		expert12_l3 [label="Expert 12 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 12" fillcolor=lightcoral shape=rectangle style=filled]
		expert12_return_l3 [label="Expert 12 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 12" fillcolor=lightgray shape=ellipse style=filled]
		expert13_comm_l3 [label="Expert 13 Communication
Input: [tokens selected for expert 13]
Output: [tokens on GPU 13]
GPU: 13" fillcolor=lightgray shape=ellipse style=filled]
		expert13_l3 [label="Expert 13 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 13" fillcolor=lightcoral shape=rectangle style=filled]
		expert13_return_l3 [label="Expert 13 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 13" fillcolor=lightgray shape=ellipse style=filled]
		expert14_comm_l3 [label="Expert 14 Communication
Input: [tokens selected for expert 14]
Output: [tokens on GPU 14]
GPU: 14" fillcolor=lightgray shape=ellipse style=filled]
		expert14_l3 [label="Expert 14 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 14" fillcolor=lightcoral shape=rectangle style=filled]
		expert14_return_l3 [label="Expert 14 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 14" fillcolor=lightgray shape=ellipse style=filled]
		expert15_comm_l3 [label="Expert 15 Communication
Input: [tokens selected for expert 15]
Output: [tokens on GPU 15]
GPU: 15" fillcolor=lightgray shape=ellipse style=filled]
		expert15_l3 [label="Expert 15 MLP
Input: [variable_tokens, hidden=8192]
Output: [variable_tokens, hidden=8192]
GPU: 15" fillcolor=lightcoral shape=rectangle style=filled]
		expert15_return_l3 [label="Expert 15 Return
Input: [processed_tokens, hidden=8192]
Output: [tokens back to original positions]
GPU: 15" fillcolor=lightgray shape=ellipse style=filled]
		aggregate_l3 [label="Expert Aggregation
Input: [processed_tokens from all experts]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightpink shape=parallelogram style=filled]
		res_add2_l3 [label="Residual Add 2
Input: [batch_size=1024, seq_len=10000, hidden=8192] (x2)
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		norm2_l3 [label="Layer Norm 2
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightgreen shape=rectangle style=filled]
		final_output [label="Final Output
Input: [batch_size=1024, seq_len=10000, hidden=8192]
Output: [batch_size=1024, seq_len=10000, hidden=8192]
GPU: all GPUs" fillcolor=lightblue shape=ellipse style=filled]
		input_l3 -> mha_qkv_l3
		mha_qkv_l3 -> mha_attn_l3
		mha_attn_l3 -> mha_out_l3
		input_l3 -> res_add_l3
		mha_out_l3 -> res_add_l3
		res_add_l3 -> norm_l3
		norm_l3 -> gate_l3
		norm_l3 -> route_l3
		route_l3 -> expert0_comm_l3
		expert0_comm_l3 -> expert0_l3
		expert0_l3 -> expert0_return_l3
		expert0_return_l3 -> aggregate_l3
		route_l3 -> expert1_comm_l3
		expert1_comm_l3 -> expert1_l3
		expert1_l3 -> expert1_return_l3
		expert1_return_l3 -> aggregate_l3
		route_l3 -> expert2_comm_l3
		expert2_comm_l3 -> expert2_l3
		expert2_l3 -> expert2_return_l3
		expert2_return_l3 -> aggregate_l3
		route_l3 -> expert3_comm_l3
		expert3_comm_l3 -> expert3_l3
		expert3_l3 -> expert3_return_l3
		expert3_return_l3 -> aggregate_l3
		route_l3 -> expert4_comm_l3
		expert4_comm_l3 -> expert4_l3
		expert4_l3 -> expert4_return_l3
		expert4_return_l3 -> aggregate_l3
		route_l3 -> expert5_comm_l3
		expert5_comm_l3 -> expert5_l3
		expert5_l3 -> expert5_return_l3
		expert5_return_l3 -> aggregate_l3
		route_l3 -> expert6_comm_l3
		expert6_comm_l3 -> expert6_l3
		expert6_l3 -> expert6_return_l3
		expert6_return_l3 -> aggregate_l3
		route_l3 -> expert7_comm_l3
		expert7_comm_l3 -> expert7_l3
		expert7_l3 -> expert7_return_l3
		expert7_return_l3 -> aggregate_l3
		route_l3 -> expert8_comm_l3
		expert8_comm_l3 -> expert8_l3
		expert8_l3 -> expert8_return_l3
		expert8_return_l3 -> aggregate_l3
		route_l3 -> expert9_comm_l3
		expert9_comm_l3 -> expert9_l3
		expert9_l3 -> expert9_return_l3
		expert9_return_l3 -> aggregate_l3
		route_l3 -> expert10_comm_l3
		expert10_comm_l3 -> expert10_l3
		expert10_l3 -> expert10_return_l3
		expert10_return_l3 -> aggregate_l3
		route_l3 -> expert11_comm_l3
		expert11_comm_l3 -> expert11_l3
		expert11_l3 -> expert11_return_l3
		expert11_return_l3 -> aggregate_l3
		route_l3 -> expert12_comm_l3
		expert12_comm_l3 -> expert12_l3
		expert12_l3 -> expert12_return_l3
		expert12_return_l3 -> aggregate_l3
		route_l3 -> expert13_comm_l3
		expert13_comm_l3 -> expert13_l3
		expert13_l3 -> expert13_return_l3
		expert13_return_l3 -> aggregate_l3
		route_l3 -> expert14_comm_l3
		expert14_comm_l3 -> expert14_l3
		expert14_l3 -> expert14_return_l3
		expert14_return_l3 -> aggregate_l3
		route_l3 -> expert15_comm_l3
		expert15_comm_l3 -> expert15_l3
		expert15_l3 -> expert15_return_l3
		expert15_return_l3 -> aggregate_l3
		aggregate_l3 -> res_add2_l3
		norm_l3 -> res_add2_l3
		res_add2_l3 -> norm2_l3
		norm2_l3 -> final_output
	}
	norm2_l0 -> input_l1
	norm2_l1 -> input_l2
	norm2_l2 -> input_l3
}
