{
  "deployment_configurations": {
    "baseline": {
      "name": "Baseline MoE (TP=8, PP=2)",
      "parallel_strategy": {
        "type": "hybrid",
        "tensor_parallelism": {
          "degree": 8,
          "partitions": {
            "attention": {
              "query_key_value": "column_parallel",
              "dense": "row_parallel"
            },
            "mlp": {
              "dense_h_to_4h": "column_parallel",
              "dense_4h_to_h": "row_parallel"
            }
          }
        },
        "pipeline_parallelism": {
          "degree": 2,
          "stages": [
            {
              "stage_id": 0,
              "layers": [0, 1],
              "gpus": [0, 1, 2, 3, 4, 5, 6, 7]
            },
            {
              "stage_id": 1,
              "layers": [2, 3],
              "gpus": [8, 9, 10, 11, 12, 13, 14, 15]
            }
          ]
        },
        "expert_parallelism": {
          "degree": 1,
          "experts_per_gpu": 8,
          "colocation": true
        }
      },
      "model_parameters": {
        "layers": 4,
        "experts_per_layer": 16,
        "expert_type": "mlp",
        "precision": "fp16",
        "token_dimension": 8192,
        "mha_heads": 16,
        "mha_head_dimension": 512,
        "mlp_hidden_size": 32768,
        "sequence_length": 10000,
        "batch_size": 1024
      },
      "device_mapping": {
        "gpu_0": {
          "stage": 0,
          "tensor_parallel_rank": 0,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_0_attention_tp0", "layer_0_mlp_tp0", "layer_1_attention_tp0", "layer_1_mlp_tp0"]
        },
        "gpu_1": {
          "stage": 0,
          "tensor_parallel_rank": 1,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_0_attention_tp1", "layer_0_mlp_tp1", "layer_1_attention_tp1", "layer_1_mlp_tp1"]
        },
        "gpu_2": {
          "stage": 0,
          "tensor_parallel_rank": 2,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_0_attention_tp2", "layer_0_mlp_tp2", "layer_1_attention_tp2", "layer_1_mlp_tp2"]
        },
        "gpu_3": {
          "stage": 0,
          "tensor_parallel_rank": 3,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_0_attention_tp3", "layer_0_mlp_tp3", "layer_1_attention_tp3", "layer_1_mlp_tp3"]
        },
        "gpu_4": {
          "stage": 0,
          "tensor_parallel_rank": 4,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_0_attention_tp4", "layer_0_mlp_tp4", "layer_1_attention_tp4", "layer_1_mlp_tp4"]
        },
        "gpu_5": {
          "stage": 0,
          "tensor_parallel_rank": 5,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_0_attention_tp5", "layer_0_mlp_tp5", "layer_1_attention_tp5", "layer_1_mlp_tp5"]
        },
        "gpu_6": {
          "stage": 0,
          "tensor_parallel_rank": 6,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_0_attention_tp6", "layer_0_mlp_tp6", "layer_1_attention_tp6", "layer_1_mlp_tp6"]
        },
        "gpu_7": {
          "stage": 0,
          "tensor_parallel_rank": 7,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_0_attention_tp7", "layer_0_mlp_tp7", "layer_1_attention_tp7", "layer_1_mlp_tp7"]
        },
        "gpu_8": {
          "stage": 1,
          "tensor_parallel_rank": 0,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_2_attention_tp0", "layer_2_mlp_tp0", "layer_3_attention_tp0", "layer_3_mlp_tp0"]
        },
        "gpu_9": {
          "stage": 1,
          "tensor_parallel_rank": 1,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_2_attention_tp1", "layer_2_mlp_tp1", "layer_3_attention_tp1", "layer_3_mlp_tp1"]
        },
        "gpu_10": {
          "stage": 1,
          "tensor_parallel_rank": 2,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_2_attention_tp2", "layer_2_mlp_tp2", "layer_3_attention_tp2", "layer_3_mlp_tp2"]
        },
        "gpu_11": {
          "stage": 1,
          "tensor_parallel_rank": 3,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_2_attention_tp3", "layer_2_mlp_tp3", "layer_3_attention_tp3", "layer_3_mlp_tp3"]
        },
        "gpu_12": {
          "stage": 1,
          "tensor_parallel_rank": 4,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_2_attention_tp4", "layer_2_mlp_tp4", "layer_3_attention_tp4", "layer_3_mlp_tp4"]
        },
        "gpu_13": {
          "stage": 1,
          "tensor_parallel_rank": 5,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_2_attention_tp5", "layer_2_mlp_tp5", "layer_3_attention_tp5", "layer_3_mlp_tp5"]
        },
        "gpu_14": {
          "stage": 1,
          "tensor_parallel_rank": 6,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_2_attention_tp6", "layer_2_mlp_tp6", "layer_3_attention_tp6", "layer_3_mlp_tp6"]
        },
        "gpu_15": {
          "stage": 1,
          "tensor_parallel_rank": 7,
          "experts": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "modules": ["layer_2_attention_tp7", "layer_2_mlp_tp7", "layer_3_attention_tp7", "layer_3_mlp_tp7"]
        }
      },
      "communication": {
        "tensor_parallel": {
          "all_reduce": "nccl",
          "buffer_size": "auto"
        },
        "pipeline_parallel": {
          "send_recv": "nccl",
          "buffer_size": 1024
        }
      }
    },
    "proposed": {
      "name": "Large-Scale Cross-Node Expert Parallelism (EP=16)",
      "parallel_strategy": {
        "type": "expert_parallelism",
        "expert_parallelism": {
          "degree": 16,
          "experts_per_gpu": 1,
          "colocation": false,
          "placement_strategy": "topology_aware"
        },
        "tensor_parallelism": {
          "degree": 1,
          "enabled": false,
          "note": "Each expert fits on single GPU"
        },
        "pipeline_parallelism": {
          "degree": 1,
          "enabled": false,
          "note": "All layers processed in parallel across experts"
        }
      },
      "model_parameters": {
        "layers": 4,
        "experts_per_layer": 16,
        "expert_type": "mlp",
        "precision": "fp16",
        "token_dimension": 8192,
        "mha_heads": 16,
        "mha_head_dimension": 512,
        "mlp_hidden_size": 32768,
        "sequence_length": 10000,
        "batch_size": 1024
      },
      "device_mapping": {
        "layer_0": {
          "expert_0": {"gpu_id": 0, "modules": ["expert_0_mlp"]},
          "expert_1": {"gpu_id": 1, "modules": ["expert_1_mlp"]},
          "expert_2": {"gpu_id": 2, "modules": ["expert_2_mlp"]},
          "expert_3": {"gpu_id": 3, "modules": ["expert_3_mlp"]},
          "expert_4": {"gpu_id": 4, "modules": ["expert_4_mlp"]},
          "expert_5": {"gpu_id": 5, "modules": ["expert_5_mlp"]},
          "expert_6": {"gpu_id": 6, "modules": ["expert_6_mlp"]},
          "expert_7": {"gpu_id": 7, "modules": ["expert_7_mlp"]},
          "expert_8": {"gpu_id": 8, "modules": ["expert_8_mlp"]},
          "expert_9": {"gpu_id": 9, "modules": ["expert_9_mlp"]},
          "expert_10": {"gpu_id": 10, "modules": ["expert_10_mlp"]},
          "expert_11": {"gpu_id": 11, "modules": ["expert_11_mlp"]},
          "expert_12": {"gpu_id": 12, "modules": ["expert_12_mlp"]},
          "expert_13": {"gpu_id": 13, "modules": ["expert_13_mlp"]},
          "expert_14": {"gpu_id": 14, "modules": ["expert_14_mlp"]},
          "expert_15": {"gpu_id": 15, "modules": ["expert_15_mlp"]}
        },
        "layer_1": {
          "expert_0": {"gpu_id": 0, "modules": ["expert_0_mlp"]},
          "expert_1": {"gpu_id": 1, "modules": ["expert_1_mlp"]},
          "expert_2": {"gpu_id": 2, "modules": ["expert_2_mlp"]},
          "expert_3": {"gpu_id": 3, "modules": ["expert_3_mlp"]},
          "expert_4": {"gpu_id": 4, "modules": ["expert_4_mlp"]},
          "expert_5": {"gpu_id": 5, "modules": ["expert_5_mlp"]},
          "expert_6": {"gpu_id": 6, "modules": ["expert_6_mlp"]},
          "expert_7": {"gpu_id": 7, "modules": ["expert_7_mlp"]},
          "expert_8": {"gpu_id": 8, "modules": ["expert_8_mlp"]},
          "expert_9": {"gpu_id": 9, "modules": ["expert_9_mlp"]},
          "expert_10": {"gpu_id": 10, "modules": ["expert_10_mlp"]},
          "expert_11": {"gpu_id": 11, "modules": ["expert_11_mlp"]},
          "expert_12": {"gpu_id": 12, "modules": ["expert_12_mlp"]},
          "expert_13": {"gpu_id": 13, "modules": ["expert_13_mlp"]},
          "expert_14": {"gpu_id": 14, "modules": ["expert_14_mlp"]},
          "expert_15": {"gpu_id": 15, "modules": ["expert_15_mlp"]}
        },
        "layer_2": {
          "expert_0": {"gpu_id": 0, "modules": ["expert_0_mlp"]},
          "expert_1": {"gpu_id": 1, "modules": ["expert_1_mlp"]},
          "expert_2": {"gpu_id": 2, "modules": ["expert_2_mlp"]},
          "expert_3": {"gpu_id": 3, "modules": ["expert_3_mlp"]},
          "expert_4": {"gpu_id": 4, "modules": ["expert_4_mlp"]},
          "expert_5": {"gpu_id": 5, "modules": ["expert_5_mlp"]},
          "expert_6": {"gpu_id": 6, "modules": ["expert_6_mlp"]},
          "expert_7": {"gpu_id": 7, "modules": ["expert_7_mlp"]},
          "expert_8": {"gpu_id": 8, "modules": ["expert_8_mlp"]},
          "expert_9": {"gpu_id": 9, "modules": ["expert_9_mlp"]},
          "expert_10": {"gpu_id": 10, "modules": ["expert_10_mlp"]},
          "expert_11": {"gpu_id": 11, "modules": ["expert_11_mlp"]},
          "expert_12": {"gpu_id": 12, "modules": ["expert_12_mlp"]},
          "expert_13": {"gpu_id": 13, "modules": ["expert_13_mlp"]},
          "expert_14": {"gpu_id": 14, "modules": ["expert_14_mlp"]},
          "expert_15": {"gpu_id": 15, "modules": ["expert_15_mlp"]}
        },
        "layer_3": {
          "expert_0": {"gpu_id": 0, "modules": ["expert_0_mlp"]},
          "expert_1": {"gpu_id": 1, "modules": ["expert_1_mlp"]},
          "expert_2": {"gpu_id": 2, "modules": ["expert_2_mlp"]},
          "expert_3": {"gpu_id": 3, "modules": ["expert_3_mlp"]},
          "expert_4": {"gpu_id": 4, "modules": ["expert_4_mlp"]},
          "expert_5": {"gpu_id": 5, "modules": ["expert_5_mlp"]},
          "expert_6": {"gpu_id": 6, "modules": ["expert_6_mlp"]},
          "expert_7": {"gpu_id": 7, "modules": ["expert_7_mlp"]},
          "expert_8": {"gpu_id": 8, "modules": ["expert_8_mlp"]},
          "expert_9": {"gpu_id": 9, "modules": ["expert_9_mlp"]},
          "expert_10": {"gpu_id": 10, "modules": ["expert_10_mlp"]},
          "expert_11": {"gpu_id": 11, "modules": ["expert_11_mlp"]},
          "expert_12": {"gpu_id": 12, "modules": ["expert_12_mlp"]},
          "expert_13": {"gpu_id": 13, "modules": ["expert_13_mlp"]},
          "expert_14": {"gpu_id": 14, "modules": ["expert_14_mlp"]},
          "expert_15": {"gpu_id": 15, "modules": ["expert_15_mlp"]}
        }
      },
      "communication": {
        "expert_parallel": {
          "token_routing": "asynchronous",
          "protocol": "nccl",
          "batching": "destination_based",
          "overlap": true
        },
        "load_balancing": {
          "dynamic_gating": true,
          "monitoring": "real_time",
          "adjustment_frequency": "per_batch"
        }
      },
      "optimization": {
        "cuda_streams": 4,
        "memory_pool": "pre_allocated",
        "communication_overlap": true,
        "pipeline_scheduling": "fine_grained"
      }
    }
  },
  "performance_metrics": {
    "baseline": {
      "tps": 120000,
      "tpot_ms": 8.3,
      "gpu_utilization": "shared",
      "communication_overhead": "moderate"
    },
    "proposed": {
      "tps": 450000,
      "tpot_ms": 2.2,
      "gpu_utilization": "dedicated",
      "communication_overhead": "overlapped"
    }
  }
}