digraph proposed_dag {
    comment="4-layer Dense Model - Proposed Layer-wise Distribution"
    rankdir=TB
    splines=ortho
    nodesep=0.5
    ranksep=1.0
    
    // Input node
    input [label="Input\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=ellipse, style=filled, fillcolor=lightblue]
    
    // Layer 0: GPUs 0-3
    subgraph cluster_layer0 {
        label="Layer 0 (GPUs 0-3)"
        style=dashed
        color=red
        
        l0_input_broadcast [label="Input Broadcast\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]×4\nGPU: 0-3", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        l0_mha_qkv_0 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_mha_attn_0 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l0_ffn_up_0 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_gate_0 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_act_0 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_down_0 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l0_residual_0 [label="Residual Add\nInput: [batch_size=128, seq_len=10000, hidden_size=1024] (x2)\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_layernorm_0 [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        // Similar nodes for GPUs 1-3
        l0_mha_qkv_1 [label="MHA QKV Linear\nGPU: 1", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_mha_attn_1 [label="MHA Attention\nGPU: 1", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_down_1 [label="FFN Down Linear\nGPU: 1", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_residual_1 [label="Residual Add\nGPU: 1", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_layernorm_1 [label="LayerNorm\nGPU: 1", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l0_mha_qkv_2 [label="MHA QKV Linear\nGPU: 2", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_mha_attn_2 [label="MHA Attention\nGPU: 2", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_down_2 [label="FFN Down Linear\nGPU: 2", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_residual_2 [label="Residual Add\nGPU: 2", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_layernorm_2 [label="LayerNorm\nGPU: 2", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l0_mha_qkv_3 [label="MHA QKV Linear\nGPU: 3", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_mha_attn_3 [label="MHA Attention\nGPU: 3", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_down_3 [label="FFN Down Linear\nGPU: 3", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_residual_3 [label="Residual Add\nGPU: 3", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_layernorm_3 [label="LayerNorm\nGPU: 3", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l0_mha_allreduce [label="MHA All-Reduce\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 0-3", shape=parallelogram, style=filled, fillcolor=lightyellow]
        l0_ffn_allreduce [label="FFN All-Reduce\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 0-3", shape=parallelogram, style=filled, fillcolor=lightyellow]
    }
    
    // Layer 1: GPUs 4-7
    subgraph cluster_layer1 {
        label="Layer 1 (GPUs 4-7)"
        style=dashed
        color=red
        
        l1_input_broadcast [label="Input Broadcast\nGPU: 4-7", shape=parallelogram, style=filled, fillcolor=lightyellow]
        l1_mha_allreduce [label="MHA All-Reduce\nGPU: 4-7", shape=parallelogram, style=filled, fillcolor=lightyellow]
        l1_ffn_allreduce [label="FFN All-Reduce\nGPU: 4-7", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        l1_mha_qkv_4 [label="MHA QKV Linear\nGPU: 4", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_mha_qkv_5 [label="MHA QKV Linear\nGPU: 5", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_mha_qkv_6 [label="MHA QKV Linear\nGPU: 6", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_mha_qkv_7 [label="MHA QKV Linear\nGPU: 7", shape=rectangle, style=filled, fillcolor=lightgreen]
    }
    
    // Layer 2: GPUs 8-11
    subgraph cluster_layer2 {
        label="Layer 2 (GPUs 8-11)"
        style=dashed
        color=red
        
        l2_input_broadcast [label="Input Broadcast\nGPU: 8-11", shape=parallelogram, style=filled, fillcolor=lightyellow]
        l2_mha_allreduce [label="MHA All-Reduce\nGPU: 8-11", shape=parallelogram, style=filled, fillcolor=lightyellow]
        l2_ffn_allreduce [label="FFN All-Reduce\nGPU: 8-11", shape=parallelogram, style=filled, fillcolor=lightyellow]
    }
    
    // Layer 3: GPUs 12-15
    subgraph cluster_layer3 {
        label="Layer 3 (GPUs 12-15)"
        style=dashed
        color=red
        
        l3_input_broadcast [label="Input Broadcast\nGPU: 12-15", shape=parallelogram, style=filled, fillcolor=lightyellow]
        l3_mha_allreduce [label="MHA All-Reduce\nGPU: 12-15", shape=parallelogram, style=filled, fillcolor=lightyellow]
        l3_ffn_allreduce [label="FFN All-Reduce\nGPU: 12-15", shape=parallelogram, style=filled, fillcolor=lightyellow]
    }
    
    // Output node
    output [label="Output\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 12-15", shape=ellipse, style=filled, fillcolor=lightblue]
    
    // Pipeline communication
    send_l0_l1 [label="Pipeline Send\nGPU: 3→4", shape=ellipse, style=filled, fillcolor=orange]
    send_l1_l2 [label="Pipeline Send\nGPU: 7→8", shape=ellipse, style=filled, fillcolor=orange]
    send_l2_l3 [label="Pipeline Send\nGPU: 11→12", shape=ellipse, style=filled, fillcolor=orange]
    
    // Connections
    input -> l0_input_broadcast
    l0_input_broadcast -> l0_mha_qkv_0
    l0_input_broadcast -> l0_mha_qkv_1
    l0_input_broadcast -> l0_mha_qkv_2
    l0_input_broadcast -> l0_mha_qkv_3
    
    l0_mha_qkv_0 -> l0_mha_attn_0 -> l0_residual_0 -> l0_layernorm_0
    l0_mha_qkv_1 -> l0_mha_attn_1 -> l0_residual_1 -> l0_layernorm_1
    l0_mha_qkv_2 -> l0_mha_attn_2 -> l0_residual_2 -> l0_layernorm_2
    l0_mha_qkv_3 -> l0_mha_attn_3 -> l0_residual_3 -> l0_layernorm_3
    
    l0_layernorm_0 -> l0_mha_allreduce -> l0_ffn_allreduce -> send_l0_l1 -> l1_input_broadcast
    l1_input_broadcast -> l1_mha_qkv_4 -> l1_mha_allreduce -> l1_ffn_allreduce -> send_l1_l2 -> l2_input_broadcast -> send_l2_l3 -> l3_input_broadcast -> output
}