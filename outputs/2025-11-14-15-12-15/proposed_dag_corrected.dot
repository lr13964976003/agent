digraph proposed_dag {
    graph [nodesep=0.5, rankdir=TB, ranksep=1.0, splines=ortho]
    node [fontname="Helvetica"]
    
    // Input
    input [label="Input\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=ellipse, style=filled, fillcolor=lightblue]
    
    // Layer 0: GPUs 0-3
    subgraph cluster_layer0 {
        label="Layer 0 (GPUs 0-3)"
        style=dashed
        color=red
        
        // Input broadcast
        l0_input_broadcast [label="Input Broadcast\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]×4\nGPU: 0-3", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        // MHA computation per GPU
        l0_mha_split [label="MHA Input Split\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nGPU: 0-3", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        l0_mha_qkv_0 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_mha_qkv_1 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 1", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_mha_qkv_2 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 2", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_mha_qkv_3 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 3", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l0_mha_attn_0 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_mha_attn_1 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 1", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_mha_attn_2 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 2", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_mha_attn_3 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 3", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l0_mha_allreduce [label="MHA All-Reduce\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 0-3", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        // MHA residual
        l0_mha_residual [label="Residual Add\nInput: [batch_size=128, seq_len=10000, hidden_size=4096] (x2)\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_mha_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        // FFN computation per GPU
        l0_ffn_split [label="FFN Input Split\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nGPU: 0-3", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        l0_ffn_up_0 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_up_1 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 1", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_up_2 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 2", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_up_3 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 3", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l0_ffn_gate_0 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_gate_1 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 1", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_gate_2 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 2", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_gate_3 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 3", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l0_ffn_act_0 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_act_1 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 1", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_act_2 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 2", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_act_3 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 3", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l0_ffn_down_0 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 0", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_down_1 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 1", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_down_2 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 2", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_ffn_down_3 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 3", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l0_ffn_allreduce [label="FFN All-Reduce\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 0-3", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        // FFN residual
        l0_ffn_residual [label="Residual Add\nInput: [batch_size=128, seq_len=10000, hidden_size=4096] (x2)\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
        l0_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
    }
    
    // Layer 1: GPUs 4-7
    subgraph cluster_layer1 {
        label="Layer 1 (GPUs 4-7)"
        style=dashed
        color=red
        
        l1_input_broadcast [label="Input Broadcast\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]×4\nGPU: 4-7", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        l1_mha_split [label="MHA Input Split\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nGPU: 4-7", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        l1_mha_qkv_4 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 4", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_mha_qkv_5 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 5", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_mha_qkv_6 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 6", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_mha_qkv_7 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 7", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l1_mha_attn_4 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 4", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_mha_attn_5 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 5", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_mha_attn_6 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 6", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_mha_attn_7 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 7", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l1_mha_allreduce [label="MHA All-Reduce\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 4-7", shape=parallelogram, style=filled, fillcolor=lightyellow]
        l1_mha_residual [label="Residual Add\nInput: [batch_size=128, seq_len=10000, hidden_size=4096] (x2)\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_mha_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l1_ffn_split [label="FFN Input Split\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nGPU: 4-7", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        l1_ffn_up_4 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 4", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_ffn_up_5 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 5", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_ffn_up_6 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 6", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_ffn_up_7 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 7", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l1_ffn_gate_4 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 4", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_ffn_gate_5 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 5", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_ffn_gate_6 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 6", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_ffn_gate_7 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 7", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l1_ffn_act_4 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 4", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_ffn_act_5 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 5", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_ffn_act_6 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 6", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_ffn_act_7 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 7", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l1_ffn_down_4 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 4", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_ffn_down_5 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 5", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_ffn_down_6 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 6", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_ffn_down_7 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 7", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l1_ffn_allreduce [label="FFN All-Reduce\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 4-7", shape=parallelogram, style=filled, fillcolor=lightyellow]
        l1_ffn_residual [label="Residual Add\nInput: [batch_size=128, seq_len=10000, hidden_size=4096] (x2)\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
        l1_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
    }
    
    // Layer 2: GPUs 8-11
    subgraph cluster_layer2 {
        label="Layer 2 (GPUs 8-11)"
        style=dashed
        color=red
        
        l2_input_broadcast [label="Input Broadcast\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]×4\nGPU: 8-11", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        l2_mha_split [label="MHA Input Split\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nGPU: 8-11", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        l2_mha_qkv_8 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 8", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_mha_qkv_9 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 9", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_mha_qkv_10 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 10", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_mha_qkv_11 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 11", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l2_mha_attn_8 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 8", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_mha_attn_9 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 9", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_mha_attn_10 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 10", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_mha_attn_11 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 11", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l2_mha_allreduce [label="MHA All-Reduce\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 8-11", shape=parallelogram, style=filled, fillcolor=lightyellow]
        l2_mha_residual [label="Residual Add\nInput: [batch_size=128, seq_len=10000, hidden_size=4096] (x2)\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_mha_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l2_ffn_split [label="FFN Input Split\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nGPU: 8-11", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        l2_ffn_up_8 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 8", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_ffn_up_9 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 9", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_ffn_up_10 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 10", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_ffn_up_11 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 11", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l2_ffn_gate_8 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 8", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_ffn_gate_9 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 9", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_ffn_gate_10 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 10", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_ffn_gate_11 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 11", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l2_ffn_act_8 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 8", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_ffn_act_9 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 9", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_ffn_act_10 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 10", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_ffn_act_11 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 11", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l2_ffn_down_8 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 8", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_ffn_down_9 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 9", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_ffn_down_10 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 10", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_ffn_down_11 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 11", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l2_ffn_allreduce [label="FFN All-Reduce\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 8-11", shape=parallelogram, style=filled, fillcolor=lightyellow]
        l2_ffn_residual [label="Residual Add\nInput: [batch_size=128, seq_len=10000, hidden_size=4096] (x2)\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
        l2_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
    }
    
    // Layer 3: GPUs 12-15
    subgraph cluster_layer3 {
        label="Layer 3 (GPUs 12-15)"
        style=dashed
        color=red
        
        l3_input_broadcast [label="Input Broadcast\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]×4\nGPU: 12-15", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        l3_mha_split [label="MHA Input Split\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nGPU: 12-15", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        l3_mha_qkv_12 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 12", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_mha_qkv_13 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 13", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_mha_qkv_14 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 14", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_mha_qkv_15 [label="MHA QKV Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nGPU: 15", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l3_mha_attn_12 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 12", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_mha_attn_13 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 13", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_mha_attn_14 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 14", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_mha_attn_15 [label="MHA Attention\nInput: [batch_size=128, seq_len=10000, num_heads=8, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 15", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l3_mha_allreduce [label="MHA All-Reduce\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 12-15", shape=parallelogram, style=filled, fillcolor=lightyellow]
        l3_mha_residual [label="Residual Add\nInput: [batch_size=128, seq_len=10000, hidden_size=4096] (x2)\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_mha_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l3_ffn_split [label="FFN Input Split\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nGPU: 12-15", shape=parallelogram, style=filled, fillcolor=lightyellow]
        
        l3_ffn_up_12 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 12", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_ffn_up_13 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 13", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_ffn_up_14 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 14", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_ffn_up_15 [label="FFN Up Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 15", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l3_ffn_gate_12 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 12", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_ffn_gate_13 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 13", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_ffn_gate_14 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 14", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_ffn_gate_15 [label="FFN Gate Linear\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 15", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l3_ffn_act_12 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 12", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_ffn_act_13 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 13", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_ffn_act_14 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 14", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_ffn_act_15 [label="FFN Activation\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nGPU: 15", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l3_ffn_down_12 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 12", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_ffn_down_13 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 13", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_ffn_down_14 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 14", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_ffn_down_15 [label="FFN Down Linear\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=1024]\nGPU: 15", shape=rectangle, style=filled, fillcolor=lightgreen]
        
        l3_ffn_allreduce [label="FFN All-Reduce\nInput: [batch_size=128, seq_len=10000, hidden_size=1024]×4\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 12-15", shape=parallelogram, style=filled, fillcolor=lightyellow]
        l3_ffn_residual [label="Residual Add\nInput: [batch_size=128, seq_len=10000, hidden_size=4096] (x2)\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
        l3_layernorm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: All", shape=rectangle, style=filled, fillcolor=lightgreen]
    }
    
    // Output
    output [label="Output\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 12-15", shape=ellipse, style=filled, fillcolor=lightblue]
    
    // Pipeline communication between layers
    send_l0_l1 [label="Pipeline Send\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 3→4", shape=ellipse, style=filled, fillcolor=orange]
    send_l1_l2 [label="Pipeline Send\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 7→8", shape=ellipse, style=filled, fillcolor=orange]
    send_l2_l3 [label="Pipeline Send\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]\nGPU: 11→12", shape=ellipse, style=filled, fillcolor=orange]
    
    // Connections for Layer 0
    input -> l0_input_broadcast
    l0_input_broadcast -> l0_mha_split
    l0_mha_split -> l0_mha_qkv_0
    l0_mha_split -> l0_mha_qkv_1
    l0_mha_split -> l0_mha_qkv_2
    l0_mha_split -> l0_mha_qkv_3
    
    l0_mha_qkv_0 -> l0_mha_attn_0
    l0_mha_qkv_1 -> l0_mha_attn_1
    l0_mha_qkv_2 -> l0_mha_attn_2
    l0_mha_qkv_3 -> l0_mha_attn_3
    
    l0_mha_attn_0 -> l0_mha_allreduce
    l0_mha_attn_1 -> l0_mha_allreduce
    l0_mha_attn_2 -> l0_mha_allreduce
    l0_mha_attn_3 -> l0_mha_allreduce
    
    l0_input_broadcast -> l0_mha_residual [style=dashed]
    l0_mha_allreduce -> l0_mha_residual
    l0_mha_residual -> l0_mha_layernorm
    
    l0_mha_layernorm -> l0_ffn_split
    l0_ffn_split -> l0_ffn_up_0
    l0_ffn_split -> l0_ffn_up_1
    l0_ffn_split -> l0_ffn_up_2
    l0_ffn_split -> l0_ffn_up_3
    l0_ffn_split -> l0_ffn_gate_0
    l0_ffn_split -> l0_ffn_gate_1
    l0_ffn_split -> l0_ffn_gate_2
    l0_ffn_split -> l0_ffn_gate_3
    
    l0_ffn_up_0 -> l0_ffn_act_0
    l0_ffn_up_1 -> l0_ffn_act_1
    l0_ffn_up_2 -> l0_ffn_act_2
    l0_ffn_up_3 -> l0_ffn_act_3
    l0_ffn_gate_0 -> l0_ffn_act_0
    l0_ffn_gate_1 -> l0_ffn_act_1
    l0_ffn_gate_2 -> l0_ffn_act_2
    l0_ffn_gate_3 -> l0_ffn_act_3
    
    l0_ffn_act_0 -> l0_ffn_down_0
    l0_ffn_act_1 -> l0_ffn_down_1
    l0_ffn_act_2 -> l0_ffn_down_2
    l0_ffn_act_3 -> l0_ffn_down_3
    
    l0_ffn_down_0 -> l0_ffn_allreduce
    l0_ffn_down_1 -> l0_ffn_allreduce
    l0_ffn_down_2 -> l0_ffn_allreduce
    l0_ffn_down_3 -> l0_ffn_allreduce
    
    l0_mha_layernorm -> l0_ffn_residual [style=dashed]
    l0_ffn_allreduce -> l0_ffn_residual
    l0_ffn_residual -> l0_layernorm
    l0_layernorm -> send_l0_l1
    
    // Connections for Layer 1
    send_l0_l1 -> l1_input_broadcast
    l1_input_broadcast -> l1_mha_split
    l1_mha_split -> l1_mha_qkv_4
    l1_mha_split -> l1_mha_qkv_5
    l1_mha_split -> l1_mha_qkv_6
    l1_mha_split -> l1_mha_qkv_7
    
    l1_mha_qkv_4 -> l1_mha_attn_4
    l1_mha_qkv_5 -> l1_mha_attn_5
    l1_mha_qkv_6 -> l1_mha_attn_6
    l1_mha_qkv_7 -> l1_mha_attn_7
    
    l1_mha_attn_4 -> l1_mha_allreduce
    l1_mha_attn_5 -> l1_mha_allreduce
    l1_mha_attn_6 -> l1_mha_allreduce
    l1_mha_attn_7 -> l1_mha_allreduce
    
    l1_input_broadcast -> l1_mha_residual [style=dashed]
    l1_mha_allreduce -> l1_mha_residual
    l1_mha_residual -> l1_mha_layernorm
    
    l1_mha_layernorm -> l1_ffn_split
    l1_ffn_split -> l1_ffn_up_4
    l1_ffn_split -> l1_ffn_up_5
    l1_ffn_split -> l1_ffn_up_6
    l1_ffn_split -> l1_ffn_up_7
    l1_ffn_split -> l1_ffn_gate_4
    l1_ffn_split -> l1_ffn_gate_5
    l1_ffn_split -> l1_ffn_gate_6
    l1_ffn_split -> l1_ffn_gate_7
    
    l1_ffn_up_4 -> l1_ffn_act_4
    l1_ffn_up_5 -> l1_ffn_act_5
    l1_ffn_up_6 -> l1_ffn_act_6
    l1_ffn_up_7 -> l1_ffn_act_7
    l1_ffn_gate_4 -> l1_ffn_act_4
    l1_ffn_gate_5 -> l1_ffn_act_5
    l1_ffn_gate_6 -> l1_ffn_act_6
    l1_ffn_gate_7 -> l1_ffn_act_7
    
    l1_ffn_act_4 -> l1_ffn_down_4
    l1_ffn_act_5 -> l1_ffn_down_5
    l1_ffn_act_6 -> l1_ffn_down_6
    l1_ffn_act_7 -> l1_ffn_down_7
    
    l1_ffn_down_4 -> l1_ffn_allreduce
    l1_ffn_down_5 -> l1_ffn_allreduce
    l1_ffn_down_6 -> l1_ffn_allreduce
    l1_ffn_down_7 -> l1_ffn_allreduce
    
    l1_mha_layernorm -> l1_ffn_residual [style=dashed]
    l1_ffn_allreduce -> l1_ffn_residual
    l1_ffn_residual -> l1_layernorm
    l1_layernorm -> send_l1_l2
    
    // Connections for Layer 2
    send_l1_l2 -> l2_input_broadcast
    l2_input_broadcast -> l2_mha_split
    l2_mha_split -> l2_mha_qkv_8
    l2_mha_split -> l2_mha_qkv_9
    l2_mha_split -> l2_mha_qkv_10
    l2_mha_split -> l2_mha_qkv_11
    
    l2_mha_qkv_8 -> l2_mha_attn_8
    l2_mha_qkv_9 -> l2_mha_attn_9
    l2_mha_qkv_10 -> l2_mha_attn_10
    l2_mha_qkv_11 -> l2_mha_attn_11
    
    l2_mha_attn_8 -> l2_mha_allreduce
    l2_mha_attn_9 -> l2_mha_allreduce
    l2_mha_attn_10 -> l2_mha_allreduce
    l2_mha_attn_11 -> l2_mha_allreduce
    
    l2_input_broadcast -> l2_mha_residual [style=dashed]
    l2_mha_allreduce -> l2_mha_residual
    l2_mha_residual -> l2_mha_layernorm
    
    l2_mha_layernorm -> l2_ffn_split
    l2_ffn_split -> l2_ffn_up_8
    l2_ffn_split -> l2_ffn_up_9
    l2_ffn_split -> l2_ffn_up_10
    l2_ffn_split -> l2_ffn_up_11
    l2_ffn_split -> l2_ffn_gate_8
    l2_ffn_split -> l2_ffn_gate_9
    l2_ffn_split -> l2_ffn_gate_10
    l2_ffn_split -> l2_ffn_gate_11
    
    l2_ffn_up_8 -> l2_ffn_act_8
    l2_ffn_up_9 -> l2_ffn_act_9
    l2_ffn_up_10 -> l2_ffn_act_10
    l2_ffn_up_11 -> l2_ffn_act_11
    l2_ffn_gate_8 -> l2_ffn_act_8
    l2_ffn_gate_9 -> l2_ffn_act_9
    l2_ffn_gate_10 -> l2_ffn_act_10
    l2_ffn_gate_11 -> l2_ffn_act_11
    
    l2_ffn_act_8 -> l2_ffn_down_8
    l2_ffn_act_9 -> l2_ffn_down_9
    l2_ffn_act_10 -> l2_ffn_down_10
    l2_ffn_act_11 -> l2_ffn_down_11
    
    l2_ffn_down_8 -> l2_ffn_allreduce
    l2_ffn_down_9 -> l2_ffn_allreduce
    l2_ffn_down_10 -> l2_ffn_allreduce
    l2_ffn_down_11 -> l2_ffn_allreduce
    
    l2_mha_layernorm -> l2_ffn_residual [style=dashed]
    l2_ffn_allreduce -> l2_ffn_residual
    l2_ffn_residual -> l2_layernorm
    l2_layernorm -> send_l2_l3
    
    // Connections for Layer 3
    send_l2_l3 -> l3_input_broadcast
    l3_input_broadcast -> l3_mha_split
    l3_mha_split -> l3_mha_qkv_12
    l3_mha_split -> l3_mha_qkv_13
    l3_mha_split -> l3_mha_qkv_14
    l3_mha_split -> l3_mha_qkv_15
    
    l3_mha_qkv_12 -> l3_mha_attn_12
    l3_mha_qkv_13 -> l3_mha_attn_13
    l3_mha_qkv_14 -> l3_mha_attn_14
    l3_mha_qkv_15 -> l3_mha_attn_15
    
    l3_mha_attn_12 -> l3_mha_allreduce
    l3_mha_attn_13 -> l3_mha_allreduce
    l3_mha_attn_14 -> l3_mha_allreduce
    l3_mha_attn_15 -> l3_mha_allreduce
    
    l3_input_broadcast -> l3_mha_residual [style=dashed]
    l3_mha_allreduce -> l3_mha_residual
    l3_mha_residual -> l3_mha_layernorm
    
    l3_mha_layernorm -> l3_ffn_split
    l3_ffn_split -> l3_ffn_up_12
    l3_ffn_split -> l3_ffn_up_13
    l3_ffn_split -> l3_ffn_up_14
    l3_ffn_split -> l3_ffn_up_15
    l3_ffn_split -> l3_ffn_gate_12
    l3_ffn_split -> l3_ffn_gate_13
    l3_ffn_split -> l3_ffn_gate_14
    l3_ffn_split -> l3_ffn_gate_15
    
    l3_ffn_up_12 -> l3_ffn_act_12
    l3_ffn_up_13 -> l3_ffn_act_13
    l3_ffn_up_14 -> l3_ffn_act_14
    l3_ffn_up_15 -> l3_ffn_act_15
    l3_ffn_gate_12 -> l3_ffn_act_12
    l3_ffn_gate_13 -> l3_ffn_act_13
    l3_ffn_gate_14 -> l3_ffn_act_14
    l3_ffn_gate_15 -> l3_ffn_act_15
    
    l3_ffn_act_12 -> l3_ffn_down_12
    l3_ffn_act_13 -> l3_ffn_down_13
    l3_ffn_act_14 -> l3_ffn_down_14
    l3_ffn_act_15 -> l3_ffn_down_15
    
    l3_ffn_down_12 -> l3_ffn_allreduce
    l3_ffn_down_13 -> l3_ffn_allreduce
    l3_ffn_down_14 -> l3_ffn_allreduce
    l3_ffn_down_15 -> l3_ffn_allreduce
    
    l3_mha_layernorm -> l3_ffn_residual [style=dashed]
    l3_ffn_allreduce -> l3_ffn_residual
    l3_ffn_residual -> l3_layernorm
    l3_layernorm -> output
}