// 4-layer Dense Model - Baseline TP=8 PP=2
digraph baseline_dag {
	graph [nodesep=0.5 rankdir=TB ranksep=1.0 splines=ortho]
	node [shape=ellipse]
	node [shape=rectangle]
	node [shape=parallelogram]
	input [label="Input
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: All" fillcolor=lightblue shape=ellipse style=filled]
	l0_input_split [label="Input Split
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0-7" fillcolor=lightyellow shape=parallelogram style=filled]
	l0_mha_qkv [label="MHA QKV Linear
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_attn [label="MHA Attention
Input: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l0_ffn_up [label="FFN Up Linear
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l0_ffn_gate [label="FFN Gate Linear
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l0_ffn_act [label="FFN Activation
Input: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
Output: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l0_ffn_down [label="FFN Down Linear
Input: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l0_residual [label="Residual Add
Input: [batch_size=128, seq_len=10000, hidden_size=4096] (x2)
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l0_layernorm [label="LayerNorm
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l0_mha_agg [label="MHA All-Reduce
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0-7" fillcolor=lightyellow shape=parallelogram style=filled]
	l0_ffn_agg [label="FFN All-Reduce
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0-7" fillcolor=lightyellow shape=parallelogram style=filled]
	l1_mha_qkv [label="MHA QKV Linear
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_attn [label="MHA Attention
Input: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l1_ffn_up [label="FFN Up Linear
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l1_ffn_gate [label="FFN Gate Linear
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l1_ffn_act [label="FFN Activation
Input: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
Output: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l1_ffn_down [label="FFN Down Linear
Input: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l1_residual [label="Residual Add
Input: [batch_size=128, seq_len=10000, hidden_size=4096] (x2)
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l1_layernorm [label="LayerNorm
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0-7" fillcolor=lightgreen shape=rectangle style=filled]
	l1_mha_agg [label="MHA All-Reduce
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0-7" fillcolor=lightyellow shape=parallelogram style=filled]
	l1_ffn_agg [label="FFN All-Reduce
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0-7" fillcolor=lightyellow shape=parallelogram style=filled]
	subgraph cluster_stage0 {
		color=red label="Pipeline Stage 0 (GPUs 0-7)" style=dashed
		subgraph cluster_layer0 {
			color=blue label="Layer 0" style=dashed
		}
		subgraph cluster_layer1 {
			color=blue label="Layer 1" style=dashed
		}
	}
	stage0_to_stage1 [label="Pipeline Send
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 0â†’8" fillcolor=orange shape=ellipse style=filled]
	l2_mha_qkv [label="MHA QKV Linear
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_attn [label="MHA Attention
Input: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l2_ffn_up [label="FFN Up Linear
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l2_ffn_gate [label="FFN Gate Linear
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l2_ffn_act [label="FFN Activation
Input: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
Output: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l2_ffn_down [label="FFN Down Linear
Input: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l2_residual [label="Residual Add
Input: [batch_size=128, seq_len=10000, hidden_size=4096] (x2)
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l2_layernorm [label="LayerNorm
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l2_mha_agg [label="MHA All-Reduce
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 8-15" fillcolor=lightyellow shape=parallelogram style=filled]
	l2_ffn_agg [label="FFN All-Reduce
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 8-15" fillcolor=lightyellow shape=parallelogram style=filled]
	l3_mha_qkv [label="MHA QKV Linear
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_attn [label="MHA Attention
Input: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l3_ffn_up [label="FFN Up Linear
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l3_ffn_gate [label="FFN Gate Linear
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l3_ffn_act [label="FFN Activation
Input: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
Output: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l3_ffn_down [label="FFN Down Linear
Input: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l3_residual [label="Residual Add
Input: [batch_size=128, seq_len=10000, hidden_size=4096] (x2)
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l3_layernorm [label="LayerNorm
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 8-15" fillcolor=lightgreen shape=rectangle style=filled]
	l3_mha_agg [label="MHA All-Reduce
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 8-15" fillcolor=lightyellow shape=parallelogram style=filled]
	l3_ffn_agg [label="FFN All-Reduce
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 8-15" fillcolor=lightyellow shape=parallelogram style=filled]
	subgraph cluster_stage1 {
		color=red label="Pipeline Stage 1 (GPUs 8-15)" style=dashed
		subgraph cluster_layer2 {
			color=blue label="Layer 2" style=dashed
		}
		subgraph cluster_layer3 {
			color=blue label="Layer 3" style=dashed
		}
	}
	output [label="Output
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: 8-15" fillcolor=lightblue shape=ellipse style=filled]
	input -> l0_input_split
	l0_input_split -> l0_mha_qkv
	l0_mha_qkv -> l0_mha_attn
	l0_mha_attn -> l0_mha_agg
	l0_mha_agg -> l0_residual
	l0_residual -> l0_layernorm
	l0_layernorm -> l0_ffn_up
	l0_layernorm -> l0_ffn_gate
	l0_ffn_up -> l0_ffn_act
	l0_ffn_gate -> l0_ffn_act
	l0_ffn_act -> l0_ffn_down
	l0_ffn_down -> l0_ffn_agg
	l0_ffn_agg -> l0_residual
	l0_residual -> l1_mha_qkv
	l1_mha_qkv -> l1_mha_attn
	l1_mha_attn -> l1_mha_agg
	l1_mha_agg -> l1_residual
	l1_residual -> l1_layernorm
	l1_layernorm -> l1_ffn_up
	l1_layernorm -> l1_ffn_gate
	l1_ffn_up -> l1_ffn_act
	l1_ffn_gate -> l1_ffn_act
	l1_ffn_act -> l1_ffn_down
	l1_ffn_down -> l1_ffn_agg
	l1_ffn_agg -> l1_residual
	l1_residual -> stage0_to_stage1
	stage0_to_stage1 -> l2_mha_qkv
	l2_mha_qkv -> l2_mha_attn
	l2_mha_attn -> l2_mha_agg
	l2_mha_agg -> l2_residual
	l2_residual -> l2_layernorm
	l2_layernorm -> l2_ffn_up
	l2_layernorm -> l2_ffn_gate
	l2_ffn_up -> l2_ffn_act
	l2_ffn_gate -> l2_ffn_act
	l2_ffn_act -> l2_ffn_down
	l2_ffn_down -> l2_ffn_agg
	l2_ffn_agg -> l2_residual
	l2_residual -> l3_mha_qkv
	l3_mha_qkv -> l3_mha_attn
	l3_mha_attn -> l3_mha_agg
	l3_mha_agg -> l3_residual
	l3_residual -> l3_layernorm
	l3_layernorm -> l3_ffn_up
	l3_layernorm -> l3_ffn_gate
	l3_ffn_up -> l3_ffn_act
	l3_ffn_gate -> l3_ffn_act
	l3_ffn_act -> l3_ffn_down
	l3_ffn_down -> l3_ffn_agg
	l3_ffn_agg -> l3_residual
	l3_residual -> output
}
