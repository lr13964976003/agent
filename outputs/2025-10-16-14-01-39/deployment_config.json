{
  "deployment_configurations": {
    "MA_Separation": {
      "model_name": "MA_Separation_MoE_Transformer",
      "parallel_strategy": {
        "name": "MA_Separation",
        "type": "hybrid_attention_moe_separation",
        "parameters": {
          "attention_gpu_count": 8,
          "moe_gpu_count": 8,
          "total_gpus": 16,
          "synchronization_interval": 100,
          "load_balancing_threshold": 0.05,
          "time_prediction_model": "neural_network_3_layers"
        }
      },
      "module_divisions": {
        "attention_modules": {
          "name": "attention_parallel",
          "parallel_strategy": "head_parallel",
          "parameters": {
            "total_attention_heads": 32,
            "heads_per_gpu": 4,
            "hidden_dimension": 4096,
            "sequence_length": 2048,
            "attention_head_dimension": 128,
            "qkv_projection": {
              "input_dimension": 4096,
              "output_dimension": 12288,
              "matrix_partition": "by_heads"
            },
            "attention_output_projection": {
              "input_dimension": 4096,
              "output_dimension": 4096,
              "aggregation_method": "all_reduce"
            }
          },
          "device_mapping": {
            "gpu_0": {"attention_heads": [0,1,2,3]},
            "gpu_1": {"attention_heads": [4,5,6,7]},
            "gpu_2": {"attention_heads": [8,9,10,11]},
            "gpu_3": {"attention_heads": [12,13,14,15]},
            "gpu_4": {"attention_heads": [16,17,18,19]},
            "gpu_5": {"attention_heads": [20,21,22,23]},
            "gpu_6": {"attention_heads": [24,25,26,27]},
            "gpu_7": {"attention_heads": [28,29,30,31]}
          }
        },
        "moe_modules": {
          "name": "moe_parallel",
          "parallel_strategy": "expert_parallel",
          "parameters": {
            "total_experts": 16,
            "experts_per_gpu": 2,
            "expert_hidden_dimension": 16384,
            "top_k_routing": 2,
            "capacity_factor": 1.0,
            "load_balancing_loss_coefficient": 0.01,
            "router_z_loss_coefficient": 0.001,
            "expert_dropout": 0.1,
            "activation_function": "SwiGLU"
          },
          "device_mapping": {
            "gpu_8": {"experts": [0,1]},
            "gpu_9": {"experts": [2,3]},
            "gpu_10": {"experts": [4,5]},
            "gpu_11": {"experts": [6,7]},
            "gpu_12": {"experts": [8,9]},
            "gpu_13": {"experts": [10,11]},
            "gpu_14": {"experts": [12,13]},
            "gpu_15": {"experts": [14,15]}
          }
        },
        "layer_norm_modules": {
          "name": "layer_norm",
          "device_mapping": "shared_across_all_gpus",
          "parameters": {
            "normalized_shape": 4096,
            "eps": 1e-05
          }
        }
      },
      "communication_patterns": {
        "attention_all_reduce": {
          "type": "hierarchical_all_reduce",
          "operation": "sum",
          "participants": ["gpu_0", "gpu_1", "gpu_2", "gpu_3", "gpu_4", "gpu_5", "gpu_6", "gpu_7"],
          "communication_backend": "nccl",
          "bandwidth": "600GBps_nvidia_nvlink"
        },
        "moe_all_to_all": {
          "type": "all_to_all",
          "operation": "token_routing",
          "participants": ["gpu_8", "gpu_9", "gpu_10", "gpu_11", "gpu_12", "gpu_13", "gpu_14", "gpu_15"],
          "communication_backend": "nccl",
          "bandwidth": "600GBps_nvidia_nvlink"
        },
        "attention_to_moe_broadcast": {
          "type": "broadcast",
          "from": ["gpu_0", "gpu_1", "gpu_2", "gpu_3", "gpu_4", "gpu_5", "gpu_6", "gpu_7"],
          "to": ["gpu_8", "gpu_9", "gpu_10", "gpu_11", "gpu_12", "gpu_13", "gpu_14", "gpu_15"],
          "communication_backend": "nccl"
        }
      }
    },
    "Tensor_Parallelism_Baseline": {
      "model_name": "TP_Baseline_MoE_Transformer",
      "parallel_strategy": {
        "name": "tensor_parallelism",
        "type": "tensor_parallel",
        "parameters": {
          "tensor_parallel_degree": 8,
          "tensor_parallel_size": 8,
          "communication_backend": "nccl"
        }
      },
      "module_divisions": {
        "tensor_parallel_modules": {
          "name": "tensor_parallel_linear",
          "parallel_strategy": "column_row_parallel",
          "parameters": {
            "column_parallel": {
              "input_dimension": 4096,
              "output_dimension": 16384,
              "partition_dimension": "output"
            },
            "row_parallel": {
              "input_dimension": 16384,
              "output_dimension": 4096,
              "partition_dimension": "input"
            }
          },
          "device_mapping": {
            "gpu_0": {"tensor_partitions": ["column_0", "row_0"]},
            "gpu_1": {"tensor_partitions": ["column_1", "row_1"]},
            "gpu_2": {"tensor_partitions": ["column_2", "row_2"]},
            "gpu_3": {"tensor_partitions": ["column_3", "row_3"]},
            "gpu_4": {"tensor_partitions": ["column_4", "row_4"]},
            "gpu_5": {"tensor_partitions": ["column_5", "row_5"]},
            "gpu_6": {"tensor_partitions": ["column_6", "row_6"]},
            "gpu_7": {"tensor_partitions": ["column_7", "row_7"]}
          }
        },
        "attention_tensor_parallel": {
          "name": "attention_tensor_parallel",
          "parameters": {
            "attention_heads": 32,
            "heads_per_gpu": 4,
            "hidden_dimension": 4096
          },
          "device_mapping": {
            "gpu_0": {"attention_heads": [0,1,2,3]},
            "gpu_1": {"attention_heads": [4,5,6,7]},
            "gpu_2": {"attention_heads": [8,9,10,11]},
            "gpu_3": {"attention_heads": [12,13,14,15]},
            "gpu_4": {"attention_heads": [16,17,18,19]},
            "gpu_5": {"attention_heads": [20,21,22,23]},
            "gpu_6": {"attention_heads": [24,25,26,27]},
            "gpu_7": {"attention_heads": [28,29,30,31]}
          }
        }
      }
    },
    "Pipeline_Parallelism_Baseline": {
      "model_name": "PP_Baseline_MoE_Transformer", 
      "parallel_strategy": {
        "name": "pipeline_parallelism",
        "type": "pipeline_parallel",
        "parameters": {
          "pipeline_parallel_degree": 2,
          "layers_per_stage": 2,
          "micro_batches": 4,
          "bubble_time_ratio": 0.25
        }
      },
      "module_divisions": {
        "pipeline_stage_0": {
          "name": "transformer_layers_0_1",
          "layers": [0, 1],
          "device_mapping": {
            "gpu_0": {"layer": 0, "full_layer": true},
            "gpu_1": {"layer": 1, "full_layer": true},
            "gpu_2": {"layer": 0, "full_layer": true},
            "gpu_3": {"layer": 1, "full_layer": true},
            "gpu_4": {"layer": 0, "full_layer": true},
            "gpu_5": {"layer": 1, "full_layer": true},
            "gpu_6": {"layer": 0, "full_layer": true},
            "gpu_7": {"layer": 1, "full_layer": true}
          }
        },
        "pipeline_stage_1": {
          "name": "transformer_layers_2_3",
          "layers": [2, 3],
          "device_mapping": {
            "gpu_8": {"layer": 2, "full_layer": true},
            "gpu_9": {"layer": 3, "full_layer": true},
            "gpu_10": {"layer": 2, "full_layer": true},
            "gpu_11": {"layer": 3, "full_layer": true},
            "gpu_12": {"layer": 2, "full_layer": true},
            "gpu_13": {"layer": 3, "full_layer": true},
            "gpu_14": {"layer": 2, "full_layer": true},
            "gpu_15": {"layer": 3, "full_layer": true}
          }
        }
      }
    },
    "Hybrid_TP_PP_Baseline": {
      "model_name": "Hybrid_TP_PP_MoE_Transformer",
      "parallel_strategy": {
        "name": "hybrid_tensor_pipeline_parallel",
        "type": "hybrid_parallel",
        "parameters": {
          "tensor_parallel_degree": 8,
          "pipeline_parallel_degree": 2,
          "layers_per_stage": 2
        }
      },
      "module_divisions": {
        "pipeline_stage_0": {
          "name": "transformer_layers_0_1_with_tensor_parallel",
          "layers": [0, 1],
          "tensor_parallel_degree": 8,
          "device_mapping": {
            "gpu_0_7": {"layer": 0, "tensor_parallel": true},
            "gpu_8_15": {"layer": 1, "tensor_parallel": true}
          }
        },
        "pipeline_stage_1": {
          "name": "transformer_layers_2_3_with_tensor_parallel",
          "layers": [2, 3],
          "tensor_parallel_degree": 8,
          "device_mapping": {
            "gpu_0_7": {"layer": 2, "tensor_parallel": true},
            "gpu_8_15": {"layer": 3, "tensor_parallel": true}
          }
        }
      }
    }
  },
  "hardware_requirements": {
    "gpus": {
      "count": 16,
      "model": "NVIDIA_A100_80GB",
      "memory_per_gpu": "80GB_HBM2e",
      "interconnect": "NVLink_3.0_600GBps"
    },
    "network": {
      "intra_node": "NVLink_mesh_topology",
      "inter_node": "InfiniBand_HDR_200Gbps",
      "topology": "fat_tree"
    },
    "cpu": {
      "model": "AMD_EPYC_7763_64_Core",
      "memory_per_node": "1TB_DDR4"
    }
  },
  "communication_specifications": {
    "bandwidth_intra_node": "600GBps",
    "bandwidth_inter_node": "200Gbps",
    "latency_intra_node": "1_microsecond",
    "latency_inter_node": "5_microseconds",
    "communication_backend": "NCCL_2.15"
  },
  "model_dimensions": {
    "vocabulary_size": 50265,
    "max_sequence_length": 2048,
    "hidden_dimension": 4096,
    "attention_heads": 32,
    "attention_head_dimension": 128,
    "expert_hidden_dimension": 16384,
    "expert_count": 16,
    "top_k_routing": 2,
    "layer_count": 4
  },
  "optimization_parameters": {
    "batch_size": 1024,
    "tokens_per_batch": 2097152,
    "learning_rate": 0.0001,
    "weight_decay": 0.1,
    "optimizer": "AdamW",
    "beta1": 0.9,
    "beta2": 0.95,
    "training_steps": 50000,
    "warmup_steps": 5000,
    "gradient_clipping": 1.0
  }
}