// Large-Scale Cross-Node Expert Parallelism MoE
digraph {
	rankdir=TB size="200,300"
	input [label="INPUT\nbatch_size=32, seq_len=2048, hidden_size=7168\nGPU: All" fillcolor=lightgreen shape=ellipse style=filled]
	d0_ln1 [label="LayerNorm\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3712" fillcolor=lightyellow style=filled]
	d0_qkv [label="QKV Projection\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, heads=128, d_k=128\nGPU: 3712" fillcolor=lightblue style=filled]
	d0_mha [label="MHA Attention\nInput: batch_size=32, seq_len=2048, heads=128, d_k=128\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3712" fillcolor=lightblue style=filled]
	d0_out [label="Output Projection\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3712" fillcolor=lightblue style=filled]
	d0_res1 [label="Residual Add\nInput1: batch_size=32, seq_len=2048, hidden_size=7168\nInput2: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3712" fillcolor=lightpink style=filled]
	d0_ln2 [label="LayerNorm\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3712" fillcolor=lightyellow style=filled]
	d0_ffn1 [label="FFN Linear1\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, ffn_hidden=2048\nGPU: 3712" fillcolor=lightblue style=filled]
	d0_gelu [label="GELU\nInput: batch_size=32, seq_len=2048, ffn_hidden=2048\nOutput: batch_size=32, seq_len=2048, ffn_hidden=2048\nGPU: 3712" fillcolor=lightgreen style=filled]
	d0_ffn2 [label="FFN Linear2\nInput: batch_size=32, seq_len=2048, ffn_hidden=2048\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3712" fillcolor=lightblue style=filled]
	d0_final [label="Residual Add\nInput1: batch_size=32, seq_len=2048, hidden_size=7168\nInput2: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3712" fillcolor=lightpink style=filled]
	d1_ln1 [label="LayerNorm\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3713" fillcolor=lightyellow style=filled]
	d1_qkv [label="QKV Projection\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, heads=128, d_k=128\nGPU: 3713" fillcolor=lightblue style=filled]
	d1_mha [label="MHA Attention\nInput: batch_size=32, seq_len=2048, heads=128, d_k=128\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3713" fillcolor=lightblue style=filled]
	d1_out [label="Output Projection\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3713" fillcolor=lightblue style=filled]
	d1_res1 [label="Residual Add\nInput1: batch_size=32, seq_len=2048, hidden_size=7168\nInput2: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3713" fillcolor=lightpink style=filled]
	d1_ln2 [label="LayerNorm\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3713" fillcolor=lightyellow style=filled]
	d1_ffn1 [label="FFN Linear1\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, ffn_hidden=2048\nGPU: 3713" fillcolor=lightblue style=filled]
	d1_gelu [label="GELU\nInput: batch_size=32, seq_len=2048, ffn_hidden=2048\nOutput: batch_size=32, seq_len=2048, ffn_hidden=2048\nGPU: 3713" fillcolor=lightgreen style=filled]
	d1_ffn2 [label="FFN Linear2\nInput: batch_size=32, seq_len=2048, ffn_hidden=2048\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3713" fillcolor=lightblue style=filled]
	d1_final [label="Residual Add\nInput1: batch_size=32, seq_len=2048, hidden_size=7168\nInput2: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3713" fillcolor=lightpink style=filled]
	d2_ln1 [label="LayerNorm\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3714" fillcolor=lightyellow style=filled]
	d2_qkv [label="QKV Projection\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, heads=128, d_k=128\nGPU: 3714" fillcolor=lightblue style=filled]
	d2_mha [label="MHA Attention\nInput: batch_size=32, seq_len=2048, heads=128, d_k=128\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3714" fillcolor=lightblue style=filled]
	d2_out [label="Output Projection\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3714" fillcolor=lightblue style=filled]
	d2_res1 [label="Residual Add\nInput1: batch_size=32, seq_len=2048, hidden_size=7168\nInput2: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3714" fillcolor=lightpink style=filled]
	d2_ln2 [label="LayerNorm\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3714" fillcolor=lightyellow style=filled]
	d2_ffn1 [label="FFN Linear1\nInput: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, ffn_hidden=2048\nGPU: 3714" fillcolor=lightblue style=filled]
	d2_gelu [label="GELU\nInput: batch_size=32, seq_len=2048, ffn_hidden=2048\nOutput: batch_size=32, seq_len=2048, ffn_hidden=2048\nGPU: 3714" fillcolor=lightgreen style=filled]
	d2_ffn2 [label="FFN Linear2\nInput: batch_size=32, seq_len=2048, ffn_hidden=2048\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3714" fillcolor=lightblue style=filled]
	d2_final [label="Residual Add\nInput1: batch_size=32, seq_len=2048, hidden_size=7168\nInput2: batch_size=32, seq_len=2048, hidden_size=7168\nOutput: batch_size=32, seq_len=2048, hidden_size=7168\nGPU: 3714" fillcolor=lightpink style=filled]
	input -> d0_ln1
	d0_ln1 -> d0_qkv
	d0_qkv -> d0_mha
	d0_mha -> d0_out
	d0_out -> d0_res1
	input -> d0_res1
	d0_res1 -> d0_ln2
	d0_ln2 -> d0_ffn1
	d0_ffn1 -> d0_gelu
	d0_gelu -> d0_ffn2
	d0_ffn2 -> d0_final
	d0_res1 -> d0_final
	d0_final -> d1_ln1
	m3_ln1 [label="LayerNorm\nGPU: 3715" fillcolor=lightyellow style=filled]
	m3_qkv [label="QKV Projection\nGPU: 3715" fillcolor=lightblue style=filled]
	m3_mha [label="MHA Attention\nGPU: 3715" fillcolor=lightblue style=filled]
	m3_out [label="Output Projection\nGPU: 3715" fillcolor=lightblue style=filled]
	m3_res1 [label="Residual Add\nGPU: 3715" fillcolor=lightpink style=filled]
	m3_gate [label="Expert Gate\nTop-2 selection\nGPU: 3715" fillcolor=orange shape=parallelogram style=filled]
	m3_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m3_expert0 [label="Expert 0\nNode 0 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m3_expert1 [label="Expert 1\nNode 0 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m3_expert2 [label="Expert 2\nNode 0 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m3_agg [label="Expert Aggregation\nGPU: 3715" fillcolor=purple shape=parallelogram style=filled]
	m3_final [label="Final Residual Add\nGPU: 3715" fillcolor=lightpink style=filled]
	d2_final -> m3_ln1
	m3_ln1 -> m3_qkv
	m3_qkv -> m3_mha
	m3_mha -> m3_out
	m3_out -> m3_res1
	d2_final -> m3_res1
	m3_res1 -> m3_gate
	m3_gate -> m3_route [style=dashed]
	m3_res1 -> m3_route
	m3_route -> m3_expert0
	m3_expert0 -> m3_agg
	m3_route -> m3_expert1
	m3_expert1 -> m3_agg
	m3_route -> m3_expert2
	m3_expert2 -> m3_agg
	m3_agg -> m3_final
	m3_res1 -> m3_final
	m4_ln1 [label="LayerNorm\nGPU: 3716" fillcolor=lightyellow style=filled]
	m4_qkv [label="QKV Projection\nGPU: 3716" fillcolor=lightblue style=filled]
	m4_mha [label="MHA Attention\nGPU: 3716" fillcolor=lightblue style=filled]
	m4_out [label="Output Projection\nGPU: 3716" fillcolor=lightblue style=filled]
	m4_res1 [label="Residual Add\nGPU: 3716" fillcolor=lightpink style=filled]
	m4_gate [label="Expert Gate\nTop-2 selection\nGPU: 3716" fillcolor=orange shape=parallelogram style=filled]
	m4_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m4_expert0 [label="Expert 0\nNode 8 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m4_expert1 [label="Expert 1\nNode 8 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m4_expert2 [label="Expert 2\nNode 8 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m4_agg [label="Expert Aggregation\nGPU: 3716" fillcolor=purple shape=parallelogram style=filled]
	m4_final [label="Final Residual Add\nGPU: 3716" fillcolor=lightpink style=filled]
	m4_ln1 -> m4_qkv
	m4_qkv -> m4_mha
	m4_mha -> m4_out
	m4_out -> m4_res1
	m3_final -> m4_res1
	m4_res1 -> m4_gate
	m4_gate -> m4_route [style=dashed]
	m4_res1 -> m4_route
	m4_route -> m4_expert0
	m4_expert0 -> m4_agg
	m4_route -> m4_expert1
	m4_expert1 -> m4_agg
	m4_route -> m4_expert2
	m4_expert2 -> m4_agg
	m4_agg -> m4_final
	m4_res1 -> m4_final
	m5_ln1 [label="LayerNorm\nGPU: 3717" fillcolor=lightyellow style=filled]
	m5_qkv [label="QKV Projection\nGPU: 3717" fillcolor=lightblue style=filled]
	m5_mha [label="MHA Attention\nGPU: 3717" fillcolor=lightblue style=filled]
	m5_out [label="Output Projection\nGPU: 3717" fillcolor=lightblue style=filled]
	m5_res1 [label="Residual Add\nGPU: 3717" fillcolor=lightpink style=filled]
	m5_gate [label="Expert Gate\nTop-2 selection\nGPU: 3717" fillcolor=orange shape=parallelogram style=filled]
	m5_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m5_expert0 [label="Expert 0\nNode 16 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m5_expert1 [label="Expert 1\nNode 16 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m5_expert2 [label="Expert 2\nNode 16 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m5_agg [label="Expert Aggregation\nGPU: 3717" fillcolor=purple shape=parallelogram style=filled]
	m5_final [label="Final Residual Add\nGPU: 3717" fillcolor=lightpink style=filled]
	m5_ln1 -> m5_qkv
	m5_qkv -> m5_mha
	m5_mha -> m5_out
	m5_out -> m5_res1
	m4_final -> m5_res1
	m5_res1 -> m5_gate
	m5_gate -> m5_route [style=dashed]
	m5_res1 -> m5_route
	m5_route -> m5_expert0
	m5_expert0 -> m5_agg
	m5_route -> m5_expert1
	m5_expert1 -> m5_agg
	m5_route -> m5_expert2
	m5_expert2 -> m5_agg
	m5_agg -> m5_final
	m5_res1 -> m5_final
	m6_ln1 [label="LayerNorm\nGPU: 3718" fillcolor=lightyellow style=filled]
	m6_qkv [label="QKV Projection\nGPU: 3718" fillcolor=lightblue style=filled]
	m6_mha [label="MHA Attention\nGPU: 3718" fillcolor=lightblue style=filled]
	m6_out [label="Output Projection\nGPU: 3718" fillcolor=lightblue style=filled]
	m6_res1 [label="Residual Add\nGPU: 3718" fillcolor=lightpink style=filled]
	m6_gate [label="Expert Gate\nTop-2 selection\nGPU: 3718" fillcolor=orange shape=parallelogram style=filled]
	m6_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m6_expert0 [label="Expert 0\nNode 24 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m6_expert1 [label="Expert 1\nNode 24 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m6_expert2 [label="Expert 2\nNode 24 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m6_agg [label="Expert Aggregation\nGPU: 3718" fillcolor=purple shape=parallelogram style=filled]
	m6_final [label="Final Residual Add\nGPU: 3718" fillcolor=lightpink style=filled]
	m6_ln1 -> m6_qkv
	m6_qkv -> m6_mha
	m6_mha -> m6_out
	m6_out -> m6_res1
	m5_final -> m6_res1
	m6_res1 -> m6_gate
	m6_gate -> m6_route [style=dashed]
	m6_res1 -> m6_route
	m6_route -> m6_expert0
	m6_expert0 -> m6_agg
	m6_route -> m6_expert1
	m6_expert1 -> m6_agg
	m6_route -> m6_expert2
	m6_expert2 -> m6_agg
	m6_agg -> m6_final
	m6_res1 -> m6_final
	m7_ln1 [label="LayerNorm\nGPU: 3719" fillcolor=lightyellow style=filled]
	m7_qkv [label="QKV Projection\nGPU: 3719" fillcolor=lightblue style=filled]
	m7_mha [label="MHA Attention\nGPU: 3719" fillcolor=lightblue style=filled]
	m7_out [label="Output Projection\nGPU: 3719" fillcolor=lightblue style=filled]
	m7_res1 [label="Residual Add\nGPU: 3719" fillcolor=lightpink style=filled]
	m7_gate [label="Expert Gate\nTop-2 selection\nGPU: 3719" fillcolor=orange shape=parallelogram style=filled]
	m7_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m7_expert0 [label="Expert 0\nNode 32 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m7_expert1 [label="Expert 1\nNode 32 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m7_expert2 [label="Expert 2\nNode 32 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m7_agg [label="Expert Aggregation\nGPU: 3719" fillcolor=purple shape=parallelogram style=filled]
	m7_final [label="Final Residual Add\nGPU: 3719" fillcolor=lightpink style=filled]
	m7_ln1 -> m7_qkv
	m7_qkv -> m7_mha
	m7_mha -> m7_out
	m7_out -> m7_res1
	m6_final -> m7_res1
	m7_res1 -> m7_gate
	m7_gate -> m7_route [style=dashed]
	m7_res1 -> m7_route
	m7_route -> m7_expert0
	m7_expert0 -> m7_agg
	m7_route -> m7_expert1
	m7_expert1 -> m7_agg
	m7_route -> m7_expert2
	m7_expert2 -> m7_agg
	m7_agg -> m7_final
	m7_res1 -> m7_final
	m8_ln1 [label="LayerNorm\nGPU: 3720" fillcolor=lightyellow style=filled]
	m8_qkv [label="QKV Projection\nGPU: 3720" fillcolor=lightblue style=filled]
	m8_mha [label="MHA Attention\nGPU: 3720" fillcolor=lightblue style=filled]
	m8_out [label="Output Projection\nGPU: 3720" fillcolor=lightblue style=filled]
	m8_res1 [label="Residual Add\nGPU: 3720" fillcolor=lightpink style=filled]
	m8_gate [label="Expert Gate\nTop-2 selection\nGPU: 3720" fillcolor=orange shape=parallelogram style=filled]
	m8_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m8_expert0 [label="Expert 0\nNode 40 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m8_expert1 [label="Expert 1\nNode 40 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m8_expert2 [label="Expert 2\nNode 40 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m8_agg [label="Expert Aggregation\nGPU: 3720" fillcolor=purple shape=parallelogram style=filled]
	m8_final [label="Final Residual Add\nGPU: 3720" fillcolor=lightpink style=filled]
	m8_ln1 -> m8_qkv
	m8_qkv -> m8_mha
	m8_mha -> m8_out
	m8_out -> m8_res1
	m7_final -> m8_res1
	m8_res1 -> m8_gate
	m8_gate -> m8_route [style=dashed]
	m8_res1 -> m8_route
	m8_route -> m8_expert0
	m8_expert0 -> m8_agg
	m8_route -> m8_expert1
	m8_expert1 -> m8_agg
	m8_route -> m8_expert2
	m8_expert2 -> m8_agg
	m8_agg -> m8_final
	m8_res1 -> m8_final
	m9_ln1 [label="LayerNorm\nGPU: 3721" fillcolor=lightyellow style=filled]
	m9_qkv [label="QKV Projection\nGPU: 3721" fillcolor=lightblue style=filled]
	m9_mha [label="MHA Attention\nGPU: 3721" fillcolor=lightblue style=filled]
	m9_out [label="Output Projection\nGPU: 3721" fillcolor=lightblue style=filled]
	m9_res1 [label="Residual Add\nGPU: 3721" fillcolor=lightpink style=filled]
	m9_gate [label="Expert Gate\nTop-2 selection\nGPU: 3721" fillcolor=orange shape=parallelogram style=filled]
	m9_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m9_expert0 [label="Expert 0\nNode 48 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m9_expert1 [label="Expert 1\nNode 48 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m9_expert2 [label="Expert 2\nNode 48 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m9_agg [label="Expert Aggregation\nGPU: 3721" fillcolor=purple shape=parallelogram style=filled]
	m9_final [label="Final Residual Add\nGPU: 3721" fillcolor=lightpink style=filled]
	m9_ln1 -> m9_qkv
	m9_qkv -> m9_mha
	m9_mha -> m9_out
	m9_out -> m9_res1
	m8_final -> m9_res1
	m9_res1 -> m9_gate
	m9_gate -> m9_route [style=dashed]
	m9_res1 -> m9_route
	m9_route -> m9_expert0
	m9_expert0 -> m9_agg
	m9_route -> m9_expert1
	m9_expert1 -> m9_agg
	m9_route -> m9_expert2
	m9_expert2 -> m9_agg
	m9_agg -> m9_final
	m9_res1 -> m9_final
	m10_ln1 [label="LayerNorm\nGPU: 3722" fillcolor=lightyellow style=filled]
	m10_qkv [label="QKV Projection\nGPU: 3722" fillcolor=lightblue style=filled]
	m10_mha [label="MHA Attention\nGPU: 3722" fillcolor=lightblue style=filled]
	m10_out [label="Output Projection\nGPU: 3722" fillcolor=lightblue style=filled]
	m10_res1 [label="Residual Add\nGPU: 3722" fillcolor=lightpink style=filled]
	m10_gate [label="Expert Gate\nTop-2 selection\nGPU: 3722" fillcolor=orange shape=parallelogram style=filled]
	m10_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m10_expert0 [label="Expert 0\nNode 56 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m10_expert1 [label="Expert 1\nNode 56 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m10_expert2 [label="Expert 2\nNode 56 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m10_agg [label="Expert Aggregation\nGPU: 3722" fillcolor=purple shape=parallelogram style=filled]
	m10_final [label="Final Residual Add\nGPU: 3722" fillcolor=lightpink style=filled]
	m10_ln1 -> m10_qkv
	m10_qkv -> m10_mha
	m10_mha -> m10_out
	m10_out -> m10_res1
	m9_final -> m10_res1
	m10_res1 -> m10_gate
	m10_gate -> m10_route [style=dashed]
	m10_res1 -> m10_route
	m10_route -> m10_expert0
	m10_expert0 -> m10_agg
	m10_route -> m10_expert1
	m10_expert1 -> m10_agg
	m10_route -> m10_expert2
	m10_expert2 -> m10_agg
	m10_agg -> m10_final
	m10_res1 -> m10_final
	m11_ln1 [label="LayerNorm\nGPU: 3723" fillcolor=lightyellow style=filled]
	m11_qkv [label="QKV Projection\nGPU: 3723" fillcolor=lightblue style=filled]
	m11_mha [label="MHA Attention\nGPU: 3723" fillcolor=lightblue style=filled]
	m11_out [label="Output Projection\nGPU: 3723" fillcolor=lightblue style=filled]
	m11_res1 [label="Residual Add\nGPU: 3723" fillcolor=lightpink style=filled]
	m11_gate [label="Expert Gate\nTop-2 selection\nGPU: 3723" fillcolor=orange shape=parallelogram style=filled]
	m11_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m11_expert0 [label="Expert 0\nNode 64 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m11_expert1 [label="Expert 1\nNode 64 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m11_expert2 [label="Expert 2\nNode 64 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m11_agg [label="Expert Aggregation\nGPU: 3723" fillcolor=purple shape=parallelogram style=filled]
	m11_final [label="Final Residual Add\nGPU: 3723" fillcolor=lightpink style=filled]
	m11_ln1 -> m11_qkv
	m11_qkv -> m11_mha
	m11_mha -> m11_out
	m11_out -> m11_res1
	m10_final -> m11_res1
	m11_res1 -> m11_gate
	m11_gate -> m11_route [style=dashed]
	m11_res1 -> m11_route
	m11_route -> m11_expert0
	m11_expert0 -> m11_agg
	m11_route -> m11_expert1
	m11_expert1 -> m11_agg
	m11_route -> m11_expert2
	m11_expert2 -> m11_agg
	m11_agg -> m11_final
	m11_res1 -> m11_final
	m12_ln1 [label="LayerNorm\nGPU: 3724" fillcolor=lightyellow style=filled]
	m12_qkv [label="QKV Projection\nGPU: 3724" fillcolor=lightblue style=filled]
	m12_mha [label="MHA Attention\nGPU: 3724" fillcolor=lightblue style=filled]
	m12_out [label="Output Projection\nGPU: 3724" fillcolor=lightblue style=filled]
	m12_res1 [label="Residual Add\nGPU: 3724" fillcolor=lightpink style=filled]
	m12_gate [label="Expert Gate\nTop-2 selection\nGPU: 3724" fillcolor=orange shape=parallelogram style=filled]
	m12_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m12_expert0 [label="Expert 0\nNode 72 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m12_expert1 [label="Expert 1\nNode 72 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m12_expert2 [label="Expert 2\nNode 72 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m12_agg [label="Expert Aggregation\nGPU: 3724" fillcolor=purple shape=parallelogram style=filled]
	m12_final [label="Final Residual Add\nGPU: 3724" fillcolor=lightpink style=filled]
	m12_ln1 -> m12_qkv
	m12_qkv -> m12_mha
	m12_mha -> m12_out
	m12_out -> m12_res1
	m11_final -> m12_res1
	m12_res1 -> m12_gate
	m12_gate -> m12_route [style=dashed]
	m12_res1 -> m12_route
	m12_route -> m12_expert0
	m12_expert0 -> m12_agg
	m12_route -> m12_expert1
	m12_expert1 -> m12_agg
	m12_route -> m12_expert2
	m12_expert2 -> m12_agg
	m12_agg -> m12_final
	m12_res1 -> m12_final
	m13_ln1 [label="LayerNorm\nGPU: 3725" fillcolor=lightyellow style=filled]
	m13_qkv [label="QKV Projection\nGPU: 3725" fillcolor=lightblue style=filled]
	m13_mha [label="MHA Attention\nGPU: 3725" fillcolor=lightblue style=filled]
	m13_out [label="Output Projection\nGPU: 3725" fillcolor=lightblue style=filled]
	m13_res1 [label="Residual Add\nGPU: 3725" fillcolor=lightpink style=filled]
	m13_gate [label="Expert Gate\nTop-2 selection\nGPU: 3725" fillcolor=orange shape=parallelogram style=filled]
	m13_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m13_expert0 [label="Expert 0\nNode 80 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m13_expert1 [label="Expert 1\nNode 80 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m13_expert2 [label="Expert 2\nNode 80 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m13_agg [label="Expert Aggregation\nGPU: 3725" fillcolor=purple shape=parallelogram style=filled]
	m13_final [label="Final Residual Add\nGPU: 3725" fillcolor=lightpink style=filled]
	m13_ln1 -> m13_qkv
	m13_qkv -> m13_mha
	m13_mha -> m13_out
	m13_out -> m13_res1
	m12_final -> m13_res1
	m13_res1 -> m13_gate
	m13_gate -> m13_route [style=dashed]
	m13_res1 -> m13_route
	m13_route -> m13_expert0
	m13_expert0 -> m13_agg
	m13_route -> m13_expert1
	m13_expert1 -> m13_agg
	m13_route -> m13_expert2
	m13_expert2 -> m13_agg
	m13_agg -> m13_final
	m13_res1 -> m13_final
	m14_ln1 [label="LayerNorm\nGPU: 3726" fillcolor=lightyellow style=filled]
	m14_qkv [label="QKV Projection\nGPU: 3726" fillcolor=lightblue style=filled]
	m14_mha [label="MHA Attention\nGPU: 3726" fillcolor=lightblue style=filled]
	m14_out [label="Output Projection\nGPU: 3726" fillcolor=lightblue style=filled]
	m14_res1 [label="Residual Add\nGPU: 3726" fillcolor=lightpink style=filled]
	m14_gate [label="Expert Gate\nTop-2 selection\nGPU: 3726" fillcolor=orange shape=parallelogram style=filled]
	m14_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m14_expert0 [label="Expert 0\nNode 88 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m14_expert1 [label="Expert 1\nNode 88 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m14_expert2 [label="Expert 2\nNode 88 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m14_agg [label="Expert Aggregation\nGPU: 3726" fillcolor=purple shape=parallelogram style=filled]
	m14_final [label="Final Residual Add\nGPU: 3726" fillcolor=lightpink style=filled]
	m14_ln1 -> m14_qkv
	m14_qkv -> m14_mha
	m14_mha -> m14_out
	m14_out -> m14_res1
	m13_final -> m14_res1
	m14_res1 -> m14_gate
	m14_gate -> m14_route [style=dashed]
	m14_res1 -> m14_route
	m14_route -> m14_expert0
	m14_expert0 -> m14_agg
	m14_route -> m14_expert1
	m14_expert1 -> m14_agg
	m14_route -> m14_expert2
	m14_expert2 -> m14_agg
	m14_agg -> m14_final
	m14_res1 -> m14_final
	m15_ln1 [label="LayerNorm\nGPU: 3727" fillcolor=lightyellow style=filled]
	m15_qkv [label="QKV Projection\nGPU: 3727" fillcolor=lightblue style=filled]
	m15_mha [label="MHA Attention\nGPU: 3727" fillcolor=lightblue style=filled]
	m15_out [label="Output Projection\nGPU: 3727" fillcolor=lightblue style=filled]
	m15_res1 [label="Residual Add\nGPU: 3727" fillcolor=lightpink style=filled]
	m15_gate [label="Expert Gate\nTop-2 selection\nGPU: 3727" fillcolor=orange shape=parallelogram style=filled]
	m15_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m15_expert0 [label="Expert 0\nNode 96 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m15_expert1 [label="Expert 1\nNode 96 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m15_expert2 [label="Expert 2\nNode 96 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m15_agg [label="Expert Aggregation\nGPU: 3727" fillcolor=purple shape=parallelogram style=filled]
	m15_final [label="Final Residual Add\nGPU: 3727" fillcolor=lightpink style=filled]
	m15_ln1 -> m15_qkv
	m15_qkv -> m15_mha
	m15_mha -> m15_out
	m15_out -> m15_res1
	m14_final -> m15_res1
	m15_res1 -> m15_gate
	m15_gate -> m15_route [style=dashed]
	m15_res1 -> m15_route
	m15_route -> m15_expert0
	m15_expert0 -> m15_agg
	m15_route -> m15_expert1
	m15_expert1 -> m15_agg
	m15_route -> m15_expert2
	m15_expert2 -> m15_agg
	m15_agg -> m15_final
	m15_res1 -> m15_final
	m16_ln1 [label="LayerNorm\nGPU: 3728" fillcolor=lightyellow style=filled]
	m16_qkv [label="QKV Projection\nGPU: 3728" fillcolor=lightblue style=filled]
	m16_mha [label="MHA Attention\nGPU: 3728" fillcolor=lightblue style=filled]
	m16_out [label="Output Projection\nGPU: 3728" fillcolor=lightblue style=filled]
	m16_res1 [label="Residual Add\nGPU: 3728" fillcolor=lightpink style=filled]
	m16_gate [label="Expert Gate\nTop-2 selection\nGPU: 3728" fillcolor=orange shape=parallelogram style=filled]
	m16_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m16_expert0 [label="Expert 0\nNode 104 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m16_expert1 [label="Expert 1\nNode 104 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m16_expert2 [label="Expert 2\nNode 104 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m16_agg [label="Expert Aggregation\nGPU: 3728" fillcolor=purple shape=parallelogram style=filled]
	m16_final [label="Final Residual Add\nGPU: 3728" fillcolor=lightpink style=filled]
	m16_ln1 -> m16_qkv
	m16_qkv -> m16_mha
	m16_mha -> m16_out
	m16_out -> m16_res1
	m15_final -> m16_res1
	m16_res1 -> m16_gate
	m16_gate -> m16_route [style=dashed]
	m16_res1 -> m16_route
	m16_route -> m16_expert0
	m16_expert0 -> m16_agg
	m16_route -> m16_expert1
	m16_expert1 -> m16_agg
	m16_route -> m16_expert2
	m16_expert2 -> m16_agg
	m16_agg -> m16_final
	m16_res1 -> m16_final
	m17_ln1 [label="LayerNorm\nGPU: 3729" fillcolor=lightyellow style=filled]
	m17_qkv [label="QKV Projection\nGPU: 3729" fillcolor=lightblue style=filled]
	m17_mha [label="MHA Attention\nGPU: 3729" fillcolor=lightblue style=filled]
	m17_out [label="Output Projection\nGPU: 3729" fillcolor=lightblue style=filled]
	m17_res1 [label="Residual Add\nGPU: 3729" fillcolor=lightpink style=filled]
	m17_gate [label="Expert Gate\nTop-2 selection\nGPU: 3729" fillcolor=orange shape=parallelogram style=filled]
	m17_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m17_expert0 [label="Expert 0\nNode 112 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m17_expert1 [label="Expert 1\nNode 112 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m17_expert2 [label="Expert 2\nNode 112 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m17_agg [label="Expert Aggregation\nGPU: 3729" fillcolor=purple shape=parallelogram style=filled]
	m17_final [label="Final Residual Add\nGPU: 3729" fillcolor=lightpink style=filled]
	m17_ln1 -> m17_qkv
	m17_qkv -> m17_mha
	m17_mha -> m17_out
	m17_out -> m17_res1
	m16_final -> m17_res1
	m17_res1 -> m17_gate
	m17_gate -> m17_route [style=dashed]
	m17_res1 -> m17_route
	m17_route -> m17_expert0
	m17_expert0 -> m17_agg
	m17_route -> m17_expert1
	m17_expert1 -> m17_agg
	m17_route -> m17_expert2
	m17_expert2 -> m17_agg
	m17_agg -> m17_final
	m17_res1 -> m17_final
	m18_ln1 [label="LayerNorm\nGPU: 3730" fillcolor=lightyellow style=filled]
	m18_qkv [label="QKV Projection\nGPU: 3730" fillcolor=lightblue style=filled]
	m18_mha [label="MHA Attention\nGPU: 3730" fillcolor=lightblue style=filled]
	m18_out [label="Output Projection\nGPU: 3730" fillcolor=lightblue style=filled]
	m18_res1 [label="Residual Add\nGPU: 3730" fillcolor=lightpink style=filled]
	m18_gate [label="Expert Gate\nTop-2 selection\nGPU: 3730" fillcolor=orange shape=parallelogram style=filled]
	m18_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m18_expert0 [label="Expert 0\nNode 120 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m18_expert1 [label="Expert 1\nNode 120 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m18_expert2 [label="Expert 2\nNode 120 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m18_agg [label="Expert Aggregation\nGPU: 3730" fillcolor=purple shape=parallelogram style=filled]
	m18_final [label="Final Residual Add\nGPU: 3730" fillcolor=lightpink style=filled]
	m18_ln1 -> m18_qkv
	m18_qkv -> m18_mha
	m18_mha -> m18_out
	m18_out -> m18_res1
	m17_final -> m18_res1
	m18_res1 -> m18_gate
	m18_gate -> m18_route [style=dashed]
	m18_res1 -> m18_route
	m18_route -> m18_expert0
	m18_expert0 -> m18_agg
	m18_route -> m18_expert1
	m18_expert1 -> m18_agg
	m18_route -> m18_expert2
	m18_expert2 -> m18_agg
	m18_agg -> m18_final
	m18_res1 -> m18_final
	m19_ln1 [label="LayerNorm\nGPU: 3731" fillcolor=lightyellow style=filled]
	m19_qkv [label="QKV Projection\nGPU: 3731" fillcolor=lightblue style=filled]
	m19_mha [label="MHA Attention\nGPU: 3731" fillcolor=lightblue style=filled]
	m19_out [label="Output Projection\nGPU: 3731" fillcolor=lightblue style=filled]
	m19_res1 [label="Residual Add\nGPU: 3731" fillcolor=lightpink style=filled]
	m19_gate [label="Expert Gate\nTop-2 selection\nGPU: 3731" fillcolor=orange shape=parallelogram style=filled]
	m19_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m19_expert0 [label="Expert 0\nNode 128 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m19_expert1 [label="Expert 1\nNode 128 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m19_expert2 [label="Expert 2\nNode 128 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m19_agg [label="Expert Aggregation\nGPU: 3731" fillcolor=purple shape=parallelogram style=filled]
	m19_final [label="Final Residual Add\nGPU: 3731" fillcolor=lightpink style=filled]
	m19_ln1 -> m19_qkv
	m19_qkv -> m19_mha
	m19_mha -> m19_out
	m19_out -> m19_res1
	m18_final -> m19_res1
	m19_res1 -> m19_gate
	m19_gate -> m19_route [style=dashed]
	m19_res1 -> m19_route
	m19_route -> m19_expert0
	m19_expert0 -> m19_agg
	m19_route -> m19_expert1
	m19_expert1 -> m19_agg
	m19_route -> m19_expert2
	m19_expert2 -> m19_agg
	m19_agg -> m19_final
	m19_res1 -> m19_final
	m20_ln1 [label="LayerNorm\nGPU: 3732" fillcolor=lightyellow style=filled]
	m20_qkv [label="QKV Projection\nGPU: 3732" fillcolor=lightblue style=filled]
	m20_mha [label="MHA Attention\nGPU: 3732" fillcolor=lightblue style=filled]
	m20_out [label="Output Projection\nGPU: 3732" fillcolor=lightblue style=filled]
	m20_res1 [label="Residual Add\nGPU: 3732" fillcolor=lightpink style=filled]
	m20_gate [label="Expert Gate\nTop-2 selection\nGPU: 3732" fillcolor=orange shape=parallelogram style=filled]
	m20_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m20_expert0 [label="Expert 0\nNode 136 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m20_expert1 [label="Expert 1\nNode 136 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m20_expert2 [label="Expert 2\nNode 136 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m20_agg [label="Expert Aggregation\nGPU: 3732" fillcolor=purple shape=parallelogram style=filled]
	m20_final [label="Final Residual Add\nGPU: 3732" fillcolor=lightpink style=filled]
	m20_ln1 -> m20_qkv
	m20_qkv -> m20_mha
	m20_mha -> m20_out
	m20_out -> m20_res1
	m19_final -> m20_res1
	m20_res1 -> m20_gate
	m20_gate -> m20_route [style=dashed]
	m20_res1 -> m20_route
	m20_route -> m20_expert0
	m20_expert0 -> m20_agg
	m20_route -> m20_expert1
	m20_expert1 -> m20_agg
	m20_route -> m20_expert2
	m20_expert2 -> m20_agg
	m20_agg -> m20_final
	m20_res1 -> m20_final
	m21_ln1 [label="LayerNorm\nGPU: 3733" fillcolor=lightyellow style=filled]
	m21_qkv [label="QKV Projection\nGPU: 3733" fillcolor=lightblue style=filled]
	m21_mha [label="MHA Attention\nGPU: 3733" fillcolor=lightblue style=filled]
	m21_out [label="Output Projection\nGPU: 3733" fillcolor=lightblue style=filled]
	m21_res1 [label="Residual Add\nGPU: 3733" fillcolor=lightpink style=filled]
	m21_gate [label="Expert Gate\nTop-2 selection\nGPU: 3733" fillcolor=orange shape=parallelogram style=filled]
	m21_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m21_expert0 [label="Expert 0\nNode 144 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m21_expert1 [label="Expert 1\nNode 144 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m21_expert2 [label="Expert 2\nNode 144 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m21_agg [label="Expert Aggregation\nGPU: 3733" fillcolor=purple shape=parallelogram style=filled]
	m21_final [label="Final Residual Add\nGPU: 3733" fillcolor=lightpink style=filled]
	m21_ln1 -> m21_qkv
	m21_qkv -> m21_mha
	m21_mha -> m21_out
	m21_out -> m21_res1
	m20_final -> m21_res1
	m21_res1 -> m21_gate
	m21_gate -> m21_route [style=dashed]
	m21_res1 -> m21_route
	m21_route -> m21_expert0
	m21_expert0 -> m21_agg
	m21_route -> m21_expert1
	m21_expert1 -> m21_agg
	m21_route -> m21_expert2
	m21_expert2 -> m21_agg
	m21_agg -> m21_final
	m21_res1 -> m21_final
	m22_ln1 [label="LayerNorm\nGPU: 3734" fillcolor=lightyellow style=filled]
	m22_qkv [label="QKV Projection\nGPU: 3734" fillcolor=lightblue style=filled]
	m22_mha [label="MHA Attention\nGPU: 3734" fillcolor=lightblue style=filled]
	m22_out [label="Output Projection\nGPU: 3734" fillcolor=lightblue style=filled]
	m22_res1 [label="Residual Add\nGPU: 3734" fillcolor=lightpink style=filled]
	m22_gate [label="Expert Gate\nTop-2 selection\nGPU: 3734" fillcolor=orange shape=parallelogram style=filled]
	m22_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m22_expert0 [label="Expert 0\nNode 152 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m22_expert1 [label="Expert 1\nNode 152 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m22_expert2 [label="Expert 2\nNode 152 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m22_agg [label="Expert Aggregation\nGPU: 3734" fillcolor=purple shape=parallelogram style=filled]
	m22_final [label="Final Residual Add\nGPU: 3734" fillcolor=lightpink style=filled]
	m22_ln1 -> m22_qkv
	m22_qkv -> m22_mha
	m22_mha -> m22_out
	m22_out -> m22_res1
	m21_final -> m22_res1
	m22_res1 -> m22_gate
	m22_gate -> m22_route [style=dashed]
	m22_res1 -> m22_route
	m22_route -> m22_expert0
	m22_expert0 -> m22_agg
	m22_route -> m22_expert1
	m22_expert1 -> m22_agg
	m22_route -> m22_expert2
	m22_expert2 -> m22_agg
	m22_agg -> m22_final
	m22_res1 -> m22_final
	m23_ln1 [label="LayerNorm\nGPU: 3735" fillcolor=lightyellow style=filled]
	m23_qkv [label="QKV Projection\nGPU: 3735" fillcolor=lightblue style=filled]
	m23_mha [label="MHA Attention\nGPU: 3735" fillcolor=lightblue style=filled]
	m23_out [label="Output Projection\nGPU: 3735" fillcolor=lightblue style=filled]
	m23_res1 [label="Residual Add\nGPU: 3735" fillcolor=lightpink style=filled]
	m23_gate [label="Expert Gate\nTop-2 selection\nGPU: 3735" fillcolor=orange shape=parallelogram style=filled]
	m23_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m23_expert0 [label="Expert 0\nNode 160 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m23_expert1 [label="Expert 1\nNode 160 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m23_expert2 [label="Expert 2\nNode 160 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m23_agg [label="Expert Aggregation\nGPU: 3735" fillcolor=purple shape=parallelogram style=filled]
	m23_final [label="Final Residual Add\nGPU: 3735" fillcolor=lightpink style=filled]
	m23_ln1 -> m23_qkv
	m23_qkv -> m23_mha
	m23_mha -> m23_out
	m23_out -> m23_res1
	m22_final -> m23_res1
	m23_res1 -> m23_gate
	m23_gate -> m23_route [style=dashed]
	m23_res1 -> m23_route
	m23_route -> m23_expert0
	m23_expert0 -> m23_agg
	m23_route -> m23_expert1
	m23_expert1 -> m23_agg
	m23_route -> m23_expert2
	m23_expert2 -> m23_agg
	m23_agg -> m23_final
	m23_res1 -> m23_final
	m24_ln1 [label="LayerNorm\nGPU: 3736" fillcolor=lightyellow style=filled]
	m24_qkv [label="QKV Projection\nGPU: 3736" fillcolor=lightblue style=filled]
	m24_mha [label="MHA Attention\nGPU: 3736" fillcolor=lightblue style=filled]
	m24_out [label="Output Projection\nGPU: 3736" fillcolor=lightblue style=filled]
	m24_res1 [label="Residual Add\nGPU: 3736" fillcolor=lightpink style=filled]
	m24_gate [label="Expert Gate\nTop-2 selection\nGPU: 3736" fillcolor=orange shape=parallelogram style=filled]
	m24_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m24_expert0 [label="Expert 0\nNode 168 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m24_expert1 [label="Expert 1\nNode 168 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m24_expert2 [label="Expert 2\nNode 168 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m24_agg [label="Expert Aggregation\nGPU: 3736" fillcolor=purple shape=parallelogram style=filled]
	m24_final [label="Final Residual Add\nGPU: 3736" fillcolor=lightpink style=filled]
	m24_ln1 -> m24_qkv
	m24_qkv -> m24_mha
	m24_mha -> m24_out
	m24_out -> m24_res1
	m23_final -> m24_res1
	m24_res1 -> m24_gate
	m24_gate -> m24_route [style=dashed]
	m24_res1 -> m24_route
	m24_route -> m24_expert0
	m24_expert0 -> m24_agg
	m24_route -> m24_expert1
	m24_expert1 -> m24_agg
	m24_route -> m24_expert2
	m24_expert2 -> m24_agg
	m24_agg -> m24_final
	m24_res1 -> m24_final
	m25_ln1 [label="LayerNorm\nGPU: 3737" fillcolor=lightyellow style=filled]
	m25_qkv [label="QKV Projection\nGPU: 3737" fillcolor=lightblue style=filled]
	m25_mha [label="MHA Attention\nGPU: 3737" fillcolor=lightblue style=filled]
	m25_out [label="Output Projection\nGPU: 3737" fillcolor=lightblue style=filled]
	m25_res1 [label="Residual Add\nGPU: 3737" fillcolor=lightpink style=filled]
	m25_gate [label="Expert Gate\nTop-2 selection\nGPU: 3737" fillcolor=orange shape=parallelogram style=filled]
	m25_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m25_expert0 [label="Expert 0\nNode 176 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m25_expert1 [label="Expert 1\nNode 176 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m25_expert2 [label="Expert 2\nNode 176 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m25_agg [label="Expert Aggregation\nGPU: 3737" fillcolor=purple shape=parallelogram style=filled]
	m25_final [label="Final Residual Add\nGPU: 3737" fillcolor=lightpink style=filled]
	m25_ln1 -> m25_qkv
	m25_qkv -> m25_mha
	m25_mha -> m25_out
	m25_out -> m25_res1
	m24_final -> m25_res1
	m25_res1 -> m25_gate
	m25_gate -> m25_route [style=dashed]
	m25_res1 -> m25_route
	m25_route -> m25_expert0
	m25_expert0 -> m25_agg
	m25_route -> m25_expert1
	m25_expert1 -> m25_agg
	m25_route -> m25_expert2
	m25_expert2 -> m25_agg
	m25_agg -> m25_final
	m25_res1 -> m25_final
	m26_ln1 [label="LayerNorm\nGPU: 3738" fillcolor=lightyellow style=filled]
	m26_qkv [label="QKV Projection\nGPU: 3738" fillcolor=lightblue style=filled]
	m26_mha [label="MHA Attention\nGPU: 3738" fillcolor=lightblue style=filled]
	m26_out [label="Output Projection\nGPU: 3738" fillcolor=lightblue style=filled]
	m26_res1 [label="Residual Add\nGPU: 3738" fillcolor=lightpink style=filled]
	m26_gate [label="Expert Gate\nTop-2 selection\nGPU: 3738" fillcolor=orange shape=parallelogram style=filled]
	m26_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m26_expert0 [label="Expert 0\nNode 184 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m26_expert1 [label="Expert 1\nNode 184 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m26_expert2 [label="Expert 2\nNode 184 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m26_agg [label="Expert Aggregation\nGPU: 3738" fillcolor=purple shape=parallelogram style=filled]
	m26_final [label="Final Residual Add\nGPU: 3738" fillcolor=lightpink style=filled]
	m26_ln1 -> m26_qkv
	m26_qkv -> m26_mha
	m26_mha -> m26_out
	m26_out -> m26_res1
	m25_final -> m26_res1
	m26_res1 -> m26_gate
	m26_gate -> m26_route [style=dashed]
	m26_res1 -> m26_route
	m26_route -> m26_expert0
	m26_expert0 -> m26_agg
	m26_route -> m26_expert1
	m26_expert1 -> m26_agg
	m26_route -> m26_expert2
	m26_expert2 -> m26_agg
	m26_agg -> m26_final
	m26_res1 -> m26_final
	m27_ln1 [label="LayerNorm\nGPU: 3739" fillcolor=lightyellow style=filled]
	m27_qkv [label="QKV Projection\nGPU: 3739" fillcolor=lightblue style=filled]
	m27_mha [label="MHA Attention\nGPU: 3739" fillcolor=lightblue style=filled]
	m27_out [label="Output Projection\nGPU: 3739" fillcolor=lightblue style=filled]
	m27_res1 [label="Residual Add\nGPU: 3739" fillcolor=lightpink style=filled]
	m27_gate [label="Expert Gate\nTop-2 selection\nGPU: 3739" fillcolor=orange shape=parallelogram style=filled]
	m27_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m27_expert0 [label="Expert 0\nNode 192 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m27_expert1 [label="Expert 1\nNode 192 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m27_expert2 [label="Expert 2\nNode 192 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m27_agg [label="Expert Aggregation\nGPU: 3739" fillcolor=purple shape=parallelogram style=filled]
	m27_final [label="Final Residual Add\nGPU: 3739" fillcolor=lightpink style=filled]
	m27_ln1 -> m27_qkv
	m27_qkv -> m27_mha
	m27_mha -> m27_out
	m27_out -> m27_res1
	m26_final -> m27_res1
	m27_res1 -> m27_gate
	m27_gate -> m27_route [style=dashed]
	m27_res1 -> m27_route
	m27_route -> m27_expert0
	m27_expert0 -> m27_agg
	m27_route -> m27_expert1
	m27_expert1 -> m27_agg
	m27_route -> m27_expert2
	m27_expert2 -> m27_agg
	m27_agg -> m27_final
	m27_res1 -> m27_final
	m28_ln1 [label="LayerNorm\nGPU: 3740" fillcolor=lightyellow style=filled]
	m28_qkv [label="QKV Projection\nGPU: 3740" fillcolor=lightblue style=filled]
	m28_mha [label="MHA Attention\nGPU: 3740" fillcolor=lightblue style=filled]
	m28_out [label="Output Projection\nGPU: 3740" fillcolor=lightblue style=filled]
	m28_res1 [label="Residual Add\nGPU: 3740" fillcolor=lightpink style=filled]
	m28_gate [label="Expert Gate\nTop-2 selection\nGPU: 3740" fillcolor=orange shape=parallelogram style=filled]
	m28_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m28_expert0 [label="Expert 0\nNode 200 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m28_expert1 [label="Expert 1\nNode 200 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m28_expert2 [label="Expert 2\nNode 200 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m28_agg [label="Expert Aggregation\nGPU: 3740" fillcolor=purple shape=parallelogram style=filled]
	m28_final [label="Final Residual Add\nGPU: 3740" fillcolor=lightpink style=filled]
	m28_ln1 -> m28_qkv
	m28_qkv -> m28_mha
	m28_mha -> m28_out
	m28_out -> m28_res1
	m27_final -> m28_res1
	m28_res1 -> m28_gate
	m28_gate -> m28_route [style=dashed]
	m28_res1 -> m28_route
	m28_route -> m28_expert0
	m28_expert0 -> m28_agg
	m28_route -> m28_expert1
	m28_expert1 -> m28_agg
	m28_route -> m28_expert2
	m28_expert2 -> m28_agg
	m28_agg -> m28_final
	m28_res1 -> m28_final
	m29_ln1 [label="LayerNorm\nGPU: 3741" fillcolor=lightyellow style=filled]
	m29_qkv [label="QKV Projection\nGPU: 3741" fillcolor=lightblue style=filled]
	m29_mha [label="MHA Attention\nGPU: 3741" fillcolor=lightblue style=filled]
	m29_out [label="Output Projection\nGPU: 3741" fillcolor=lightblue style=filled]
	m29_res1 [label="Residual Add\nGPU: 3741" fillcolor=lightpink style=filled]
	m29_gate [label="Expert Gate\nTop-2 selection\nGPU: 3741" fillcolor=orange shape=parallelogram style=filled]
	m29_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m29_expert0 [label="Expert 0\nNode 208 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m29_expert1 [label="Expert 1\nNode 208 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m29_expert2 [label="Expert 2\nNode 208 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m29_agg [label="Expert Aggregation\nGPU: 3741" fillcolor=purple shape=parallelogram style=filled]
	m29_final [label="Final Residual Add\nGPU: 3741" fillcolor=lightpink style=filled]
	m29_ln1 -> m29_qkv
	m29_qkv -> m29_mha
	m29_mha -> m29_out
	m29_out -> m29_res1
	m28_final -> m29_res1
	m29_res1 -> m29_gate
	m29_gate -> m29_route [style=dashed]
	m29_res1 -> m29_route
	m29_route -> m29_expert0
	m29_expert0 -> m29_agg
	m29_route -> m29_expert1
	m29_expert1 -> m29_agg
	m29_route -> m29_expert2
	m29_expert2 -> m29_agg
	m29_agg -> m29_final
	m29_res1 -> m29_final
	m30_ln1 [label="LayerNorm\nGPU: 3742" fillcolor=lightyellow style=filled]
	m30_qkv [label="QKV Projection\nGPU: 3742" fillcolor=lightblue style=filled]
	m30_mha [label="MHA Attention\nGPU: 3742" fillcolor=lightblue style=filled]
	m30_out [label="Output Projection\nGPU: 3742" fillcolor=lightblue style=filled]
	m30_res1 [label="Residual Add\nGPU: 3742" fillcolor=lightpink style=filled]
	m30_gate [label="Expert Gate\nTop-2 selection\nGPU: 3742" fillcolor=orange shape=parallelogram style=filled]
	m30_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m30_expert0 [label="Expert 0\nNode 216 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m30_expert1 [label="Expert 1\nNode 216 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m30_expert2 [label="Expert 2\nNode 216 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m30_agg [label="Expert Aggregation\nGPU: 3742" fillcolor=purple shape=parallelogram style=filled]
	m30_final [label="Final Residual Add\nGPU: 3742" fillcolor=lightpink style=filled]
	m30_ln1 -> m30_qkv
	m30_qkv -> m30_mha
	m30_mha -> m30_out
	m30_out -> m30_res1
	m29_final -> m30_res1
	m30_res1 -> m30_gate
	m30_gate -> m30_route [style=dashed]
	m30_res1 -> m30_route
	m30_route -> m30_expert0
	m30_expert0 -> m30_agg
	m30_route -> m30_expert1
	m30_expert1 -> m30_agg
	m30_route -> m30_expert2
	m30_expert2 -> m30_agg
	m30_agg -> m30_final
	m30_res1 -> m30_final
	m31_ln1 [label="LayerNorm\nGPU: 3743" fillcolor=lightyellow style=filled]
	m31_qkv [label="QKV Projection\nGPU: 3743" fillcolor=lightblue style=filled]
	m31_mha [label="MHA Attention\nGPU: 3743" fillcolor=lightblue style=filled]
	m31_out [label="Output Projection\nGPU: 3743" fillcolor=lightblue style=filled]
	m31_res1 [label="Residual Add\nGPU: 3743" fillcolor=lightpink style=filled]
	m31_gate [label="Expert Gate\nTop-2 selection\nGPU: 3743" fillcolor=orange shape=parallelogram style=filled]
	m31_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m31_expert0 [label="Expert 0\nNode 224 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m31_expert1 [label="Expert 1\nNode 224 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m31_expert2 [label="Expert 2\nNode 224 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m31_agg [label="Expert Aggregation\nGPU: 3743" fillcolor=purple shape=parallelogram style=filled]
	m31_final [label="Final Residual Add\nGPU: 3743" fillcolor=lightpink style=filled]
	m31_ln1 -> m31_qkv
	m31_qkv -> m31_mha
	m31_mha -> m31_out
	m31_out -> m31_res1
	m30_final -> m31_res1
	m31_res1 -> m31_gate
	m31_gate -> m31_route [style=dashed]
	m31_res1 -> m31_route
	m31_route -> m31_expert0
	m31_expert0 -> m31_agg
	m31_route -> m31_expert1
	m31_expert1 -> m31_agg
	m31_route -> m31_expert2
	m31_expert2 -> m31_agg
	m31_agg -> m31_final
	m31_res1 -> m31_final
	m32_ln1 [label="LayerNorm\nGPU: 3744" fillcolor=lightyellow style=filled]
	m32_qkv [label="QKV Projection\nGPU: 3744" fillcolor=lightblue style=filled]
	m32_mha [label="MHA Attention\nGPU: 3744" fillcolor=lightblue style=filled]
	m32_out [label="Output Projection\nGPU: 3744" fillcolor=lightblue style=filled]
	m32_res1 [label="Residual Add\nGPU: 3744" fillcolor=lightpink style=filled]
	m32_gate [label="Expert Gate\nTop-2 selection\nGPU: 3744" fillcolor=orange shape=parallelogram style=filled]
	m32_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m32_expert0 [label="Expert 0\nNode 232 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m32_expert1 [label="Expert 1\nNode 232 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m32_expert2 [label="Expert 2\nNode 232 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m32_agg [label="Expert Aggregation\nGPU: 3744" fillcolor=purple shape=parallelogram style=filled]
	m32_final [label="Final Residual Add\nGPU: 3744" fillcolor=lightpink style=filled]
	m32_ln1 -> m32_qkv
	m32_qkv -> m32_mha
	m32_mha -> m32_out
	m32_out -> m32_res1
	m31_final -> m32_res1
	m32_res1 -> m32_gate
	m32_gate -> m32_route [style=dashed]
	m32_res1 -> m32_route
	m32_route -> m32_expert0
	m32_expert0 -> m32_agg
	m32_route -> m32_expert1
	m32_expert1 -> m32_agg
	m32_route -> m32_expert2
	m32_expert2 -> m32_agg
	m32_agg -> m32_final
	m32_res1 -> m32_final
	m33_ln1 [label="LayerNorm\nGPU: 3745" fillcolor=lightyellow style=filled]
	m33_qkv [label="QKV Projection\nGPU: 3745" fillcolor=lightblue style=filled]
	m33_mha [label="MHA Attention\nGPU: 3745" fillcolor=lightblue style=filled]
	m33_out [label="Output Projection\nGPU: 3745" fillcolor=lightblue style=filled]
	m33_res1 [label="Residual Add\nGPU: 3745" fillcolor=lightpink style=filled]
	m33_gate [label="Expert Gate\nTop-2 selection\nGPU: 3745" fillcolor=orange shape=parallelogram style=filled]
	m33_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m33_expert0 [label="Expert 0\nNode 240 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m33_expert1 [label="Expert 1\nNode 240 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m33_expert2 [label="Expert 2\nNode 240 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m33_agg [label="Expert Aggregation\nGPU: 3745" fillcolor=purple shape=parallelogram style=filled]
	m33_final [label="Final Residual Add\nGPU: 3745" fillcolor=lightpink style=filled]
	m33_ln1 -> m33_qkv
	m33_qkv -> m33_mha
	m33_mha -> m33_out
	m33_out -> m33_res1
	m32_final -> m33_res1
	m33_res1 -> m33_gate
	m33_gate -> m33_route [style=dashed]
	m33_res1 -> m33_route
	m33_route -> m33_expert0
	m33_expert0 -> m33_agg
	m33_route -> m33_expert1
	m33_expert1 -> m33_agg
	m33_route -> m33_expert2
	m33_expert2 -> m33_agg
	m33_agg -> m33_final
	m33_res1 -> m33_final
	m34_ln1 [label="LayerNorm\nGPU: 3746" fillcolor=lightyellow style=filled]
	m34_qkv [label="QKV Projection\nGPU: 3746" fillcolor=lightblue style=filled]
	m34_mha [label="MHA Attention\nGPU: 3746" fillcolor=lightblue style=filled]
	m34_out [label="Output Projection\nGPU: 3746" fillcolor=lightblue style=filled]
	m34_res1 [label="Residual Add\nGPU: 3746" fillcolor=lightpink style=filled]
	m34_gate [label="Expert Gate\nTop-2 selection\nGPU: 3746" fillcolor=orange shape=parallelogram style=filled]
	m34_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m34_expert0 [label="Expert 0\nNode 248 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m34_expert1 [label="Expert 1\nNode 248 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m34_expert2 [label="Expert 2\nNode 248 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m34_agg [label="Expert Aggregation\nGPU: 3746" fillcolor=purple shape=parallelogram style=filled]
	m34_final [label="Final Residual Add\nGPU: 3746" fillcolor=lightpink style=filled]
	m34_ln1 -> m34_qkv
	m34_qkv -> m34_mha
	m34_mha -> m34_out
	m34_out -> m34_res1
	m33_final -> m34_res1
	m34_res1 -> m34_gate
	m34_gate -> m34_route [style=dashed]
	m34_res1 -> m34_route
	m34_route -> m34_expert0
	m34_expert0 -> m34_agg
	m34_route -> m34_expert1
	m34_expert1 -> m34_agg
	m34_route -> m34_expert2
	m34_expert2 -> m34_agg
	m34_agg -> m34_final
	m34_res1 -> m34_final
	m35_ln1 [label="LayerNorm\nGPU: 3747" fillcolor=lightyellow style=filled]
	m35_qkv [label="QKV Projection\nGPU: 3747" fillcolor=lightblue style=filled]
	m35_mha [label="MHA Attention\nGPU: 3747" fillcolor=lightblue style=filled]
	m35_out [label="Output Projection\nGPU: 3747" fillcolor=lightblue style=filled]
	m35_res1 [label="Residual Add\nGPU: 3747" fillcolor=lightpink style=filled]
	m35_gate [label="Expert Gate\nTop-2 selection\nGPU: 3747" fillcolor=orange shape=parallelogram style=filled]
	m35_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m35_expert0 [label="Expert 0\nNode 256 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m35_expert1 [label="Expert 1\nNode 256 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m35_expert2 [label="Expert 2\nNode 256 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m35_agg [label="Expert Aggregation\nGPU: 3747" fillcolor=purple shape=parallelogram style=filled]
	m35_final [label="Final Residual Add\nGPU: 3747" fillcolor=lightpink style=filled]
	m35_ln1 -> m35_qkv
	m35_qkv -> m35_mha
	m35_mha -> m35_out
	m35_out -> m35_res1
	m34_final -> m35_res1
	m35_res1 -> m35_gate
	m35_gate -> m35_route [style=dashed]
	m35_res1 -> m35_route
	m35_route -> m35_expert0
	m35_expert0 -> m35_agg
	m35_route -> m35_expert1
	m35_expert1 -> m35_agg
	m35_route -> m35_expert2
	m35_expert2 -> m35_agg
	m35_agg -> m35_final
	m35_res1 -> m35_final
	m36_ln1 [label="LayerNorm\nGPU: 3748" fillcolor=lightyellow style=filled]
	m36_qkv [label="QKV Projection\nGPU: 3748" fillcolor=lightblue style=filled]
	m36_mha [label="MHA Attention\nGPU: 3748" fillcolor=lightblue style=filled]
	m36_out [label="Output Projection\nGPU: 3748" fillcolor=lightblue style=filled]
	m36_res1 [label="Residual Add\nGPU: 3748" fillcolor=lightpink style=filled]
	m36_gate [label="Expert Gate\nTop-2 selection\nGPU: 3748" fillcolor=orange shape=parallelogram style=filled]
	m36_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m36_expert0 [label="Expert 0\nNode 264 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m36_expert1 [label="Expert 1\nNode 264 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m36_expert2 [label="Expert 2\nNode 264 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m36_agg [label="Expert Aggregation\nGPU: 3748" fillcolor=purple shape=parallelogram style=filled]
	m36_final [label="Final Residual Add\nGPU: 3748" fillcolor=lightpink style=filled]
	m36_ln1 -> m36_qkv
	m36_qkv -> m36_mha
	m36_mha -> m36_out
	m36_out -> m36_res1
	m35_final -> m36_res1
	m36_res1 -> m36_gate
	m36_gate -> m36_route [style=dashed]
	m36_res1 -> m36_route
	m36_route -> m36_expert0
	m36_expert0 -> m36_agg
	m36_route -> m36_expert1
	m36_expert1 -> m36_agg
	m36_route -> m36_expert2
	m36_expert2 -> m36_agg
	m36_agg -> m36_final
	m36_res1 -> m36_final
	m37_ln1 [label="LayerNorm\nGPU: 3749" fillcolor=lightyellow style=filled]
	m37_qkv [label="QKV Projection\nGPU: 3749" fillcolor=lightblue style=filled]
	m37_mha [label="MHA Attention\nGPU: 3749" fillcolor=lightblue style=filled]
	m37_out [label="Output Projection\nGPU: 3749" fillcolor=lightblue style=filled]
	m37_res1 [label="Residual Add\nGPU: 3749" fillcolor=lightpink style=filled]
	m37_gate [label="Expert Gate\nTop-2 selection\nGPU: 3749" fillcolor=orange shape=parallelogram style=filled]
	m37_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m37_expert0 [label="Expert 0\nNode 272 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m37_expert1 [label="Expert 1\nNode 272 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m37_expert2 [label="Expert 2\nNode 272 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m37_agg [label="Expert Aggregation\nGPU: 3749" fillcolor=purple shape=parallelogram style=filled]
	m37_final [label="Final Residual Add\nGPU: 3749" fillcolor=lightpink style=filled]
	m37_ln1 -> m37_qkv
	m37_qkv -> m37_mha
	m37_mha -> m37_out
	m37_out -> m37_res1
	m36_final -> m37_res1
	m37_res1 -> m37_gate
	m37_gate -> m37_route [style=dashed]
	m37_res1 -> m37_route
	m37_route -> m37_expert0
	m37_expert0 -> m37_agg
	m37_route -> m37_expert1
	m37_expert1 -> m37_agg
	m37_route -> m37_expert2
	m37_expert2 -> m37_agg
	m37_agg -> m37_final
	m37_res1 -> m37_final
	m38_ln1 [label="LayerNorm\nGPU: 3750" fillcolor=lightyellow style=filled]
	m38_qkv [label="QKV Projection\nGPU: 3750" fillcolor=lightblue style=filled]
	m38_mha [label="MHA Attention\nGPU: 3750" fillcolor=lightblue style=filled]
	m38_out [label="Output Projection\nGPU: 3750" fillcolor=lightblue style=filled]
	m38_res1 [label="Residual Add\nGPU: 3750" fillcolor=lightpink style=filled]
	m38_gate [label="Expert Gate\nTop-2 selection\nGPU: 3750" fillcolor=orange shape=parallelogram style=filled]
	m38_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m38_expert0 [label="Expert 0\nNode 280 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m38_expert1 [label="Expert 1\nNode 280 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m38_expert2 [label="Expert 2\nNode 280 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m38_agg [label="Expert Aggregation\nGPU: 3750" fillcolor=purple shape=parallelogram style=filled]
	m38_final [label="Final Residual Add\nGPU: 3750" fillcolor=lightpink style=filled]
	m38_ln1 -> m38_qkv
	m38_qkv -> m38_mha
	m38_mha -> m38_out
	m38_out -> m38_res1
	m37_final -> m38_res1
	m38_res1 -> m38_gate
	m38_gate -> m38_route [style=dashed]
	m38_res1 -> m38_route
	m38_route -> m38_expert0
	m38_expert0 -> m38_agg
	m38_route -> m38_expert1
	m38_expert1 -> m38_agg
	m38_route -> m38_expert2
	m38_expert2 -> m38_agg
	m38_agg -> m38_final
	m38_res1 -> m38_final
	m39_ln1 [label="LayerNorm\nGPU: 3751" fillcolor=lightyellow style=filled]
	m39_qkv [label="QKV Projection\nGPU: 3751" fillcolor=lightblue style=filled]
	m39_mha [label="MHA Attention\nGPU: 3751" fillcolor=lightblue style=filled]
	m39_out [label="Output Projection\nGPU: 3751" fillcolor=lightblue style=filled]
	m39_res1 [label="Residual Add\nGPU: 3751" fillcolor=lightpink style=filled]
	m39_gate [label="Expert Gate\nTop-2 selection\nGPU: 3751" fillcolor=orange shape=parallelogram style=filled]
	m39_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m39_expert0 [label="Expert 0\nNode 288 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m39_expert1 [label="Expert 1\nNode 288 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m39_expert2 [label="Expert 2\nNode 288 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m39_agg [label="Expert Aggregation\nGPU: 3751" fillcolor=purple shape=parallelogram style=filled]
	m39_final [label="Final Residual Add\nGPU: 3751" fillcolor=lightpink style=filled]
	m39_ln1 -> m39_qkv
	m39_qkv -> m39_mha
	m39_mha -> m39_out
	m39_out -> m39_res1
	m38_final -> m39_res1
	m39_res1 -> m39_gate
	m39_gate -> m39_route [style=dashed]
	m39_res1 -> m39_route
	m39_route -> m39_expert0
	m39_expert0 -> m39_agg
	m39_route -> m39_expert1
	m39_expert1 -> m39_agg
	m39_route -> m39_expert2
	m39_expert2 -> m39_agg
	m39_agg -> m39_final
	m39_res1 -> m39_final
	m40_ln1 [label="LayerNorm\nGPU: 3752" fillcolor=lightyellow style=filled]
	m40_qkv [label="QKV Projection\nGPU: 3752" fillcolor=lightblue style=filled]
	m40_mha [label="MHA Attention\nGPU: 3752" fillcolor=lightblue style=filled]
	m40_out [label="Output Projection\nGPU: 3752" fillcolor=lightblue style=filled]
	m40_res1 [label="Residual Add\nGPU: 3752" fillcolor=lightpink style=filled]
	m40_gate [label="Expert Gate\nTop-2 selection\nGPU: 3752" fillcolor=orange shape=parallelogram style=filled]
	m40_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m40_expert0 [label="Expert 0\nNode 296 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m40_expert1 [label="Expert 1\nNode 296 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m40_expert2 [label="Expert 2\nNode 296 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m40_agg [label="Expert Aggregation\nGPU: 3752" fillcolor=purple shape=parallelogram style=filled]
	m40_final [label="Final Residual Add\nGPU: 3752" fillcolor=lightpink style=filled]
	m40_ln1 -> m40_qkv
	m40_qkv -> m40_mha
	m40_mha -> m40_out
	m40_out -> m40_res1
	m39_final -> m40_res1
	m40_res1 -> m40_gate
	m40_gate -> m40_route [style=dashed]
	m40_res1 -> m40_route
	m40_route -> m40_expert0
	m40_expert0 -> m40_agg
	m40_route -> m40_expert1
	m40_expert1 -> m40_agg
	m40_route -> m40_expert2
	m40_expert2 -> m40_agg
	m40_agg -> m40_final
	m40_res1 -> m40_final
	m41_ln1 [label="LayerNorm\nGPU: 3753" fillcolor=lightyellow style=filled]
	m41_qkv [label="QKV Projection\nGPU: 3753" fillcolor=lightblue style=filled]
	m41_mha [label="MHA Attention\nGPU: 3753" fillcolor=lightblue style=filled]
	m41_out [label="Output Projection\nGPU: 3753" fillcolor=lightblue style=filled]
	m41_res1 [label="Residual Add\nGPU: 3753" fillcolor=lightpink style=filled]
	m41_gate [label="Expert Gate\nTop-2 selection\nGPU: 3753" fillcolor=orange shape=parallelogram style=filled]
	m41_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m41_expert0 [label="Expert 0\nNode 304 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m41_expert1 [label="Expert 1\nNode 304 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m41_expert2 [label="Expert 2\nNode 304 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m41_agg [label="Expert Aggregation\nGPU: 3753" fillcolor=purple shape=parallelogram style=filled]
	m41_final [label="Final Residual Add\nGPU: 3753" fillcolor=lightpink style=filled]
	m41_ln1 -> m41_qkv
	m41_qkv -> m41_mha
	m41_mha -> m41_out
	m41_out -> m41_res1
	m40_final -> m41_res1
	m41_res1 -> m41_gate
	m41_gate -> m41_route [style=dashed]
	m41_res1 -> m41_route
	m41_route -> m41_expert0
	m41_expert0 -> m41_agg
	m41_route -> m41_expert1
	m41_expert1 -> m41_agg
	m41_route -> m41_expert2
	m41_expert2 -> m41_agg
	m41_agg -> m41_final
	m41_res1 -> m41_final
	m42_ln1 [label="LayerNorm\nGPU: 3754" fillcolor=lightyellow style=filled]
	m42_qkv [label="QKV Projection\nGPU: 3754" fillcolor=lightblue style=filled]
	m42_mha [label="MHA Attention\nGPU: 3754" fillcolor=lightblue style=filled]
	m42_out [label="Output Projection\nGPU: 3754" fillcolor=lightblue style=filled]
	m42_res1 [label="Residual Add\nGPU: 3754" fillcolor=lightpink style=filled]
	m42_gate [label="Expert Gate\nTop-2 selection\nGPU: 3754" fillcolor=orange shape=parallelogram style=filled]
	m42_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m42_expert0 [label="Expert 0\nNode 312 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m42_expert1 [label="Expert 1\nNode 312 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m42_expert2 [label="Expert 2\nNode 312 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m42_agg [label="Expert Aggregation\nGPU: 3754" fillcolor=purple shape=parallelogram style=filled]
	m42_final [label="Final Residual Add\nGPU: 3754" fillcolor=lightpink style=filled]
	m42_ln1 -> m42_qkv
	m42_qkv -> m42_mha
	m42_mha -> m42_out
	m42_out -> m42_res1
	m41_final -> m42_res1
	m42_res1 -> m42_gate
	m42_gate -> m42_route [style=dashed]
	m42_res1 -> m42_route
	m42_route -> m42_expert0
	m42_expert0 -> m42_agg
	m42_route -> m42_expert1
	m42_expert1 -> m42_agg
	m42_route -> m42_expert2
	m42_expert2 -> m42_agg
	m42_agg -> m42_final
	m42_res1 -> m42_final
	m43_ln1 [label="LayerNorm\nGPU: 3755" fillcolor=lightyellow style=filled]
	m43_qkv [label="QKV Projection\nGPU: 3755" fillcolor=lightblue style=filled]
	m43_mha [label="MHA Attention\nGPU: 3755" fillcolor=lightblue style=filled]
	m43_out [label="Output Projection\nGPU: 3755" fillcolor=lightblue style=filled]
	m43_res1 [label="Residual Add\nGPU: 3755" fillcolor=lightpink style=filled]
	m43_gate [label="Expert Gate\nTop-2 selection\nGPU: 3755" fillcolor=orange shape=parallelogram style=filled]
	m43_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m43_expert0 [label="Expert 0\nNode 320 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m43_expert1 [label="Expert 1\nNode 320 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m43_expert2 [label="Expert 2\nNode 320 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m43_agg [label="Expert Aggregation\nGPU: 3755" fillcolor=purple shape=parallelogram style=filled]
	m43_final [label="Final Residual Add\nGPU: 3755" fillcolor=lightpink style=filled]
	m43_ln1 -> m43_qkv
	m43_qkv -> m43_mha
	m43_mha -> m43_out
	m43_out -> m43_res1
	m42_final -> m43_res1
	m43_res1 -> m43_gate
	m43_gate -> m43_route [style=dashed]
	m43_res1 -> m43_route
	m43_route -> m43_expert0
	m43_expert0 -> m43_agg
	m43_route -> m43_expert1
	m43_expert1 -> m43_agg
	m43_route -> m43_expert2
	m43_expert2 -> m43_agg
	m43_agg -> m43_final
	m43_res1 -> m43_final
	m44_ln1 [label="LayerNorm\nGPU: 3756" fillcolor=lightyellow style=filled]
	m44_qkv [label="QKV Projection\nGPU: 3756" fillcolor=lightblue style=filled]
	m44_mha [label="MHA Attention\nGPU: 3756" fillcolor=lightblue style=filled]
	m44_out [label="Output Projection\nGPU: 3756" fillcolor=lightblue style=filled]
	m44_res1 [label="Residual Add\nGPU: 3756" fillcolor=lightpink style=filled]
	m44_gate [label="Expert Gate\nTop-2 selection\nGPU: 3756" fillcolor=orange shape=parallelogram style=filled]
	m44_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m44_expert0 [label="Expert 0\nNode 328 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m44_expert1 [label="Expert 1\nNode 328 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m44_expert2 [label="Expert 2\nNode 328 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m44_agg [label="Expert Aggregation\nGPU: 3756" fillcolor=purple shape=parallelogram style=filled]
	m44_final [label="Final Residual Add\nGPU: 3756" fillcolor=lightpink style=filled]
	m44_ln1 -> m44_qkv
	m44_qkv -> m44_mha
	m44_mha -> m44_out
	m44_out -> m44_res1
	m43_final -> m44_res1
	m44_res1 -> m44_gate
	m44_gate -> m44_route [style=dashed]
	m44_res1 -> m44_route
	m44_route -> m44_expert0
	m44_expert0 -> m44_agg
	m44_route -> m44_expert1
	m44_expert1 -> m44_agg
	m44_route -> m44_expert2
	m44_expert2 -> m44_agg
	m44_agg -> m44_final
	m44_res1 -> m44_final
	m45_ln1 [label="LayerNorm\nGPU: 3757" fillcolor=lightyellow style=filled]
	m45_qkv [label="QKV Projection\nGPU: 3757" fillcolor=lightblue style=filled]
	m45_mha [label="MHA Attention\nGPU: 3757" fillcolor=lightblue style=filled]
	m45_out [label="Output Projection\nGPU: 3757" fillcolor=lightblue style=filled]
	m45_res1 [label="Residual Add\nGPU: 3757" fillcolor=lightpink style=filled]
	m45_gate [label="Expert Gate\nTop-2 selection\nGPU: 3757" fillcolor=orange shape=parallelogram style=filled]
	m45_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m45_expert0 [label="Expert 0\nNode 336 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m45_expert1 [label="Expert 1\nNode 336 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m45_expert2 [label="Expert 2\nNode 336 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m45_agg [label="Expert Aggregation\nGPU: 3757" fillcolor=purple shape=parallelogram style=filled]
	m45_final [label="Final Residual Add\nGPU: 3757" fillcolor=lightpink style=filled]
	m45_ln1 -> m45_qkv
	m45_qkv -> m45_mha
	m45_mha -> m45_out
	m45_out -> m45_res1
	m44_final -> m45_res1
	m45_res1 -> m45_gate
	m45_gate -> m45_route [style=dashed]
	m45_res1 -> m45_route
	m45_route -> m45_expert0
	m45_expert0 -> m45_agg
	m45_route -> m45_expert1
	m45_expert1 -> m45_agg
	m45_route -> m45_expert2
	m45_expert2 -> m45_agg
	m45_agg -> m45_final
	m45_res1 -> m45_final
	m46_ln1 [label="LayerNorm\nGPU: 3758" fillcolor=lightyellow style=filled]
	m46_qkv [label="QKV Projection\nGPU: 3758" fillcolor=lightblue style=filled]
	m46_mha [label="MHA Attention\nGPU: 3758" fillcolor=lightblue style=filled]
	m46_out [label="Output Projection\nGPU: 3758" fillcolor=lightblue style=filled]
	m46_res1 [label="Residual Add\nGPU: 3758" fillcolor=lightpink style=filled]
	m46_gate [label="Expert Gate\nTop-2 selection\nGPU: 3758" fillcolor=orange shape=parallelogram style=filled]
	m46_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m46_expert0 [label="Expert 0\nNode 344 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m46_expert1 [label="Expert 1\nNode 344 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m46_expert2 [label="Expert 2\nNode 344 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m46_agg [label="Expert Aggregation\nGPU: 3758" fillcolor=purple shape=parallelogram style=filled]
	m46_final [label="Final Residual Add\nGPU: 3758" fillcolor=lightpink style=filled]
	m46_ln1 -> m46_qkv
	m46_qkv -> m46_mha
	m46_mha -> m46_out
	m46_out -> m46_res1
	m45_final -> m46_res1
	m46_res1 -> m46_gate
	m46_gate -> m46_route [style=dashed]
	m46_res1 -> m46_route
	m46_route -> m46_expert0
	m46_expert0 -> m46_agg
	m46_route -> m46_expert1
	m46_expert1 -> m46_agg
	m46_route -> m46_expert2
	m46_expert2 -> m46_agg
	m46_agg -> m46_final
	m46_res1 -> m46_final
	m47_ln1 [label="LayerNorm\nGPU: 3759" fillcolor=lightyellow style=filled]
	m47_qkv [label="QKV Projection\nGPU: 3759" fillcolor=lightblue style=filled]
	m47_mha [label="MHA Attention\nGPU: 3759" fillcolor=lightblue style=filled]
	m47_out [label="Output Projection\nGPU: 3759" fillcolor=lightblue style=filled]
	m47_res1 [label="Residual Add\nGPU: 3759" fillcolor=lightpink style=filled]
	m47_gate [label="Expert Gate\nTop-2 selection\nGPU: 3759" fillcolor=orange shape=parallelogram style=filled]
	m47_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m47_expert0 [label="Expert 0\nNode 352 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m47_expert1 [label="Expert 1\nNode 352 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m47_expert2 [label="Expert 2\nNode 352 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m47_agg [label="Expert Aggregation\nGPU: 3759" fillcolor=purple shape=parallelogram style=filled]
	m47_final [label="Final Residual Add\nGPU: 3759" fillcolor=lightpink style=filled]
	m47_ln1 -> m47_qkv
	m47_qkv -> m47_mha
	m47_mha -> m47_out
	m47_out -> m47_res1
	m46_final -> m47_res1
	m47_res1 -> m47_gate
	m47_gate -> m47_route [style=dashed]
	m47_res1 -> m47_route
	m47_route -> m47_expert0
	m47_expert0 -> m47_agg
	m47_route -> m47_expert1
	m47_expert1 -> m47_agg
	m47_route -> m47_expert2
	m47_expert2 -> m47_agg
	m47_agg -> m47_final
	m47_res1 -> m47_final
	m48_ln1 [label="LayerNorm\nGPU: 3760" fillcolor=lightyellow style=filled]
	m48_qkv [label="QKV Projection\nGPU: 3760" fillcolor=lightblue style=filled]
	m48_mha [label="MHA Attention\nGPU: 3760" fillcolor=lightblue style=filled]
	m48_out [label="Output Projection\nGPU: 3760" fillcolor=lightblue style=filled]
	m48_res1 [label="Residual Add\nGPU: 3760" fillcolor=lightpink style=filled]
	m48_gate [label="Expert Gate\nTop-2 selection\nGPU: 3760" fillcolor=orange shape=parallelogram style=filled]
	m48_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m48_expert0 [label="Expert 0\nNode 360 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m48_expert1 [label="Expert 1\nNode 360 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m48_expert2 [label="Expert 2\nNode 360 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m48_agg [label="Expert Aggregation\nGPU: 3760" fillcolor=purple shape=parallelogram style=filled]
	m48_final [label="Final Residual Add\nGPU: 3760" fillcolor=lightpink style=filled]
	m48_ln1 -> m48_qkv
	m48_qkv -> m48_mha
	m48_mha -> m48_out
	m48_out -> m48_res1
	m47_final -> m48_res1
	m48_res1 -> m48_gate
	m48_gate -> m48_route [style=dashed]
	m48_res1 -> m48_route
	m48_route -> m48_expert0
	m48_expert0 -> m48_agg
	m48_route -> m48_expert1
	m48_expert1 -> m48_agg
	m48_route -> m48_expert2
	m48_expert2 -> m48_agg
	m48_agg -> m48_final
	m48_res1 -> m48_final
	m49_ln1 [label="LayerNorm\nGPU: 3761" fillcolor=lightyellow style=filled]
	m49_qkv [label="QKV Projection\nGPU: 3761" fillcolor=lightblue style=filled]
	m49_mha [label="MHA Attention\nGPU: 3761" fillcolor=lightblue style=filled]
	m49_out [label="Output Projection\nGPU: 3761" fillcolor=lightblue style=filled]
	m49_res1 [label="Residual Add\nGPU: 3761" fillcolor=lightpink style=filled]
	m49_gate [label="Expert Gate\nTop-2 selection\nGPU: 3761" fillcolor=orange shape=parallelogram style=filled]
	m49_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m49_expert0 [label="Expert 0\nNode 368 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m49_expert1 [label="Expert 1\nNode 368 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m49_expert2 [label="Expert 2\nNode 368 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m49_agg [label="Expert Aggregation\nGPU: 3761" fillcolor=purple shape=parallelogram style=filled]
	m49_final [label="Final Residual Add\nGPU: 3761" fillcolor=lightpink style=filled]
	m49_ln1 -> m49_qkv
	m49_qkv -> m49_mha
	m49_mha -> m49_out
	m49_out -> m49_res1
	m48_final -> m49_res1
	m49_res1 -> m49_gate
	m49_gate -> m49_route [style=dashed]
	m49_res1 -> m49_route
	m49_route -> m49_expert0
	m49_expert0 -> m49_agg
	m49_route -> m49_expert1
	m49_expert1 -> m49_agg
	m49_route -> m49_expert2
	m49_expert2 -> m49_agg
	m49_agg -> m49_final
	m49_res1 -> m49_final
	m50_ln1 [label="LayerNorm\nGPU: 3762" fillcolor=lightyellow style=filled]
	m50_qkv [label="QKV Projection\nGPU: 3762" fillcolor=lightblue style=filled]
	m50_mha [label="MHA Attention\nGPU: 3762" fillcolor=lightblue style=filled]
	m50_out [label="Output Projection\nGPU: 3762" fillcolor=lightblue style=filled]
	m50_res1 [label="Residual Add\nGPU: 3762" fillcolor=lightpink style=filled]
	m50_gate [label="Expert Gate\nTop-2 selection\nGPU: 3762" fillcolor=orange shape=parallelogram style=filled]
	m50_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m50_expert0 [label="Expert 0\nNode 376 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m50_expert1 [label="Expert 1\nNode 376 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m50_expert2 [label="Expert 2\nNode 376 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m50_agg [label="Expert Aggregation\nGPU: 3762" fillcolor=purple shape=parallelogram style=filled]
	m50_final [label="Final Residual Add\nGPU: 3762" fillcolor=lightpink style=filled]
	m50_ln1 -> m50_qkv
	m50_qkv -> m50_mha
	m50_mha -> m50_out
	m50_out -> m50_res1
	m49_final -> m50_res1
	m50_res1 -> m50_gate
	m50_gate -> m50_route [style=dashed]
	m50_res1 -> m50_route
	m50_route -> m50_expert0
	m50_expert0 -> m50_agg
	m50_route -> m50_expert1
	m50_expert1 -> m50_agg
	m50_route -> m50_expert2
	m50_expert2 -> m50_agg
	m50_agg -> m50_final
	m50_res1 -> m50_final
	m51_ln1 [label="LayerNorm\nGPU: 3763" fillcolor=lightyellow style=filled]
	m51_qkv [label="QKV Projection\nGPU: 3763" fillcolor=lightblue style=filled]
	m51_mha [label="MHA Attention\nGPU: 3763" fillcolor=lightblue style=filled]
	m51_out [label="Output Projection\nGPU: 3763" fillcolor=lightblue style=filled]
	m51_res1 [label="Residual Add\nGPU: 3763" fillcolor=lightpink style=filled]
	m51_gate [label="Expert Gate\nTop-2 selection\nGPU: 3763" fillcolor=orange shape=parallelogram style=filled]
	m51_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m51_expert0 [label="Expert 0\nNode 384 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m51_expert1 [label="Expert 1\nNode 384 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m51_expert2 [label="Expert 2\nNode 384 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m51_agg [label="Expert Aggregation\nGPU: 3763" fillcolor=purple shape=parallelogram style=filled]
	m51_final [label="Final Residual Add\nGPU: 3763" fillcolor=lightpink style=filled]
	m51_ln1 -> m51_qkv
	m51_qkv -> m51_mha
	m51_mha -> m51_out
	m51_out -> m51_res1
	m50_final -> m51_res1
	m51_res1 -> m51_gate
	m51_gate -> m51_route [style=dashed]
	m51_res1 -> m51_route
	m51_route -> m51_expert0
	m51_expert0 -> m51_agg
	m51_route -> m51_expert1
	m51_expert1 -> m51_agg
	m51_route -> m51_expert2
	m51_expert2 -> m51_agg
	m51_agg -> m51_final
	m51_res1 -> m51_final
	m52_ln1 [label="LayerNorm\nGPU: 3764" fillcolor=lightyellow style=filled]
	m52_qkv [label="QKV Projection\nGPU: 3764" fillcolor=lightblue style=filled]
	m52_mha [label="MHA Attention\nGPU: 3764" fillcolor=lightblue style=filled]
	m52_out [label="Output Projection\nGPU: 3764" fillcolor=lightblue style=filled]
	m52_res1 [label="Residual Add\nGPU: 3764" fillcolor=lightpink style=filled]
	m52_gate [label="Expert Gate\nTop-2 selection\nGPU: 3764" fillcolor=orange shape=parallelogram style=filled]
	m52_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m52_expert0 [label="Expert 0\nNode 392 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m52_expert1 [label="Expert 1\nNode 392 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m52_expert2 [label="Expert 2\nNode 392 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m52_agg [label="Expert Aggregation\nGPU: 3764" fillcolor=purple shape=parallelogram style=filled]
	m52_final [label="Final Residual Add\nGPU: 3764" fillcolor=lightpink style=filled]
	m52_ln1 -> m52_qkv
	m52_qkv -> m52_mha
	m52_mha -> m52_out
	m52_out -> m52_res1
	m51_final -> m52_res1
	m52_res1 -> m52_gate
	m52_gate -> m52_route [style=dashed]
	m52_res1 -> m52_route
	m52_route -> m52_expert0
	m52_expert0 -> m52_agg
	m52_route -> m52_expert1
	m52_expert1 -> m52_agg
	m52_route -> m52_expert2
	m52_expert2 -> m52_agg
	m52_agg -> m52_final
	m52_res1 -> m52_final
	m53_ln1 [label="LayerNorm\nGPU: 3765" fillcolor=lightyellow style=filled]
	m53_qkv [label="QKV Projection\nGPU: 3765" fillcolor=lightblue style=filled]
	m53_mha [label="MHA Attention\nGPU: 3765" fillcolor=lightblue style=filled]
	m53_out [label="Output Projection\nGPU: 3765" fillcolor=lightblue style=filled]
	m53_res1 [label="Residual Add\nGPU: 3765" fillcolor=lightpink style=filled]
	m53_gate [label="Expert Gate\nTop-2 selection\nGPU: 3765" fillcolor=orange shape=parallelogram style=filled]
	m53_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m53_expert0 [label="Expert 0\nNode 400 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m53_expert1 [label="Expert 1\nNode 400 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m53_expert2 [label="Expert 2\nNode 400 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m53_agg [label="Expert Aggregation\nGPU: 3765" fillcolor=purple shape=parallelogram style=filled]
	m53_final [label="Final Residual Add\nGPU: 3765" fillcolor=lightpink style=filled]
	m53_ln1 -> m53_qkv
	m53_qkv -> m53_mha
	m53_mha -> m53_out
	m53_out -> m53_res1
	m52_final -> m53_res1
	m53_res1 -> m53_gate
	m53_gate -> m53_route [style=dashed]
	m53_res1 -> m53_route
	m53_route -> m53_expert0
	m53_expert0 -> m53_agg
	m53_route -> m53_expert1
	m53_expert1 -> m53_agg
	m53_route -> m53_expert2
	m53_expert2 -> m53_agg
	m53_agg -> m53_final
	m53_res1 -> m53_final
	m54_ln1 [label="LayerNorm\nGPU: 3766" fillcolor=lightyellow style=filled]
	m54_qkv [label="QKV Projection\nGPU: 3766" fillcolor=lightblue style=filled]
	m54_mha [label="MHA Attention\nGPU: 3766" fillcolor=lightblue style=filled]
	m54_out [label="Output Projection\nGPU: 3766" fillcolor=lightblue style=filled]
	m54_res1 [label="Residual Add\nGPU: 3766" fillcolor=lightpink style=filled]
	m54_gate [label="Expert Gate\nTop-2 selection\nGPU: 3766" fillcolor=orange shape=parallelogram style=filled]
	m54_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m54_expert0 [label="Expert 0\nNode 408 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m54_expert1 [label="Expert 1\nNode 408 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m54_expert2 [label="Expert 2\nNode 408 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m54_agg [label="Expert Aggregation\nGPU: 3766" fillcolor=purple shape=parallelogram style=filled]
	m54_final [label="Final Residual Add\nGPU: 3766" fillcolor=lightpink style=filled]
	m54_ln1 -> m54_qkv
	m54_qkv -> m54_mha
	m54_mha -> m54_out
	m54_out -> m54_res1
	m53_final -> m54_res1
	m54_res1 -> m54_gate
	m54_gate -> m54_route [style=dashed]
	m54_res1 -> m54_route
	m54_route -> m54_expert0
	m54_expert0 -> m54_agg
	m54_route -> m54_expert1
	m54_expert1 -> m54_agg
	m54_route -> m54_expert2
	m54_expert2 -> m54_agg
	m54_agg -> m54_final
	m54_res1 -> m54_final
	m55_ln1 [label="LayerNorm\nGPU: 3767" fillcolor=lightyellow style=filled]
	m55_qkv [label="QKV Projection\nGPU: 3767" fillcolor=lightblue style=filled]
	m55_mha [label="MHA Attention\nGPU: 3767" fillcolor=lightblue style=filled]
	m55_out [label="Output Projection\nGPU: 3767" fillcolor=lightblue style=filled]
	m55_res1 [label="Residual Add\nGPU: 3767" fillcolor=lightpink style=filled]
	m55_gate [label="Expert Gate\nTop-2 selection\nGPU: 3767" fillcolor=orange shape=parallelogram style=filled]
	m55_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m55_expert0 [label="Expert 0\nNode 416 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m55_expert1 [label="Expert 1\nNode 416 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m55_expert2 [label="Expert 2\nNode 416 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m55_agg [label="Expert Aggregation\nGPU: 3767" fillcolor=purple shape=parallelogram style=filled]
	m55_final [label="Final Residual Add\nGPU: 3767" fillcolor=lightpink style=filled]
	m55_ln1 -> m55_qkv
	m55_qkv -> m55_mha
	m55_mha -> m55_out
	m55_out -> m55_res1
	m54_final -> m55_res1
	m55_res1 -> m55_gate
	m55_gate -> m55_route [style=dashed]
	m55_res1 -> m55_route
	m55_route -> m55_expert0
	m55_expert0 -> m55_agg
	m55_route -> m55_expert1
	m55_expert1 -> m55_agg
	m55_route -> m55_expert2
	m55_expert2 -> m55_agg
	m55_agg -> m55_final
	m55_res1 -> m55_final
	m56_ln1 [label="LayerNorm\nGPU: 3768" fillcolor=lightyellow style=filled]
	m56_qkv [label="QKV Projection\nGPU: 3768" fillcolor=lightblue style=filled]
	m56_mha [label="MHA Attention\nGPU: 3768" fillcolor=lightblue style=filled]
	m56_out [label="Output Projection\nGPU: 3768" fillcolor=lightblue style=filled]
	m56_res1 [label="Residual Add\nGPU: 3768" fillcolor=lightpink style=filled]
	m56_gate [label="Expert Gate\nTop-2 selection\nGPU: 3768" fillcolor=orange shape=parallelogram style=filled]
	m56_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m56_expert0 [label="Expert 0\nNode 424 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m56_expert1 [label="Expert 1\nNode 424 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m56_expert2 [label="Expert 2\nNode 424 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m56_agg [label="Expert Aggregation\nGPU: 3768" fillcolor=purple shape=parallelogram style=filled]
	m56_final [label="Final Residual Add\nGPU: 3768" fillcolor=lightpink style=filled]
	m56_ln1 -> m56_qkv
	m56_qkv -> m56_mha
	m56_mha -> m56_out
	m56_out -> m56_res1
	m55_final -> m56_res1
	m56_res1 -> m56_gate
	m56_gate -> m56_route [style=dashed]
	m56_res1 -> m56_route
	m56_route -> m56_expert0
	m56_expert0 -> m56_agg
	m56_route -> m56_expert1
	m56_expert1 -> m56_agg
	m56_route -> m56_expert2
	m56_expert2 -> m56_agg
	m56_agg -> m56_final
	m56_res1 -> m56_final
	m57_ln1 [label="LayerNorm\nGPU: 3769" fillcolor=lightyellow style=filled]
	m57_qkv [label="QKV Projection\nGPU: 3769" fillcolor=lightblue style=filled]
	m57_mha [label="MHA Attention\nGPU: 3769" fillcolor=lightblue style=filled]
	m57_out [label="Output Projection\nGPU: 3769" fillcolor=lightblue style=filled]
	m57_res1 [label="Residual Add\nGPU: 3769" fillcolor=lightpink style=filled]
	m57_gate [label="Expert Gate\nTop-2 selection\nGPU: 3769" fillcolor=orange shape=parallelogram style=filled]
	m57_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m57_expert0 [label="Expert 0\nNode 432 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m57_expert1 [label="Expert 1\nNode 432 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m57_expert2 [label="Expert 2\nNode 432 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m57_agg [label="Expert Aggregation\nGPU: 3769" fillcolor=purple shape=parallelogram style=filled]
	m57_final [label="Final Residual Add\nGPU: 3769" fillcolor=lightpink style=filled]
	m57_ln1 -> m57_qkv
	m57_qkv -> m57_mha
	m57_mha -> m57_out
	m57_out -> m57_res1
	m56_final -> m57_res1
	m57_res1 -> m57_gate
	m57_gate -> m57_route [style=dashed]
	m57_res1 -> m57_route
	m57_route -> m57_expert0
	m57_expert0 -> m57_agg
	m57_route -> m57_expert1
	m57_expert1 -> m57_agg
	m57_route -> m57_expert2
	m57_expert2 -> m57_agg
	m57_agg -> m57_final
	m57_res1 -> m57_final
	m58_ln1 [label="LayerNorm\nGPU: 3770" fillcolor=lightyellow style=filled]
	m58_qkv [label="QKV Projection\nGPU: 3770" fillcolor=lightblue style=filled]
	m58_mha [label="MHA Attention\nGPU: 3770" fillcolor=lightblue style=filled]
	m58_out [label="Output Projection\nGPU: 3770" fillcolor=lightblue style=filled]
	m58_res1 [label="Residual Add\nGPU: 3770" fillcolor=lightpink style=filled]
	m58_gate [label="Expert Gate\nTop-2 selection\nGPU: 3770" fillcolor=orange shape=parallelogram style=filled]
	m58_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m58_expert0 [label="Expert 0\nNode 440 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m58_expert1 [label="Expert 1\nNode 440 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m58_expert2 [label="Expert 2\nNode 440 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m58_agg [label="Expert Aggregation\nGPU: 3770" fillcolor=purple shape=parallelogram style=filled]
	m58_final [label="Final Residual Add\nGPU: 3770" fillcolor=lightpink style=filled]
	m58_ln1 -> m58_qkv
	m58_qkv -> m58_mha
	m58_mha -> m58_out
	m58_out -> m58_res1
	m57_final -> m58_res1
	m58_res1 -> m58_gate
	m58_gate -> m58_route [style=dashed]
	m58_res1 -> m58_route
	m58_route -> m58_expert0
	m58_expert0 -> m58_agg
	m58_route -> m58_expert1
	m58_expert1 -> m58_agg
	m58_route -> m58_expert2
	m58_expert2 -> m58_agg
	m58_agg -> m58_final
	m58_res1 -> m58_final
	m59_ln1 [label="LayerNorm\nGPU: 3771" fillcolor=lightyellow style=filled]
	m59_qkv [label="QKV Projection\nGPU: 3771" fillcolor=lightblue style=filled]
	m59_mha [label="MHA Attention\nGPU: 3771" fillcolor=lightblue style=filled]
	m59_out [label="Output Projection\nGPU: 3771" fillcolor=lightblue style=filled]
	m59_res1 [label="Residual Add\nGPU: 3771" fillcolor=lightpink style=filled]
	m59_gate [label="Expert Gate\nTop-2 selection\nGPU: 3771" fillcolor=orange shape=parallelogram style=filled]
	m59_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m59_expert0 [label="Expert 0\nNode 448 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m59_expert1 [label="Expert 1\nNode 448 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m59_expert2 [label="Expert 2\nNode 448 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m59_agg [label="Expert Aggregation\nGPU: 3771" fillcolor=purple shape=parallelogram style=filled]
	m59_final [label="Final Residual Add\nGPU: 3771" fillcolor=lightpink style=filled]
	m59_ln1 -> m59_qkv
	m59_qkv -> m59_mha
	m59_mha -> m59_out
	m59_out -> m59_res1
	m58_final -> m59_res1
	m59_res1 -> m59_gate
	m59_gate -> m59_route [style=dashed]
	m59_res1 -> m59_route
	m59_route -> m59_expert0
	m59_expert0 -> m59_agg
	m59_route -> m59_expert1
	m59_expert1 -> m59_agg
	m59_route -> m59_expert2
	m59_expert2 -> m59_agg
	m59_agg -> m59_final
	m59_res1 -> m59_final
	m60_ln1 [label="LayerNorm\nGPU: 3772" fillcolor=lightyellow style=filled]
	m60_qkv [label="QKV Projection\nGPU: 3772" fillcolor=lightblue style=filled]
	m60_mha [label="MHA Attention\nGPU: 3772" fillcolor=lightblue style=filled]
	m60_out [label="Output Projection\nGPU: 3772" fillcolor=lightblue style=filled]
	m60_res1 [label="Residual Add\nGPU: 3772" fillcolor=lightpink style=filled]
	m60_gate [label="Expert Gate\nTop-2 selection\nGPU: 3772" fillcolor=orange shape=parallelogram style=filled]
	m60_route [label="Expert Router\nDistribute tokens\nGPU: All" fillcolor=lightgray shape=ellipse style=filled]
	m60_expert0 [label="Expert 0\nNode 456 GPU 0\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m60_expert1 [label="Expert 1\nNode 456 GPU 1\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m60_expert2 [label="Expert 2\nNode 456 GPU 2\nInput: batch_size=32, seq_len=2048, hidden_size=7168" fillcolor=lightblue style=filled]
	m60_agg [label="Expert Aggregation\nGPU: 3772" fillcolor=purple shape=parallelogram style=filled]
	m60_final [label="Final Residual Add\nGPU: 3772" fillcolor=lightpink style=filled]
	m60_ln1 -> m60_qkv
	m60_qkv -> m60_mha
	m60_mha -> m60_out
	m60_out -> m60_res1
	m59_final -> m60_res1
	m60_res1 -> m60_gate
	m60_gate -> m60_route [style=dashed]
	m60_res1 -> m60_route
	m60_route -> m60_expert0
	m60_expert0 -> m60_agg
	m60_route -> m60_expert1
	m60_expert1 -> m60_agg
	m60_route -> m60_expert2
	m60_expert2 -> m60_agg
	m60_agg -> m60_final
	m60_res1 -> m60_final
	output [label="OUTPUT\nbatch_size=32, seq_len=2048, hidden_size=7168\nGPU: All" fillcolor=lightcoral shape=ellipse style=filled]
	m60_final -> output
}
