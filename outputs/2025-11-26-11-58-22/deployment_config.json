{
  "deployment_config": {
    "model_specifications": {
      "architecture": "61-layer MoE Transformer",
      "layers": {
        "dense_layers": 3,
        "moe_layers": 58,
        "total_layers": 61
      },
      "dimensions": {
        "token_dimension": 7168,
        "mha_heads": 128,
        "head_dimension": 128,
        "mlp_hidden_size": 2048,
        "precision": "BF16"
      },
      "experts": {
        "experts_per_moe_layer": 64,
        "total_experts": 3712,
        "expert_memory_mb": 29.36
      }
    },
    "hardware_setup": {
      "total_gpus": 3904,
      "nodes": 488,
      "gpus_per_node": 8,
      "gpu_type": "H100",
      "gpu_memory_gb": 64,
      "single_gpu_flops": "400TFlops",
      "mfu_utilization": "60%",
      "effective_compute": "240TFlops",
      "vram_bandwidth": "1.8TBps",
      "bandwidth_utilization": "80%",
      "effective_bandwidth": "1.44TBps"
    },
    "parallel_strategies": {
      "proposed_method": {
        "name": "Large-Scale Cross-Node Expert Parallelism",
        "expert_parallelism_degree": 3712,
        "strategy": "one_expert_per_gpu",
        "cross_node_distribution": true,
        "gpus_used": 3715,
        "unused_gpus": 189,
        "placement_algorithm": "layer_index * 64 + expert_index",
        "device_mapping": {
          "dense_layers": {
            "layer_0": {
              "gpu_id": 3712,
              "node_id": 464,
              "gpu_within_node": 0
            },
            "layer_1": {
              "gpu_id": 3713,
              "node_id": 464,
              "gpu_within_node": 1
            },
            "layer_2": {
              "gpu_id": 3714,
              "node_id": 464,
              "gpu_within_node": 2
            }
          },
          "moe_layers": {
            "layer_3": {
              "expert_0": {"gpu_id": 0, "node_id": 0, "gpu_within_node": 0},
              "expert_1": {"gpu_id": 1, "node_id": 0, "gpu_within_node": 1},
              "expert_2": {"gpu_id": 2, "node_id": 0, "gpu_within_node": 2},
              "expert_3": {"gpu_id": 3, "node_id": 0, "gpu_within_node": 3},
              "expert_4": {"gpu_id": 4, "node_id": 0, "gpu_within_node": 4},
              "expert_5": {"gpu_id": 5, "node_id": 0, "gpu_within_node": 5},
              "expert_6": {"gpu_id": 6, "node_id": 0, "gpu_within_node": 6},
              "expert_7": {"gpu_id": 7, "node_id": 0, "gpu_within_node": 7},
              "expert_8": {"gpu_id": 8, "node_id": 1, "gpu_within_node": 0},
              "expert_9": {"gpu_id": 9, "node_id": 1, "gpu_within_node": 1},
              "expert_10": {"gpu_id": 10, "node_id": 1, "gpu_within_node": 2},
              "expert_11": {"gpu_id": 11, "node_id": 1, "gpu_within_node": 3},
              "expert_12": {"gpu_id": 12, "node_id": 1, "gpu_within_node": 4},
              "expert_13": {"gpu_id": 13, "node_id": 1, "gpu_within_node": 5},
              "expert_14": {"gpu_id": 14, "node_id": 1, "gpu_within_node": 6},
              "expert_15": {"gpu_id": 15, "node_id": 1, "gpu_within_node": 7},
              "expert_16": {"gpu_id": 16, "node_id": 2, "gpu_within_node": 0},
              "expert_17": {"gpu_id": 17, "node_id": 2, "gpu_within_node": 1},
              "expert_18": {"gpu_id": 18, "node_id": 2, "gpu_within_node": 2},
              "expert_19": {"gpu_id": 19, "node_id": 2, "gpu_within_node": 3},
              "expert_20": {"gpu_id": 20, "node_id": 2, "gpu_within_node": 4},
              "expert_21": {"gpu_id": 21, "node_id": 2, "gpu_within_node": 5},
              "expert_22": {"gpu_id": 22, "node_id": 2, "gpu_within_node": 6},
              "expert_23": {"gpu_id": 23, "node_id": 2, "gpu_within_node": 7},
              "expert_24": {"gpu_id": 24, "node_id": 3, "gpu_within_node": 0},
              "expert_25": {"gpu_id": 25, "node_id": 3, "gpu_within_node": 1},
              "expert_26": {"gpu_id": 26, "node_id": 3, "gpu_within_node": 2},
              "expert_27": {"gpu_id": 27, "node_id": 3, "gpu_within_node": 3},
              "expert_28": {"gpu_id": 28, "node_id": 3, "gpu_within_node": 4},
              "expert_29": {"gpu_id": 29, "node_id": 3, "gpu_within_node": 5},
              "expert_30": {"gpu_id": 30, "node_id": 3, "gpu_within_node": 6},
              "expert_31": {"gpu_id": 31, "node_id": 3, "gpu_within_node": 7},
              "expert_32": {"gpu_id": 32, "node_id": 4, "gpu_within_node": 0},
              "expert_33": {"gpu_id": 33, "node_id": 4, "gpu_within_node": 1},
              "expert_34": {"gpu_id": 34, "node_id": 4, "gpu_within_node": 2},
              "expert_35": {"gpu_id": 35, "node_id": 4, "gpu_within_node": 3},
              "expert_36": {"gpu_id": 36, "node_id": 4, "gpu_within_node": 4},
              "expert_37": {"gpu_id": 37, "node_id": 4, "gpu_within_node": 5},
              "expert_38": {"gpu_id": 38, "node_id": 4, "gpu_within_node": 6},
              "expert_39": {"gpu_id": 39, "node_id": 4, "gpu_within_node": 7},
              "expert_40": {"gpu_id": 40, "node_id": 5, "gpu_within_node": 0},
              "expert_41": {"gpu_id": 41, "node_id": 5, "gpu_within_node": 1},
              "expert_42": {"gpu_id": 42, "node_id": 5, "gpu_within_node": 2},
              "expert_43": {"gpu_id": 43, "node_id": 5, "gpu_within_node": 3},
              "expert_44": {"gpu_id": 44, "node_id": 5, "gpu_within_node": 4},
              "expert_45": {"gpu_id": 45, "node_id": 5, "gpu_within_node": 5},
              "expert_46": {"gpu_id": 46, "node_id": 5, "gpu_within_node": 6},
              "expert_47": {"gpu_id": 47, "node_id": 5, "gpu_within_node": 7},
              "expert_48": {"gpu_id": 48, "node_id": 6, "gpu_within_node": 0},
              "expert_49": {"gpu_id": 49, "node_id": 6, "gpu_within_node": 1},
              "expert_50": {"gpu_id": 50, "node_id": 6, "gpu_within_node": 2},
              "expert_51": {"gpu_id": 51, "node_id": 6, "gpu_within_node": 3},
              "expert_52": {"gpu_id": 52, "node_id": 6, "gpu_within_node": 4},
              "expert_53": {"gpu_id": 53, "node_id": 6, "gpu_within_node": 5},
              "expert_54": {"gpu_id": 54, "node_id": 6, "gpu_within_node": 6},
              "expert_55": {"gpu_id": 55, "node_id": 6, "gpu_within_node": 7},
              "expert_56": {"gpu_id": 56, "node_id": 7, "gpu_within_node": 0},
              "expert_57": {"gpu_id": 57, "node_id": 7, "gpu_within_node": 1},
              "expert_58": {"gpu_id": 58, "node_id": 7, "gpu_within_node": 2},
              "expert_59": {"gpu_id": 59, "node_id": 7, "gpu_within_node": 3},
              "expert_60": {"gpu_id": 60, "node_id": 7, "gpu_within_node": 4},
              "expert_61": {"gpu_id": 61, "node_id": 7, "gpu_within_node": 5},
              "expert_62": {"gpu_id": 62, "node_id": 7, "gpu_within_node": 6},
              "expert_63": {"gpu_id": 63, "node_id": 7, "gpu_within_node": 7}
            },
            "layer_4": {
              "expert_0": {"gpu_id": 64, "node_id": 8, "gpu_within_node": 0},
              "expert_1": {"gpu_id": 65, "node_id": 8, "gpu_within_node": 1},
              "expert_2": {"gpu_id": 66, "node_id": 8, "gpu_within_node": 2},
              "expert_3": {"gpu_id": 67, "node_id": 8, "gpu_within_node": 3},
              "expert_4": {"gpu_id": 68, "node_id": 8, "gpu_within_node": 4},
              "expert_5": {"gpu_id": 69, "node_id": 8, "gpu_within_node": 5},
              "expert_6": {"gpu_id": 70, "node_id": 8, "gpu_within_node": 6},
              "expert_7": {"gpu_id": 71, "node_id": 8, "gpu_within_node": 7},
              "expert_8": {"gpu_id": 72, "node_id": 9, "gpu_within_node": 0},
              "expert_9": {"gpu_id": 73, "node_id": 9, "gpu_within_node": 1},
              "expert_10": {"gpu_id": 74, "node_id": 9, "gpu_within_node": 2},
              "expert_11": {"gpu_id": 75, "node_id": 9, "gpu_within_node": 3},
              "expert_12": {"gpu_id": 76, "node_id": 9, "gpu_within_node": 4},
              "expert_13": {"gpu_id": 77, "node_id": 9, "gpu_within_node": 5},
              "expert_14": {"gpu_id": 78, "node_id": 9, "gpu_within_node": 6},
              "expert_15": {"gpu_id": 79, "node_id": 9, "gpu_within_node": 7},
              "expert_16": {"gpu_id": 80, "node_id": 10, "gpu_within_node": 0},
              "expert_17": {"gpu_id": 81, "node_id": 10, "gpu_within_node": 1},
              "expert_18": {"gpu_id": 82, "node_id": 10, "gpu_within_node": 2},
              "expert_19": {"gpu_id": 83, "node_id": 10, "gpu_within_node": 3},
              "expert_20": {"gpu_id": 84, "node_id": 10, "gpu_within_node": 4},
              "expert_21": {"gpu_id": 85, "node_id": 10, "gpu_within_node": 5},
              "expert_22": {"gpu_id": 86, "node_id": 10, "gpu_within_node": 6},
              "expert_23": {"gpu_id": 87, "node_id": 10, "gpu_within_node": 7},
              "expert_24": {"gpu_id": 88, "node_id": 11, "gpu_within_node": 0},
              "expert_25": {"gpu_id": 89, "node_id": 11, "gpu_within_node": 1},
              "expert_26": {"gpu_id": 90, "node_id": 11, "gpu_within_node": 2},
              "expert_27": {"gpu_id": 91, "node_id": 11, "gpu_within_node": 3},
              "expert_28": {"gpu_id": 92, "node_id": 11, "gpu_within_node": 4},
              "expert_29": {"gpu_id": 93, "node_id": 11, "gpu_within_node": 5},
              "expert_30": {"gpu_id": 94, "node_id": 11, "gpu_within_node": 6},
              "expert_31": {"gpu_id": 95, "node_id": 11, "gpu_within_node": 7},
              "expert_32": {"gpu_id": 96, "node_id": 12, "gpu_within_node": 0},
              "expert_33": {"gpu_id": 97, "node_id": 12, "gpu_within_node": 1},
              "expert_34": {"gpu_id": 98, "node_id": 12, "gpu_within_node": 2},
              "expert_35": {"gpu_id": 99, "node_id": 12, "gpu_within_node": 3},
              "expert_36": {"gpu_id": 100, "node_id": 12, "gpu_within_node": 4},
              "expert_37": {"gpu_id": 101, "node_id": 12, "gpu_within_node": 5},
              "expert_38": {"gpu_id": 102, "node_id": 12, "gpu_within_node": 6},
              "expert_39": {"gpu_id": 103, "node_id": 12, "gpu_within_node": 7},
              "expert_40": {"gpu_id": 104, "node_id": 13, "gpu_within_node": 0},
              "expert_41": {"gpu_id": 105, "node_id": 13, "gpu_within_node": 1},
              "expert_42": {"gpu_id": 106, "node_id": 13, "gpu_within_node": 2},
              "expert_43": {"gpu_id": 107, "node_id": 13, "gpu_within_node": 3},
              "expert_44": {"gpu_id": 108, "node_id": 13, "gpu_within_node": 4},
              "expert_45": {"gpu_id": 109, "node_id": 13, "gpu_within_node": 5},
              "expert_46": {"gpu_id": 110, "node_id": 13, "gpu_within_node": 6},
              "expert_47": {"gpu_id": 111, "node_id": 13, "gpu_within_node": 7},
              "expert_48": {"gpu_id": 112, "node_id": 14, "gpu_within_node": 0},
              "expert_49": {"gpu_id": 113, "node_id": 14, "gpu_within_node": 1},
              "expert_50": {"gpu_id": 114, "node_id": 14, "gpu_within_node": 2},
              "expert_51": {"gpu_id": 115, "node_id": 14, "gpu_within_node": 3},
              "expert_52": {"gpu_id": 116, "node_id": 14, "gpu_within_node": 4},
              "expert_53": {"gpu_id": 117, "node_id": 14, "gpu_within_node": 5},
              "expert_54": {"gpu_id": 118, "node_id": 14, "gpu_within_node": 6},
              "expert_55": {"gpu_id": 119, "node_id": 14, "gpu_within_node": 7},
              "expert_56": {"gpu_id": 120, "node_id": 15, "gpu_within_node": 0},
              "expert_57": {"gpu_id": 121, "node_id": 15, "gpu_within_node": 1},
              "expert_58": {"gpu_id": 122, "node_id": 15, "gpu_within_node": 2},
              "expert_59": {"gpu_id": 123, "node_id": 15, "gpu_within_node": 3},
              "expert_60": {"gpu_id": 124, "node_id": 15, "gpu_within_node": 4},
              "expert_61": {"gpu_id": 125, "node_id": 15, "gpu_within_node": 5},
              "expert_62": {"gpu_id": 126, "node_id": 15, "gpu_within_node": 6},
              "expert_63": {"gpu_id": 127, "node_id": 15, "gpu_within_node": 7}
            },
            "layer_5_to_59": {
              "note": "Following the same pattern, each layer uses the next 64 GPUs",
              "layer_5": "GPUs 128-191 (Nodes 16-23)",
              "layer_6": "GPUs 192-255 (Nodes 24-31)",
              "layer_7": "GPUs 256-319 (Nodes 32-39)",
              "layer_8": "GPUs 320-383 (Nodes 40-47)",
              "layer_9": "GPUs 384-447 (Nodes 48-55)",
              "layer_10": "GPUs 448-511 (Nodes 56-63)",
              "layer_11": "GPUs 512-575 (Nodes 64-71)",
              "layer_12": "GPUs 576-639 (Nodes 72-79)",
              "layer_13": "GPUs 640-703 (Nodes 80-87)",
              "layer_14": "GPUs 704-767 (Nodes 88-95)",
              "layer_15": "GPUs 768-831 (Nodes 96-103)",
              "layer_16": "GPUs 832-895 (Nodes 104-111)",
              "layer_17": "GPUs 896-959 (Nodes 112-119)",
              "layer_18": "GPUs 960-1023 (Nodes 120-127)",
              "layer_19": "GPUs 1024-1087 (Nodes 128-135)",
              "layer_20": "GPUs 1088-1151 (Nodes 136-143)",
              "layer_21": "GPUs 1152-1215 (Nodes 144-151)",
              "layer_22": "GPUs 1216-1279 (Nodes 152-159)",
              "layer_23": "GPUs 1280-1343 (Nodes 160-167)",
              "layer_24": "GPUs 1344-1407 (Nodes 168-175)",
              "layer_25": "GPUs 1408-1471 (Nodes 176-183)",
              "layer_26": "GPUs 1472-1535 (Nodes 184-191)",
              "layer_27": "GPUs 1536-1599 (Nodes 192-199)",
              "layer_28": "GPUs 1600-1663 (Nodes 200-207)",
              "layer_29": "GPUs 1664-1727 (Nodes 208-215)",
              "layer_30": "GPUs 1728-1791 (Nodes 216-223)",
              "layer_31": "GPUs 1792-1855 (Nodes 224-231)",
              "layer_32": "GPUs 1856-1919 (Nodes 232-239)",
              "layer_33": "GPUs 1920-1983 (Nodes 240-247)",
              "layer_34": "GPUs 1984-2047 (Nodes 248-255)",
              "layer_35": "GPUs 2048-2111 (Nodes 256-263)",
              "layer_36": "GPUs 2112-2175 (Nodes 264-271)",
              "layer_37": "GPUs 2176-2239 (Nodes 272-279)",
              "layer_38": "GPUs 2240-2303 (Nodes 280-287)",
              "layer_39": "GPUs 2304-2367 (Nodes 288-295)",
              "layer_40": "GPUs 2368-2431 (Nodes 296-303)",
              "layer_41": "GPUs 2432-2495 (Nodes 304-311)",
              "layer_42": "GPUs 2496-2559 (Nodes 312-319)",
              "layer_43": "GPUs 2560-2623 (Nodes 320-327)",
              "layer_44": "GPUs 2624-2687 (Nodes 328-335)",
              "layer_45": "GPUs 2688-2751 (Nodes 336-343)",
              "layer_46": "GPUs 2752-2815 (Nodes 344-351)",
              "layer_47": "GPUs 2816-2879 (Nodes 352-359)",
              "layer_48": "GPUs 2880-2943 (Nodes 360-367)",
              "layer_49": "GPUs 2944-3007 (Nodes 368-375)",
              "layer_50": "GPUs 3008-3071 (Nodes 376-383)",
              "layer_51": "GPUs 3072-3135 (Nodes 384-391)",
              "layer_52": "GPUs 3136-3199 (Nodes 392-399)",
              "layer_53": "GPUs 3200-3263 (Nodes 400-407)",
              "layer_54": "GPUs 3264-3327 (Nodes 408-415)",
              "layer_55": "GPUs 3328-3391 (Nodes 416-423)",
              "layer_56": "GPUs 3392-3455 (Nodes 424-431)",
              "layer_57": "GPUs 3456-3519 (Nodes 432-439)",
              "layer_58": "GPUs 3520-3583 (Nodes 440-447)",
              "layer_59": "GPUs 3584-3647 (Nodes 448-455)"
            },
            "layer_60": {
              "expert_0": {"gpu_id": 3648, "node_id": 456, "gpu_within_node": 0},
              "expert_1": {"gpu_id": 3649, "node_id": 456, "gpu_within_node": 1},
              "expert_2": {"gpu_id": 3650, "node_id": 456, "gpu_within_node": 2},
              "expert_3": {"gpu_id": 3651, "node_id": 456, "gpu_within_node": 3},
              "expert_4": {"gpu_id": 3652, "node_id": 456, "gpu_within_node": 4},
              "expert_5": {"gpu_id": 3653, "node_id": 456, "gpu_within_node": 5},
              "expert_6": {"gpu_id": 3654, "node_id": 456, "gpu_within_node": 6},
              "expert_7": {"gpu_id": 3655, "node_id": 456, "gpu_within_node": 7},
              "expert_8": {"gpu_id": 3656, "node_id": 457, "gpu_within_node": 0},
              "expert_9": {"gpu_id": 3657, "node_id": 457, "gpu_within_node": 1},
              "expert_10": {"gpu_id": 3658, "node_id": 457, "gpu_within_node": 2},
              "expert_11": {"gpu_id": 3659, "node_id": 457, "gpu_within_node": 3},
              "expert_12": {"gpu_id": 3660, "node_id": 457, "gpu_within_node": 4},
              "expert_13": {"gpu_id": 3661, "node_id": 457, "gpu_within_node": 5},
              "expert_14": {"gpu_id": 3662, "node_id": 457, "gpu_within_node": 6},
              "expert_15": {"gpu_id": 3663, "node_id": 457, "gpu_within_node": 7},
              "expert_16": {"gpu_id": 3664, "node_id": 458, "gpu_within_node": 0},
              "expert_17": {"gpu_id": 3665, "node_id": 458, "gpu_within_node": 1},
              "expert_18": {"gpu_id": 3666, "node_id": 458, "gpu_within_node": 2},
              "expert_19": {"gpu_id": 3667, "node_id": 458, "gpu_within_node": 3},
              "expert_20": {"gpu_id": 3668, "node_id": 458, "gpu_within_node": 4},
              "expert_21": {"gpu_id": 3669, "node_id": 458, "gpu_within_node": 5},
              "expert_22": {"gpu_id": 3670, "node_id": 458, "gpu_within_node": 6},
              "expert_23": {"gpu_id": 3671, "node_id": 458, "gpu_within_node": 7},
              "expert_24": {"gpu_id": 3672, "node_id": 459, "gpu_within_node": 0},
              "expert_25": {"gpu_id": 3673, "node_id": 459, "gpu_within_node": 1},
              "expert_26": {"gpu_id": 3674, "node_id": 459, "gpu_within_node": 2},
              "expert_27": {"gpu_id": 3675, "node_id": 459, "gpu_within_node": 3},
              "expert_28": {"gpu_id": 3676, "node_id": 459, "gpu_within_node": 4},
              "expert_29": {"gpu_id": 3677, "node_id": 459, "gpu_within_node": 5},
              "expert_30": {"gpu_id": 3678, "node_id": 459, "gpu_within_node": 6},
              "expert_31": {"gpu_id": 3679, "node_id": 459, "gpu_within_node": 7},
              "expert_32": {"gpu_id": 3680, "node_id": 460, "gpu_within_node": 0},
              "expert_33": {"gpu_id": 3681, "node_id": 460, "gpu_within_node": 1},
              "expert_34": {"gpu_id": 3682, "node_id": 460, "gpu_within_node": 2},
              "expert_35": {"gpu_id": 3683, "node_id": 460, "gpu_within_node": 3},
              "expert_36": {"gpu_id": 3684, "node_id": 460, "gpu_within_node": 4},
              "expert_37": {"gpu_id": 3685, "node_id": 460, "gpu_within_node": 5},
              "expert_38": {"gpu_id": 3686, "node_id": 460, "gpu_within_node": 6},
              "expert_39": {"gpu_id": 3687, "node_id": 460, "gpu_within_node": 7},
              "expert_40": {"gpu_id": 3688, "node_id": 461, "gpu_within_node": 0},
              "expert_41": {"gpu_id": 3689, "node_id": 461, "gpu_within_node": 1},
              "expert_42": {"gpu_id": 3690, "node_id": 461, "gpu_within_node": 2},
              "expert_43": {"gpu_id": 3691, "node_id": 461, "gpu_within_node": 3},
              "expert_44": {"gpu_id": 3692, "node_id": 461, "gpu_within_node": 4},
              "expert_45": {"gpu_id": 3693, "node_id": 461, "gpu_within_node": 5},
              "expert_46": {"gpu_id": 3694, "node_id": 461, "gpu_within_node": 6},
              "expert_47": {"gpu_id": 3695, "node_id": 461, "gpu_within_node": 7},
              "expert_48": {"gpu_id": 3696, "node_id": 462, "gpu_within_node": 0},
              "expert_49": {"gpu_id": 3697, "node_id": 462, "gpu_within_node": 1},
              "expert_50": {"gpu_id": 3698, "node_id": 462, "gpu_within_node": 2},
              "expert_51": {"gpu_id": 3699, "node_id": 462, "gpu_within_node": 3},
              "expert_52": {"gpu_id": 3700, "node_id": 462, "gpu_within_node": 4},
              "expert_53": {"gpu_id": 3701, "node_id": 462, "gpu_within_node": 5},
              "expert_54": {"gpu_id": 3702, "node_id": 462, "gpu_within_node": 6},
              "expert_55": {"gpu_id": 3703, "node_id": 462, "gpu_within_node": 7},
              "expert_56": {"gpu_id": 3704, "node_id": 463, "gpu_within_node": 0},
              "expert_57": {"gpu_id": 3705, "node_id": 463, "gpu_within_node": 1},
              "expert_58": {"gpu_id": 3706, "node_id": 463, "gpu_within_node": 2},
              "expert_59": {"gpu_id": 3707, "node_id": 463, "gpu_within_node": 3},
              "expert_60": {"gpu_id": 3708, "node_id": 463, "gpu_within_node": 4},
              "expert_61": {"gpu_id": 3709, "node_id": 463, "gpu_within_node": 5},
              "expert_62": {"gpu_id": 3710, "node_id": 463, "gpu_within_node": 6},
              "expert_63": {"gpu_id": 3711, "node_id": 463, "gpu_within_node": 7}
            }
          }
        },
        "communication": {
          "network_topology": "InfiniBand_between_nodes_NVLink_within_nodes",
          "bandwidth_utilization": "80%",
          "peak_bandwidth_per_gpu": "1.44TBps",
          "async_communication": true,
          "token_batching": true,
          "compute_communication_overlap": "95%+",
          "message_size_bytes": "batch_size * 7168 * 2"
        },
        "performance": {
          "tokens_per_gpu_per_sec": "7.8M",
          "scaling_efficiency": "98%",
          "expert_utilization": "98%",
          "gpu_utilization": "95%+",
          "communication_overhead": "<5%"
        }
      },
      "baseline_method": {
        "name": "Traditional Expert Parallelism",
        "expert_parallelism_degree": 16,
        "strategy": "multiple_experts_per_gpu",
        "cross_node_distribution": false,
        "gpus_used": 64,
        "experts_per_gpu": 4,
        "device_mapping": {
          "layer_3": {
            "expert_0_to_15": "GPUs 0-15 (Nodes 0-1, 8 experts per GPU)",
            "expert_16_to_31": "GPUs 16-31 (Nodes 2-3, 8 experts per GPU)",
            "expert_32_to_47": "GPUs 32-47 (Nodes 4-5, 8 experts per GPU)",
            "expert_48_to_63": "GPUs 48-63 (Nodes 6-7, 8 experts per GPU)"
          },
          "layer_4_to_60": "Same pattern repeated for all MoE layers"
        },
        "communication": {
          "network_topology": "Primarily_within_nodes",
          "bandwidth_utilization": "95%",
          "expert_contention": "25-30%",
          "scaling_efficiency": "72%"
        },
        "performance": {
          "tokens_per_gpu_per_sec": "5.5M",
          "expert_utilization": "75%",
          "gpu_utilization": "70%",
          "communication_overhead": "25-30%"
        }
      }
    },
    "deployment_requirements": {
      "setting": "inference-only",
      "minimum_gpus": 64,
      "recommended_gpus": 3904,
      "network_requirements": "InfiniBand + NVLink",
      "memory_per_gpu": "64GB",
      "framework": "CUDA-aware_MPI_NCCL",
      "precision": "BF16",
      "batch_size": "variable",
      "sequence_length": "variable",
      "top_k_experts": 2,
      "gating_temperature": 1.0
    },
    "key_advantages": [
      "Maximized Expert Parallelism: One expert per GPU ensures minimal contention",
      "Balanced Load: Topology-aware placement prevents network bottlenecks",
      "Scalable Communication: Asynchronous token routing enables near-linear scaling",
      "High Utilization: 95%+ GPU utilization through compute-communication overlap",
      "Large Model Compatibility: Integrates with TP and DP for memory constraints"
    ]
  }
}