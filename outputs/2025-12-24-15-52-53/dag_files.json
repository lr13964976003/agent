{
  "dag_files": {
    "python_generator": "../outputs/2025-12-24-15-52-53/generate_dag.py",
    "dot_file": "../outputs/2025-12-24-15-52-53/llm_parallel_strategy_dag.dot",
    "svg_file": "../outputs/2025-12-24-15-52-53/llm_parallel_strategy_dag.svg",
    "png_file": "../outputs/2025-12-24-15-52-53/llm_parallel_strategy_dag.png"
  },
  "dag_characteristics": {
    "parallel_strategy": "TP × EP × PP",
    "total_gpus": 32,
    "pipeline_stages": 4,
    "expert_parallelism": 4,
    "tensor_parallelism": 2,
    "total_layers": 16,
    "experts_per_layer": 16,
    "has_cycles": false,
    "total_nodes": "~1000+",
    "total_edges": "~2000+"
  },
  "model_specifications": {
    "batch_size": 128,
    "sequence_length": 10240,
    "hidden_dimension": 512,
    "attention_heads": 16,
    "head_dimension": 32,
    "moe_hidden_size": 1024,    "precision": "FP16"
  },
  "node_types": {
    "compute_nodes": ["QKV Linear", "Attention", "Expert", "MLP Output", "LayerNorm"],
    "communication_nodes": ["All-Reduce", "All-to-All"],
    "routing_nodes": ["MoE Gate/Router"],
    "shapes": {
      "compute": "rectangle",
      "communication": "ellipse",
      "routing": "parallelogram (dashed)"
    }
  }
}