// Detailed Two-Level Attention Partitioning
digraph Detailed_Proposed_Model_DAG {
	rankdir=TB size="25,20"
	node [fillcolor=lightgray shape=rectangle style=filled]
	input [label="Model_Input
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightgreen shape=ellipse]
	subgraph cluster_layer0_partition0 {
		label="Partition 0 (GPU 0)
Heads: [0-7], Dim: [0-31]"
		q_0_0 [label="Q_Projection_L0_P0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		k_0_0 [label="K_Projection_L0_P0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		v_0_0 [label="V_Projection_L0_P0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		attn_0_0 [label="Attention_L0_P0
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
	}
	subgraph cluster_layer0_partition1 {
		label="Partition 1 (GPU 1)
Heads: [0-7], Dim: [32-63]"
		q_0_1 [label="Q_Projection_L0_P1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		k_0_1 [label="K_Projection_L0_P1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		v_0_1 [label="V_Projection_L0_P1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		attn_0_1 [label="Attention_L0_P1
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
	}
	subgraph cluster_layer0_partition2 {
		label="Partition 2 (GPU 2)
Heads: [0-7], Dim: [64-95]"
		q_0_2 [label="Q_Projection_L0_P2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		k_0_2 [label="K_Projection_L0_P2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		v_0_2 [label="V_Projection_L0_P2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		attn_0_2 [label="Attention_L0_P2
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
	}
	subgraph cluster_layer0_partition3 {
		label="Partition 3 (GPU 3)
Heads: [0-7], Dim: [96-127]"
		q_0_3 [label="Q_Projection_L0_P3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		k_0_3 [label="K_Projection_L0_P3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		v_0_3 [label="V_Projection_L0_P3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		attn_0_3 [label="Attention_L0_P3
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
	}
	subgraph cluster_layer0_partition4 {
		label="Partition 4 (GPU 4)
Heads: [8-15], Dim: [0-31]"
		q_0_4 [label="Q_Projection_L0_P4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		k_0_4 [label="K_Projection_L0_P4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		v_0_4 [label="V_Projection_L0_P4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		attn_0_4 [label="Attention_L0_P4
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
	}
	subgraph cluster_layer0_partition5 {
		label="Partition 5 (GPU 5)
Heads: [8-15], Dim: [32-63]"
		q_0_5 [label="Q_Projection_L0_P5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		k_0_5 [label="K_Projection_L0_P5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		v_0_5 [label="V_Projection_L0_P5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		attn_0_5 [label="Attention_L0_P5
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
	}
	subgraph cluster_layer0_partition6 {
		label="Partition 6 (GPU 6)
Heads: [8-15], Dim: [64-95]"
		q_0_6 [label="Q_Projection_L0_P6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		k_0_6 [label="K_Projection_L0_P6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		v_0_6 [label="V_Projection_L0_P6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		attn_0_6 [label="Attention_L0_P6
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
	}
	subgraph cluster_layer0_partition7 {
		label="Partition 7 (GPU 7)
Heads: [8-15], Dim: [96-127]"
		q_0_7 [label="Q_Projection_L0_P7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		k_0_7 [label="K_Projection_L0_P7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		v_0_7 [label="V_Projection_L0_P7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		attn_0_7 [label="Attention_L0_P7
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
	}
	subgraph cluster_layer0_partition8 {
		label="Partition 8 (GPU 8)
Heads: [16-23], Dim: [0-31]"
		q_0_8 [label="Q_Projection_L0_P8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		k_0_8 [label="K_Projection_L0_P8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		v_0_8 [label="V_Projection_L0_P8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		attn_0_8 [label="Attention_L0_P8
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
	}
	subgraph cluster_layer0_partition9 {
		label="Partition 9 (GPU 9)
Heads: [16-23], Dim: [32-63]"
		q_0_9 [label="Q_Projection_L0_P9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		k_0_9 [label="K_Projection_L0_P9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		v_0_9 [label="V_Projection_L0_P9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		attn_0_9 [label="Attention_L0_P9
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
	}
	subgraph cluster_layer0_partition10 {
		label="Partition 10 (GPU 10)
Heads: [16-23], Dim: [64-95]"
		q_0_10 [label="Q_Projection_L0_P10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		k_0_10 [label="K_Projection_L0_P10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		v_0_10 [label="V_Projection_L0_P10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		attn_0_10 [label="Attention_L0_P10
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
	}
	subgraph cluster_layer0_partition11 {
		label="Partition 11 (GPU 11)
Heads: [16-23], Dim: [96-127]"
		q_0_11 [label="Q_Projection_L0_P11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		k_0_11 [label="K_Projection_L0_P11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		v_0_11 [label="V_Projection_L0_P11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		attn_0_11 [label="Attention_L0_P11
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
	}
	subgraph cluster_layer0_partition12 {
		label="Partition 12 (GPU 12)
Heads: [24-31], Dim: [0-31]"
		q_0_12 [label="Q_Projection_L0_P12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		k_0_12 [label="K_Projection_L0_P12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		v_0_12 [label="V_Projection_L0_P12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		attn_0_12 [label="Attention_L0_P12
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
	}
	subgraph cluster_layer0_partition13 {
		label="Partition 13 (GPU 13)
Heads: [24-31], Dim: [32-63]"
		q_0_13 [label="Q_Projection_L0_P13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		k_0_13 [label="K_Projection_L0_P13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		v_0_13 [label="V_Projection_L0_P13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		attn_0_13 [label="Attention_L0_P13
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
	}
	subgraph cluster_layer0_partition14 {
		label="Partition 14 (GPU 14)
Heads: [24-31], Dim: [64-95]"
		q_0_14 [label="Q_Projection_L0_P14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		k_0_14 [label="K_Projection_L0_P14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		v_0_14 [label="V_Projection_L0_P14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		attn_0_14 [label="Attention_L0_P14
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
	}
	subgraph cluster_layer0_partition15 {
		label="Partition 15 (GPU 15)
Heads: [24-31], Dim: [96-127]"
		q_0_15 [label="Q_Projection_L0_P15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		k_0_15 [label="K_Projection_L0_P15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		v_0_15 [label="V_Projection_L0_P15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		attn_0_15 [label="Attention_L0_P15
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
	}
	subgraph cluster_layer0 {
		color=blue label="Layer 0 - Two-Level Attention Partitioning" style=dashed
		ll0_input [label="Layer0_Input
Input: [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" shape=ellipse]
		l0_dim_concat [label="Dimension_Concatenation_L0
Input: [128,10000,256]×4×4
Output: [128,10000,4096]
GPU: 0" fillcolor=yellow shape=parallelogram]
		l0_add [label="Add&Norm_L0_Attention
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
		mlp_fc1_0_0 [label="MLP_FC1_L0_P0
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 0" shape=rectangle]
		mlp_fc2_0_0 [label="MLP_FC2_L0_P0
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		mlp_fc1_0_1 [label="MLP_FC1_L0_P1
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 1" shape=rectangle]
		mlp_fc2_0_1 [label="MLP_FC2_L0_P1
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		mlp_fc1_0_2 [label="MLP_FC1_L0_P2
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 2" shape=rectangle]
		mlp_fc2_0_2 [label="MLP_FC2_L0_P2
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		mlp_fc1_0_3 [label="MLP_FC1_L0_P3
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 3" shape=rectangle]
		mlp_fc2_0_3 [label="MLP_FC2_L0_P3
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		mlp_fc1_0_4 [label="MLP_FC1_L0_P4
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 4" shape=rectangle]
		mlp_fc2_0_4 [label="MLP_FC2_L0_P4
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		mlp_fc1_0_5 [label="MLP_FC1_L0_P5
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 5" shape=rectangle]
		mlp_fc2_0_5 [label="MLP_FC2_L0_P5
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		mlp_fc1_0_6 [label="MLP_FC1_L0_P6
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 6" shape=rectangle]
		mlp_fc2_0_6 [label="MLP_FC2_L0_P6
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		mlp_fc1_0_7 [label="MLP_FC1_L0_P7
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 7" shape=rectangle]
		mlp_fc2_0_7 [label="MLP_FC2_L0_P7
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		mlp_fc1_0_8 [label="MLP_FC1_L0_P8
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 8" shape=rectangle]
		mlp_fc2_0_8 [label="MLP_FC2_L0_P8
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		mlp_fc1_0_9 [label="MLP_FC1_L0_P9
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 9" shape=rectangle]
		mlp_fc2_0_9 [label="MLP_FC2_L0_P9
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		mlp_fc1_0_10 [label="MLP_FC1_L0_P10
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 10" shape=rectangle]
		mlp_fc2_0_10 [label="MLP_FC2_L0_P10
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		mlp_fc1_0_11 [label="MLP_FC1_L0_P11
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 11" shape=rectangle]
		mlp_fc2_0_11 [label="MLP_FC2_L0_P11
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		mlp_fc1_0_12 [label="MLP_FC1_L0_P12
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 12" shape=rectangle]
		mlp_fc2_0_12 [label="MLP_FC2_L0_P12
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		mlp_fc1_0_13 [label="MLP_FC1_L0_P13
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 13" shape=rectangle]
		mlp_fc2_0_13 [label="MLP_FC2_L0_P13
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		mlp_fc1_0_14 [label="MLP_FC1_L0_P14
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 14" shape=rectangle]
		mlp_fc2_0_14 [label="MLP_FC2_L0_P14
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		mlp_fc1_0_15 [label="MLP_FC1_L0_P15
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 15" shape=rectangle]
		mlp_fc2_0_15 [label="MLP_FC2_L0_P15
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		l0_mlp_concat [label="MLP_Concatenation_L0
Input: [128,10000,256]×16
Output: [128,10000,4096]
GPU: 0" fillcolor=yellow shape=parallelogram]
		l0_mlp_add [label="Add&Norm_L0_MLP
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
	}
	subgraph cluster_layer1_partition0 {
		label="Partition 0 (GPU 0)
Heads: [0-7], Dim: [0-31]"
		q_1_0 [label="Q_Projection_L1_P0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		k_1_0 [label="K_Projection_L1_P0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		v_1_0 [label="V_Projection_L1_P0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		attn_1_0 [label="Attention_L1_P0
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
	}
	subgraph cluster_layer1_partition1 {
		label="Partition 1 (GPU 1)
Heads: [0-7], Dim: [32-63]"
		q_1_1 [label="Q_Projection_L1_P1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		k_1_1 [label="K_Projection_L1_P1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		v_1_1 [label="V_Projection_L1_P1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		attn_1_1 [label="Attention_L1_P1
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
	}
	subgraph cluster_layer1_partition2 {
		label="Partition 2 (GPU 2)
Heads: [0-7], Dim: [64-95]"
		q_1_2 [label="Q_Projection_L1_P2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		k_1_2 [label="K_Projection_L1_P2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		v_1_2 [label="V_Projection_L1_P2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		attn_1_2 [label="Attention_L1_P2
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
	}
	subgraph cluster_layer1_partition3 {
		label="Partition 3 (GPU 3)
Heads: [0-7], Dim: [96-127]"
		q_1_3 [label="Q_Projection_L1_P3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		k_1_3 [label="K_Projection_L1_P3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		v_1_3 [label="V_Projection_L1_P3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		attn_1_3 [label="Attention_L1_P3
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
	}
	subgraph cluster_layer1_partition4 {
		label="Partition 4 (GPU 4)
Heads: [8-15], Dim: [0-31]"
		q_1_4 [label="Q_Projection_L1_P4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		k_1_4 [label="K_Projection_L1_P4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		v_1_4 [label="V_Projection_L1_P4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		attn_1_4 [label="Attention_L1_P4
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
	}
	subgraph cluster_layer1_partition5 {
		label="Partition 5 (GPU 5)
Heads: [8-15], Dim: [32-63]"
		q_1_5 [label="Q_Projection_L1_P5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		k_1_5 [label="K_Projection_L1_P5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		v_1_5 [label="V_Projection_L1_P5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		attn_1_5 [label="Attention_L1_P5
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
	}
	subgraph cluster_layer1_partition6 {
		label="Partition 6 (GPU 6)
Heads: [8-15], Dim: [64-95]"
		q_1_6 [label="Q_Projection_L1_P6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		k_1_6 [label="K_Projection_L1_P6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		v_1_6 [label="V_Projection_L1_P6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		attn_1_6 [label="Attention_L1_P6
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
	}
	subgraph cluster_layer1_partition7 {
		label="Partition 7 (GPU 7)
Heads: [8-15], Dim: [96-127]"
		q_1_7 [label="Q_Projection_L1_P7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		k_1_7 [label="K_Projection_L1_P7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		v_1_7 [label="V_Projection_L1_P7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		attn_1_7 [label="Attention_L1_P7
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
	}
	subgraph cluster_layer1_partition8 {
		label="Partition 8 (GPU 8)
Heads: [16-23], Dim: [0-31]"
		q_1_8 [label="Q_Projection_L1_P8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		k_1_8 [label="K_Projection_L1_P8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		v_1_8 [label="V_Projection_L1_P8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		attn_1_8 [label="Attention_L1_P8
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
	}
	subgraph cluster_layer1_partition9 {
		label="Partition 9 (GPU 9)
Heads: [16-23], Dim: [32-63]"
		q_1_9 [label="Q_Projection_L1_P9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		k_1_9 [label="K_Projection_L1_P9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		v_1_9 [label="V_Projection_L1_P9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		attn_1_9 [label="Attention_L1_P9
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
	}
	subgraph cluster_layer1_partition10 {
		label="Partition 10 (GPU 10)
Heads: [16-23], Dim: [64-95]"
		q_1_10 [label="Q_Projection_L1_P10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		k_1_10 [label="K_Projection_L1_P10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		v_1_10 [label="V_Projection_L1_P10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		attn_1_10 [label="Attention_L1_P10
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
	}
	subgraph cluster_layer1_partition11 {
		label="Partition 11 (GPU 11)
Heads: [16-23], Dim: [96-127]"
		q_1_11 [label="Q_Projection_L1_P11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		k_1_11 [label="K_Projection_L1_P11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		v_1_11 [label="V_Projection_L1_P11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		attn_1_11 [label="Attention_L1_P11
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
	}
	subgraph cluster_layer1_partition12 {
		label="Partition 12 (GPU 12)
Heads: [24-31], Dim: [0-31]"
		q_1_12 [label="Q_Projection_L1_P12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		k_1_12 [label="K_Projection_L1_P12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		v_1_12 [label="V_Projection_L1_P12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		attn_1_12 [label="Attention_L1_P12
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
	}
	subgraph cluster_layer1_partition13 {
		label="Partition 13 (GPU 13)
Heads: [24-31], Dim: [32-63]"
		q_1_13 [label="Q_Projection_L1_P13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		k_1_13 [label="K_Projection_L1_P13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		v_1_13 [label="V_Projection_L1_P13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		attn_1_13 [label="Attention_L1_P13
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
	}
	subgraph cluster_layer1_partition14 {
		label="Partition 14 (GPU 14)
Heads: [24-31], Dim: [64-95]"
		q_1_14 [label="Q_Projection_L1_P14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		k_1_14 [label="K_Projection_L1_P14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		v_1_14 [label="V_Projection_L1_P14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		attn_1_14 [label="Attention_L1_P14
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
	}
	subgraph cluster_layer1_partition15 {
		label="Partition 15 (GPU 15)
Heads: [24-31], Dim: [96-127]"
		q_1_15 [label="Q_Projection_L1_P15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		k_1_15 [label="K_Projection_L1_P15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		v_1_15 [label="V_Projection_L1_P15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		attn_1_15 [label="Attention_L1_P15
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
	}
	subgraph cluster_layer1 {
		color=blue label="Layer 1 - Two-Level Attention Partitioning" style=dashed
		ll1_input [label="Layer1_Input
Input: [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" shape=ellipse]
		l1_dim_concat [label="Dimension_Concatenation_L1
Input: [128,10000,256]×4×4
Output: [128,10000,4096]
GPU: 0" fillcolor=yellow shape=parallelogram]
		l1_add [label="Add&Norm_L1_Attention
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
		mlp_fc1_1_0 [label="MLP_FC1_L1_P0
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 0" shape=rectangle]
		mlp_fc2_1_0 [label="MLP_FC2_L1_P0
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		mlp_fc1_1_1 [label="MLP_FC1_L1_P1
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 1" shape=rectangle]
		mlp_fc2_1_1 [label="MLP_FC2_L1_P1
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		mlp_fc1_1_2 [label="MLP_FC1_L1_P2
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 2" shape=rectangle]
		mlp_fc2_1_2 [label="MLP_FC2_L1_P2
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		mlp_fc1_1_3 [label="MLP_FC1_L1_P3
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 3" shape=rectangle]
		mlp_fc2_1_3 [label="MLP_FC2_L1_P3
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		mlp_fc1_1_4 [label="MLP_FC1_L1_P4
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 4" shape=rectangle]
		mlp_fc2_1_4 [label="MLP_FC2_L1_P4
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		mlp_fc1_1_5 [label="MLP_FC1_L1_P5
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 5" shape=rectangle]
		mlp_fc2_1_5 [label="MLP_FC2_L1_P5
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		mlp_fc1_1_6 [label="MLP_FC1_L1_P6
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 6" shape=rectangle]
		mlp_fc2_1_6 [label="MLP_FC2_L1_P6
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		mlp_fc1_1_7 [label="MLP_FC1_L1_P7
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 7" shape=rectangle]
		mlp_fc2_1_7 [label="MLP_FC2_L1_P7
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		mlp_fc1_1_8 [label="MLP_FC1_L1_P8
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 8" shape=rectangle]
		mlp_fc2_1_8 [label="MLP_FC2_L1_P8
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		mlp_fc1_1_9 [label="MLP_FC1_L1_P9
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 9" shape=rectangle]
		mlp_fc2_1_9 [label="MLP_FC2_L1_P9
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		mlp_fc1_1_10 [label="MLP_FC1_L1_P10
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 10" shape=rectangle]
		mlp_fc2_1_10 [label="MLP_FC2_L1_P10
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		mlp_fc1_1_11 [label="MLP_FC1_L1_P11
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 11" shape=rectangle]
		mlp_fc2_1_11 [label="MLP_FC2_L1_P11
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		mlp_fc1_1_12 [label="MLP_FC1_L1_P12
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 12" shape=rectangle]
		mlp_fc2_1_12 [label="MLP_FC2_L1_P12
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		mlp_fc1_1_13 [label="MLP_FC1_L1_P13
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 13" shape=rectangle]
		mlp_fc2_1_13 [label="MLP_FC2_L1_P13
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		mlp_fc1_1_14 [label="MLP_FC1_L1_P14
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 14" shape=rectangle]
		mlp_fc2_1_14 [label="MLP_FC2_L1_P14
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		mlp_fc1_1_15 [label="MLP_FC1_L1_P15
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 15" shape=rectangle]
		mlp_fc2_1_15 [label="MLP_FC2_L1_P15
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		l1_mlp_concat [label="MLP_Concatenation_L1
Input: [128,10000,256]×16
Output: [128,10000,4096]
GPU: 0" fillcolor=yellow shape=parallelogram]
		l1_mlp_add [label="Add&Norm_L1_MLP
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
	}
	subgraph cluster_layer2_partition0 {
		label="Partition 0 (GPU 0)
Heads: [0-7], Dim: [0-31]"
		q_2_0 [label="Q_Projection_L2_P0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		k_2_0 [label="K_Projection_L2_P0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		v_2_0 [label="V_Projection_L2_P0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		attn_2_0 [label="Attention_L2_P0
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
	}
	subgraph cluster_layer2_partition1 {
		label="Partition 1 (GPU 1)
Heads: [0-7], Dim: [32-63]"
		q_2_1 [label="Q_Projection_L2_P1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		k_2_1 [label="K_Projection_L2_P1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		v_2_1 [label="V_Projection_L2_P1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		attn_2_1 [label="Attention_L2_P1
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
	}
	subgraph cluster_layer2_partition2 {
		label="Partition 2 (GPU 2)
Heads: [0-7], Dim: [64-95]"
		q_2_2 [label="Q_Projection_L2_P2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		k_2_2 [label="K_Projection_L2_P2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		v_2_2 [label="V_Projection_L2_P2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		attn_2_2 [label="Attention_L2_P2
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
	}
	subgraph cluster_layer2_partition3 {
		label="Partition 3 (GPU 3)
Heads: [0-7], Dim: [96-127]"
		q_2_3 [label="Q_Projection_L2_P3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		k_2_3 [label="K_Projection_L2_P3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		v_2_3 [label="V_Projection_L2_P3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		attn_2_3 [label="Attention_L2_P3
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
	}
	subgraph cluster_layer2_partition4 {
		label="Partition 4 (GPU 4)
Heads: [8-15], Dim: [0-31]"
		q_2_4 [label="Q_Projection_L2_P4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		k_2_4 [label="K_Projection_L2_P4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		v_2_4 [label="V_Projection_L2_P4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		attn_2_4 [label="Attention_L2_P4
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
	}
	subgraph cluster_layer2_partition5 {
		label="Partition 5 (GPU 5)
Heads: [8-15], Dim: [32-63]"
		q_2_5 [label="Q_Projection_L2_P5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		k_2_5 [label="K_Projection_L2_P5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		v_2_5 [label="V_Projection_L2_P5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		attn_2_5 [label="Attention_L2_P5
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
	}
	subgraph cluster_layer2_partition6 {
		label="Partition 6 (GPU 6)
Heads: [8-15], Dim: [64-95]"
		q_2_6 [label="Q_Projection_L2_P6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		k_2_6 [label="K_Projection_L2_P6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		v_2_6 [label="V_Projection_L2_P6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		attn_2_6 [label="Attention_L2_P6
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
	}
	subgraph cluster_layer2_partition7 {
		label="Partition 7 (GPU 7)
Heads: [8-15], Dim: [96-127]"
		q_2_7 [label="Q_Projection_L2_P7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		k_2_7 [label="K_Projection_L2_P7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		v_2_7 [label="V_Projection_L2_P7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		attn_2_7 [label="Attention_L2_P7
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
	}
	subgraph cluster_layer2_partition8 {
		label="Partition 8 (GPU 8)
Heads: [16-23], Dim: [0-31]"
		q_2_8 [label="Q_Projection_L2_P8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		k_2_8 [label="K_Projection_L2_P8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		v_2_8 [label="V_Projection_L2_P8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		attn_2_8 [label="Attention_L2_P8
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
	}
	subgraph cluster_layer2_partition9 {
		label="Partition 9 (GPU 9)
Heads: [16-23], Dim: [32-63]"
		q_2_9 [label="Q_Projection_L2_P9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		k_2_9 [label="K_Projection_L2_P9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		v_2_9 [label="V_Projection_L2_P9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		attn_2_9 [label="Attention_L2_P9
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
	}
	subgraph cluster_layer2_partition10 {
		label="Partition 10 (GPU 10)
Heads: [16-23], Dim: [64-95]"
		q_2_10 [label="Q_Projection_L2_P10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		k_2_10 [label="K_Projection_L2_P10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		v_2_10 [label="V_Projection_L2_P10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		attn_2_10 [label="Attention_L2_P10
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
	}
	subgraph cluster_layer2_partition11 {
		label="Partition 11 (GPU 11)
Heads: [16-23], Dim: [96-127]"
		q_2_11 [label="Q_Projection_L2_P11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		k_2_11 [label="K_Projection_L2_P11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		v_2_11 [label="V_Projection_L2_P11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		attn_2_11 [label="Attention_L2_P11
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
	}
	subgraph cluster_layer2_partition12 {
		label="Partition 12 (GPU 12)
Heads: [24-31], Dim: [0-31]"
		q_2_12 [label="Q_Projection_L2_P12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		k_2_12 [label="K_Projection_L2_P12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		v_2_12 [label="V_Projection_L2_P12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		attn_2_12 [label="Attention_L2_P12
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
	}
	subgraph cluster_layer2_partition13 {
		label="Partition 13 (GPU 13)
Heads: [24-31], Dim: [32-63]"
		q_2_13 [label="Q_Projection_L2_P13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		k_2_13 [label="K_Projection_L2_P13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		v_2_13 [label="V_Projection_L2_P13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		attn_2_13 [label="Attention_L2_P13
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
	}
	subgraph cluster_layer2_partition14 {
		label="Partition 14 (GPU 14)
Heads: [24-31], Dim: [64-95]"
		q_2_14 [label="Q_Projection_L2_P14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		k_2_14 [label="K_Projection_L2_P14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		v_2_14 [label="V_Projection_L2_P14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		attn_2_14 [label="Attention_L2_P14
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
	}
	subgraph cluster_layer2_partition15 {
		label="Partition 15 (GPU 15)
Heads: [24-31], Dim: [96-127]"
		q_2_15 [label="Q_Projection_L2_P15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		k_2_15 [label="K_Projection_L2_P15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		v_2_15 [label="V_Projection_L2_P15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		attn_2_15 [label="Attention_L2_P15
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
	}
	subgraph cluster_layer2 {
		color=blue label="Layer 2 - Two-Level Attention Partitioning" style=dashed
		ll2_input [label="Layer2_Input
Input: [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" shape=ellipse]
		l2_dim_concat [label="Dimension_Concatenation_L2
Input: [128,10000,256]×4×4
Output: [128,10000,4096]
GPU: 0" fillcolor=yellow shape=parallelogram]
		l2_add [label="Add&Norm_L2_Attention
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
		mlp_fc1_2_0 [label="MLP_FC1_L2_P0
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 0" shape=rectangle]
		mlp_fc2_2_0 [label="MLP_FC2_L2_P0
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		mlp_fc1_2_1 [label="MLP_FC1_L2_P1
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 1" shape=rectangle]
		mlp_fc2_2_1 [label="MLP_FC2_L2_P1
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		mlp_fc1_2_2 [label="MLP_FC1_L2_P2
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 2" shape=rectangle]
		mlp_fc2_2_2 [label="MLP_FC2_L2_P2
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		mlp_fc1_2_3 [label="MLP_FC1_L2_P3
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 3" shape=rectangle]
		mlp_fc2_2_3 [label="MLP_FC2_L2_P3
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		mlp_fc1_2_4 [label="MLP_FC1_L2_P4
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 4" shape=rectangle]
		mlp_fc2_2_4 [label="MLP_FC2_L2_P4
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		mlp_fc1_2_5 [label="MLP_FC1_L2_P5
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 5" shape=rectangle]
		mlp_fc2_2_5 [label="MLP_FC2_L2_P5
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		mlp_fc1_2_6 [label="MLP_FC1_L2_P6
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 6" shape=rectangle]
		mlp_fc2_2_6 [label="MLP_FC2_L2_P6
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		mlp_fc1_2_7 [label="MLP_FC1_L2_P7
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 7" shape=rectangle]
		mlp_fc2_2_7 [label="MLP_FC2_L2_P7
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		mlp_fc1_2_8 [label="MLP_FC1_L2_P8
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 8" shape=rectangle]
		mlp_fc2_2_8 [label="MLP_FC2_L2_P8
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		mlp_fc1_2_9 [label="MLP_FC1_L2_P9
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 9" shape=rectangle]
		mlp_fc2_2_9 [label="MLP_FC2_L2_P9
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		mlp_fc1_2_10 [label="MLP_FC1_L2_P10
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 10" shape=rectangle]
		mlp_fc2_2_10 [label="MLP_FC2_L2_P10
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		mlp_fc1_2_11 [label="MLP_FC1_L2_P11
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 11" shape=rectangle]
		mlp_fc2_2_11 [label="MLP_FC2_L2_P11
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		mlp_fc1_2_12 [label="MLP_FC1_L2_P12
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 12" shape=rectangle]
		mlp_fc2_2_12 [label="MLP_FC2_L2_P12
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		mlp_fc1_2_13 [label="MLP_FC1_L2_P13
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 13" shape=rectangle]
		mlp_fc2_2_13 [label="MLP_FC2_L2_P13
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		mlp_fc1_2_14 [label="MLP_FC1_L2_P14
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 14" shape=rectangle]
		mlp_fc2_2_14 [label="MLP_FC2_L2_P14
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		mlp_fc1_2_15 [label="MLP_FC1_L2_P15
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 15" shape=rectangle]
		mlp_fc2_2_15 [label="MLP_FC2_L2_P15
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		l2_mlp_concat [label="MLP_Concatenation_L2
Input: [128,10000,256]×16
Output: [128,10000,4096]
GPU: 0" fillcolor=yellow shape=parallelogram]
		l2_mlp_add [label="Add&Norm_L2_MLP
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
	}
	subgraph cluster_layer3_partition0 {
		label="Partition 0 (GPU 0)
Heads: [0-7], Dim: [0-31]"
		q_3_0 [label="Q_Projection_L3_P0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		k_3_0 [label="K_Projection_L3_P0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		v_3_0 [label="V_Projection_L3_P0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		attn_3_0 [label="Attention_L3_P0
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
	}
	subgraph cluster_layer3_partition1 {
		label="Partition 1 (GPU 1)
Heads: [0-7], Dim: [32-63]"
		q_3_1 [label="Q_Projection_L3_P1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		k_3_1 [label="K_Projection_L3_P1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		v_3_1 [label="V_Projection_L3_P1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		attn_3_1 [label="Attention_L3_P1
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
	}
	subgraph cluster_layer3_partition2 {
		label="Partition 2 (GPU 2)
Heads: [0-7], Dim: [64-95]"
		q_3_2 [label="Q_Projection_L3_P2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		k_3_2 [label="K_Projection_L3_P2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		v_3_2 [label="V_Projection_L3_P2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		attn_3_2 [label="Attention_L3_P2
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
	}
	subgraph cluster_layer3_partition3 {
		label="Partition 3 (GPU 3)
Heads: [0-7], Dim: [96-127]"
		q_3_3 [label="Q_Projection_L3_P3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		k_3_3 [label="K_Projection_L3_P3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		v_3_3 [label="V_Projection_L3_P3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		attn_3_3 [label="Attention_L3_P3
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
	}
	subgraph cluster_layer3_partition4 {
		label="Partition 4 (GPU 4)
Heads: [8-15], Dim: [0-31]"
		q_3_4 [label="Q_Projection_L3_P4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		k_3_4 [label="K_Projection_L3_P4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		v_3_4 [label="V_Projection_L3_P4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		attn_3_4 [label="Attention_L3_P4
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
	}
	subgraph cluster_layer3_partition5 {
		label="Partition 5 (GPU 5)
Heads: [8-15], Dim: [32-63]"
		q_3_5 [label="Q_Projection_L3_P5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		k_3_5 [label="K_Projection_L3_P5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		v_3_5 [label="V_Projection_L3_P5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		attn_3_5 [label="Attention_L3_P5
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
	}
	subgraph cluster_layer3_partition6 {
		label="Partition 6 (GPU 6)
Heads: [8-15], Dim: [64-95]"
		q_3_6 [label="Q_Projection_L3_P6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		k_3_6 [label="K_Projection_L3_P6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		v_3_6 [label="V_Projection_L3_P6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		attn_3_6 [label="Attention_L3_P6
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
	}
	subgraph cluster_layer3_partition7 {
		label="Partition 7 (GPU 7)
Heads: [8-15], Dim: [96-127]"
		q_3_7 [label="Q_Projection_L3_P7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		k_3_7 [label="K_Projection_L3_P7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		v_3_7 [label="V_Projection_L3_P7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		attn_3_7 [label="Attention_L3_P7
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
	}
	subgraph cluster_layer3_partition8 {
		label="Partition 8 (GPU 8)
Heads: [16-23], Dim: [0-31]"
		q_3_8 [label="Q_Projection_L3_P8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		k_3_8 [label="K_Projection_L3_P8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		v_3_8 [label="V_Projection_L3_P8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		attn_3_8 [label="Attention_L3_P8
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
	}
	subgraph cluster_layer3_partition9 {
		label="Partition 9 (GPU 9)
Heads: [16-23], Dim: [32-63]"
		q_3_9 [label="Q_Projection_L3_P9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		k_3_9 [label="K_Projection_L3_P9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		v_3_9 [label="V_Projection_L3_P9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		attn_3_9 [label="Attention_L3_P9
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
	}
	subgraph cluster_layer3_partition10 {
		label="Partition 10 (GPU 10)
Heads: [16-23], Dim: [64-95]"
		q_3_10 [label="Q_Projection_L3_P10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		k_3_10 [label="K_Projection_L3_P10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		v_3_10 [label="V_Projection_L3_P10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		attn_3_10 [label="Attention_L3_P10
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
	}
	subgraph cluster_layer3_partition11 {
		label="Partition 11 (GPU 11)
Heads: [16-23], Dim: [96-127]"
		q_3_11 [label="Q_Projection_L3_P11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		k_3_11 [label="K_Projection_L3_P11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		v_3_11 [label="V_Projection_L3_P11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		attn_3_11 [label="Attention_L3_P11
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
	}
	subgraph cluster_layer3_partition12 {
		label="Partition 12 (GPU 12)
Heads: [24-31], Dim: [0-31]"
		q_3_12 [label="Q_Projection_L3_P12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		k_3_12 [label="K_Projection_L3_P12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		v_3_12 [label="V_Projection_L3_P12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		attn_3_12 [label="Attention_L3_P12
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
	}
	subgraph cluster_layer3_partition13 {
		label="Partition 13 (GPU 13)
Heads: [24-31], Dim: [32-63]"
		q_3_13 [label="Q_Projection_L3_P13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		k_3_13 [label="K_Projection_L3_P13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		v_3_13 [label="V_Projection_L3_P13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		attn_3_13 [label="Attention_L3_P13
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
	}
	subgraph cluster_layer3_partition14 {
		label="Partition 14 (GPU 14)
Heads: [24-31], Dim: [64-95]"
		q_3_14 [label="Q_Projection_L3_P14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		k_3_14 [label="K_Projection_L3_P14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		v_3_14 [label="V_Projection_L3_P14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		attn_3_14 [label="Attention_L3_P14
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
	}
	subgraph cluster_layer3_partition15 {
		label="Partition 15 (GPU 15)
Heads: [24-31], Dim: [96-127]"
		q_3_15 [label="Q_Projection_L3_P15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		k_3_15 [label="K_Projection_L3_P15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		v_3_15 [label="V_Projection_L3_P15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		attn_3_15 [label="Attention_L3_P15
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
	}
	subgraph cluster_layer3 {
		color=blue label="Layer 3 - Two-Level Attention Partitioning" style=dashed
		ll3_input [label="Layer3_Input
Input: [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" shape=ellipse]
		l3_dim_concat [label="Dimension_Concatenation_L3
Input: [128,10000,256]×4×4
Output: [128,10000,4096]
GPU: 0" fillcolor=yellow shape=parallelogram]
		l3_add [label="Add&Norm_L3_Attention
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
		mlp_fc1_3_0 [label="MLP_FC1_L3_P0
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 0" shape=rectangle]
		mlp_fc2_3_0 [label="MLP_FC2_L3_P0
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		mlp_fc1_3_1 [label="MLP_FC1_L3_P1
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 1" shape=rectangle]
		mlp_fc2_3_1 [label="MLP_FC2_L3_P1
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		mlp_fc1_3_2 [label="MLP_FC1_L3_P2
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 2" shape=rectangle]
		mlp_fc2_3_2 [label="MLP_FC2_L3_P2
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		mlp_fc1_3_3 [label="MLP_FC1_L3_P3
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 3" shape=rectangle]
		mlp_fc2_3_3 [label="MLP_FC2_L3_P3
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		mlp_fc1_3_4 [label="MLP_FC1_L3_P4
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 4" shape=rectangle]
		mlp_fc2_3_4 [label="MLP_FC2_L3_P4
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		mlp_fc1_3_5 [label="MLP_FC1_L3_P5
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 5" shape=rectangle]
		mlp_fc2_3_5 [label="MLP_FC2_L3_P5
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		mlp_fc1_3_6 [label="MLP_FC1_L3_P6
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 6" shape=rectangle]
		mlp_fc2_3_6 [label="MLP_FC2_L3_P6
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		mlp_fc1_3_7 [label="MLP_FC1_L3_P7
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 7" shape=rectangle]
		mlp_fc2_3_7 [label="MLP_FC2_L3_P7
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		mlp_fc1_3_8 [label="MLP_FC1_L3_P8
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 8" shape=rectangle]
		mlp_fc2_3_8 [label="MLP_FC2_L3_P8
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		mlp_fc1_3_9 [label="MLP_FC1_L3_P9
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 9" shape=rectangle]
		mlp_fc2_3_9 [label="MLP_FC2_L3_P9
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		mlp_fc1_3_10 [label="MLP_FC1_L3_P10
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 10" shape=rectangle]
		mlp_fc2_3_10 [label="MLP_FC2_L3_P10
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		mlp_fc1_3_11 [label="MLP_FC1_L3_P11
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 11" shape=rectangle]
		mlp_fc2_3_11 [label="MLP_FC2_L3_P11
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		mlp_fc1_3_12 [label="MLP_FC1_L3_P12
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 12" shape=rectangle]
		mlp_fc2_3_12 [label="MLP_FC2_L3_P12
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		mlp_fc1_3_13 [label="MLP_FC1_L3_P13
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 13" shape=rectangle]
		mlp_fc2_3_13 [label="MLP_FC2_L3_P13
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		mlp_fc1_3_14 [label="MLP_FC1_L3_P14
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 14" shape=rectangle]
		mlp_fc2_3_14 [label="MLP_FC2_L3_P14
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		mlp_fc1_3_15 [label="MLP_FC1_L3_P15
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 15" shape=rectangle]
		mlp_fc2_3_15 [label="MLP_FC2_L3_P15
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		l3_mlp_concat [label="MLP_Concatenation_L3
Input: [128,10000,256]×16
Output: [128,10000,4096]
GPU: 0" fillcolor=yellow shape=parallelogram]
		l3_mlp_add [label="Add&Norm_L3_MLP
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
	}
	output [label="Model_Output
Input: [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightgreen shape=ellipse]
	input -> ll0_input
	ll0_input -> q_0_0
	ll0_input -> k_0_0
	ll0_input -> v_0_0
	q_0_0 -> attn_0_0
	k_0_0 -> attn_0_0
	v_0_0 -> attn_0_0
	attn_0_0 -> l0_dim_concat
	ll0_input -> q_0_1
	ll0_input -> k_0_1
	ll0_input -> v_0_1
	q_0_1 -> attn_0_1
	k_0_1 -> attn_0_1
	v_0_1 -> attn_0_1
	attn_0_1 -> l0_dim_concat
	ll0_input -> q_0_2
	ll0_input -> k_0_2
	ll0_input -> v_0_2
	q_0_2 -> attn_0_2
	k_0_2 -> attn_0_2
	v_0_2 -> attn_0_2
	attn_0_2 -> l0_dim_concat
	ll0_input -> q_0_3
	ll0_input -> k_0_3
	ll0_input -> v_0_3
	q_0_3 -> attn_0_3
	k_0_3 -> attn_0_3
	v_0_3 -> attn_0_3
	attn_0_3 -> l0_dim_concat
	ll0_input -> q_0_4
	ll0_input -> k_0_4
	ll0_input -> v_0_4
	q_0_4 -> attn_0_4
	k_0_4 -> attn_0_4
	v_0_4 -> attn_0_4
	attn_0_4 -> l0_dim_concat
	ll0_input -> q_0_5
	ll0_input -> k_0_5
	ll0_input -> v_0_5
	q_0_5 -> attn_0_5
	k_0_5 -> attn_0_5
	v_0_5 -> attn_0_5
	attn_0_5 -> l0_dim_concat
	ll0_input -> q_0_6
	ll0_input -> k_0_6
	ll0_input -> v_0_6
	q_0_6 -> attn_0_6
	k_0_6 -> attn_0_6
	v_0_6 -> attn_0_6
	attn_0_6 -> l0_dim_concat
	ll0_input -> q_0_7
	ll0_input -> k_0_7
	ll0_input -> v_0_7
	q_0_7 -> attn_0_7
	k_0_7 -> attn_0_7
	v_0_7 -> attn_0_7
	attn_0_7 -> l0_dim_concat
	ll0_input -> q_0_8
	ll0_input -> k_0_8
	ll0_input -> v_0_8
	q_0_8 -> attn_0_8
	k_0_8 -> attn_0_8
	v_0_8 -> attn_0_8
	attn_0_8 -> l0_dim_concat
	ll0_input -> q_0_9
	ll0_input -> k_0_9
	ll0_input -> v_0_9
	q_0_9 -> attn_0_9
	k_0_9 -> attn_0_9
	v_0_9 -> attn_0_9
	attn_0_9 -> l0_dim_concat
	ll0_input -> q_0_10
	ll0_input -> k_0_10
	ll0_input -> v_0_10
	q_0_10 -> attn_0_10
	k_0_10 -> attn_0_10
	v_0_10 -> attn_0_10
	attn_0_10 -> l0_dim_concat
	ll0_input -> q_0_11
	ll0_input -> k_0_11
	ll0_input -> v_0_11
	q_0_11 -> attn_0_11
	k_0_11 -> attn_0_11
	v_0_11 -> attn_0_11
	attn_0_11 -> l0_dim_concat
	ll0_input -> q_0_12
	ll0_input -> k_0_12
	ll0_input -> v_0_12
	q_0_12 -> attn_0_12
	k_0_12 -> attn_0_12
	v_0_12 -> attn_0_12
	attn_0_12 -> l0_dim_concat
	ll0_input -> q_0_13
	ll0_input -> k_0_13
	ll0_input -> v_0_13
	q_0_13 -> attn_0_13
	k_0_13 -> attn_0_13
	v_0_13 -> attn_0_13
	attn_0_13 -> l0_dim_concat
	ll0_input -> q_0_14
	ll0_input -> k_0_14
	ll0_input -> v_0_14
	q_0_14 -> attn_0_14
	k_0_14 -> attn_0_14
	v_0_14 -> attn_0_14
	attn_0_14 -> l0_dim_concat
	ll0_input -> q_0_15
	ll0_input -> k_0_15
	ll0_input -> v_0_15
	q_0_15 -> attn_0_15
	k_0_15 -> attn_0_15
	v_0_15 -> attn_0_15
	attn_0_15 -> l0_dim_concat
	l0_dim_concat -> l0_add
	ll0_input -> l0_add
	l0_add -> mlp_fc1_0_0
	mlp_fc1_0_0 -> mlp_fc2_0_0
	mlp_fc2_0_0 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_1
	mlp_fc1_0_1 -> mlp_fc2_0_1
	mlp_fc2_0_1 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_2
	mlp_fc1_0_2 -> mlp_fc2_0_2
	mlp_fc2_0_2 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_3
	mlp_fc1_0_3 -> mlp_fc2_0_3
	mlp_fc2_0_3 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_4
	mlp_fc1_0_4 -> mlp_fc2_0_4
	mlp_fc2_0_4 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_5
	mlp_fc1_0_5 -> mlp_fc2_0_5
	mlp_fc2_0_5 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_6
	mlp_fc1_0_6 -> mlp_fc2_0_6
	mlp_fc2_0_6 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_7
	mlp_fc1_0_7 -> mlp_fc2_0_7
	mlp_fc2_0_7 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_8
	mlp_fc1_0_8 -> mlp_fc2_0_8
	mlp_fc2_0_8 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_9
	mlp_fc1_0_9 -> mlp_fc2_0_9
	mlp_fc2_0_9 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_10
	mlp_fc1_0_10 -> mlp_fc2_0_10
	mlp_fc2_0_10 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_11
	mlp_fc1_0_11 -> mlp_fc2_0_11
	mlp_fc2_0_11 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_12
	mlp_fc1_0_12 -> mlp_fc2_0_12
	mlp_fc2_0_12 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_13
	mlp_fc1_0_13 -> mlp_fc2_0_13
	mlp_fc2_0_13 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_14
	mlp_fc1_0_14 -> mlp_fc2_0_14
	mlp_fc2_0_14 -> l0_mlp_concat
	l0_add -> mlp_fc1_0_15
	mlp_fc1_0_15 -> mlp_fc2_0_15
	mlp_fc2_0_15 -> l0_mlp_concat
	l0_mlp_concat -> l0_mlp_add
	l0_add -> l0_mlp_add
	l0_mlp_add -> ll1_input
	ll1_input -> q_1_0
	ll1_input -> k_1_0
	ll1_input -> v_1_0
	q_1_0 -> attn_1_0
	k_1_0 -> attn_1_0
	v_1_0 -> attn_1_0
	attn_1_0 -> l1_dim_concat
	ll1_input -> q_1_1
	ll1_input -> k_1_1
	ll1_input -> v_1_1
	q_1_1 -> attn_1_1
	k_1_1 -> attn_1_1
	v_1_1 -> attn_1_1
	attn_1_1 -> l1_dim_concat
	ll1_input -> q_1_2
	ll1_input -> k_1_2
	ll1_input -> v_1_2
	q_1_2 -> attn_1_2
	k_1_2 -> attn_1_2
	v_1_2 -> attn_1_2
	attn_1_2 -> l1_dim_concat
	ll1_input -> q_1_3
	ll1_input -> k_1_3
	ll1_input -> v_1_3
	q_1_3 -> attn_1_3
	k_1_3 -> attn_1_3
	v_1_3 -> attn_1_3
	attn_1_3 -> l1_dim_concat
	ll1_input -> q_1_4
	ll1_input -> k_1_4
	ll1_input -> v_1_4
	q_1_4 -> attn_1_4
	k_1_4 -> attn_1_4
	v_1_4 -> attn_1_4
	attn_1_4 -> l1_dim_concat
	ll1_input -> q_1_5
	ll1_input -> k_1_5
	ll1_input -> v_1_5
	q_1_5 -> attn_1_5
	k_1_5 -> attn_1_5
	v_1_5 -> attn_1_5
	attn_1_5 -> l1_dim_concat
	ll1_input -> q_1_6
	ll1_input -> k_1_6
	ll1_input -> v_1_6
	q_1_6 -> attn_1_6
	k_1_6 -> attn_1_6
	v_1_6 -> attn_1_6
	attn_1_6 -> l1_dim_concat
	ll1_input -> q_1_7
	ll1_input -> k_1_7
	ll1_input -> v_1_7
	q_1_7 -> attn_1_7
	k_1_7 -> attn_1_7
	v_1_7 -> attn_1_7
	attn_1_7 -> l1_dim_concat
	ll1_input -> q_1_8
	ll1_input -> k_1_8
	ll1_input -> v_1_8
	q_1_8 -> attn_1_8
	k_1_8 -> attn_1_8
	v_1_8 -> attn_1_8
	attn_1_8 -> l1_dim_concat
	ll1_input -> q_1_9
	ll1_input -> k_1_9
	ll1_input -> v_1_9
	q_1_9 -> attn_1_9
	k_1_9 -> attn_1_9
	v_1_9 -> attn_1_9
	attn_1_9 -> l1_dim_concat
	ll1_input -> q_1_10
	ll1_input -> k_1_10
	ll1_input -> v_1_10
	q_1_10 -> attn_1_10
	k_1_10 -> attn_1_10
	v_1_10 -> attn_1_10
	attn_1_10 -> l1_dim_concat
	ll1_input -> q_1_11
	ll1_input -> k_1_11
	ll1_input -> v_1_11
	q_1_11 -> attn_1_11
	k_1_11 -> attn_1_11
	v_1_11 -> attn_1_11
	attn_1_11 -> l1_dim_concat
	ll1_input -> q_1_12
	ll1_input -> k_1_12
	ll1_input -> v_1_12
	q_1_12 -> attn_1_12
	k_1_12 -> attn_1_12
	v_1_12 -> attn_1_12
	attn_1_12 -> l1_dim_concat
	ll1_input -> q_1_13
	ll1_input -> k_1_13
	ll1_input -> v_1_13
	q_1_13 -> attn_1_13
	k_1_13 -> attn_1_13
	v_1_13 -> attn_1_13
	attn_1_13 -> l1_dim_concat
	ll1_input -> q_1_14
	ll1_input -> k_1_14
	ll1_input -> v_1_14
	q_1_14 -> attn_1_14
	k_1_14 -> attn_1_14
	v_1_14 -> attn_1_14
	attn_1_14 -> l1_dim_concat
	ll1_input -> q_1_15
	ll1_input -> k_1_15
	ll1_input -> v_1_15
	q_1_15 -> attn_1_15
	k_1_15 -> attn_1_15
	v_1_15 -> attn_1_15
	attn_1_15 -> l1_dim_concat
	l1_dim_concat -> l1_add
	ll1_input -> l1_add
	l1_add -> mlp_fc1_1_0
	mlp_fc1_1_0 -> mlp_fc2_1_0
	mlp_fc2_1_0 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_1
	mlp_fc1_1_1 -> mlp_fc2_1_1
	mlp_fc2_1_1 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_2
	mlp_fc1_1_2 -> mlp_fc2_1_2
	mlp_fc2_1_2 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_3
	mlp_fc1_1_3 -> mlp_fc2_1_3
	mlp_fc2_1_3 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_4
	mlp_fc1_1_4 -> mlp_fc2_1_4
	mlp_fc2_1_4 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_5
	mlp_fc1_1_5 -> mlp_fc2_1_5
	mlp_fc2_1_5 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_6
	mlp_fc1_1_6 -> mlp_fc2_1_6
	mlp_fc2_1_6 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_7
	mlp_fc1_1_7 -> mlp_fc2_1_7
	mlp_fc2_1_7 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_8
	mlp_fc1_1_8 -> mlp_fc2_1_8
	mlp_fc2_1_8 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_9
	mlp_fc1_1_9 -> mlp_fc2_1_9
	mlp_fc2_1_9 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_10
	mlp_fc1_1_10 -> mlp_fc2_1_10
	mlp_fc2_1_10 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_11
	mlp_fc1_1_11 -> mlp_fc2_1_11
	mlp_fc2_1_11 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_12
	mlp_fc1_1_12 -> mlp_fc2_1_12
	mlp_fc2_1_12 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_13
	mlp_fc1_1_13 -> mlp_fc2_1_13
	mlp_fc2_1_13 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_14
	mlp_fc1_1_14 -> mlp_fc2_1_14
	mlp_fc2_1_14 -> l1_mlp_concat
	l1_add -> mlp_fc1_1_15
	mlp_fc1_1_15 -> mlp_fc2_1_15
	mlp_fc2_1_15 -> l1_mlp_concat
	l1_mlp_concat -> l1_mlp_add
	l1_add -> l1_mlp_add
	l1_mlp_add -> ll2_input
	ll2_input -> q_2_0
	ll2_input -> k_2_0
	ll2_input -> v_2_0
	q_2_0 -> attn_2_0
	k_2_0 -> attn_2_0
	v_2_0 -> attn_2_0
	attn_2_0 -> l2_dim_concat
	ll2_input -> q_2_1
	ll2_input -> k_2_1
	ll2_input -> v_2_1
	q_2_1 -> attn_2_1
	k_2_1 -> attn_2_1
	v_2_1 -> attn_2_1
	attn_2_1 -> l2_dim_concat
	ll2_input -> q_2_2
	ll2_input -> k_2_2
	ll2_input -> v_2_2
	q_2_2 -> attn_2_2
	k_2_2 -> attn_2_2
	v_2_2 -> attn_2_2
	attn_2_2 -> l2_dim_concat
	ll2_input -> q_2_3
	ll2_input -> k_2_3
	ll2_input -> v_2_3
	q_2_3 -> attn_2_3
	k_2_3 -> attn_2_3
	v_2_3 -> attn_2_3
	attn_2_3 -> l2_dim_concat
	ll2_input -> q_2_4
	ll2_input -> k_2_4
	ll2_input -> v_2_4
	q_2_4 -> attn_2_4
	k_2_4 -> attn_2_4
	v_2_4 -> attn_2_4
	attn_2_4 -> l2_dim_concat
	ll2_input -> q_2_5
	ll2_input -> k_2_5
	ll2_input -> v_2_5
	q_2_5 -> attn_2_5
	k_2_5 -> attn_2_5
	v_2_5 -> attn_2_5
	attn_2_5 -> l2_dim_concat
	ll2_input -> q_2_6
	ll2_input -> k_2_6
	ll2_input -> v_2_6
	q_2_6 -> attn_2_6
	k_2_6 -> attn_2_6
	v_2_6 -> attn_2_6
	attn_2_6 -> l2_dim_concat
	ll2_input -> q_2_7
	ll2_input -> k_2_7
	ll2_input -> v_2_7
	q_2_7 -> attn_2_7
	k_2_7 -> attn_2_7
	v_2_7 -> attn_2_7
	attn_2_7 -> l2_dim_concat
	ll2_input -> q_2_8
	ll2_input -> k_2_8
	ll2_input -> v_2_8
	q_2_8 -> attn_2_8
	k_2_8 -> attn_2_8
	v_2_8 -> attn_2_8
	attn_2_8 -> l2_dim_concat
	ll2_input -> q_2_9
	ll2_input -> k_2_9
	ll2_input -> v_2_9
	q_2_9 -> attn_2_9
	k_2_9 -> attn_2_9
	v_2_9 -> attn_2_9
	attn_2_9 -> l2_dim_concat
	ll2_input -> q_2_10
	ll2_input -> k_2_10
	ll2_input -> v_2_10
	q_2_10 -> attn_2_10
	k_2_10 -> attn_2_10
	v_2_10 -> attn_2_10
	attn_2_10 -> l2_dim_concat
	ll2_input -> q_2_11
	ll2_input -> k_2_11
	ll2_input -> v_2_11
	q_2_11 -> attn_2_11
	k_2_11 -> attn_2_11
	v_2_11 -> attn_2_11
	attn_2_11 -> l2_dim_concat
	ll2_input -> q_2_12
	ll2_input -> k_2_12
	ll2_input -> v_2_12
	q_2_12 -> attn_2_12
	k_2_12 -> attn_2_12
	v_2_12 -> attn_2_12
	attn_2_12 -> l2_dim_concat
	ll2_input -> q_2_13
	ll2_input -> k_2_13
	ll2_input -> v_2_13
	q_2_13 -> attn_2_13
	k_2_13 -> attn_2_13
	v_2_13 -> attn_2_13
	attn_2_13 -> l2_dim_concat
	ll2_input -> q_2_14
	ll2_input -> k_2_14
	ll2_input -> v_2_14
	q_2_14 -> attn_2_14
	k_2_14 -> attn_2_14
	v_2_14 -> attn_2_14
	attn_2_14 -> l2_dim_concat
	ll2_input -> q_2_15
	ll2_input -> k_2_15
	ll2_input -> v_2_15
	q_2_15 -> attn_2_15
	k_2_15 -> attn_2_15
	v_2_15 -> attn_2_15
	attn_2_15 -> l2_dim_concat
	l2_dim_concat -> l2_add
	ll2_input -> l2_add
	l2_add -> mlp_fc1_2_0
	mlp_fc1_2_0 -> mlp_fc2_2_0
	mlp_fc2_2_0 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_1
	mlp_fc1_2_1 -> mlp_fc2_2_1
	mlp_fc2_2_1 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_2
	mlp_fc1_2_2 -> mlp_fc2_2_2
	mlp_fc2_2_2 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_3
	mlp_fc1_2_3 -> mlp_fc2_2_3
	mlp_fc2_2_3 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_4
	mlp_fc1_2_4 -> mlp_fc2_2_4
	mlp_fc2_2_4 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_5
	mlp_fc1_2_5 -> mlp_fc2_2_5
	mlp_fc2_2_5 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_6
	mlp_fc1_2_6 -> mlp_fc2_2_6
	mlp_fc2_2_6 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_7
	mlp_fc1_2_7 -> mlp_fc2_2_7
	mlp_fc2_2_7 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_8
	mlp_fc1_2_8 -> mlp_fc2_2_8
	mlp_fc2_2_8 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_9
	mlp_fc1_2_9 -> mlp_fc2_2_9
	mlp_fc2_2_9 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_10
	mlp_fc1_2_10 -> mlp_fc2_2_10
	mlp_fc2_2_10 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_11
	mlp_fc1_2_11 -> mlp_fc2_2_11
	mlp_fc2_2_11 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_12
	mlp_fc1_2_12 -> mlp_fc2_2_12
	mlp_fc2_2_12 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_13
	mlp_fc1_2_13 -> mlp_fc2_2_13
	mlp_fc2_2_13 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_14
	mlp_fc1_2_14 -> mlp_fc2_2_14
	mlp_fc2_2_14 -> l2_mlp_concat
	l2_add -> mlp_fc1_2_15
	mlp_fc1_2_15 -> mlp_fc2_2_15
	mlp_fc2_2_15 -> l2_mlp_concat
	l2_mlp_concat -> l2_mlp_add
	l2_add -> l2_mlp_add
	l2_mlp_add -> ll3_input
	ll3_input -> q_3_0
	ll3_input -> k_3_0
	ll3_input -> v_3_0
	q_3_0 -> attn_3_0
	k_3_0 -> attn_3_0
	v_3_0 -> attn_3_0
	attn_3_0 -> l3_dim_concat
	ll3_input -> q_3_1
	ll3_input -> k_3_1
	ll3_input -> v_3_1
	q_3_1 -> attn_3_1
	k_3_1 -> attn_3_1
	v_3_1 -> attn_3_1
	attn_3_1 -> l3_dim_concat
	ll3_input -> q_3_2
	ll3_input -> k_3_2
	ll3_input -> v_3_2
	q_3_2 -> attn_3_2
	k_3_2 -> attn_3_2
	v_3_2 -> attn_3_2
	attn_3_2 -> l3_dim_concat
	ll3_input -> q_3_3
	ll3_input -> k_3_3
	ll3_input -> v_3_3
	q_3_3 -> attn_3_3
	k_3_3 -> attn_3_3
	v_3_3 -> attn_3_3
	attn_3_3 -> l3_dim_concat
	ll3_input -> q_3_4
	ll3_input -> k_3_4
	ll3_input -> v_3_4
	q_3_4 -> attn_3_4
	k_3_4 -> attn_3_4
	v_3_4 -> attn_3_4
	attn_3_4 -> l3_dim_concat
	ll3_input -> q_3_5
	ll3_input -> k_3_5
	ll3_input -> v_3_5
	q_3_5 -> attn_3_5
	k_3_5 -> attn_3_5
	v_3_5 -> attn_3_5
	attn_3_5 -> l3_dim_concat
	ll3_input -> q_3_6
	ll3_input -> k_3_6
	ll3_input -> v_3_6
	q_3_6 -> attn_3_6
	k_3_6 -> attn_3_6
	v_3_6 -> attn_3_6
	attn_3_6 -> l3_dim_concat
	ll3_input -> q_3_7
	ll3_input -> k_3_7
	ll3_input -> v_3_7
	q_3_7 -> attn_3_7
	k_3_7 -> attn_3_7
	v_3_7 -> attn_3_7
	attn_3_7 -> l3_dim_concat
	ll3_input -> q_3_8
	ll3_input -> k_3_8
	ll3_input -> v_3_8
	q_3_8 -> attn_3_8
	k_3_8 -> attn_3_8
	v_3_8 -> attn_3_8
	attn_3_8 -> l3_dim_concat
	ll3_input -> q_3_9
	ll3_input -> k_3_9
	ll3_input -> v_3_9
	q_3_9 -> attn_3_9
	k_3_9 -> attn_3_9
	v_3_9 -> attn_3_9
	attn_3_9 -> l3_dim_concat
	ll3_input -> q_3_10
	ll3_input -> k_3_10
	ll3_input -> v_3_10
	q_3_10 -> attn_3_10
	k_3_10 -> attn_3_10
	v_3_10 -> attn_3_10
	attn_3_10 -> l3_dim_concat
	ll3_input -> q_3_11
	ll3_input -> k_3_11
	ll3_input -> v_3_11
	q_3_11 -> attn_3_11
	k_3_11 -> attn_3_11
	v_3_11 -> attn_3_11
	attn_3_11 -> l3_dim_concat
	ll3_input -> q_3_12
	ll3_input -> k_3_12
	ll3_input -> v_3_12
	q_3_12 -> attn_3_12
	k_3_12 -> attn_3_12
	v_3_12 -> attn_3_12
	attn_3_12 -> l3_dim_concat
	ll3_input -> q_3_13
	ll3_input -> k_3_13
	ll3_input -> v_3_13
	q_3_13 -> attn_3_13
	k_3_13 -> attn_3_13
	v_3_13 -> attn_3_13
	attn_3_13 -> l3_dim_concat
	ll3_input -> q_3_14
	ll3_input -> k_3_14
	ll3_input -> v_3_14
	q_3_14 -> attn_3_14
	k_3_14 -> attn_3_14
	v_3_14 -> attn_3_14
	attn_3_14 -> l3_dim_concat
	ll3_input -> q_3_15
	ll3_input -> k_3_15
	ll3_input -> v_3_15
	q_3_15 -> attn_3_15
	k_3_15 -> attn_3_15
	v_3_15 -> attn_3_15
	attn_3_15 -> l3_dim_concat
	l3_dim_concat -> l3_add
	ll3_input -> l3_add
	l3_add -> mlp_fc1_3_0
	mlp_fc1_3_0 -> mlp_fc2_3_0
	mlp_fc2_3_0 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_1
	mlp_fc1_3_1 -> mlp_fc2_3_1
	mlp_fc2_3_1 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_2
	mlp_fc1_3_2 -> mlp_fc2_3_2
	mlp_fc2_3_2 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_3
	mlp_fc1_3_3 -> mlp_fc2_3_3
	mlp_fc2_3_3 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_4
	mlp_fc1_3_4 -> mlp_fc2_3_4
	mlp_fc2_3_4 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_5
	mlp_fc1_3_5 -> mlp_fc2_3_5
	mlp_fc2_3_5 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_6
	mlp_fc1_3_6 -> mlp_fc2_3_6
	mlp_fc2_3_6 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_7
	mlp_fc1_3_7 -> mlp_fc2_3_7
	mlp_fc2_3_7 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_8
	mlp_fc1_3_8 -> mlp_fc2_3_8
	mlp_fc2_3_8 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_9
	mlp_fc1_3_9 -> mlp_fc2_3_9
	mlp_fc2_3_9 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_10
	mlp_fc1_3_10 -> mlp_fc2_3_10
	mlp_fc2_3_10 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_11
	mlp_fc1_3_11 -> mlp_fc2_3_11
	mlp_fc2_3_11 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_12
	mlp_fc1_3_12 -> mlp_fc2_3_12
	mlp_fc2_3_12 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_13
	mlp_fc1_3_13 -> mlp_fc2_3_13
	mlp_fc2_3_13 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_14
	mlp_fc1_3_14 -> mlp_fc2_3_14
	mlp_fc2_3_14 -> l3_mlp_concat
	l3_add -> mlp_fc1_3_15
	mlp_fc1_3_15 -> mlp_fc2_3_15
	mlp_fc2_3_15 -> l3_mlp_concat
	l3_mlp_concat -> l3_mlp_add
	l3_add -> l3_mlp_add
	l3_mlp_add -> output
}
