{
  "generated_dags": {
    "baseline": {
      "dot_file": "../outputs/2025-11-24-20-37-50/baseline_dag.dot",
      "svg_file": "../outputs/2025-11-24-20-37-50/baseline_dag.svg",
      "description": "Traditional Tensor Parallelism (TP=8) + Pipeline Parallelism (PP=2) baseline configuration across 16 GPUs"
    },
    "proposed": {
      "standard_dot": "../outputs/2025-11-24-20-37-50/proposed_dag.dot",
      "standard_svg": "../outputs/2025-11-24-20-37-50/proposed_dag.svg",
      "detailed_dot": "../outputs/2025-11-24-20-37-50/proposed_detailed_dag.dot", 
      "detailed_svg": "../outputs/2025-11-24-20-37-50/proposed_detailed_dag.svg",
      "description": "Two-Level Attention Partitioning with m√ón=16 partitions across 16 GPUs"
    }
  },
  "analysis_summary": {
    "baseline_strategy": {
      "parallel_method": "Tensor Parallelism + Pipeline Parallelism",
      "tensor_parallel_degree": 8,
      "pipeline_parallel_degree": 2,
      "total_gpus": 16,
      "layers_per_stage": 2,
      "attention_heads_partitioning": "heads_split_across_8_gpus",
      "dimension_partitioning": "none"
    },
    "proposed_strategy": {
      "parallel_method": "Two-Level Attention Partitioning",
      "total_partitions": 16,
      "head_partitions": 4,
      "dimension_partitions": 4,
      "heads_per_partition": 8,
      "dimensions_per_partition": 32,
      "total_gpus": 16,
      "unique_partitions_per_gpu": 1
    },
    "verification": {
      "baseline_dag_valid": true,
      "proposed_dag_valid": true,
      "no_cycles_detected": true,
      "all_nodes_connected": true,
      "dimensions_aligned": true
    }
  }
}