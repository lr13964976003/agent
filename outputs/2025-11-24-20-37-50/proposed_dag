// Dense Transformer with Two-Level Attention Partitioning
digraph Proposed_Model_DAG {
	rankdir=TB size="20,15"
	node [fillcolor=lightgray shape=rectangle style=filled]
	subgraph cluster_partition_0 {
		color=green label="Partition 0 (GPU 0)
Heads: [0-7], Dim: [0-31]" style=dotted
		l0_q_0 [label="Q_Projection_0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		l0_k_0 [label="K_Projection_0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		l0_v_0 [label="V_Projection_0
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
		l0_attn_0 [label="Attention_0
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
	}
	subgraph cluster_partition_1 {
		color=green label="Partition 1 (GPU 1)
Heads: [0-7], Dim: [32-63]" style=dotted
		l0_q_1 [label="Q_Projection_1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		l0_k_1 [label="K_Projection_1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		l0_v_1 [label="V_Projection_1
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
		l0_attn_1 [label="Attention_1
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
	}
	subgraph cluster_partition_2 {
		color=green label="Partition 2 (GPU 2)
Heads: [0-7], Dim: [64-95]" style=dotted
		l0_q_2 [label="Q_Projection_2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		l0_k_2 [label="K_Projection_2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		l0_v_2 [label="V_Projection_2
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
		l0_attn_2 [label="Attention_2
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
	}
	subgraph cluster_partition_3 {
		color=green label="Partition 3 (GPU 3)
Heads: [0-7], Dim: [96-127]" style=dotted
		l0_q_3 [label="Q_Projection_3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		l0_k_3 [label="K_Projection_3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		l0_v_3 [label="V_Projection_3
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
		l0_attn_3 [label="Attention_3
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
	}
	subgraph cluster_partition_4 {
		color=green label="Partition 4 (GPU 4)
Heads: [8-15], Dim: [0-31]" style=dotted
		l0_q_4 [label="Q_Projection_4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		l0_k_4 [label="K_Projection_4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		l0_v_4 [label="V_Projection_4
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
		l0_attn_4 [label="Attention_4
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
	}
	subgraph cluster_partition_5 {
		color=green label="Partition 5 (GPU 5)
Heads: [8-15], Dim: [32-63]" style=dotted
		l0_q_5 [label="Q_Projection_5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		l0_k_5 [label="K_Projection_5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		l0_v_5 [label="V_Projection_5
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
		l0_attn_5 [label="Attention_5
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
	}
	subgraph cluster_partition_6 {
		color=green label="Partition 6 (GPU 6)
Heads: [8-15], Dim: [64-95]" style=dotted
		l0_q_6 [label="Q_Projection_6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		l0_k_6 [label="K_Projection_6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		l0_v_6 [label="V_Projection_6
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
		l0_attn_6 [label="Attention_6
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
	}
	subgraph cluster_partition_7 {
		color=green label="Partition 7 (GPU 7)
Heads: [8-15], Dim: [96-127]" style=dotted
		l0_q_7 [label="Q_Projection_7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		l0_k_7 [label="K_Projection_7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		l0_v_7 [label="V_Projection_7
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
		l0_attn_7 [label="Attention_7
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
	}
	subgraph cluster_partition_8 {
		color=green label="Partition 8 (GPU 8)
Heads: [16-23], Dim: [0-31]" style=dotted
		l0_q_8 [label="Q_Projection_8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		l0_k_8 [label="K_Projection_8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		l0_v_8 [label="V_Projection_8
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
		l0_attn_8 [label="Attention_8
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
	}
	subgraph cluster_partition_9 {
		color=green label="Partition 9 (GPU 9)
Heads: [16-23], Dim: [32-63]" style=dotted
		l0_q_9 [label="Q_Projection_9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		l0_k_9 [label="K_Projection_9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		l0_v_9 [label="V_Projection_9
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
		l0_attn_9 [label="Attention_9
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
	}
	subgraph cluster_partition_10 {
		color=green label="Partition 10 (GPU 10)
Heads: [16-23], Dim: [64-95]" style=dotted
		l0_q_10 [label="Q_Projection_10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		l0_k_10 [label="K_Projection_10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		l0_v_10 [label="V_Projection_10
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
		l0_attn_10 [label="Attention_10
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
	}
	subgraph cluster_partition_11 {
		color=green label="Partition 11 (GPU 11)
Heads: [16-23], Dim: [96-127]" style=dotted
		l0_q_11 [label="Q_Projection_11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		l0_k_11 [label="K_Projection_11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		l0_v_11 [label="V_Projection_11
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
		l0_attn_11 [label="Attention_11
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
	}
	subgraph cluster_partition_12 {
		color=green label="Partition 12 (GPU 12)
Heads: [24-31], Dim: [0-31]" style=dotted
		l0_q_12 [label="Q_Projection_12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		l0_k_12 [label="K_Projection_12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		l0_v_12 [label="V_Projection_12
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
		l0_attn_12 [label="Attention_12
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
	}
	subgraph cluster_partition_13 {
		color=green label="Partition 13 (GPU 13)
Heads: [24-31], Dim: [32-63]" style=dotted
		l0_q_13 [label="Q_Projection_13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		l0_k_13 [label="K_Projection_13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		l0_v_13 [label="V_Projection_13
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
		l0_attn_13 [label="Attention_13
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
	}
	subgraph cluster_partition_14 {
		color=green label="Partition 14 (GPU 14)
Heads: [24-31], Dim: [64-95]" style=dotted
		l0_q_14 [label="Q_Projection_14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		l0_k_14 [label="K_Projection_14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		l0_v_14 [label="V_Projection_14
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
		l0_attn_14 [label="Attention_14
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
	}
	subgraph cluster_partition_15 {
		color=green label="Partition 15 (GPU 15)
Heads: [24-31], Dim: [96-127]" style=dotted
		l0_q_15 [label="Q_Projection_15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		l0_k_15 [label="K_Projection_15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		l0_v_15 [label="V_Projection_15
Input: [128,10000,4096]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
		l0_attn_15 [label="Attention_15
Input: Q:[128,10000,256], K:[128,10000,256], V:[128,10000,256]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
	}
	subgraph cluster_l0 {
		color=blue label="Layer 0: Two-Level Attention Partitioning (m×n=16)" style=dashed
		input_l0 [label="Input_Layer0
Input: [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightgreen shape=ellipse]
	}
	l0_stage1_concat [label="Layer0_Dimension_Concatenation
Input: [128,10000,256]×4×4
Output: [128,10000,1024]×4
GPU: 0,4,8,12" fillcolor=yellow shape=parallelogram]
	l0_stage2_concat [label="Layer0_Head_Concatenation
Input: [128,10000,1024]×4
Output: [128,10000,4096]
GPU: 0" fillcolor=yellow shape=parallelogram]
	l0_add [label="Layer0_Add&Norm
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
	l0_mlp_fc1_0 [label="MLP_FC1_0
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 0" shape=rectangle]
	l0_mlp_fc2_0 [label="MLP_FC2_0
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 0" shape=rectangle]
	l0_mlp_fc1_1 [label="MLP_FC1_1
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 1" shape=rectangle]
	l0_mlp_fc2_1 [label="MLP_FC2_1
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 1" shape=rectangle]
	l0_mlp_fc1_2 [label="MLP_FC1_2
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 2" shape=rectangle]
	l0_mlp_fc2_2 [label="MLP_FC2_2
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 2" shape=rectangle]
	l0_mlp_fc1_3 [label="MLP_FC1_3
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 3" shape=rectangle]
	l0_mlp_fc2_3 [label="MLP_FC2_3
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 3" shape=rectangle]
	l0_mlp_fc1_4 [label="MLP_FC1_4
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 4" shape=rectangle]
	l0_mlp_fc2_4 [label="MLP_FC2_4
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 4" shape=rectangle]
	l0_mlp_fc1_5 [label="MLP_FC1_5
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 5" shape=rectangle]
	l0_mlp_fc2_5 [label="MLP_FC2_5
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 5" shape=rectangle]
	l0_mlp_fc1_6 [label="MLP_FC1_6
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 6" shape=rectangle]
	l0_mlp_fc2_6 [label="MLP_FC2_6
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 6" shape=rectangle]
	l0_mlp_fc1_7 [label="MLP_FC1_7
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 7" shape=rectangle]
	l0_mlp_fc2_7 [label="MLP_FC2_7
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 7" shape=rectangle]
	l0_mlp_fc1_8 [label="MLP_FC1_8
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 8" shape=rectangle]
	l0_mlp_fc2_8 [label="MLP_FC2_8
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 8" shape=rectangle]
	l0_mlp_fc1_9 [label="MLP_FC1_9
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 9" shape=rectangle]
	l0_mlp_fc2_9 [label="MLP_FC2_9
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 9" shape=rectangle]
	l0_mlp_fc1_10 [label="MLP_FC1_10
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 10" shape=rectangle]
	l0_mlp_fc2_10 [label="MLP_FC2_10
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 10" shape=rectangle]
	l0_mlp_fc1_11 [label="MLP_FC1_11
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 11" shape=rectangle]
	l0_mlp_fc2_11 [label="MLP_FC2_11
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 11" shape=rectangle]
	l0_mlp_fc1_12 [label="MLP_FC1_12
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 12" shape=rectangle]
	l0_mlp_fc2_12 [label="MLP_FC2_12
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 12" shape=rectangle]
	l0_mlp_fc1_13 [label="MLP_FC1_13
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 13" shape=rectangle]
	l0_mlp_fc2_13 [label="MLP_FC2_13
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 13" shape=rectangle]
	l0_mlp_fc1_14 [label="MLP_FC1_14
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 14" shape=rectangle]
	l0_mlp_fc2_14 [label="MLP_FC2_14
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 14" shape=rectangle]
	l0_mlp_fc1_15 [label="MLP_FC1_15
Input: [128,10000,256]
Output: [128,10000,1024]
GPU: 15" shape=rectangle]
	l0_mlp_fc2_15 [label="MLP_FC2_15
Input: [128,10000,1024]
Output: [128,10000,256]
GPU: 15" shape=rectangle]
	l0_mlp_stage1_concat [label="Layer0_MLP_Dimension_Concat
Input: [128,10000,256]×4×4
Output: [128,10000,1024]×4
GPU: 0,4,8,12" fillcolor=yellow shape=parallelogram]
	l0_mlp_stage2_concat [label="Layer0_MLP_Head_Concat
Input: [128,10000,1024]×4
Output: [128,10000,4096]
GPU: 0" fillcolor=yellow shape=parallelogram]
	l0_mlp_add [label="Layer0_MLP_Add&Norm
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
	layer1_block [label="Layer 1
Complete Layer with Two-Level Partitioning
Input: [128,10000,4096]
Output: [128,10000,4096]
GPU: 0-15
(16 partitions, similar to Layer 0)" fillcolor=lightcyan shape=rectangle]
	layer2_block [label="Layer 2
Complete Layer with Two-Level Partitioning
Input: [128,10000,4096]
Output: [128,10000,4096]
GPU: 0-15
(16 partitions, similar to Layer 0)" fillcolor=lightcyan shape=rectangle]
	layer3_block [label="Layer 3
Complete Layer with Two-Level Partitioning
Input: [128,10000,4096]
Output: [128,10000,4096]
GPU: 0-15
(16 partitions, similar to Layer 0)" fillcolor=lightcyan shape=rectangle]
	output [label="Final_Output
Input: [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightgreen shape=ellipse]
	input_l0 -> l0_q_0
	input_l0 -> l0_k_0
	input_l0 -> l0_v_0
	l0_q_0 -> l0_attn_0
	l0_k_0 -> l0_attn_0
	l0_v_0 -> l0_attn_0
	l0_attn_0 -> l0_stage1_concat
	input_l0 -> l0_q_1
	input_l0 -> l0_k_1
	input_l0 -> l0_v_1
	l0_q_1 -> l0_attn_1
	l0_k_1 -> l0_attn_1
	l0_v_1 -> l0_attn_1
	l0_attn_1 -> l0_stage1_concat
	input_l0 -> l0_q_2
	input_l0 -> l0_k_2
	input_l0 -> l0_v_2
	l0_q_2 -> l0_attn_2
	l0_k_2 -> l0_attn_2
	l0_v_2 -> l0_attn_2
	l0_attn_2 -> l0_stage1_concat
	input_l0 -> l0_q_3
	input_l0 -> l0_k_3
	input_l0 -> l0_v_3
	l0_q_3 -> l0_attn_3
	l0_k_3 -> l0_attn_3
	l0_v_3 -> l0_attn_3
	l0_attn_3 -> l0_stage1_concat
	input_l0 -> l0_q_4
	input_l0 -> l0_k_4
	input_l0 -> l0_v_4
	l0_q_4 -> l0_attn_4
	l0_k_4 -> l0_attn_4
	l0_v_4 -> l0_attn_4
	l0_attn_4 -> l0_stage1_concat
	input_l0 -> l0_q_5
	input_l0 -> l0_k_5
	input_l0 -> l0_v_5
	l0_q_5 -> l0_attn_5
	l0_k_5 -> l0_attn_5
	l0_v_5 -> l0_attn_5
	l0_attn_5 -> l0_stage1_concat
	input_l0 -> l0_q_6
	input_l0 -> l0_k_6
	input_l0 -> l0_v_6
	l0_q_6 -> l0_attn_6
	l0_k_6 -> l0_attn_6
	l0_v_6 -> l0_attn_6
	l0_attn_6 -> l0_stage1_concat
	input_l0 -> l0_q_7
	input_l0 -> l0_k_7
	input_l0 -> l0_v_7
	l0_q_7 -> l0_attn_7
	l0_k_7 -> l0_attn_7
	l0_v_7 -> l0_attn_7
	l0_attn_7 -> l0_stage1_concat
	input_l0 -> l0_q_8
	input_l0 -> l0_k_8
	input_l0 -> l0_v_8
	l0_q_8 -> l0_attn_8
	l0_k_8 -> l0_attn_8
	l0_v_8 -> l0_attn_8
	l0_attn_8 -> l0_stage1_concat
	input_l0 -> l0_q_9
	input_l0 -> l0_k_9
	input_l0 -> l0_v_9
	l0_q_9 -> l0_attn_9
	l0_k_9 -> l0_attn_9
	l0_v_9 -> l0_attn_9
	l0_attn_9 -> l0_stage1_concat
	input_l0 -> l0_q_10
	input_l0 -> l0_k_10
	input_l0 -> l0_v_10
	l0_q_10 -> l0_attn_10
	l0_k_10 -> l0_attn_10
	l0_v_10 -> l0_attn_10
	l0_attn_10 -> l0_stage1_concat
	input_l0 -> l0_q_11
	input_l0 -> l0_k_11
	input_l0 -> l0_v_11
	l0_q_11 -> l0_attn_11
	l0_k_11 -> l0_attn_11
	l0_v_11 -> l0_attn_11
	l0_attn_11 -> l0_stage1_concat
	input_l0 -> l0_q_12
	input_l0 -> l0_k_12
	input_l0 -> l0_v_12
	l0_q_12 -> l0_attn_12
	l0_k_12 -> l0_attn_12
	l0_v_12 -> l0_attn_12
	l0_attn_12 -> l0_stage1_concat
	input_l0 -> l0_q_13
	input_l0 -> l0_k_13
	input_l0 -> l0_v_13
	l0_q_13 -> l0_attn_13
	l0_k_13 -> l0_attn_13
	l0_v_13 -> l0_attn_13
	l0_attn_13 -> l0_stage1_concat
	input_l0 -> l0_q_14
	input_l0 -> l0_k_14
	input_l0 -> l0_v_14
	l0_q_14 -> l0_attn_14
	l0_k_14 -> l0_attn_14
	l0_v_14 -> l0_attn_14
	l0_attn_14 -> l0_stage1_concat
	input_l0 -> l0_q_15
	input_l0 -> l0_k_15
	input_l0 -> l0_v_15
	l0_q_15 -> l0_attn_15
	l0_k_15 -> l0_attn_15
	l0_v_15 -> l0_attn_15
	l0_attn_15 -> l0_stage1_concat
	l0_attn_0 -> l0_stage1_concat
	l0_attn_1 -> l0_stage1_concat
	l0_attn_2 -> l0_stage1_concat
	l0_attn_3 -> l0_stage1_concat
	l0_attn_4 -> l0_stage1_concat
	l0_attn_5 -> l0_stage1_concat
	l0_attn_6 -> l0_stage1_concat
	l0_attn_7 -> l0_stage1_concat
	l0_attn_8 -> l0_stage1_concat
	l0_attn_9 -> l0_stage1_concat
	l0_attn_10 -> l0_stage1_concat
	l0_attn_11 -> l0_stage1_concat
	l0_attn_12 -> l0_stage1_concat
	l0_attn_13 -> l0_stage1_concat
	l0_attn_14 -> l0_stage1_concat
	l0_attn_15 -> l0_stage1_concat
	l0_stage1_concat -> l0_stage2_concat
	l0_stage2_concat -> l0_add
	input_l0 -> l0_add
	l0_add -> l0_mlp_fc1_0
	l0_mlp_fc1_0 -> l0_mlp_fc2_0
	l0_mlp_fc2_0 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_1
	l0_mlp_fc1_1 -> l0_mlp_fc2_1
	l0_mlp_fc2_1 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_2
	l0_mlp_fc1_2 -> l0_mlp_fc2_2
	l0_mlp_fc2_2 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_3
	l0_mlp_fc1_3 -> l0_mlp_fc2_3
	l0_mlp_fc2_3 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_4
	l0_mlp_fc1_4 -> l0_mlp_fc2_4
	l0_mlp_fc2_4 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_5
	l0_mlp_fc1_5 -> l0_mlp_fc2_5
	l0_mlp_fc2_5 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_6
	l0_mlp_fc1_6 -> l0_mlp_fc2_6
	l0_mlp_fc2_6 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_7
	l0_mlp_fc1_7 -> l0_mlp_fc2_7
	l0_mlp_fc2_7 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_8
	l0_mlp_fc1_8 -> l0_mlp_fc2_8
	l0_mlp_fc2_8 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_9
	l0_mlp_fc1_9 -> l0_mlp_fc2_9
	l0_mlp_fc2_9 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_10
	l0_mlp_fc1_10 -> l0_mlp_fc2_10
	l0_mlp_fc2_10 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_11
	l0_mlp_fc1_11 -> l0_mlp_fc2_11
	l0_mlp_fc2_11 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_12
	l0_mlp_fc1_12 -> l0_mlp_fc2_12
	l0_mlp_fc2_12 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_13
	l0_mlp_fc1_13 -> l0_mlp_fc2_13
	l0_mlp_fc2_13 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_14
	l0_mlp_fc1_14 -> l0_mlp_fc2_14
	l0_mlp_fc2_14 -> l0_mlp_stage1_concat
	l0_add -> l0_mlp_fc1_15
	l0_mlp_fc1_15 -> l0_mlp_fc2_15
	l0_mlp_fc2_15 -> l0_mlp_stage1_concat
	l0_mlp_stage1_concat -> l0_mlp_stage2_concat
	l0_mlp_stage2_concat -> l0_mlp_add
	l0_add -> l0_mlp_add
	l0_mlp_add -> layer1_block
	layer1_block -> layer2_block
	layer2_block -> layer3_block
	layer3_block -> output
}
