// Dense Transformer with TP=8, PP=2
digraph Baseline_Model_DAG {
	rankdir=TB size="20,15"
	node [fillcolor=lightgray shape=rectangle style=filled]
	subgraph cluster_0 {
		color=blue label="Stage 0: GPUs 0-7 (Layers 0-1)" style=dashed
		input [label="Input
Input: [batch_size=128, seq_len=10000, hidden_size=4096]
Output: [batch_size=128, seq_len=10000, hidden_size=4096]
GPU: all GPUs" fillcolor=lightgreen shape=ellipse]
		l0_q_0 [label="Layer0_Q_Projection_0
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 0" shape=rectangle]
		l0_k_0 [label="Layer0_K_Projection_0
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 0" shape=rectangle]
		l0_v_0 [label="Layer0_V_Projection_0
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 0" shape=rectangle]
		l0_attn_0 [label="Layer0_Attention_0
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 0" shape=rectangle]
		l0_out_0 [label="Layer0_Output_Projection_0
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 0" shape=rectangle]
		l0_q_1 [label="Layer0_Q_Projection_1
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 1" shape=rectangle]
		l0_k_1 [label="Layer0_K_Projection_1
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 1" shape=rectangle]
		l0_v_1 [label="Layer0_V_Projection_1
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 1" shape=rectangle]
		l0_attn_1 [label="Layer0_Attention_1
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 1" shape=rectangle]
		l0_out_1 [label="Layer0_Output_Projection_1
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 1" shape=rectangle]
		l0_q_2 [label="Layer0_Q_Projection_2
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 2" shape=rectangle]
		l0_k_2 [label="Layer0_K_Projection_2
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 2" shape=rectangle]
		l0_v_2 [label="Layer0_V_Projection_2
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 2" shape=rectangle]
		l0_attn_2 [label="Layer0_Attention_2
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 2" shape=rectangle]
		l0_out_2 [label="Layer0_Output_Projection_2
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 2" shape=rectangle]
		l0_q_3 [label="Layer0_Q_Projection_3
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 3" shape=rectangle]
		l0_k_3 [label="Layer0_K_Projection_3
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 3" shape=rectangle]
		l0_v_3 [label="Layer0_V_Projection_3
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 3" shape=rectangle]
		l0_attn_3 [label="Layer0_Attention_3
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 3" shape=rectangle]
		l0_out_3 [label="Layer0_Output_Projection_3
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 3" shape=rectangle]
		l0_q_4 [label="Layer0_Q_Projection_4
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 4" shape=rectangle]
		l0_k_4 [label="Layer0_K_Projection_4
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 4" shape=rectangle]
		l0_v_4 [label="Layer0_V_Projection_4
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 4" shape=rectangle]
		l0_attn_4 [label="Layer0_Attention_4
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 4" shape=rectangle]
		l0_out_4 [label="Layer0_Output_Projection_4
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 4" shape=rectangle]
		l0_q_5 [label="Layer0_Q_Projection_5
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 5" shape=rectangle]
		l0_k_5 [label="Layer0_K_Projection_5
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 5" shape=rectangle]
		l0_v_5 [label="Layer0_V_Projection_5
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 5" shape=rectangle]
		l0_attn_5 [label="Layer0_Attention_5
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 5" shape=rectangle]
		l0_out_5 [label="Layer0_Output_Projection_5
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 5" shape=rectangle]
		l0_q_6 [label="Layer0_Q_Projection_6
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 6" shape=rectangle]
		l0_k_6 [label="Layer0_K_Projection_6
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 6" shape=rectangle]
		l0_v_6 [label="Layer0_V_Projection_6
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 6" shape=rectangle]
		l0_attn_6 [label="Layer0_Attention_6
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 6" shape=rectangle]
		l0_out_6 [label="Layer0_Output_Projection_6
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 6" shape=rectangle]
		l0_q_7 [label="Layer0_Q_Projection_7
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 7" shape=rectangle]
		l0_k_7 [label="Layer0_K_Projection_7
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 7" shape=rectangle]
		l0_v_7 [label="Layer0_V_Projection_7
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 7" shape=rectangle]
		l0_attn_7 [label="Layer0_Attention_7
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 7" shape=rectangle]
		l0_out_7 [label="Layer0_Output_Projection_7
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 7" shape=rectangle]
		l0_reduce [label="Layer0_Attention_Reduce
Input: [128,10000,512]×8
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=yellow shape=parallelogram]
		l0_add [label="Layer0_Add&Norm
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
		l0_mlp_fc1_0 [label="Layer0_MLP_FC1_0
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 0" shape=rectangle]
		l0_mlp_fc2_0 [label="Layer0_MLP_FC2_0
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 0" shape=rectangle]
		l0_mlp_fc1_1 [label="Layer0_MLP_FC1_1
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 1" shape=rectangle]
		l0_mlp_fc2_1 [label="Layer0_MLP_FC2_1
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 1" shape=rectangle]
		l0_mlp_fc1_2 [label="Layer0_MLP_FC1_2
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 2" shape=rectangle]
		l0_mlp_fc2_2 [label="Layer0_MLP_FC2_2
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 2" shape=rectangle]
		l0_mlp_fc1_3 [label="Layer0_MLP_FC1_3
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 3" shape=rectangle]
		l0_mlp_fc2_3 [label="Layer0_MLP_FC2_3
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 3" shape=rectangle]
		l0_mlp_fc1_4 [label="Layer0_MLP_FC1_4
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 4" shape=rectangle]
		l0_mlp_fc2_4 [label="Layer0_MLP_FC2_4
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 4" shape=rectangle]
		l0_mlp_fc1_5 [label="Layer0_MLP_FC1_5
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 5" shape=rectangle]
		l0_mlp_fc2_5 [label="Layer0_MLP_FC2_5
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 5" shape=rectangle]
		l0_mlp_fc1_6 [label="Layer0_MLP_FC1_6
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 6" shape=rectangle]
		l0_mlp_fc2_6 [label="Layer0_MLP_FC2_6
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 6" shape=rectangle]
		l0_mlp_fc1_7 [label="Layer0_MLP_FC1_7
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 7" shape=rectangle]
		l0_mlp_fc2_7 [label="Layer0_MLP_FC2_7
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 7" shape=rectangle]
		l0_mlp_reduce [label="Layer0_MLP_Reduce
Input: [128,10000,512]×8
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=yellow shape=parallelogram]
		l0_mlp_add [label="Layer0_MLP_Add&Norm
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
	}
	subgraph cluster_1 {
		color=red label="Stage 1: GPUs 8-15 (Layers 2-3)" style=dashed
		comm_0_1 [label="Pipeline_Communication_0→1
Input: [128,10000,4096]
Output: [128,10000,4096]
GPU: 7→8" fillcolor=pink shape=ellipse style=dashed]
		l2_q_8 [label="Layer2_Q_Projection_8
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 8" shape=rectangle]
		l2_k_8 [label="Layer2_K_Projection_8
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 8" shape=rectangle]
		l2_v_8 [label="Layer2_V_Projection_8
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 8" shape=rectangle]
		l2_attn_8 [label="Layer2_Attention_8
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 8" shape=rectangle]
		l2_out_8 [label="Layer2_Output_Projection_8
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 8" shape=rectangle]
		l2_q_9 [label="Layer2_Q_Projection_9
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 9" shape=rectangle]
		l2_k_9 [label="Layer2_K_Projection_9
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 9" shape=rectangle]
		l2_v_9 [label="Layer2_V_Projection_9
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 9" shape=rectangle]
		l2_attn_9 [label="Layer2_Attention_9
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 9" shape=rectangle]
		l2_out_9 [label="Layer2_Output_Projection_9
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 9" shape=rectangle]
		l2_q_10 [label="Layer2_Q_Projection_10
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 10" shape=rectangle]
		l2_k_10 [label="Layer2_K_Projection_10
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 10" shape=rectangle]
		l2_v_10 [label="Layer2_V_Projection_10
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 10" shape=rectangle]
		l2_attn_10 [label="Layer2_Attention_10
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 10" shape=rectangle]
		l2_out_10 [label="Layer2_Output_Projection_10
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 10" shape=rectangle]
		l2_q_11 [label="Layer2_Q_Projection_11
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 11" shape=rectangle]
		l2_k_11 [label="Layer2_K_Projection_11
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 11" shape=rectangle]
		l2_v_11 [label="Layer2_V_Projection_11
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 11" shape=rectangle]
		l2_attn_11 [label="Layer2_Attention_11
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 11" shape=rectangle]
		l2_out_11 [label="Layer2_Output_Projection_11
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 11" shape=rectangle]
		l2_q_12 [label="Layer2_Q_Projection_12
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 12" shape=rectangle]
		l2_k_12 [label="Layer2_K_Projection_12
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 12" shape=rectangle]
		l2_v_12 [label="Layer2_V_Projection_12
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 12" shape=rectangle]
		l2_attn_12 [label="Layer2_Attention_12
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 12" shape=rectangle]
		l2_out_12 [label="Layer2_Output_Projection_12
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 12" shape=rectangle]
		l2_q_13 [label="Layer2_Q_Projection_13
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 13" shape=rectangle]
		l2_k_13 [label="Layer2_K_Projection_13
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 13" shape=rectangle]
		l2_v_13 [label="Layer2_V_Projection_13
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 13" shape=rectangle]
		l2_attn_13 [label="Layer2_Attention_13
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 13" shape=rectangle]
		l2_out_13 [label="Layer2_Output_Projection_13
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 13" shape=rectangle]
		l2_q_14 [label="Layer2_Q_Projection_14
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 14" shape=rectangle]
		l2_k_14 [label="Layer2_K_Projection_14
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 14" shape=rectangle]
		l2_v_14 [label="Layer2_V_Projection_14
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 14" shape=rectangle]
		l2_attn_14 [label="Layer2_Attention_14
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 14" shape=rectangle]
		l2_out_14 [label="Layer2_Output_Projection_14
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 14" shape=rectangle]
		l2_q_15 [label="Layer2_Q_Projection_15
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 15" shape=rectangle]
		l2_k_15 [label="Layer2_K_Projection_15
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 15" shape=rectangle]
		l2_v_15 [label="Layer2_V_Projection_15
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 15" shape=rectangle]
		l2_attn_15 [label="Layer2_Attention_15
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 15" shape=rectangle]
		l2_out_15 [label="Layer2_Output_Projection_15
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 15" shape=rectangle]
		l2_reduce [label="Layer2_Attention_Reduce
Input: [128,10000,512]×8
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=yellow shape=parallelogram]
		l2_add [label="Layer2_Add&Norm
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
		l2_mlp_fc1_8 [label="Layer2_MLP_FC1_8
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 8" shape=rectangle]
		l2_mlp_fc2_8 [label="Layer2_MLP_FC2_8
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 8" shape=rectangle]
		l2_mlp_fc1_9 [label="Layer2_MLP_FC1_9
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 9" shape=rectangle]
		l2_mlp_fc2_9 [label="Layer2_MLP_FC2_9
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 9" shape=rectangle]
		l2_mlp_fc1_10 [label="Layer2_MLP_FC1_10
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 10" shape=rectangle]
		l2_mlp_fc2_10 [label="Layer2_MLP_FC2_10
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 10" shape=rectangle]
		l2_mlp_fc1_11 [label="Layer2_MLP_FC1_11
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 11" shape=rectangle]
		l2_mlp_fc2_11 [label="Layer2_MLP_FC2_11
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 11" shape=rectangle]
		l2_mlp_fc1_12 [label="Layer2_MLP_FC1_12
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 12" shape=rectangle]
		l2_mlp_fc2_12 [label="Layer2_MLP_FC2_12
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 12" shape=rectangle]
		l2_mlp_fc1_13 [label="Layer2_MLP_FC1_13
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 13" shape=rectangle]
		l2_mlp_fc2_13 [label="Layer2_MLP_FC2_13
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 13" shape=rectangle]
		l2_mlp_fc1_14 [label="Layer2_MLP_FC1_14
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 14" shape=rectangle]
		l2_mlp_fc2_14 [label="Layer2_MLP_FC2_14
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 14" shape=rectangle]
		l2_mlp_fc1_15 [label="Layer2_MLP_FC1_15
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 15" shape=rectangle]
		l2_mlp_fc2_15 [label="Layer2_MLP_FC2_15
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 15" shape=rectangle]
		l2_mlp_reduce [label="Layer2_MLP_Reduce
Input: [128,10000,512]×8
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=yellow shape=parallelogram]
		l2_mlp_add [label="Layer2_MLP_Add&Norm
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
		l3_q_8 [label="Layer3_Q_Projection_8
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 8" shape=rectangle]
		l3_k_8 [label="Layer3_K_Projection_8
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 8" shape=rectangle]
		l3_v_8 [label="Layer3_V_Projection_8
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 8" shape=rectangle]
		l3_attn_8 [label="Layer3_Attention_8
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 8" shape=rectangle]
		l3_out_8 [label="Layer3_Output_Projection_8
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 8" shape=rectangle]
		l3_q_9 [label="Layer3_Q_Projection_9
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 9" shape=rectangle]
		l3_k_9 [label="Layer3_K_Projection_9
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 9" shape=rectangle]
		l3_v_9 [label="Layer3_V_Projection_9
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 9" shape=rectangle]
		l3_attn_9 [label="Layer3_Attention_9
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 9" shape=rectangle]
		l3_out_9 [label="Layer3_Output_Projection_9
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 9" shape=rectangle]
		l3_q_10 [label="Layer3_Q_Projection_10
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 10" shape=rectangle]
		l3_k_10 [label="Layer3_K_Projection_10
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 10" shape=rectangle]
		l3_v_10 [label="Layer3_V_Projection_10
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 10" shape=rectangle]
		l3_attn_10 [label="Layer3_Attention_10
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 10" shape=rectangle]
		l3_out_10 [label="Layer3_Output_Projection_10
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 10" shape=rectangle]
		l3_q_11 [label="Layer3_Q_Projection_11
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 11" shape=rectangle]
		l3_k_11 [label="Layer3_K_Projection_11
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 11" shape=rectangle]
		l3_v_11 [label="Layer3_V_Projection_11
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 11" shape=rectangle]
		l3_attn_11 [label="Layer3_Attention_11
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 11" shape=rectangle]
		l3_out_11 [label="Layer3_Output_Projection_11
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 11" shape=rectangle]
		l3_q_12 [label="Layer3_Q_Projection_12
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 12" shape=rectangle]
		l3_k_12 [label="Layer3_K_Projection_12
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 12" shape=rectangle]
		l3_v_12 [label="Layer3_V_Projection_12
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 12" shape=rectangle]
		l3_attn_12 [label="Layer3_Attention_12
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 12" shape=rectangle]
		l3_out_12 [label="Layer3_Output_Projection_12
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 12" shape=rectangle]
		l3_q_13 [label="Layer3_Q_Projection_13
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 13" shape=rectangle]
		l3_k_13 [label="Layer3_K_Projection_13
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 13" shape=rectangle]
		l3_v_13 [label="Layer3_V_Projection_13
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 13" shape=rectangle]
		l3_attn_13 [label="Layer3_Attention_13
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 13" shape=rectangle]
		l3_out_13 [label="Layer3_Output_Projection_13
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 13" shape=rectangle]
		l3_q_14 [label="Layer3_Q_Projection_14
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 14" shape=rectangle]
		l3_k_14 [label="Layer3_K_Projection_14
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 14" shape=rectangle]
		l3_v_14 [label="Layer3_V_Projection_14
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 14" shape=rectangle]
		l3_attn_14 [label="Layer3_Attention_14
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 14" shape=rectangle]
		l3_out_14 [label="Layer3_Output_Projection_14
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 14" shape=rectangle]
		l3_q_15 [label="Layer3_Q_Projection_15
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 15" shape=rectangle]
		l3_k_15 [label="Layer3_K_Projection_15
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 15" shape=rectangle]
		l3_v_15 [label="Layer3_V_Projection_15
Input: [128,10000,4096]
Output: [128,10000,512]
GPU: 15" shape=rectangle]
		l3_attn_15 [label="Layer3_Attention_15
Input: Q:[128,10000,512], K:[128,10000,512], V:[128,10000,512]
Output: [128,10000,512]
GPU: 15" shape=rectangle]
		l3_out_15 [label="Layer3_Output_Projection_15
Input: [128,10000,512]
Output: [128,10000,512]
GPU: 15" shape=rectangle]
		l3_reduce [label="Layer3_Attention_Reduce
Input: [128,10000,512]×8
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=yellow shape=parallelogram]
		l3_add [label="Layer3_Add&Norm
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
		l3_mlp_fc1_8 [label="Layer3_MLP_FC1_8
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 8" shape=rectangle]
		l3_mlp_fc2_8 [label="Layer3_MLP_FC2_8
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 8" shape=rectangle]
		l3_mlp_fc1_9 [label="Layer3_MLP_FC1_9
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 9" shape=rectangle]
		l3_mlp_fc2_9 [label="Layer3_MLP_FC2_9
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 9" shape=rectangle]
		l3_mlp_fc1_10 [label="Layer3_MLP_FC1_10
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 10" shape=rectangle]
		l3_mlp_fc2_10 [label="Layer3_MLP_FC2_10
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 10" shape=rectangle]
		l3_mlp_fc1_11 [label="Layer3_MLP_FC1_11
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 11" shape=rectangle]
		l3_mlp_fc2_11 [label="Layer3_MLP_FC2_11
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 11" shape=rectangle]
		l3_mlp_fc1_12 [label="Layer3_MLP_FC1_12
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 12" shape=rectangle]
		l3_mlp_fc2_12 [label="Layer3_MLP_FC2_12
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 12" shape=rectangle]
		l3_mlp_fc1_13 [label="Layer3_MLP_FC1_13
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 13" shape=rectangle]
		l3_mlp_fc2_13 [label="Layer3_MLP_FC2_13
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 13" shape=rectangle]
		l3_mlp_fc1_14 [label="Layer3_MLP_FC1_14
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 14" shape=rectangle]
		l3_mlp_fc2_14 [label="Layer3_MLP_FC2_14
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 14" shape=rectangle]
		l3_mlp_fc1_15 [label="Layer3_MLP_FC1_15
Input: [128,10000,512]
Output: [128,10000,2048]
GPU: 15" shape=rectangle]
		l3_mlp_fc2_15 [label="Layer3_MLP_FC2_15
Input: [128,10000,2048]
Output: [128,10000,512]
GPU: 15" shape=rectangle]
		l3_mlp_reduce [label="Layer3_MLP_Reduce
Input: [128,10000,512]×8
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=yellow shape=parallelogram]
		l3_mlp_add [label="Layer3_MLP_Add&Norm
Input: [128,10000,4096], [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightblue shape=rectangle]
		output [label="Final_Output
Input: [128,10000,4096]
Output: [128,10000,4096]
GPU: all GPUs" fillcolor=lightgreen shape=ellipse]
	}
	input -> l0_q_0
	input -> l0_k_0
	input -> l0_v_0
	l0_q_0 -> l0_attn_0
	l0_k_0 -> l0_attn_0
	l0_v_0 -> l0_attn_0
	l0_attn_0 -> l0_out_0
	input -> l0_q_1
	input -> l0_k_1
	input -> l0_v_1
	l0_q_1 -> l0_attn_1
	l0_k_1 -> l0_attn_1
	l0_v_1 -> l0_attn_1
	l0_attn_1 -> l0_out_1
	input -> l0_q_2
	input -> l0_k_2
	input -> l0_v_2
	l0_q_2 -> l0_attn_2
	l0_k_2 -> l0_attn_2
	l0_v_2 -> l0_attn_2
	l0_attn_2 -> l0_out_2
	input -> l0_q_3
	input -> l0_k_3
	input -> l0_v_3
	l0_q_3 -> l0_attn_3
	l0_k_3 -> l0_attn_3
	l0_v_3 -> l0_attn_3
	l0_attn_3 -> l0_out_3
	input -> l0_q_4
	input -> l0_k_4
	input -> l0_v_4
	l0_q_4 -> l0_attn_4
	l0_k_4 -> l0_attn_4
	l0_v_4 -> l0_attn_4
	l0_attn_4 -> l0_out_4
	input -> l0_q_5
	input -> l0_k_5
	input -> l0_v_5
	l0_q_5 -> l0_attn_5
	l0_k_5 -> l0_attn_5
	l0_v_5 -> l0_attn_5
	l0_attn_5 -> l0_out_5
	input -> l0_q_6
	input -> l0_k_6
	input -> l0_v_6
	l0_q_6 -> l0_attn_6
	l0_k_6 -> l0_attn_6
	l0_v_6 -> l0_attn_6
	l0_attn_6 -> l0_out_6
	input -> l0_q_7
	input -> l0_k_7
	input -> l0_v_7
	l0_q_7 -> l0_attn_7
	l0_k_7 -> l0_attn_7
	l0_v_7 -> l0_attn_7
	l0_attn_7 -> l0_out_7
	l0_out_0 -> l0_reduce
	l0_out_1 -> l0_reduce
	l0_out_2 -> l0_reduce
	l0_out_3 -> l0_reduce
	l0_out_4 -> l0_reduce
	l0_out_5 -> l0_reduce
	l0_out_6 -> l0_reduce
	l0_out_7 -> l0_reduce
	l0_reduce -> l0_add
	input -> l0_add
	l0_add -> l0_mlp_fc1_0
	l0_mlp_fc1_0 -> l0_mlp_fc2_0
	l0_mlp_fc2_0 -> l0_mlp_reduce
	l0_add -> l0_mlp_fc1_1
	l0_mlp_fc1_1 -> l0_mlp_fc2_1
	l0_mlp_fc2_1 -> l0_mlp_reduce
	l0_add -> l0_mlp_fc1_2
	l0_mlp_fc1_2 -> l0_mlp_fc2_2
	l0_mlp_fc2_2 -> l0_mlp_reduce
	l0_add -> l0_mlp_fc1_3
	l0_mlp_fc1_3 -> l0_mlp_fc2_3
	l0_mlp_fc2_3 -> l0_mlp_reduce
	l0_add -> l0_mlp_fc1_4
	l0_mlp_fc1_4 -> l0_mlp_fc2_4
	l0_mlp_fc2_4 -> l0_mlp_reduce
	l0_add -> l0_mlp_fc1_5
	l0_mlp_fc1_5 -> l0_mlp_fc2_5
	l0_mlp_fc2_5 -> l0_mlp_reduce
	l0_add -> l0_mlp_fc1_6
	l0_mlp_fc1_6 -> l0_mlp_fc2_6
	l0_mlp_fc2_6 -> l0_mlp_reduce
	l0_add -> l0_mlp_fc1_7
	l0_mlp_fc1_7 -> l0_mlp_fc2_7
	l0_mlp_fc2_7 -> l0_mlp_reduce
	l0_mlp_reduce -> l0_mlp_add
	l0_add -> l0_mlp_add
	l0_mlp_add -> comm_0_1
	comm_0_1 -> l2_q_8
	comm_0_1 -> l2_k_8
	comm_0_1 -> l2_v_8
	l2_q_8 -> l2_attn_8
	l2_k_8 -> l2_attn_8
	l2_v_8 -> l2_attn_8
	l2_attn_8 -> l2_out_8
	comm_0_1 -> l2_q_9
	comm_0_1 -> l2_k_9
	comm_0_1 -> l2_v_9
	l2_q_9 -> l2_attn_9
	l2_k_9 -> l2_attn_9
	l2_v_9 -> l2_attn_9
	l2_attn_9 -> l2_out_9
	comm_0_1 -> l2_q_10
	comm_0_1 -> l2_k_10
	comm_0_1 -> l2_v_10
	l2_q_10 -> l2_attn_10
	l2_k_10 -> l2_attn_10
	l2_v_10 -> l2_attn_10
	l2_attn_10 -> l2_out_10
	comm_0_1 -> l2_q_11
	comm_0_1 -> l2_k_11
	comm_0_1 -> l2_v_11
	l2_q_11 -> l2_attn_11
	l2_k_11 -> l2_attn_11
	l2_v_11 -> l2_attn_11
	l2_attn_11 -> l2_out_11
	comm_0_1 -> l2_q_12
	comm_0_1 -> l2_k_12
	comm_0_1 -> l2_v_12
	l2_q_12 -> l2_attn_12
	l2_k_12 -> l2_attn_12
	l2_v_12 -> l2_attn_12
	l2_attn_12 -> l2_out_12
	comm_0_1 -> l2_q_13
	comm_0_1 -> l2_k_13
	comm_0_1 -> l2_v_13
	l2_q_13 -> l2_attn_13
	l2_k_13 -> l2_attn_13
	l2_v_13 -> l2_attn_13
	l2_attn_13 -> l2_out_13
	comm_0_1 -> l2_q_14
	comm_0_1 -> l2_k_14
	comm_0_1 -> l2_v_14
	l2_q_14 -> l2_attn_14
	l2_k_14 -> l2_attn_14
	l2_v_14 -> l2_attn_14
	l2_attn_14 -> l2_out_14
	comm_0_1 -> l2_q_15
	comm_0_1 -> l2_k_15
	comm_0_1 -> l2_v_15
	l2_q_15 -> l2_attn_15
	l2_k_15 -> l2_attn_15
	l2_v_15 -> l2_attn_15
	l2_attn_15 -> l2_out_15
	l2_out_8 -> l2_reduce
	l2_out_9 -> l2_reduce
	l2_out_10 -> l2_reduce
	l2_out_11 -> l2_reduce
	l2_out_12 -> l2_reduce
	l2_out_13 -> l2_reduce
	l2_out_14 -> l2_reduce
	l2_out_15 -> l2_reduce
	l2_reduce -> l2_add
	comm_0_1 -> l2_add
	l2_add -> l2_mlp_fc1_8
	l2_mlp_fc1_8 -> l2_mlp_fc2_8
	l2_mlp_fc2_8 -> l2_mlp_reduce
	l2_add -> l2_mlp_fc1_9
	l2_mlp_fc1_9 -> l2_mlp_fc2_9
	l2_mlp_fc2_9 -> l2_mlp_reduce
	l2_add -> l2_mlp_fc1_10
	l2_mlp_fc1_10 -> l2_mlp_fc2_10
	l2_mlp_fc2_10 -> l2_mlp_reduce
	l2_add -> l2_mlp_fc1_11
	l2_mlp_fc1_11 -> l2_mlp_fc2_11
	l2_mlp_fc2_11 -> l2_mlp_reduce
	l2_add -> l2_mlp_fc1_12
	l2_mlp_fc1_12 -> l2_mlp_fc2_12
	l2_mlp_fc2_12 -> l2_mlp_reduce
	l2_add -> l2_mlp_fc1_13
	l2_mlp_fc1_13 -> l2_mlp_fc2_13
	l2_mlp_fc2_13 -> l2_mlp_reduce
	l2_add -> l2_mlp_fc1_14
	l2_mlp_fc1_14 -> l2_mlp_fc2_14
	l2_mlp_fc2_14 -> l2_mlp_reduce
	l2_add -> l2_mlp_fc1_15
	l2_mlp_fc1_15 -> l2_mlp_fc2_15
	l2_mlp_fc2_15 -> l2_mlp_reduce
	l2_mlp_reduce -> l2_mlp_add
	l2_add -> l2_mlp_add
	l2_mlp_add -> l3_q_8
	l2_mlp_add -> l3_k_8
	l2_mlp_add -> l3_v_8
	l3_q_8 -> l3_attn_8
	l3_k_8 -> l3_attn_8
	l3_v_8 -> l3_attn_8
	l3_attn_8 -> l3_out_8
	l2_mlp_add -> l3_q_9
	l2_mlp_add -> l3_k_9
	l2_mlp_add -> l3_v_9
	l3_q_9 -> l3_attn_9
	l3_k_9 -> l3_attn_9
	l3_v_9 -> l3_attn_9
	l3_attn_9 -> l3_out_9
	l2_mlp_add -> l3_q_10
	l2_mlp_add -> l3_k_10
	l2_mlp_add -> l3_v_10
	l3_q_10 -> l3_attn_10
	l3_k_10 -> l3_attn_10
	l3_v_10 -> l3_attn_10
	l3_attn_10 -> l3_out_10
	l2_mlp_add -> l3_q_11
	l2_mlp_add -> l3_k_11
	l2_mlp_add -> l3_v_11
	l3_q_11 -> l3_attn_11
	l3_k_11 -> l3_attn_11
	l3_v_11 -> l3_attn_11
	l3_attn_11 -> l3_out_11
	l2_mlp_add -> l3_q_12
	l2_mlp_add -> l3_k_12
	l2_mlp_add -> l3_v_12
	l3_q_12 -> l3_attn_12
	l3_k_12 -> l3_attn_12
	l3_v_12 -> l3_attn_12
	l3_attn_12 -> l3_out_12
	l2_mlp_add -> l3_q_13
	l2_mlp_add -> l3_k_13
	l2_mlp_add -> l3_v_13
	l3_q_13 -> l3_attn_13
	l3_k_13 -> l3_attn_13
	l3_v_13 -> l3_attn_13
	l3_attn_13 -> l3_out_13
	l2_mlp_add -> l3_q_14
	l2_mlp_add -> l3_k_14
	l2_mlp_add -> l3_v_14
	l3_q_14 -> l3_attn_14
	l3_k_14 -> l3_attn_14
	l3_v_14 -> l3_attn_14
	l3_attn_14 -> l3_out_14
	l2_mlp_add -> l3_q_15
	l2_mlp_add -> l3_k_15
	l2_mlp_add -> l3_v_15
	l3_q_15 -> l3_attn_15
	l3_k_15 -> l3_attn_15
	l3_v_15 -> l3_attn_15
	l3_attn_15 -> l3_out_15
	l3_out_8 -> l3_reduce
	l3_out_9 -> l3_reduce
	l3_out_10 -> l3_reduce
	l3_out_11 -> l3_reduce
	l3_out_12 -> l3_reduce
	l3_out_13 -> l3_reduce
	l3_out_14 -> l3_reduce
	l3_out_15 -> l3_reduce
	l3_reduce -> l3_add
	l2_mlp_add -> l3_add
	l3_add -> l3_mlp_fc1_8
	l3_mlp_fc1_8 -> l3_mlp_fc2_8
	l3_mlp_fc2_8 -> l3_mlp_reduce
	l3_add -> l3_mlp_fc1_9
	l3_mlp_fc1_9 -> l3_mlp_fc2_9
	l3_mlp_fc2_9 -> l3_mlp_reduce
	l3_add -> l3_mlp_fc1_10
	l3_mlp_fc1_10 -> l3_mlp_fc2_10
	l3_mlp_fc2_10 -> l3_mlp_reduce
	l3_add -> l3_mlp_fc1_11
	l3_mlp_fc1_11 -> l3_mlp_fc2_11
	l3_mlp_fc2_11 -> l3_mlp_reduce
	l3_add -> l3_mlp_fc1_12
	l3_mlp_fc1_12 -> l3_mlp_fc2_12
	l3_mlp_fc2_12 -> l3_mlp_reduce
	l3_add -> l3_mlp_fc1_13
	l3_mlp_fc1_13 -> l3_mlp_fc2_13
	l3_mlp_fc2_13 -> l3_mlp_reduce
	l3_add -> l3_mlp_fc1_14
	l3_mlp_fc1_14 -> l3_mlp_fc2_14
	l3_mlp_fc2_14 -> l3_mlp_reduce
	l3_add -> l3_mlp_fc1_15
	l3_mlp_fc1_15 -> l3_mlp_fc2_15
	l3_mlp_fc2_15 -> l3_mlp_reduce
	l3_mlp_reduce -> l3_mlp_add
	l3_add -> l3_mlp_add
	l3_mlp_add -> output
}
