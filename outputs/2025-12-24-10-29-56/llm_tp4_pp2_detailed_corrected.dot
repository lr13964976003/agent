// Corrected TP4xPP2 Hybrid Configuration
digraph LLM_TP4_PP2_Detailed_Corrected {
	graph [bgcolor=white fontname=Arial fontsize=12 nodesep=0.5 rankdir=TB ranksep=0.8]
	input [label="INPUT\n[batch_size=1, seq_len=2048, hidden=8192]\nGPU: Host" fillcolor=lightcoral fontname=Arial fontsize=10 shape=hexagon style=filled]
	stage0_label [label="PIPELINE STAGE 0\nGPUs 0-3 (TP Group 0)\nLayers 0-39" fillcolor=lightgray fontsize=12 shape=box style="rounded,filled"]
	embed_s0 [label="Embedding Layer 0\n[Input: [1,2048], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	rmsnorm_0_s0 [label="RMSNorm Layer 0\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	qkv_0_s0 [label="QKV Linear Layer 0\n[Input: [1,2048,8192], Output: [1,2048,12288]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	attn_split_0_s0 [label="Attention Head Split\n[Input: [1,2048,12288], Output: [1,64,2048,128]]\nGPU: 0-3 (TP4)" fillcolor=lightgreen fontname=Arial fontsize=10 shape=parallelogram style=filled]
	attn_compute_0_s0 [label="Attention Compute\n[Input: [1,64,2048,128], Output: [1,64,2048,128]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	attn_ar_0_s0 [label="Attention All-Reduce\n[Input: [1,64,2048,128], Output: [1,64,2048,128]]\nGPU: 0-3 (TP4)" fillcolor=lightyellow fontname=Arial fontsize=10 shape=ellipse style=filled]
	attn_out_0_s0 [label="Attention Output Linear\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_up_0_s0 [label="FFN Up-Projection\n[Input: [1,2048,8192], Output: [1,2048,28672]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_act_0_s0 [label="FFN Activation (SiLU)\n[Input: [1,2048,28672], Output: [1,2048,28672]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_down_0_s0 [label="FFN Down-Projection\n[Input: [1,2048,28672], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_ar_0_s0 [label="FFN All-Reduce\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightyellow fontname=Arial fontsize=10 shape=ellipse style=filled]
	rmsnorm_1_s0 [label="RMSNorm Layer 1\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	qkv_1_s0 [label="QKV Linear Layer 1\n[Input: [1,2048,8192], Output: [1,2048,12288]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	attn_split_1_s0 [label="Attention Head Split\n[Input: [1,2048,12288], Output: [1,64,2048,128]]\nGPU: 0-3 (TP4)" fillcolor=lightgreen fontname=Arial fontsize=10 shape=parallelogram style=filled]
	attn_compute_1_s0 [label="Attention Compute\n[Input: [1,64,2048,128], Output: [1,64,2048,128]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	attn_ar_1_s0 [label="Attention All-Reduce\n[Input: [1,64,2048,128], Output: [1,64,2048,128]]\nGPU: 0-3 (TP4)" fillcolor=lightyellow fontname=Arial fontsize=10 shape=ellipse style=filled]
	attn_out_1_s0 [label="Attention Output Linear\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_up_1_s0 [label="FFN Up-Projection\n[Input: [1,2048,8192], Output: [1,2048,28672]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_act_1_s0 [label="FFN Activation (SiLU)\n[Input: [1,2048,28672], Output: [1,2048,28672]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_down_1_s0 [label="FFN Down-Projection\n[Input: [1,2048,28672], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_ar_1_s0 [label="FFN All-Reduce\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightyellow fontname=Arial fontsize=10 shape=ellipse style=filled]
	rmsnorm_2_s0 [label="RMSNorm Layer 2\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	qkv_2_s0 [label="QKV Linear Layer 2\n[Input: [1,2048,8192], Output: [1,2048,12288]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	attn_split_2_s0 [label="Attention Head Split\n[Input: [1,2048,12288], Output: [1,64,2048,128]]\nGPU: 0-3 (TP4)" fillcolor=lightgreen fontname=Arial fontsize=10 shape=parallelogram style=filled]
	attn_compute_2_s0 [label="Attention Compute\n[Input: [1,64,2048,128], Output: [1,64,2048,128]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	attn_ar_2_s0 [label="Attention All-Reduce\n[Input: [1,64,2048,128], Output: [1,64,2048,128]]\nGPU: 0-3 (TP4)" fillcolor=lightyellow fontname=Arial fontsize=10 shape=ellipse style=filled]
	attn_out_2_s0 [label="Attention Output Linear\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_up_2_s0 [label="FFN Up-Projection\n[Input: [1,2048,8192], Output: [1,2048,28672]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_act_2_s0 [label="FFN Activation (SiLU)\n[Input: [1,2048,28672], Output: [1,2048,28672]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_down_2_s0 [label="FFN Down-Projection\n[Input: [1,2048,28672], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_ar_2_s0 [label="FFN All-Reduce\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightyellow fontname=Arial fontsize=10 shape=ellipse style=filled]
	intermediate_s0 [label="Intermediate Layers 3-38\n[36 transformer layers]\nSame pattern as layers 0-2\nGPU: 0-3 (TP4)" fillcolor=lightgray fontsize=10 shape=note style=filled]
	rmsnorm_39_s0 [label="RMSNorm Layer 39\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 0-3 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	pipe_comm_s0_s1 [label="Pipeline Communication\nStage 0 â†’ Stage 1\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 0-3 â†’ 4-7" fillcolor=lightyellow fontname=Arial fontsize=10 shape=ellipse style=filled]
	stage1_label [label="PIPELINE STAGE 1\nGPUs 4-7 (TP Group 1)\nLayers 40-79" fillcolor=lightgray fontsize=12 shape=box style="rounded,filled"]
	rmsnorm_40_s1 [label="RMSNorm Layer 40\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	qkv_40_s1 [label="QKV Linear Layer 40\n[Input: [1,2048,8192], Output: [1,2048,12288]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	attn_split_40_s1 [label="Attention Head Split\n[Input: [1,2048,12288], Output: [1,64,2048,128]]\nGPU: 4-7 (TP4)" fillcolor=lightgreen fontname=Arial fontsize=10 shape=parallelogram style=filled]
	attn_compute_40_s1 [label="Attention Compute\n[Input: [1,64,2048,128], Output: [1,64,2048,128]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	attn_ar_40_s1 [label="Attention All-Reduce\n[Input: [1,64,2048,128], Output: [1,64,2048,128]]\nGPU: 4-7 (TP4)" fillcolor=lightyellow fontname=Arial fontsize=10 shape=ellipse style=filled]
	attn_out_40_s1 [label="Attention Output Linear\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_up_40_s1 [label="FFN Up-Projection\n[Input: [1,2048,8192], Output: [1,2048,28672]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_act_40_s1 [label="FFN Activation (SiLU)\n[Input: [1,2048,28672], Output: [1,2048,28672]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_down_40_s1 [label="FFN Down-Projection\n[Input: [1,2048,28672], Output: [1,2048,8192]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_ar_40_s1 [label="FFN All-Reduce\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 4-7 (TP4)" fillcolor=lightyellow fontname=Arial fontsize=10 shape=ellipse style=filled]
	rmsnorm_41_s1 [label="RMSNorm Layer 41\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	qkv_41_s1 [label="QKV Linear Layer 41\n[Input: [1,2048,8192], Output: [1,2048,12288]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	attn_split_41_s1 [label="Attention Head Split\n[Input: [1,2048,12288], Output: [1,64,2048,128]]\nGPU: 4-7 (TP4)" fillcolor=lightgreen fontname=Arial fontsize=10 shape=parallelogram style=filled]
	attn_compute_41_s1 [label="Attention Compute\n[Input: [1,64,2048,128], Output: [1,64,2048,128]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	attn_ar_41_s1 [label="Attention All-Reduce\n[Input: [1,64,2048,128], Output: [1,64,2048,128]]\nGPU: 4-7 (TP4)" fillcolor=lightyellow fontname=Arial fontsize=10 shape=ellipse style=filled]
	attn_out_41_s1 [label="Attention Output Linear\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_up_41_s1 [label="FFN Up-Projection\n[Input: [1,2048,8192], Output: [1,2048,28672]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_act_41_s1 [label="FFN Activation (SiLU)\n[Input: [1,2048,28672], Output: [1,2048,28672]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_down_41_s1 [label="FFN Down-Projection\n[Input: [1,2048,28672], Output: [1,2048,8192]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_ar_41_s1 [label="FFN All-Reduce\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 4-7 (TP4)" fillcolor=lightyellow fontname=Arial fontsize=10 shape=ellipse style=filled]
	rmsnorm_42_s1 [label="RMSNorm Layer 42\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	qkv_42_s1 [label="QKV Linear Layer 42\n[Input: [1,2048,8192], Output: [1,2048,12288]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	attn_split_42_s1 [label="Attention Head Split\n[Input: [1,2048,12288], Output: [1,64,2048,128]]\nGPU: 4-7 (TP4)" fillcolor=lightgreen fontname=Arial fontsize=10 shape=parallelogram style=filled]
	attn_compute_42_s1 [label="Attention Compute\n[Input: [1,64,2048,128], Output: [1,64,2048,128]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	attn_ar_42_s1 [label="Attention All-Reduce\n[Input: [1,64,2048,128], Output: [1,64,2048,128]]\nGPU: 4-7 (TP4)" fillcolor=lightyellow fontname=Arial fontsize=10 shape=ellipse style=filled]
	attn_out_42_s1 [label="Attention Output Linear\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_up_42_s1 [label="FFN Up-Projection\n[Input: [1,2048,8192], Output: [1,2048,28672]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_act_42_s1 [label="FFN Activation (SiLU)\n[Input: [1,2048,28672], Output: [1,2048,28672]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_down_42_s1 [label="FFN Down-Projection\n[Input: [1,2048,28672], Output: [1,2048,8192]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	ffn_ar_42_s1 [label="FFN All-Reduce\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 4-7 (TP4)" fillcolor=lightyellow fontname=Arial fontsize=10 shape=ellipse style=filled]
	intermediate_s1 [label="Intermediate Layers 43-78\n[36 transformer layers]\nSame pattern as layers 40-42\nGPU: 4-7 (TP4)" fillcolor=lightgray fontsize=10 shape=note style=filled]
	rmsnorm_79_s1 [label="RMSNorm Layer 79\n[Input: [1,2048,8192], Output: [1,2048,8192]]\nGPU: 4-7 (TP4)" fillcolor=lightblue fontname=Arial fontsize=10 shape=rectangle style=filled]
	output [label="OUTPUT\n[batch_size=1, seq_len=2048, hidden=8192]\nGPU: 4-7 (TP4)" fillcolor=lightcoral fontname=Arial fontsize=10 shape=hexagon style=filled]
	input -> embed_s0
	embed_s0 -> rmsnorm_0_s0
	rmsnorm_0_s0 -> qkv_0_s0
	qkv_0_s0 -> attn_split_0_s0
	attn_split_0_s0 -> attn_compute_0_s0
	attn_compute_0_s0 -> attn_ar_0_s0
	attn_ar_0_s0 -> attn_out_0_s0
	attn_out_0_s0 -> ffn_up_0_s0
	ffn_up_0_s0 -> ffn_act_0_s0
	ffn_act_0_s0 -> ffn_down_0_s0
	ffn_down_0_s0 -> ffn_ar_0_s0
	ffn_ar_0_s0 -> rmsnorm_1_s0
	rmsnorm_1_s0 -> qkv_1_s0
	qkv_1_s0 -> attn_split_1_s0
	attn_split_1_s0 -> attn_compute_1_s0
	attn_compute_1_s0 -> attn_ar_1_s0
	attn_ar_1_s0 -> attn_out_1_s0
	attn_out_1_s0 -> ffn_up_1_s0
	ffn_up_1_s0 -> ffn_act_1_s0
	ffn_act_1_s0 -> ffn_down_1_s0
	ffn_down_1_s0 -> ffn_ar_1_s0
	ffn_ar_1_s0 -> rmsnorm_2_s0
	rmsnorm_2_s0 -> qkv_2_s0
	qkv_2_s0 -> attn_split_2_s0
	attn_split_2_s0 -> attn_compute_2_s0
	attn_compute_2_s0 -> attn_ar_2_s0
	attn_ar_2_s0 -> attn_out_2_s0
	attn_out_2_s0 -> ffn_up_2_s0
	ffn_up_2_s0 -> ffn_act_2_s0
	ffn_act_2_s0 -> ffn_down_2_s0
	ffn_down_2_s0 -> ffn_ar_2_s0
	ffn_ar_2_s0 -> intermediate_s0
	intermediate_s0 -> rmsnorm_39_s0
	rmsnorm_39_s0 -> pipe_comm_s0_s1
	pipe_comm_s0_s1 -> rmsnorm_40_s1
	rmsnorm_40_s1 -> qkv_40_s1
	qkv_40_s1 -> attn_split_40_s1
	attn_split_40_s1 -> attn_compute_40_s1
	attn_compute_40_s1 -> attn_ar_40_s1
	attn_ar_40_s1 -> attn_out_40_s1
	attn_out_40_s1 -> ffn_up_40_s1
	ffn_up_40_s1 -> ffn_act_40_s1
	ffn_act_40_s1 -> ffn_down_40_s1
	ffn_down_40_s1 -> ffn_ar_40_s1
	ffn_ar_40_s1 -> rmsnorm_41_s1
	rmsnorm_41_s1 -> qkv_41_s1
	qkv_41_s1 -> attn_split_41_s1
	attn_split_41_s1 -> attn_compute_41_s1
	attn_compute_41_s1 -> attn_ar_41_s1
	attn_ar_41_s1 -> attn_out_41_s1
	attn_out_41_s1 -> ffn_up_41_s1
	ffn_up_41_s1 -> ffn_act_41_s1
	ffn_act_41_s1 -> ffn_down_41_s1
	ffn_down_41_s1 -> ffn_ar_41_s1
	ffn_ar_41_s1 -> rmsnorm_42_s1
	rmsnorm_42_s1 -> qkv_42_s1
	qkv_42_s1 -> attn_split_42_s1
	attn_split_42_s1 -> attn_compute_42_s1
	attn_compute_42_s1 -> attn_ar_42_s1
	attn_ar_42_s1 -> attn_out_42_s1
	attn_out_42_s1 -> ffn_up_42_s1
	ffn_up_42_s1 -> ffn_act_42_s1
	ffn_act_42_s1 -> ffn_down_42_s1
	ffn_down_42_s1 -> ffn_ar_42_s1
	ffn_ar_42_s1 -> intermediate_s1
	intermediate_s1 -> rmsnorm_79_s1
	rmsnorm_79_s1 -> output
}
