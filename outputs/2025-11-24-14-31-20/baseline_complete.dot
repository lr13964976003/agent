digraph Baseline_MoE_Final {
    rankdir=TB;
    splines=ortho;
    node [fontname="Arial", fontsize=10];
    
    input [label="Model Input\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=box, style=filled, fillcolor=lightblue]
    layer0_mha [label="MHA Layer 0\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer0_ln1 [label="LayerNorm MHA 0\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer0_add1 [label="MHA Residual Add 0\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer0_gate [label="Gate Layer 0\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer0_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_moe_agg [label="MoE Final Aggregation 0\nTP=8 across GPUs 0-7\nStage 0\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer0_ln2 [label="LayerNorm MoE 0\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer0_moe_output [label="MoE Residual Add 0\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer1_mha [label="MHA Layer 1\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer1_ln1 [label="LayerNorm MHA 1\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer1_add1 [label="MHA Residual Add 1\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer1_gate [label="Gate Layer 1\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer1_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_moe_agg [label="MoE Final Aggregation 1\nTP=8 across GPUs 0-7\nStage 0\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer1_ln2 [label="LayerNorm MoE 1\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer1_moe_output [label="MoE Residual Add 1\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer2_mha [label="MHA Layer 2\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer2_ln1 [label="LayerNorm MHA 2\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer2_add1 [label="MHA Residual Add 2\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer2_gate [label="Gate Layer 2\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer2_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_moe_agg [label="MoE Final Aggregation 2\nTP=8 across GPUs 0-7\nStage 0\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer2_ln2 [label="LayerNorm MoE 2\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer2_moe_output [label="MoE Residual Add 2\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer3_mha [label="MHA Layer 3\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer3_ln1 [label="LayerNorm MHA 3\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer3_add1 [label="MHA Residual Add 3\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer3_gate [label="Gate Layer 3\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer3_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_moe_agg [label="MoE Final Aggregation 3\nTP=8 across GPUs 0-7\nStage 0\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer3_ln2 [label="LayerNorm MoE 3\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer3_moe_output [label="MoE Residual Add 3\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer4_mha [label="MHA Layer 4\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer4_ln1 [label="LayerNorm MHA 4\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer4_add1 [label="MHA Residual Add 4\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer4_gate [label="Gate Layer 4\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer4_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_moe_agg [label="MoE Final Aggregation 4\nTP=8 across GPUs 0-7\nStage 0\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer4_ln2 [label="LayerNorm MoE 4\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer4_moe_output [label="MoE Residual Add 4\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer5_mha [label="MHA Layer 5\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer5_ln1 [label="LayerNorm MHA 5\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer5_add1 [label="MHA Residual Add 5\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer5_gate [label="Gate Layer 5\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer5_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_moe_agg [label="MoE Final Aggregation 5\nTP=8 across GPUs 0-7\nStage 0\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer5_ln2 [label="LayerNorm MoE 5\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer5_moe_output [label="MoE Residual Add 5\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer6_mha [label="MHA Layer 6\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer6_ln1 [label="LayerNorm MHA 6\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer6_add1 [label="MHA Residual Add 6\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer6_gate [label="Gate Layer 6\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer6_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_moe_agg [label="MoE Final Aggregation 6\nTP=8 across GPUs 0-7\nStage 0\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer6_ln2 [label="LayerNorm MoE 6\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer6_moe_output [label="MoE Residual Add 6\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer7_mha [label="MHA Layer 7\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer7_ln1 [label="LayerNorm MHA 7\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer7_add1 [label="MHA Residual Add 7\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer7_gate [label="Gate Layer 7\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer7_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu0 [label="Route to GPU 0\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu0 [label="8 Experts GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu0 [label="Aggregate GPU 0\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu1 [label="Route to GPU 1\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu1 [label="8 Experts GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu1 [label="Aggregate GPU 1\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu2 [label="Route to GPU 2\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu2 [label="8 Experts GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu2 [label="Aggregate GPU 2\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu3 [label="Route to GPU 3\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu3 [label="8 Experts GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu3 [label="Aggregate GPU 3\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu4 [label="Route to GPU 4\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu4 [label="8 Experts GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu4 [label="Aggregate GPU 4\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu5 [label="Route to GPU 5\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu5 [label="8 Experts GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu5 [label="Aggregate GPU 5\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu6 [label="Route to GPU 6\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu6 [label="8 Experts GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu6 [label="Aggregate GPU 6\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_gpu7 [label="Route to GPU 7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_experts_gpu7 [label="8 Experts GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_gpu7 [label="Aggregate GPU 7\nStage 0\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_moe_agg [label="MoE Final Aggregation 7\nTP=8 across GPUs 0-7\nStage 0\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer7_ln2 [label="LayerNorm MoE 7\nTP=8 across GPUs 0-7\nStage 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer7_moe_output [label="MoE Residual Add 7\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer8_mha [label="MHA Layer 8\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer8_ln1 [label="LayerNorm MHA 8\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer8_add1 [label="MHA Residual Add 8\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer8_gate [label="Gate Layer 8\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer8_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_moe_agg [label="MoE Final Aggregation 8\nTP=8 across GPUs 8-15\nStage 1\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer8_ln2 [label="LayerNorm MoE 8\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer8_moe_output [label="MoE Residual Add 8\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer9_mha [label="MHA Layer 9\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer9_ln1 [label="LayerNorm MHA 9\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer9_add1 [label="MHA Residual Add 9\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer9_gate [label="Gate Layer 9\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer9_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_moe_agg [label="MoE Final Aggregation 9\nTP=8 across GPUs 8-15\nStage 1\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer9_ln2 [label="LayerNorm MoE 9\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer9_moe_output [label="MoE Residual Add 9\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer10_mha [label="MHA Layer 10\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer10_ln1 [label="LayerNorm MHA 10\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer10_add1 [label="MHA Residual Add 10\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer10_gate [label="Gate Layer 10\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer10_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_moe_agg [label="MoE Final Aggregation 10\nTP=8 across GPUs 8-15\nStage 1\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer10_ln2 [label="LayerNorm MoE 10\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer10_moe_output [label="MoE Residual Add 10\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer11_mha [label="MHA Layer 11\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer11_ln1 [label="LayerNorm MHA 11\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer11_add1 [label="MHA Residual Add 11\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer11_gate [label="Gate Layer 11\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer11_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_moe_agg [label="MoE Final Aggregation 11\nTP=8 across GPUs 8-15\nStage 1\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer11_ln2 [label="LayerNorm MoE 11\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer11_moe_output [label="MoE Residual Add 11\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer12_mha [label="MHA Layer 12\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer12_ln1 [label="LayerNorm MHA 12\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer12_add1 [label="MHA Residual Add 12\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer12_gate [label="Gate Layer 12\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer12_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_moe_agg [label="MoE Final Aggregation 12\nTP=8 across GPUs 8-15\nStage 1\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer12_ln2 [label="LayerNorm MoE 12\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer12_moe_output [label="MoE Residual Add 12\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer13_mha [label="MHA Layer 13\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer13_ln1 [label="LayerNorm MHA 13\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer13_add1 [label="MHA Residual Add 13\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer13_gate [label="Gate Layer 13\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer13_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_moe_agg [label="MoE Final Aggregation 13\nTP=8 across GPUs 8-15\nStage 1\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer13_ln2 [label="LayerNorm MoE 13\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer13_moe_output [label="MoE Residual Add 13\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer14_mha [label="MHA Layer 14\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer14_ln1 [label="LayerNorm MHA 14\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer14_add1 [label="MHA Residual Add 14\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer14_gate [label="Gate Layer 14\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer14_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_moe_agg [label="MoE Final Aggregation 14\nTP=8 across GPUs 8-15\nStage 1\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer14_ln2 [label="LayerNorm MoE 14\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer14_moe_output [label="MoE Residual Add 14\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer15_mha [label="MHA Layer 15\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer15_ln1 [label="LayerNorm MHA 15\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer15_add1 [label="MHA Residual Add 15\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer15_gate [label="Gate Layer 15\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer15_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu8 [label="Route to GPU 8\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu8 [label="8 Experts GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu8 [label="Aggregate GPU 8\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu9 [label="Route to GPU 9\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu9 [label="8 Experts GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu9 [label="Aggregate GPU 9\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu10 [label="Route to GPU 10\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu10 [label="8 Experts GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu10 [label="Aggregate GPU 10\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu11 [label="Route to GPU 11\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu11 [label="8 Experts GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu11 [label="Aggregate GPU 11\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu12 [label="Route to GPU 12\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu12 [label="8 Experts GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu12 [label="Aggregate GPU 12\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu13 [label="Route to GPU 13\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu13 [label="8 Experts GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu13 [label="Aggregate GPU 13\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu14 [label="Route to GPU 14\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu14 [label="8 Experts GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu14 [label="Aggregate GPU 14\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_gpu15 [label="Route to GPU 15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_experts_gpu15 [label="8 Experts GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [tokens_per_gpu, hidden_dim=4096]\nHidden: [tokens_per_gpu, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_gpu15 [label="Aggregate GPU 15\nStage 1\nInput: [tokens_per_gpu, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_moe_agg [label="MoE Final Aggregation 15\nTP=8 across GPUs 8-15\nStage 1\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer15_ln2 [label="LayerNorm MoE 15\nTP=8 across GPUs 8-15\nStage 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer15_moe_output [label="MoE Residual Add 15\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    output [label="Model Output\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=box, style=filled, fillcolor=lightblue]
    
    input -> layer0_mha
    layer0_mha -> layer0_ln1
    layer0_ln1 -> layer0_add1
    input -> layer0_add1
    layer0_add1 -> layer0_gate
    layer0_gate -> layer0_route_gpu0
    layer0_route_gpu0 -> layer0_experts_gpu0
    layer0_experts_gpu0 -> layer0_agg_gpu0
    layer0_agg_gpu0 -> layer0_moe_agg
    layer0_gate -> layer0_route_gpu1
    layer0_route_gpu1 -> layer0_experts_gpu1
    layer0_experts_gpu1 -> layer0_agg_gpu1
    layer0_agg_gpu1 -> layer0_moe_agg
    layer0_gate -> layer0_route_gpu2
    layer0_route_gpu2 -> layer0_experts_gpu2
    layer0_experts_gpu2 -> layer0_agg_gpu2
    layer0_agg_gpu2 -> layer0_moe_agg
    layer0_gate -> layer0_route_gpu3
    layer0_route_gpu3 -> layer0_experts_gpu3
    layer0_experts_gpu3 -> layer0_agg_gpu3
    layer0_agg_gpu3 -> layer0_moe_agg
    layer0_gate -> layer0_route_gpu4
    layer0_route_gpu4 -> layer0_experts_gpu4
    layer0_experts_gpu4 -> layer0_agg_gpu4
    layer0_agg_gpu4 -> layer0_moe_agg
    layer0_gate -> layer0_route_gpu5
    layer0_route_gpu5 -> layer0_experts_gpu5
    layer0_experts_gpu5 -> layer0_agg_gpu5
    layer0_agg_gpu5 -> layer0_moe_agg
    layer0_gate -> layer0_route_gpu6
    layer0_route_gpu6 -> layer0_experts_gpu6
    layer0_experts_gpu6 -> layer0_agg_gpu6
    layer0_agg_gpu6 -> layer0_moe_agg
    layer0_gate -> layer0_route_gpu7
    layer0_route_gpu7 -> layer0_experts_gpu7
    layer0_experts_gpu7 -> layer0_agg_gpu7
    layer0_agg_gpu7 -> layer0_moe_agg
    layer0_moe_agg -> layer0_ln2
    layer0_ln2 -> layer0_moe_output
    layer0_add1 -> layer0_moe_output
    layer0_moe_output -> layer1_mha
    layer1_mha -> layer1_ln1
    layer1_ln1 -> layer1_add1
    layer0_moe_output -> layer1_add1
    layer1_add1 -> layer1_gate
    layer1_gate -> layer1_route_gpu0
    layer1_route_gpu0 -> layer1_experts_gpu0
    layer1_experts_gpu0 -> layer1_agg_gpu0
    layer1_agg_gpu0 -> layer1_moe_agg
    layer1_gate -> layer1_route_gpu1
    layer1_route_gpu1 -> layer1_experts_gpu1
    layer1_experts_gpu1 -> layer1_agg_gpu1
    layer1_agg_gpu1 -> layer1_moe_agg
    layer1_gate -> layer1_route_gpu2
    layer1_route_gpu2 -> layer1_experts_gpu2
    layer1_experts_gpu2 -> layer1_agg_gpu2
    layer1_agg_gpu2 -> layer1_moe_agg
    layer1_gate -> layer1_route_gpu3
    layer1_route_gpu3 -> layer1_experts_gpu3
    layer1_experts_gpu3 -> layer1_agg_gpu3
    layer1_agg_gpu3 -> layer1_moe_agg
    layer1_gate -> layer1_route_gpu4
    layer1_route_gpu4 -> layer1_experts_gpu4
    layer1_experts_gpu4 -> layer1_agg_gpu4
    layer1_agg_gpu4 -> layer1_moe_agg
    layer1_gate -> layer1_route_gpu5
    layer1_route_gpu5 -> layer1_experts_gpu5
    layer1_experts_gpu5 -> layer1_agg_gpu5
    layer1_agg_gpu5 -> layer1_moe_agg
    layer1_gate -> layer1_route_gpu6
    layer1_route_gpu6 -> layer1_experts_gpu6
    layer1_experts_gpu6 -> layer1_agg_gpu6
    layer1_agg_gpu6 -> layer1_moe_agg
    layer1_gate -> layer1_route_gpu7
    layer1_route_gpu7 -> layer1_experts_gpu7
    layer1_experts_gpu7 -> layer1_agg_gpu7
    layer1_agg_gpu7 -> layer1_moe_agg
    layer1_moe_agg -> layer1_ln2
    layer1_ln2 -> layer1_moe_output
    layer1_add1 -> layer1_moe_output
    layer1_moe_output -> layer2_mha
    layer2_mha -> layer2_ln1
    layer2_ln1 -> layer2_add1
    layer1_moe_output -> layer2_add1
    layer2_add1 -> layer2_gate
    layer2_gate -> layer2_route_gpu0
    layer2_route_gpu0 -> layer2_experts_gpu0
    layer2_experts_gpu0 -> layer2_agg_gpu0
    layer2_agg_gpu0 -> layer2_moe_agg
    layer2_gate -> layer2_route_gpu1
    layer2_route_gpu1 -> layer2_experts_gpu1
    layer2_experts_gpu1 -> layer2_agg_gpu1
    layer2_agg_gpu1 -> layer2_moe_agg
    layer2_gate -> layer2_route_gpu2
    layer2_route_gpu2 -> layer2_experts_gpu2
    layer2_experts_gpu2 -> layer2_agg_gpu2
    layer2_agg_gpu2 -> layer2_moe_agg
    layer2_gate -> layer2_route_gpu3
    layer2_route_gpu3 -> layer2_experts_gpu3
    layer2_experts_gpu3 -> layer2_agg_gpu3
    layer2_agg_gpu3 -> layer2_moe_agg
    layer2_gate -> layer2_route_gpu4
    layer2_route_gpu4 -> layer2_experts_gpu4
    layer2_experts_gpu4 -> layer2_agg_gpu4
    layer2_agg_gpu4 -> layer2_moe_agg
    layer2_gate -> layer2_route_gpu5
    layer2_route_gpu5 -> layer2_experts_gpu5
    layer2_experts_gpu5 -> layer2_agg_gpu5
    layer2_agg_gpu5 -> layer2_moe_agg
    layer2_gate -> layer2_route_gpu6
    layer2_route_gpu6 -> layer2_experts_gpu6
    layer2_experts_gpu6 -> layer2_agg_gpu6
    layer2_agg_gpu6 -> layer2_moe_agg
    layer2_gate -> layer2_route_gpu7
    layer2_route_gpu7 -> layer2_experts_gpu7
    layer2_experts_gpu7 -> layer2_agg_gpu7
    layer2_agg_gpu7 -> layer2_moe_agg
    layer2_moe_agg -> layer2_ln2
    layer2_ln2 -> layer2_moe_output
    layer2_add1 -> layer2_moe_output
    layer2_moe_output -> layer3_mha
    layer3_mha -> layer3_ln1
    layer3_ln1 -> layer3_add1
    layer2_moe_output -> layer3_add1
    layer3_add1 -> layer3_gate
    layer3_gate -> layer3_route_gpu0
    layer3_route_gpu0 -> layer3_experts_gpu0
    layer3_experts_gpu0 -> layer3_agg_gpu0
    layer3_agg_gpu0 -> layer3_moe_agg
    layer3_gate -> layer3_route_gpu1
    layer3_route_gpu1 -> layer3_experts_gpu1
    layer3_experts_gpu1 -> layer3_agg_gpu1
    layer3_agg_gpu1 -> layer3_moe_agg
    layer3_gate -> layer3_route_gpu2
    layer3_route_gpu2 -> layer3_experts_gpu2
    layer3_experts_gpu2 -> layer3_agg_gpu2
    layer3_agg_gpu2 -> layer3_moe_agg
    layer3_gate -> layer3_route_gpu3
    layer3_route_gpu3 -> layer3_experts_gpu3
    layer3_experts_gpu3 -> layer3_agg_gpu3
    layer3_agg_gpu3 -> layer3_moe_agg
    layer3_gate -> layer3_route_gpu4
    layer3_route_gpu4 -> layer3_experts_gpu4
    layer3_experts_gpu4 -> layer3_agg_gpu4
    layer3_agg_gpu4 -> layer3_moe_agg
    layer3_gate -> layer3_route_gpu5
    layer3_route_gpu5 -> layer3_experts_gpu5
    layer3_experts_gpu5 -> layer3_agg_gpu5
    layer3_agg_gpu5 -> layer3_moe_agg
    layer3_gate -> layer3_route_gpu6
    layer3_route_gpu6 -> layer3_experts_gpu6
    layer3_experts_gpu6 -> layer3_agg_gpu6
    layer3_agg_gpu6 -> layer3_moe_agg
    layer3_gate -> layer3_route_gpu7
    layer3_route_gpu7 -> layer3_experts_gpu7
    layer3_experts_gpu7 -> layer3_agg_gpu7
    layer3_agg_gpu7 -> layer3_moe_agg
    layer3_moe_agg -> layer3_ln2
    layer3_ln2 -> layer3_moe_output
    layer3_add1 -> layer3_moe_output
    layer3_moe_output -> layer4_mha
    layer4_mha -> layer4_ln1
    layer4_ln1 -> layer4_add1
    layer3_moe_output -> layer4_add1
    layer4_add1 -> layer4_gate
    layer4_gate -> layer4_route_gpu0
    layer4_route_gpu0 -> layer4_experts_gpu0
    layer4_experts_gpu0 -> layer4_agg_gpu0
    layer4_agg_gpu0 -> layer4_moe_agg
    layer4_gate -> layer4_route_gpu1
    layer4_route_gpu1 -> layer4_experts_gpu1
    layer4_experts_gpu1 -> layer4_agg_gpu1
    layer4_agg_gpu1 -> layer4_moe_agg
    layer4_gate -> layer4_route_gpu2
    layer4_route_gpu2 -> layer4_experts_gpu2
    layer4_experts_gpu2 -> layer4_agg_gpu2
    layer4_agg_gpu2 -> layer4_moe_agg
    layer4_gate -> layer4_route_gpu3
    layer4_route_gpu3 -> layer4_experts_gpu3
    layer4_experts_gpu3 -> layer4_agg_gpu3
    layer4_agg_gpu3 -> layer4_moe_agg
    layer4_gate -> layer4_route_gpu4
    layer4_route_gpu4 -> layer4_experts_gpu4
    layer4_experts_gpu4 -> layer4_agg_gpu4
    layer4_agg_gpu4 -> layer4_moe_agg
    layer4_gate -> layer4_route_gpu5
    layer4_route_gpu5 -> layer4_experts_gpu5
    layer4_experts_gpu5 -> layer4_agg_gpu5
    layer4_agg_gpu5 -> layer4_moe_agg
    layer4_gate -> layer4_route_gpu6
    layer4_route_gpu6 -> layer4_experts_gpu6
    layer4_experts_gpu6 -> layer4_agg_gpu6
    layer4_agg_gpu6 -> layer4_moe_agg
    layer4_gate -> layer4_route_gpu7
    layer4_route_gpu7 -> layer4_experts_gpu7
    layer4_experts_gpu7 -> layer4_agg_gpu7
    layer4_agg_gpu7 -> layer4_moe_agg
    layer4_moe_agg -> layer4_ln2
    layer4_ln2 -> layer4_moe_output
    layer4_add1 -> layer4_moe_output
    layer4_moe_output -> layer5_mha
    layer5_mha -> layer5_ln1
    layer5_ln1 -> layer5_add1
    layer4_moe_output -> layer5_add1
    layer5_add1 -> layer5_gate
    layer5_gate -> layer5_route_gpu0
    layer5_route_gpu0 -> layer5_experts_gpu0
    layer5_experts_gpu0 -> layer5_agg_gpu0
    layer5_agg_gpu0 -> layer5_moe_agg
    layer5_gate -> layer5_route_gpu1
    layer5_route_gpu1 -> layer5_experts_gpu1
    layer5_experts_gpu1 -> layer5_agg_gpu1
    layer5_agg_gpu1 -> layer5_moe_agg
    layer5_gate -> layer5_route_gpu2
    layer5_route_gpu2 -> layer5_experts_gpu2
    layer5_experts_gpu2 -> layer5_agg_gpu2
    layer5_agg_gpu2 -> layer5_moe_agg
    layer5_gate -> layer5_route_gpu3
    layer5_route_gpu3 -> layer5_experts_gpu3
    layer5_experts_gpu3 -> layer5_agg_gpu3
    layer5_agg_gpu3 -> layer5_moe_agg
    layer5_gate -> layer5_route_gpu4
    layer5_route_gpu4 -> layer5_experts_gpu4
    layer5_experts_gpu4 -> layer5_agg_gpu4
    layer5_agg_gpu4 -> layer5_moe_agg
    layer5_gate -> layer5_route_gpu5
    layer5_route_gpu5 -> layer5_experts_gpu5
    layer5_experts_gpu5 -> layer5_agg_gpu5
    layer5_agg_gpu5 -> layer5_moe_agg
    layer5_gate -> layer5_route_gpu6
    layer5_route_gpu6 -> layer5_experts_gpu6
    layer5_experts_gpu6 -> layer5_agg_gpu6
    layer5_agg_gpu6 -> layer5_moe_agg
    layer5_gate -> layer5_route_gpu7
    layer5_route_gpu7 -> layer5_experts_gpu7
    layer5_experts_gpu7 -> layer5_agg_gpu7
    layer5_agg_gpu7 -> layer5_moe_agg
    layer5_moe_agg -> layer5_ln2
    layer5_ln2 -> layer5_moe_output
    layer5_add1 -> layer5_moe_output
    layer5_moe_output -> layer6_mha
    layer6_mha -> layer6_ln1
    layer6_ln1 -> layer6_add1
    layer5_moe_output -> layer6_add1
    layer6_add1 -> layer6_gate
    layer6_gate -> layer6_route_gpu0
    layer6_route_gpu0 -> layer6_experts_gpu0
    layer6_experts_gpu0 -> layer6_agg_gpu0
    layer6_agg_gpu0 -> layer6_moe_agg
    layer6_gate -> layer6_route_gpu1
    layer6_route_gpu1 -> layer6_experts_gpu1
    layer6_experts_gpu1 -> layer6_agg_gpu1
    layer6_agg_gpu1 -> layer6_moe_agg
    layer6_gate -> layer6_route_gpu2
    layer6_route_gpu2 -> layer6_experts_gpu2
    layer6_experts_gpu2 -> layer6_agg_gpu2
    layer6_agg_gpu2 -> layer6_moe_agg
    layer6_gate -> layer6_route_gpu3
    layer6_route_gpu3 -> layer6_experts_gpu3
    layer6_experts_gpu3 -> layer6_agg_gpu3
    layer6_agg_gpu3 -> layer6_moe_agg
    layer6_gate -> layer6_route_gpu4
    layer6_route_gpu4 -> layer6_experts_gpu4
    layer6_experts_gpu4 -> layer6_agg_gpu4
    layer6_agg_gpu4 -> layer6_moe_agg
    layer6_gate -> layer6_route_gpu5
    layer6_route_gpu5 -> layer6_experts_gpu5
    layer6_experts_gpu5 -> layer6_agg_gpu5
    layer6_agg_gpu5 -> layer6_moe_agg
    layer6_gate -> layer6_route_gpu6
    layer6_route_gpu6 -> layer6_experts_gpu6
    layer6_experts_gpu6 -> layer6_agg_gpu6
    layer6_agg_gpu6 -> layer6_moe_agg
    layer6_gate -> layer6_route_gpu7
    layer6_route_gpu7 -> layer6_experts_gpu7
    layer6_experts_gpu7 -> layer6_agg_gpu7
    layer6_agg_gpu7 -> layer6_moe_agg
    layer6_moe_agg -> layer6_ln2
    layer6_ln2 -> layer6_moe_output
    layer6_add1 -> layer6_moe_output
    layer6_moe_output -> layer7_mha
    layer7_mha -> layer7_ln1
    layer7_ln1 -> layer7_add1
    layer6_moe_output -> layer7_add1
    layer7_add1 -> layer7_gate
    layer7_gate -> layer7_route_gpu0
    layer7_route_gpu0 -> layer7_experts_gpu0
    layer7_experts_gpu0 -> layer7_agg_gpu0
    layer7_agg_gpu0 -> layer7_moe_agg
    layer7_gate -> layer7_route_gpu1
    layer7_route_gpu1 -> layer7_experts_gpu1
    layer7_experts_gpu1 -> layer7_agg_gpu1
    layer7_agg_gpu1 -> layer7_moe_agg
    layer7_gate -> layer7_route_gpu2
    layer7_route_gpu2 -> layer7_experts_gpu2
    layer7_experts_gpu2 -> layer7_agg_gpu2
    layer7_agg_gpu2 -> layer7_moe_agg
    layer7_gate -> layer7_route_gpu3
    layer7_route_gpu3 -> layer7_experts_gpu3
    layer7_experts_gpu3 -> layer7_agg_gpu3
    layer7_agg_gpu3 -> layer7_moe_agg
    layer7_gate -> layer7_route_gpu4
    layer7_route_gpu4 -> layer7_experts_gpu4
    layer7_experts_gpu4 -> layer7_agg_gpu4
    layer7_agg_gpu4 -> layer7_moe_agg
    layer7_gate -> layer7_route_gpu5
    layer7_route_gpu5 -> layer7_experts_gpu5
    layer7_experts_gpu5 -> layer7_agg_gpu5
    layer7_agg_gpu5 -> layer7_moe_agg
    layer7_gate -> layer7_route_gpu6
    layer7_route_gpu6 -> layer7_experts_gpu6
    layer7_experts_gpu6 -> layer7_agg_gpu6
    layer7_agg_gpu6 -> layer7_moe_agg
    layer7_gate -> layer7_route_gpu7
    layer7_route_gpu7 -> layer7_experts_gpu7
    layer7_experts_gpu7 -> layer7_agg_gpu7
    layer7_agg_gpu7 -> layer7_moe_agg
    layer7_moe_agg -> layer7_ln2
    layer7_ln2 -> layer7_moe_output
    layer7_add1 -> layer7_moe_output
    layer7_moe_output -> layer8_mha
    layer8_mha -> layer8_ln1
    layer8_ln1 -> layer8_add1
    layer7_moe_output -> layer8_add1
    layer8_add1 -> layer8_gate
    layer8_gate -> layer8_route_gpu8
    layer8_route_gpu8 -> layer8_experts_gpu8
    layer8_experts_gpu8 -> layer8_agg_gpu8
    layer8_agg_gpu8 -> layer8_moe_agg
    layer8_gate -> layer8_route_gpu9
    layer8_route_gpu9 -> layer8_experts_gpu9
    layer8_experts_gpu9 -> layer8_agg_gpu9
    layer8_agg_gpu9 -> layer8_moe_agg
    layer8_gate -> layer8_route_gpu10
    layer8_route_gpu10 -> layer8_experts_gpu10
    layer8_experts_gpu10 -> layer8_agg_gpu10
    layer8_agg_gpu10 -> layer8_moe_agg
    layer8_gate -> layer8_route_gpu11
    layer8_route_gpu11 -> layer8_experts_gpu11
    layer8_experts_gpu11 -> layer8_agg_gpu11
    layer8_agg_gpu11 -> layer8_moe_agg
    layer8_gate -> layer8_route_gpu12
    layer8_route_gpu12 -> layer8_experts_gpu12
    layer8_experts_gpu12 -> layer8_agg_gpu12
    layer8_agg_gpu12 -> layer8_moe_agg
    layer8_gate -> layer8_route_gpu13
    layer8_route_gpu13 -> layer8_experts_gpu13
    layer8_experts_gpu13 -> layer8_agg_gpu13
    layer8_agg_gpu13 -> layer8_moe_agg
    layer8_gate -> layer8_route_gpu14
    layer8_route_gpu14 -> layer8_experts_gpu14
    layer8_experts_gpu14 -> layer8_agg_gpu14
    layer8_agg_gpu14 -> layer8_moe_agg
    layer8_gate -> layer8_route_gpu15
    layer8_route_gpu15 -> layer8_experts_gpu15
    layer8_experts_gpu15 -> layer8_agg_gpu15
    layer8_agg_gpu15 -> layer8_moe_agg
    layer8_moe_agg -> layer8_ln2
    layer8_ln2 -> layer8_moe_output
    layer8_add1 -> layer8_moe_output
    layer8_moe_output -> layer9_mha
    layer9_mha -> layer9_ln1
    layer9_ln1 -> layer9_add1
    layer8_moe_output -> layer9_add1
    layer9_add1 -> layer9_gate
    layer9_gate -> layer9_route_gpu8
    layer9_route_gpu8 -> layer9_experts_gpu8
    layer9_experts_gpu8 -> layer9_agg_gpu8
    layer9_agg_gpu8 -> layer9_moe_agg
    layer9_gate -> layer9_route_gpu9
    layer9_route_gpu9 -> layer9_experts_gpu9
    layer9_experts_gpu9 -> layer9_agg_gpu9
    layer9_agg_gpu9 -> layer9_moe_agg
    layer9_gate -> layer9_route_gpu10
    layer9_route_gpu10 -> layer9_experts_gpu10
    layer9_experts_gpu10 -> layer9_agg_gpu10
    layer9_agg_gpu10 -> layer9_moe_agg
    layer9_gate -> layer9_route_gpu11
    layer9_route_gpu11 -> layer9_experts_gpu11
    layer9_experts_gpu11 -> layer9_agg_gpu11
    layer9_agg_gpu11 -> layer9_moe_agg
    layer9_gate -> layer9_route_gpu12
    layer9_route_gpu12 -> layer9_experts_gpu12
    layer9_experts_gpu12 -> layer9_agg_gpu12
    layer9_agg_gpu12 -> layer9_moe_agg
    layer9_gate -> layer9_route_gpu13
    layer9_route_gpu13 -> layer9_experts_gpu13
    layer9_experts_gpu13 -> layer9_agg_gpu13
    layer9_agg_gpu13 -> layer9_moe_agg
    layer9_gate -> layer9_route_gpu14
    layer9_route_gpu14 -> layer9_experts_gpu14
    layer9_experts_gpu14 -> layer9_agg_gpu14
    layer9_agg_gpu14 -> layer9_moe_agg
    layer9_gate -> layer9_route_gpu15
    layer9_route_gpu15 -> layer9_experts_gpu15
    layer9_experts_gpu15 -> layer9_agg_gpu15
    layer9_agg_gpu15 -> layer9_moe_agg
    layer9_moe_agg -> layer9_ln2
    layer9_ln2 -> layer9_moe_output
    layer9_add1 -> layer9_moe_output
    layer9_moe_output -> layer10_mha
    layer10_mha -> layer10_ln1
    layer10_ln1 -> layer10_add1
    layer9_moe_output -> layer10_add1
    layer10_add1 -> layer10_gate
    layer10_gate -> layer10_route_gpu8
    layer10_route_gpu8 -> layer10_experts_gpu8
    layer10_experts_gpu8 -> layer10_agg_gpu8
    layer10_agg_gpu8 -> layer10_moe_agg
    layer10_gate -> layer10_route_gpu9
    layer10_route_gpu9 -> layer10_experts_gpu9
    layer10_experts_gpu9 -> layer10_agg_gpu9
    layer10_agg_gpu9 -> layer10_moe_agg
    layer10_gate -> layer10_route_gpu10
    layer10_route_gpu10 -> layer10_experts_gpu10
    layer10_experts_gpu10 -> layer10_agg_gpu10
    layer10_agg_gpu10 -> layer10_moe_agg
    layer10_gate -> layer10_route_gpu11
    layer10_route_gpu11 -> layer10_experts_gpu11
    layer10_experts_gpu11 -> layer10_agg_gpu11
    layer10_agg_gpu11 -> layer10_moe_agg
    layer10_gate -> layer10_route_gpu12
    layer10_route_gpu12 -> layer10_experts_gpu12
    layer10_experts_gpu12 -> layer10_agg_gpu12
    layer10_agg_gpu12 -> layer10_moe_agg
    layer10_gate -> layer10_route_gpu13
    layer10_route_gpu13 -> layer10_experts_gpu13
    layer10_experts_gpu13 -> layer10_agg_gpu13
    layer10_agg_gpu13 -> layer10_moe_agg
    layer10_gate -> layer10_route_gpu14
    layer10_route_gpu14 -> layer10_experts_gpu14
    layer10_experts_gpu14 -> layer10_agg_gpu14
    layer10_agg_gpu14 -> layer10_moe_agg
    layer10_gate -> layer10_route_gpu15
    layer10_route_gpu15 -> layer10_experts_gpu15
    layer10_experts_gpu15 -> layer10_agg_gpu15
    layer10_agg_gpu15 -> layer10_moe_agg
    layer10_moe_agg -> layer10_ln2
    layer10_ln2 -> layer10_moe_output
    layer10_add1 -> layer10_moe_output
    layer10_moe_output -> layer11_mha
    layer11_mha -> layer11_ln1
    layer11_ln1 -> layer11_add1
    layer10_moe_output -> layer11_add1
    layer11_add1 -> layer11_gate
    layer11_gate -> layer11_route_gpu8
    layer11_route_gpu8 -> layer11_experts_gpu8
    layer11_experts_gpu8 -> layer11_agg_gpu8
    layer11_agg_gpu8 -> layer11_moe_agg
    layer11_gate -> layer11_route_gpu9
    layer11_route_gpu9 -> layer11_experts_gpu9
    layer11_experts_gpu9 -> layer11_agg_gpu9
    layer11_agg_gpu9 -> layer11_moe_agg
    layer11_gate -> layer11_route_gpu10
    layer11_route_gpu10 -> layer11_experts_gpu10
    layer11_experts_gpu10 -> layer11_agg_gpu10
    layer11_agg_gpu10 -> layer11_moe_agg
    layer11_gate -> layer11_route_gpu11
    layer11_route_gpu11 -> layer11_experts_gpu11
    layer11_experts_gpu11 -> layer11_agg_gpu11
    layer11_agg_gpu11 -> layer11_moe_agg
    layer11_gate -> layer11_route_gpu12
    layer11_route_gpu12 -> layer11_experts_gpu12
    layer11_experts_gpu12 -> layer11_agg_gpu12
    layer11_agg_gpu12 -> layer11_moe_agg
    layer11_gate -> layer11_route_gpu13
    layer11_route_gpu13 -> layer11_experts_gpu13
    layer11_experts_gpu13 -> layer11_agg_gpu13
    layer11_agg_gpu13 -> layer11_moe_agg
    layer11_gate -> layer11_route_gpu14
    layer11_route_gpu14 -> layer11_experts_gpu14
    layer11_experts_gpu14 -> layer11_agg_gpu14
    layer11_agg_gpu14 -> layer11_moe_agg
    layer11_gate -> layer11_route_gpu15
    layer11_route_gpu15 -> layer11_experts_gpu15
    layer11_experts_gpu15 -> layer11_agg_gpu15
    layer11_agg_gpu15 -> layer11_moe_agg
    layer11_moe_agg -> layer11_ln2
    layer11_ln2 -> layer11_moe_output
    layer11_add1 -> layer11_moe_output
    layer11_moe_output -> layer12_mha
    layer12_mha -> layer12_ln1
    layer12_ln1 -> layer12_add1
    layer11_moe_output -> layer12_add1
    layer12_add1 -> layer12_gate
    layer12_gate -> layer12_route_gpu8
    layer12_route_gpu8 -> layer12_experts_gpu8
    layer12_experts_gpu8 -> layer12_agg_gpu8
    layer12_agg_gpu8 -> layer12_moe_agg
    layer12_gate -> layer12_route_gpu9
    layer12_route_gpu9 -> layer12_experts_gpu9
    layer12_experts_gpu9 -> layer12_agg_gpu9
    layer12_agg_gpu9 -> layer12_moe_agg
    layer12_gate -> layer12_route_gpu10
    layer12_route_gpu10 -> layer12_experts_gpu10
    layer12_experts_gpu10 -> layer12_agg_gpu10
    layer12_agg_gpu10 -> layer12_moe_agg
    layer12_gate -> layer12_route_gpu11
    layer12_route_gpu11 -> layer12_experts_gpu11
    layer12_experts_gpu11 -> layer12_agg_gpu11
    layer12_agg_gpu11 -> layer12_moe_agg
    layer12_gate -> layer12_route_gpu12
    layer12_route_gpu12 -> layer12_experts_gpu12
    layer12_experts_gpu12 -> layer12_agg_gpu12
    layer12_agg_gpu12 -> layer12_moe_agg
    layer12_gate -> layer12_route_gpu13
    layer12_route_gpu13 -> layer12_experts_gpu13
    layer12_experts_gpu13 -> layer12_agg_gpu13
    layer12_agg_gpu13 -> layer12_moe_agg
    layer12_gate -> layer12_route_gpu14
    layer12_route_gpu14 -> layer12_experts_gpu14
    layer12_experts_gpu14 -> layer12_agg_gpu14
    layer12_agg_gpu14 -> layer12_moe_agg
    layer12_gate -> layer12_route_gpu15
    layer12_route_gpu15 -> layer12_experts_gpu15
    layer12_experts_gpu15 -> layer12_agg_gpu15
    layer12_agg_gpu15 -> layer12_moe_agg
    layer12_moe_agg -> layer12_ln2
    layer12_ln2 -> layer12_moe_output
    layer12_add1 -> layer12_moe_output
    layer12_moe_output -> layer13_mha
    layer13_mha -> layer13_ln1
    layer13_ln1 -> layer13_add1
    layer12_moe_output -> layer13_add1
    layer13_add1 -> layer13_gate
    layer13_gate -> layer13_route_gpu8
    layer13_route_gpu8 -> layer13_experts_gpu8
    layer13_experts_gpu8 -> layer13_agg_gpu8
    layer13_agg_gpu8 -> layer13_moe_agg
    layer13_gate -> layer13_route_gpu9
    layer13_route_gpu9 -> layer13_experts_gpu9
    layer13_experts_gpu9 -> layer13_agg_gpu9
    layer13_agg_gpu9 -> layer13_moe_agg
    layer13_gate -> layer13_route_gpu10
    layer13_route_gpu10 -> layer13_experts_gpu10
    layer13_experts_gpu10 -> layer13_agg_gpu10
    layer13_agg_gpu10 -> layer13_moe_agg
    layer13_gate -> layer13_route_gpu11
    layer13_route_gpu11 -> layer13_experts_gpu11
    layer13_experts_gpu11 -> layer13_agg_gpu11
    layer13_agg_gpu11 -> layer13_moe_agg
    layer13_gate -> layer13_route_gpu12
    layer13_route_gpu12 -> layer13_experts_gpu12
    layer13_experts_gpu12 -> layer13_agg_gpu12
    layer13_agg_gpu12 -> layer13_moe_agg
    layer13_gate -> layer13_route_gpu13
    layer13_route_gpu13 -> layer13_experts_gpu13
    layer13_experts_gpu13 -> layer13_agg_gpu13
    layer13_agg_gpu13 -> layer13_moe_agg
    layer13_gate -> layer13_route_gpu14
    layer13_route_gpu14 -> layer13_experts_gpu14
    layer13_experts_gpu14 -> layer13_agg_gpu14
    layer13_agg_gpu14 -> layer13_moe_agg
    layer13_gate -> layer13_route_gpu15
    layer13_route_gpu15 -> layer13_experts_gpu15
    layer13_experts_gpu15 -> layer13_agg_gpu15
    layer13_agg_gpu15 -> layer13_moe_agg
    layer13_moe_agg -> layer13_ln2
    layer13_ln2 -> layer13_moe_output
    layer13_add1 -> layer13_moe_output
    layer13_moe_output -> layer14_mha
    layer14_mha -> layer14_ln1
    layer14_ln1 -> layer14_add1
    layer13_moe_output -> layer14_add1
    layer14_add1 -> layer14_gate
    layer14_gate -> layer14_route_gpu8
    layer14_route_gpu8 -> layer14_experts_gpu8
    layer14_experts_gpu8 -> layer14_agg_gpu8
    layer14_agg_gpu8 -> layer14_moe_agg
    layer14_gate -> layer14_route_gpu9
    layer14_route_gpu9 -> layer14_experts_gpu9
    layer14_experts_gpu9 -> layer14_agg_gpu9
    layer14_agg_gpu9 -> layer14_moe_agg
    layer14_gate -> layer14_route_gpu10
    layer14_route_gpu10 -> layer14_experts_gpu10
    layer14_experts_gpu10 -> layer14_agg_gpu10
    layer14_agg_gpu10 -> layer14_moe_agg
    layer14_gate -> layer14_route_gpu11
    layer14_route_gpu11 -> layer14_experts_gpu11
    layer14_experts_gpu11 -> layer14_agg_gpu11
    layer14_agg_gpu11 -> layer14_moe_agg
    layer14_gate -> layer14_route_gpu12
    layer14_route_gpu12 -> layer14_experts_gpu12
    layer14_experts_gpu12 -> layer14_agg_gpu12
    layer14_agg_gpu12 -> layer14_moe_agg
    layer14_gate -> layer14_route_gpu13
    layer14_route_gpu13 -> layer14_experts_gpu13
    layer14_experts_gpu13 -> layer14_agg_gpu13
    layer14_agg_gpu13 -> layer14_moe_agg
    layer14_gate -> layer14_route_gpu14
    layer14_route_gpu14 -> layer14_experts_gpu14
    layer14_experts_gpu14 -> layer14_agg_gpu14
    layer14_agg_gpu14 -> layer14_moe_agg
    layer14_gate -> layer14_route_gpu15
    layer14_route_gpu15 -> layer14_experts_gpu15
    layer14_experts_gpu15 -> layer14_agg_gpu15
    layer14_agg_gpu15 -> layer14_moe_agg
    layer14_moe_agg -> layer14_ln2
    layer14_ln2 -> layer14_moe_output
    layer14_add1 -> layer14_moe_output
    layer14_moe_output -> layer15_mha
    layer15_mha -> layer15_ln1
    layer15_ln1 -> layer15_add1
    layer14_moe_output -> layer15_add1
    layer15_add1 -> layer15_gate
    layer15_gate -> layer15_route_gpu8
    layer15_route_gpu8 -> layer15_experts_gpu8
    layer15_experts_gpu8 -> layer15_agg_gpu8
    layer15_agg_gpu8 -> layer15_moe_agg
    layer15_gate -> layer15_route_gpu9
    layer15_route_gpu9 -> layer15_experts_gpu9
    layer15_experts_gpu9 -> layer15_agg_gpu9
    layer15_agg_gpu9 -> layer15_moe_agg
    layer15_gate -> layer15_route_gpu10
    layer15_route_gpu10 -> layer15_experts_gpu10
    layer15_experts_gpu10 -> layer15_agg_gpu10
    layer15_agg_gpu10 -> layer15_moe_agg
    layer15_gate -> layer15_route_gpu11
    layer15_route_gpu11 -> layer15_experts_gpu11
    layer15_experts_gpu11 -> layer15_agg_gpu11
    layer15_agg_gpu11 -> layer15_moe_agg
    layer15_gate -> layer15_route_gpu12
    layer15_route_gpu12 -> layer15_experts_gpu12
    layer15_experts_gpu12 -> layer15_agg_gpu12
    layer15_agg_gpu12 -> layer15_moe_agg
    layer15_gate -> layer15_route_gpu13
    layer15_route_gpu13 -> layer15_experts_gpu13
    layer15_experts_gpu13 -> layer15_agg_gpu13
    layer15_agg_gpu13 -> layer15_moe_agg
    layer15_gate -> layer15_route_gpu14
    layer15_route_gpu14 -> layer15_experts_gpu14
    layer15_experts_gpu14 -> layer15_agg_gpu14
    layer15_agg_gpu14 -> layer15_moe_agg
    layer15_gate -> layer15_route_gpu15
    layer15_route_gpu15 -> layer15_experts_gpu15
    layer15_experts_gpu15 -> layer15_agg_gpu15
    layer15_agg_gpu15 -> layer15_moe_agg
    layer15_moe_agg -> layer15_ln2
    layer15_ln2 -> layer15_moe_output
    layer15_add1 -> layer15_moe_output
    layer15_moe_output -> output
}