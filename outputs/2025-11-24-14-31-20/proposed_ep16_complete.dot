digraph Proposed_Large_EP16_Final {
    rankdir=TB;
    splines=ortho;
    node [fontname="Arial", fontsize=10];
    
    input [label="Model Input\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=box, style=filled, fillcolor=lightblue]
    layer0_mha [label="MHA Layer 0\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer0_ln1 [label="LayerNorm MHA 0\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer0_add1 [label="MHA Residual Add 0\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer0_gate [label="Gate Layer 0\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer0_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer0_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer0_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer0_moe_agg [label="MoE Final Aggregation 0\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer0_ln2 [label="LayerNorm MoE 0\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer0_moe_output [label="MoE Residual Add 0\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer1_mha [label="MHA Layer 1\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer1_ln1 [label="LayerNorm MHA 1\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer1_add1 [label="MHA Residual Add 1\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer1_gate [label="Gate Layer 1\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer1_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer1_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer1_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer1_moe_agg [label="MoE Final Aggregation 1\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer1_ln2 [label="LayerNorm MoE 1\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer1_moe_output [label="MoE Residual Add 1\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer2_mha [label="MHA Layer 2\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer2_ln1 [label="LayerNorm MHA 2\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer2_add1 [label="MHA Residual Add 2\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer2_gate [label="Gate Layer 2\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer2_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer2_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer2_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer2_moe_agg [label="MoE Final Aggregation 2\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer2_ln2 [label="LayerNorm MoE 2\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer2_moe_output [label="MoE Residual Add 2\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer3_mha [label="MHA Layer 3\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer3_ln1 [label="LayerNorm MHA 3\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer3_add1 [label="MHA Residual Add 3\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer3_gate [label="Gate Layer 3\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer3_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer3_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer3_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer3_moe_agg [label="MoE Final Aggregation 3\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer3_ln2 [label="LayerNorm MoE 3\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer3_moe_output [label="MoE Residual Add 3\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer4_mha [label="MHA Layer 4\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer4_ln1 [label="LayerNorm MHA 4\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer4_add1 [label="MHA Residual Add 4\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer4_gate [label="Gate Layer 4\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer4_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer4_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer4_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer4_moe_agg [label="MoE Final Aggregation 4\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer4_ln2 [label="LayerNorm MoE 4\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer4_moe_output [label="MoE Residual Add 4\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer5_mha [label="MHA Layer 5\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer5_ln1 [label="LayerNorm MHA 5\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer5_add1 [label="MHA Residual Add 5\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer5_gate [label="Gate Layer 5\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer5_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer5_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer5_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer5_moe_agg [label="MoE Final Aggregation 5\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer5_ln2 [label="LayerNorm MoE 5\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer5_moe_output [label="MoE Residual Add 5\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer6_mha [label="MHA Layer 6\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer6_ln1 [label="LayerNorm MHA 6\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer6_add1 [label="MHA Residual Add 6\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer6_gate [label="Gate Layer 6\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer6_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer6_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer6_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer6_moe_agg [label="MoE Final Aggregation 6\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer6_ln2 [label="LayerNorm MoE 6\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer6_moe_output [label="MoE Residual Add 6\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer7_mha [label="MHA Layer 7\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer7_ln1 [label="LayerNorm MHA 7\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer7_add1 [label="MHA Residual Add 7\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer7_gate [label="Gate Layer 7\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer7_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer7_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer7_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer7_moe_agg [label="MoE Final Aggregation 7\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer7_ln2 [label="LayerNorm MoE 7\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer7_moe_output [label="MoE Residual Add 7\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer8_mha [label="MHA Layer 8\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer8_ln1 [label="LayerNorm MHA 8\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer8_add1 [label="MHA Residual Add 8\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer8_gate [label="Gate Layer 8\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer8_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer8_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer8_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer8_moe_agg [label="MoE Final Aggregation 8\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer8_ln2 [label="LayerNorm MoE 8\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer8_moe_output [label="MoE Residual Add 8\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer9_mha [label="MHA Layer 9\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer9_ln1 [label="LayerNorm MHA 9\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer9_add1 [label="MHA Residual Add 9\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer9_gate [label="Gate Layer 9\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer9_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer9_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer9_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer9_moe_agg [label="MoE Final Aggregation 9\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer9_ln2 [label="LayerNorm MoE 9\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer9_moe_output [label="MoE Residual Add 9\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer10_mha [label="MHA Layer 10\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer10_ln1 [label="LayerNorm MHA 10\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer10_add1 [label="MHA Residual Add 10\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer10_gate [label="Gate Layer 10\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer10_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer10_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer10_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer10_moe_agg [label="MoE Final Aggregation 10\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer10_ln2 [label="LayerNorm MoE 10\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer10_moe_output [label="MoE Residual Add 10\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer11_mha [label="MHA Layer 11\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer11_ln1 [label="LayerNorm MHA 11\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer11_add1 [label="MHA Residual Add 11\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer11_gate [label="Gate Layer 11\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer11_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer11_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer11_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer11_moe_agg [label="MoE Final Aggregation 11\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer11_ln2 [label="LayerNorm MoE 11\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer11_moe_output [label="MoE Residual Add 11\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer12_mha [label="MHA Layer 12\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer12_ln1 [label="LayerNorm MHA 12\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer12_add1 [label="MHA Residual Add 12\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer12_gate [label="Gate Layer 12\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer12_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer12_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer12_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer12_moe_agg [label="MoE Final Aggregation 12\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer12_ln2 [label="LayerNorm MoE 12\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer12_moe_output [label="MoE Residual Add 12\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer13_mha [label="MHA Layer 13\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer13_ln1 [label="LayerNorm MHA 13\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer13_add1 [label="MHA Residual Add 13\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer13_gate [label="Gate Layer 13\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer13_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer13_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer13_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer13_moe_agg [label="MoE Final Aggregation 13\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer13_ln2 [label="LayerNorm MoE 13\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer13_moe_output [label="MoE Residual Add 13\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer14_mha [label="MHA Layer 14\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer14_ln1 [label="LayerNorm MHA 14\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer14_add1 [label="MHA Residual Add 14\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer14_gate [label="Gate Layer 14\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer14_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer14_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer14_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer14_moe_agg [label="MoE Final Aggregation 14\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer14_ln2 [label="LayerNorm MoE 14\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer14_moe_output [label="MoE Residual Add 14\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer15_mha [label="MHA Layer 15\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=lightcyan]
    layer15_ln1 [label="LayerNorm MHA 15\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer15_add1 [label="MHA Residual Add 15\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    layer15_gate [label="Gate Layer 15\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", shape=parallelogram, style=filled, fillcolor=lightgreen]
    layer15_route_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_route_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", shape=ellipse, style=dashed, fillcolor=orange]
    layer15_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]\nHidden: [tokens_per_expert, 16384]", shape=rectangle, style=filled, fillcolor=lightcoral]
    layer15_agg_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=lightpink]
    layer15_moe_agg [label="MoE Final Aggregation 15\nAll GPUs\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=ellipse, style=filled, fillcolor=gold]
    layer15_ln2 [label="LayerNorm MoE 15\nAll GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle]
    layer15_moe_output [label="MoE Residual Add 15\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=rectangle, style=filled, fillcolor=yellow]
    output [label="Model Output\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]", shape=box, style=filled, fillcolor=lightblue]
    
    input -> layer0_mha
    layer0_mha -> layer0_ln1
    layer0_ln1 -> layer0_add1
    input -> layer0_add1
    layer0_add1 -> layer0_gate
    layer0_gate -> layer0_route_exp0
    layer0_route_exp0 -> layer0_expert0
    layer0_expert0 -> layer0_agg_exp0
    layer0_agg_exp0 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp1
    layer0_route_exp1 -> layer0_expert1
    layer0_expert1 -> layer0_agg_exp1
    layer0_agg_exp1 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp2
    layer0_route_exp2 -> layer0_expert2
    layer0_expert2 -> layer0_agg_exp2
    layer0_agg_exp2 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp3
    layer0_route_exp3 -> layer0_expert3
    layer0_expert3 -> layer0_agg_exp3
    layer0_agg_exp3 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp4
    layer0_route_exp4 -> layer0_expert4
    layer0_expert4 -> layer0_agg_exp4
    layer0_agg_exp4 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp5
    layer0_route_exp5 -> layer0_expert5
    layer0_expert5 -> layer0_agg_exp5
    layer0_agg_exp5 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp6
    layer0_route_exp6 -> layer0_expert6
    layer0_expert6 -> layer0_agg_exp6
    layer0_agg_exp6 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp7
    layer0_route_exp7 -> layer0_expert7
    layer0_expert7 -> layer0_agg_exp7
    layer0_agg_exp7 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp8
    layer0_route_exp8 -> layer0_expert8
    layer0_expert8 -> layer0_agg_exp8
    layer0_agg_exp8 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp9
    layer0_route_exp9 -> layer0_expert9
    layer0_expert9 -> layer0_agg_exp9
    layer0_agg_exp9 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp10
    layer0_route_exp10 -> layer0_expert10
    layer0_expert10 -> layer0_agg_exp10
    layer0_agg_exp10 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp11
    layer0_route_exp11 -> layer0_expert11
    layer0_expert11 -> layer0_agg_exp11
    layer0_agg_exp11 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp12
    layer0_route_exp12 -> layer0_expert12
    layer0_expert12 -> layer0_agg_exp12
    layer0_agg_exp12 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp13
    layer0_route_exp13 -> layer0_expert13
    layer0_expert13 -> layer0_agg_exp13
    layer0_agg_exp13 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp14
    layer0_route_exp14 -> layer0_expert14
    layer0_expert14 -> layer0_agg_exp14
    layer0_agg_exp14 -> layer0_moe_agg
    layer0_gate -> layer0_route_exp15
    layer0_route_exp15 -> layer0_expert15
    layer0_expert15 -> layer0_agg_exp15
    layer0_agg_exp15 -> layer0_moe_agg
    layer0_moe_agg -> layer0_ln2
    layer0_ln2 -> layer0_moe_output
    layer0_add1 -> layer0_moe_output
    layer0_moe_output -> layer1_mha
    layer1_mha -> layer1_ln1
    layer1_ln1 -> layer1_add1
    layer0_moe_output -> layer1_add1
    layer1_add1 -> layer1_gate
    layer1_gate -> layer1_route_exp0
    layer1_route_exp0 -> layer1_expert0
    layer1_expert0 -> layer1_agg_exp0
    layer1_agg_exp0 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp1
    layer1_route_exp1 -> layer1_expert1
    layer1_expert1 -> layer1_agg_exp1
    layer1_agg_exp1 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp2
    layer1_route_exp2 -> layer1_expert2
    layer1_expert2 -> layer1_agg_exp2
    layer1_agg_exp2 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp3
    layer1_route_exp3 -> layer1_expert3
    layer1_expert3 -> layer1_agg_exp3
    layer1_agg_exp3 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp4
    layer1_route_exp4 -> layer1_expert4
    layer1_expert4 -> layer1_agg_exp4
    layer1_agg_exp4 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp5
    layer1_route_exp5 -> layer1_expert5
    layer1_expert5 -> layer1_agg_exp5
    layer1_agg_exp5 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp6
    layer1_route_exp6 -> layer1_expert6
    layer1_expert6 -> layer1_agg_exp6
    layer1_agg_exp6 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp7
    layer1_route_exp7 -> layer1_expert7
    layer1_expert7 -> layer1_agg_exp7
    layer1_agg_exp7 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp8
    layer1_route_exp8 -> layer1_expert8
    layer1_expert8 -> layer1_agg_exp8
    layer1_agg_exp8 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp9
    layer1_route_exp9 -> layer1_expert9
    layer1_expert9 -> layer1_agg_exp9
    layer1_agg_exp9 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp10
    layer1_route_exp10 -> layer1_expert10
    layer1_expert10 -> layer1_agg_exp10
    layer1_agg_exp10 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp11
    layer1_route_exp11 -> layer1_expert11
    layer1_expert11 -> layer1_agg_exp11
    layer1_agg_exp11 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp12
    layer1_route_exp12 -> layer1_expert12
    layer1_expert12 -> layer1_agg_exp12
    layer1_agg_exp12 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp13
    layer1_route_exp13 -> layer1_expert13
    layer1_expert13 -> layer1_agg_exp13
    layer1_agg_exp13 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp14
    layer1_route_exp14 -> layer1_expert14
    layer1_expert14 -> layer1_agg_exp14
    layer1_agg_exp14 -> layer1_moe_agg
    layer1_gate -> layer1_route_exp15
    layer1_route_exp15 -> layer1_expert15
    layer1_expert15 -> layer1_agg_exp15
    layer1_agg_exp15 -> layer1_moe_agg
    layer1_moe_agg -> layer1_ln2
    layer1_ln2 -> layer1_moe_output
    layer1_add1 -> layer1_moe_output
    layer1_moe_output -> layer2_mha
    layer2_mha -> layer2_ln1
    layer2_ln1 -> layer2_add1
    layer1_moe_output -> layer2_add1
    layer2_add1 -> layer2_gate
    layer2_gate -> layer2_route_exp0
    layer2_route_exp0 -> layer2_expert0
    layer2_expert0 -> layer2_agg_exp0
    layer2_agg_exp0 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp1
    layer2_route_exp1 -> layer2_expert1
    layer2_expert1 -> layer2_agg_exp1
    layer2_agg_exp1 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp2
    layer2_route_exp2 -> layer2_expert2
    layer2_expert2 -> layer2_agg_exp2
    layer2_agg_exp2 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp3
    layer2_route_exp3 -> layer2_expert3
    layer2_expert3 -> layer2_agg_exp3
    layer2_agg_exp3 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp4
    layer2_route_exp4 -> layer2_expert4
    layer2_expert4 -> layer2_agg_exp4
    layer2_agg_exp4 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp5
    layer2_route_exp5 -> layer2_expert5
    layer2_expert5 -> layer2_agg_exp5
    layer2_agg_exp5 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp6
    layer2_route_exp6 -> layer2_expert6
    layer2_expert6 -> layer2_agg_exp6
    layer2_agg_exp6 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp7
    layer2_route_exp7 -> layer2_expert7
    layer2_expert7 -> layer2_agg_exp7
    layer2_agg_exp7 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp8
    layer2_route_exp8 -> layer2_expert8
    layer2_expert8 -> layer2_agg_exp8
    layer2_agg_exp8 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp9
    layer2_route_exp9 -> layer2_expert9
    layer2_expert9 -> layer2_agg_exp9
    layer2_agg_exp9 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp10
    layer2_route_exp10 -> layer2_expert10
    layer2_expert10 -> layer2_agg_exp10
    layer2_agg_exp10 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp11
    layer2_route_exp11 -> layer2_expert11
    layer2_expert11 -> layer2_agg_exp11
    layer2_agg_exp11 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp12
    layer2_route_exp12 -> layer2_expert12
    layer2_expert12 -> layer2_agg_exp12
    layer2_agg_exp12 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp13
    layer2_route_exp13 -> layer2_expert13
    layer2_expert13 -> layer2_agg_exp13
    layer2_agg_exp13 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp14
    layer2_route_exp14 -> layer2_expert14
    layer2_expert14 -> layer2_agg_exp14
    layer2_agg_exp14 -> layer2_moe_agg
    layer2_gate -> layer2_route_exp15
    layer2_route_exp15 -> layer2_expert15
    layer2_expert15 -> layer2_agg_exp15
    layer2_agg_exp15 -> layer2_moe_agg
    layer2_moe_agg -> layer2_ln2
    layer2_ln2 -> layer2_moe_output
    layer2_add1 -> layer2_moe_output
    layer2_moe_output -> layer3_mha
    layer3_mha -> layer3_ln1
    layer3_ln1 -> layer3_add1
    layer2_moe_output -> layer3_add1
    layer3_add1 -> layer3_gate
    layer3_gate -> layer3_route_exp0
    layer3_route_exp0 -> layer3_expert0
    layer3_expert0 -> layer3_agg_exp0
    layer3_agg_exp0 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp1
    layer3_route_exp1 -> layer3_expert1
    layer3_expert1 -> layer3_agg_exp1
    layer3_agg_exp1 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp2
    layer3_route_exp2 -> layer3_expert2
    layer3_expert2 -> layer3_agg_exp2
    layer3_agg_exp2 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp3
    layer3_route_exp3 -> layer3_expert3
    layer3_expert3 -> layer3_agg_exp3
    layer3_agg_exp3 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp4
    layer3_route_exp4 -> layer3_expert4
    layer3_expert4 -> layer3_agg_exp4
    layer3_agg_exp4 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp5
    layer3_route_exp5 -> layer3_expert5
    layer3_expert5 -> layer3_agg_exp5
    layer3_agg_exp5 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp6
    layer3_route_exp6 -> layer3_expert6
    layer3_expert6 -> layer3_agg_exp6
    layer3_agg_exp6 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp7
    layer3_route_exp7 -> layer3_expert7
    layer3_expert7 -> layer3_agg_exp7
    layer3_agg_exp7 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp8
    layer3_route_exp8 -> layer3_expert8
    layer3_expert8 -> layer3_agg_exp8
    layer3_agg_exp8 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp9
    layer3_route_exp9 -> layer3_expert9
    layer3_expert9 -> layer3_agg_exp9
    layer3_agg_exp9 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp10
    layer3_route_exp10 -> layer3_expert10
    layer3_expert10 -> layer3_agg_exp10
    layer3_agg_exp10 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp11
    layer3_route_exp11 -> layer3_expert11
    layer3_expert11 -> layer3_agg_exp11
    layer3_agg_exp11 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp12
    layer3_route_exp12 -> layer3_expert12
    layer3_expert12 -> layer3_agg_exp12
    layer3_agg_exp12 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp13
    layer3_route_exp13 -> layer3_expert13
    layer3_expert13 -> layer3_agg_exp13
    layer3_agg_exp13 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp14
    layer3_route_exp14 -> layer3_expert14
    layer3_expert14 -> layer3_agg_exp14
    layer3_agg_exp14 -> layer3_moe_agg
    layer3_gate -> layer3_route_exp15
    layer3_route_exp15 -> layer3_expert15
    layer3_expert15 -> layer3_agg_exp15
    layer3_agg_exp15 -> layer3_moe_agg
    layer3_moe_agg -> layer3_ln2
    layer3_ln2 -> layer3_moe_output
    layer3_add1 -> layer3_moe_output
    layer3_moe_output -> layer4_mha
    layer4_mha -> layer4_ln1
    layer4_ln1 -> layer4_add1
    layer3_moe_output -> layer4_add1
    layer4_add1 -> layer4_gate
    layer4_gate -> layer4_route_exp0
    layer4_route_exp0 -> layer4_expert0
    layer4_expert0 -> layer4_agg_exp0
    layer4_agg_exp0 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp1
    layer4_route_exp1 -> layer4_expert1
    layer4_expert1 -> layer4_agg_exp1
    layer4_agg_exp1 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp2
    layer4_route_exp2 -> layer4_expert2
    layer4_expert2 -> layer4_agg_exp2
    layer4_agg_exp2 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp3
    layer4_route_exp3 -> layer4_expert3
    layer4_expert3 -> layer4_agg_exp3
    layer4_agg_exp3 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp4
    layer4_route_exp4 -> layer4_expert4
    layer4_expert4 -> layer4_agg_exp4
    layer4_agg_exp4 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp5
    layer4_route_exp5 -> layer4_expert5
    layer4_expert5 -> layer4_agg_exp5
    layer4_agg_exp5 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp6
    layer4_route_exp6 -> layer4_expert6
    layer4_expert6 -> layer4_agg_exp6
    layer4_agg_exp6 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp7
    layer4_route_exp7 -> layer4_expert7
    layer4_expert7 -> layer4_agg_exp7
    layer4_agg_exp7 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp8
    layer4_route_exp8 -> layer4_expert8
    layer4_expert8 -> layer4_agg_exp8
    layer4_agg_exp8 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp9
    layer4_route_exp9 -> layer4_expert9
    layer4_expert9 -> layer4_agg_exp9
    layer4_agg_exp9 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp10
    layer4_route_exp10 -> layer4_expert10
    layer4_expert10 -> layer4_agg_exp10
    layer4_agg_exp10 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp11
    layer4_route_exp11 -> layer4_expert11
    layer4_expert11 -> layer4_agg_exp11
    layer4_agg_exp11 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp12
    layer4_route_exp12 -> layer4_expert12
    layer4_expert12 -> layer4_agg_exp12
    layer4_agg_exp12 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp13
    layer4_route_exp13 -> layer4_expert13
    layer4_expert13 -> layer4_agg_exp13
    layer4_agg_exp13 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp14
    layer4_route_exp14 -> layer4_expert14
    layer4_expert14 -> layer4_agg_exp14
    layer4_agg_exp14 -> layer4_moe_agg
    layer4_gate -> layer4_route_exp15
    layer4_route_exp15 -> layer4_expert15
    layer4_expert15 -> layer4_agg_exp15
    layer4_agg_exp15 -> layer4_moe_agg
    layer4_moe_agg -> layer4_ln2
    layer4_ln2 -> layer4_moe_output
    layer4_add1 -> layer4_moe_output
    layer4_moe_output -> layer5_mha
    layer5_mha -> layer5_ln1
    layer5_ln1 -> layer5_add1
    layer4_moe_output -> layer5_add1
    layer5_add1 -> layer5_gate
    layer5_gate -> layer5_route_exp0
    layer5_route_exp0 -> layer5_expert0
    layer5_expert0 -> layer5_agg_exp0
    layer5_agg_exp0 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp1
    layer5_route_exp1 -> layer5_expert1
    layer5_expert1 -> layer5_agg_exp1
    layer5_agg_exp1 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp2
    layer5_route_exp2 -> layer5_expert2
    layer5_expert2 -> layer5_agg_exp2
    layer5_agg_exp2 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp3
    layer5_route_exp3 -> layer5_expert3
    layer5_expert3 -> layer5_agg_exp3
    layer5_agg_exp3 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp4
    layer5_route_exp4 -> layer5_expert4
    layer5_expert4 -> layer5_agg_exp4
    layer5_agg_exp4 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp5
    layer5_route_exp5 -> layer5_expert5
    layer5_expert5 -> layer5_agg_exp5
    layer5_agg_exp5 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp6
    layer5_route_exp6 -> layer5_expert6
    layer5_expert6 -> layer5_agg_exp6
    layer5_agg_exp6 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp7
    layer5_route_exp7 -> layer5_expert7
    layer5_expert7 -> layer5_agg_exp7
    layer5_agg_exp7 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp8
    layer5_route_exp8 -> layer5_expert8
    layer5_expert8 -> layer5_agg_exp8
    layer5_agg_exp8 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp9
    layer5_route_exp9 -> layer5_expert9
    layer5_expert9 -> layer5_agg_exp9
    layer5_agg_exp9 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp10
    layer5_route_exp10 -> layer5_expert10
    layer5_expert10 -> layer5_agg_exp10
    layer5_agg_exp10 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp11
    layer5_route_exp11 -> layer5_expert11
    layer5_expert11 -> layer5_agg_exp11
    layer5_agg_exp11 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp12
    layer5_route_exp12 -> layer5_expert12
    layer5_expert12 -> layer5_agg_exp12
    layer5_agg_exp12 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp13
    layer5_route_exp13 -> layer5_expert13
    layer5_expert13 -> layer5_agg_exp13
    layer5_agg_exp13 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp14
    layer5_route_exp14 -> layer5_expert14
    layer5_expert14 -> layer5_agg_exp14
    layer5_agg_exp14 -> layer5_moe_agg
    layer5_gate -> layer5_route_exp15
    layer5_route_exp15 -> layer5_expert15
    layer5_expert15 -> layer5_agg_exp15
    layer5_agg_exp15 -> layer5_moe_agg
    layer5_moe_agg -> layer5_ln2
    layer5_ln2 -> layer5_moe_output
    layer5_add1 -> layer5_moe_output
    layer5_moe_output -> layer6_mha
    layer6_mha -> layer6_ln1
    layer6_ln1 -> layer6_add1
    layer5_moe_output -> layer6_add1
    layer6_add1 -> layer6_gate
    layer6_gate -> layer6_route_exp0
    layer6_route_exp0 -> layer6_expert0
    layer6_expert0 -> layer6_agg_exp0
    layer6_agg_exp0 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp1
    layer6_route_exp1 -> layer6_expert1
    layer6_expert1 -> layer6_agg_exp1
    layer6_agg_exp1 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp2
    layer6_route_exp2 -> layer6_expert2
    layer6_expert2 -> layer6_agg_exp2
    layer6_agg_exp2 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp3
    layer6_route_exp3 -> layer6_expert3
    layer6_expert3 -> layer6_agg_exp3
    layer6_agg_exp3 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp4
    layer6_route_exp4 -> layer6_expert4
    layer6_expert4 -> layer6_agg_exp4
    layer6_agg_exp4 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp5
    layer6_route_exp5 -> layer6_expert5
    layer6_expert5 -> layer6_agg_exp5
    layer6_agg_exp5 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp6
    layer6_route_exp6 -> layer6_expert6
    layer6_expert6 -> layer6_agg_exp6
    layer6_agg_exp6 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp7
    layer6_route_exp7 -> layer6_expert7
    layer6_expert7 -> layer6_agg_exp7
    layer6_agg_exp7 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp8
    layer6_route_exp8 -> layer6_expert8
    layer6_expert8 -> layer6_agg_exp8
    layer6_agg_exp8 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp9
    layer6_route_exp9 -> layer6_expert9
    layer6_expert9 -> layer6_agg_exp9
    layer6_agg_exp9 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp10
    layer6_route_exp10 -> layer6_expert10
    layer6_expert10 -> layer6_agg_exp10
    layer6_agg_exp10 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp11
    layer6_route_exp11 -> layer6_expert11
    layer6_expert11 -> layer6_agg_exp11
    layer6_agg_exp11 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp12
    layer6_route_exp12 -> layer6_expert12
    layer6_expert12 -> layer6_agg_exp12
    layer6_agg_exp12 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp13
    layer6_route_exp13 -> layer6_expert13
    layer6_expert13 -> layer6_agg_exp13
    layer6_agg_exp13 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp14
    layer6_route_exp14 -> layer6_expert14
    layer6_expert14 -> layer6_agg_exp14
    layer6_agg_exp14 -> layer6_moe_agg
    layer6_gate -> layer6_route_exp15
    layer6_route_exp15 -> layer6_expert15
    layer6_expert15 -> layer6_agg_exp15
    layer6_agg_exp15 -> layer6_moe_agg
    layer6_moe_agg -> layer6_ln2
    layer6_ln2 -> layer6_moe_output
    layer6_add1 -> layer6_moe_output
    layer6_moe_output -> layer7_mha
    layer7_mha -> layer7_ln1
    layer7_ln1 -> layer7_add1
    layer6_moe_output -> layer7_add1
    layer7_add1 -> layer7_gate
    layer7_gate -> layer7_route_exp0
    layer7_route_exp0 -> layer7_expert0
    layer7_expert0 -> layer7_agg_exp0
    layer7_agg_exp0 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp1
    layer7_route_exp1 -> layer7_expert1
    layer7_expert1 -> layer7_agg_exp1
    layer7_agg_exp1 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp2
    layer7_route_exp2 -> layer7_expert2
    layer7_expert2 -> layer7_agg_exp2
    layer7_agg_exp2 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp3
    layer7_route_exp3 -> layer7_expert3
    layer7_expert3 -> layer7_agg_exp3
    layer7_agg_exp3 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp4
    layer7_route_exp4 -> layer7_expert4
    layer7_expert4 -> layer7_agg_exp4
    layer7_agg_exp4 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp5
    layer7_route_exp5 -> layer7_expert5
    layer7_expert5 -> layer7_agg_exp5
    layer7_agg_exp5 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp6
    layer7_route_exp6 -> layer7_expert6
    layer7_expert6 -> layer7_agg_exp6
    layer7_agg_exp6 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp7
    layer7_route_exp7 -> layer7_expert7
    layer7_expert7 -> layer7_agg_exp7
    layer7_agg_exp7 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp8
    layer7_route_exp8 -> layer7_expert8
    layer7_expert8 -> layer7_agg_exp8
    layer7_agg_exp8 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp9
    layer7_route_exp9 -> layer7_expert9
    layer7_expert9 -> layer7_agg_exp9
    layer7_agg_exp9 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp10
    layer7_route_exp10 -> layer7_expert10
    layer7_expert10 -> layer7_agg_exp10
    layer7_agg_exp10 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp11
    layer7_route_exp11 -> layer7_expert11
    layer7_expert11 -> layer7_agg_exp11
    layer7_agg_exp11 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp12
    layer7_route_exp12 -> layer7_expert12
    layer7_expert12 -> layer7_agg_exp12
    layer7_agg_exp12 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp13
    layer7_route_exp13 -> layer7_expert13
    layer7_expert13 -> layer7_agg_exp13
    layer7_agg_exp13 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp14
    layer7_route_exp14 -> layer7_expert14
    layer7_expert14 -> layer7_agg_exp14
    layer7_agg_exp14 -> layer7_moe_agg
    layer7_gate -> layer7_route_exp15
    layer7_route_exp15 -> layer7_expert15
    layer7_expert15 -> layer7_agg_exp15
    layer7_agg_exp15 -> layer7_moe_agg
    layer7_moe_agg -> layer7_ln2
    layer7_ln2 -> layer7_moe_output
    layer7_add1 -> layer7_moe_output
    layer7_moe_output -> layer8_mha
    layer8_mha -> layer8_ln1
    layer8_ln1 -> layer8_add1
    layer7_moe_output -> layer8_add1
    layer8_add1 -> layer8_gate
    layer8_gate -> layer8_route_exp0
    layer8_route_exp0 -> layer8_expert0
    layer8_expert0 -> layer8_agg_exp0
    layer8_agg_exp0 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp1
    layer8_route_exp1 -> layer8_expert1
    layer8_expert1 -> layer8_agg_exp1
    layer8_agg_exp1 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp2
    layer8_route_exp2 -> layer8_expert2
    layer8_expert2 -> layer8_agg_exp2
    layer8_agg_exp2 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp3
    layer8_route_exp3 -> layer8_expert3
    layer8_expert3 -> layer8_agg_exp3
    layer8_agg_exp3 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp4
    layer8_route_exp4 -> layer8_expert4
    layer8_expert4 -> layer8_agg_exp4
    layer8_agg_exp4 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp5
    layer8_route_exp5 -> layer8_expert5
    layer8_expert5 -> layer8_agg_exp5
    layer8_agg_exp5 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp6
    layer8_route_exp6 -> layer8_expert6
    layer8_expert6 -> layer8_agg_exp6
    layer8_agg_exp6 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp7
    layer8_route_exp7 -> layer8_expert7
    layer8_expert7 -> layer8_agg_exp7
    layer8_agg_exp7 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp8
    layer8_route_exp8 -> layer8_expert8
    layer8_expert8 -> layer8_agg_exp8
    layer8_agg_exp8 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp9
    layer8_route_exp9 -> layer8_expert9
    layer8_expert9 -> layer8_agg_exp9
    layer8_agg_exp9 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp10
    layer8_route_exp10 -> layer8_expert10
    layer8_expert10 -> layer8_agg_exp10
    layer8_agg_exp10 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp11
    layer8_route_exp11 -> layer8_expert11
    layer8_expert11 -> layer8_agg_exp11
    layer8_agg_exp11 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp12
    layer8_route_exp12 -> layer8_expert12
    layer8_expert12 -> layer8_agg_exp12
    layer8_agg_exp12 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp13
    layer8_route_exp13 -> layer8_expert13
    layer8_expert13 -> layer8_agg_exp13
    layer8_agg_exp13 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp14
    layer8_route_exp14 -> layer8_expert14
    layer8_expert14 -> layer8_agg_exp14
    layer8_agg_exp14 -> layer8_moe_agg
    layer8_gate -> layer8_route_exp15
    layer8_route_exp15 -> layer8_expert15
    layer8_expert15 -> layer8_agg_exp15
    layer8_agg_exp15 -> layer8_moe_agg
    layer8_moe_agg -> layer8_ln2
    layer8_ln2 -> layer8_moe_output
    layer8_add1 -> layer8_moe_output
    layer8_moe_output -> layer9_mha
    layer9_mha -> layer9_ln1
    layer9_ln1 -> layer9_add1
    layer8_moe_output -> layer9_add1
    layer9_add1 -> layer9_gate
    layer9_gate -> layer9_route_exp0
    layer9_route_exp0 -> layer9_expert0
    layer9_expert0 -> layer9_agg_exp0
    layer9_agg_exp0 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp1
    layer9_route_exp1 -> layer9_expert1
    layer9_expert1 -> layer9_agg_exp1
    layer9_agg_exp1 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp2
    layer9_route_exp2 -> layer9_expert2
    layer9_expert2 -> layer9_agg_exp2
    layer9_agg_exp2 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp3
    layer9_route_exp3 -> layer9_expert3
    layer9_expert3 -> layer9_agg_exp3
    layer9_agg_exp3 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp4
    layer9_route_exp4 -> layer9_expert4
    layer9_expert4 -> layer9_agg_exp4
    layer9_agg_exp4 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp5
    layer9_route_exp5 -> layer9_expert5
    layer9_expert5 -> layer9_agg_exp5
    layer9_agg_exp5 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp6
    layer9_route_exp6 -> layer9_expert6
    layer9_expert6 -> layer9_agg_exp6
    layer9_agg_exp6 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp7
    layer9_route_exp7 -> layer9_expert7
    layer9_expert7 -> layer9_agg_exp7
    layer9_agg_exp7 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp8
    layer9_route_exp8 -> layer9_expert8
    layer9_expert8 -> layer9_agg_exp8
    layer9_agg_exp8 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp9
    layer9_route_exp9 -> layer9_expert9
    layer9_expert9 -> layer9_agg_exp9
    layer9_agg_exp9 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp10
    layer9_route_exp10 -> layer9_expert10
    layer9_expert10 -> layer9_agg_exp10
    layer9_agg_exp10 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp11
    layer9_route_exp11 -> layer9_expert11
    layer9_expert11 -> layer9_agg_exp11
    layer9_agg_exp11 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp12
    layer9_route_exp12 -> layer9_expert12
    layer9_expert12 -> layer9_agg_exp12
    layer9_agg_exp12 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp13
    layer9_route_exp13 -> layer9_expert13
    layer9_expert13 -> layer9_agg_exp13
    layer9_agg_exp13 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp14
    layer9_route_exp14 -> layer9_expert14
    layer9_expert14 -> layer9_agg_exp14
    layer9_agg_exp14 -> layer9_moe_agg
    layer9_gate -> layer9_route_exp15
    layer9_route_exp15 -> layer9_expert15
    layer9_expert15 -> layer9_agg_exp15
    layer9_agg_exp15 -> layer9_moe_agg
    layer9_moe_agg -> layer9_ln2
    layer9_ln2 -> layer9_moe_output
    layer9_add1 -> layer9_moe_output
    layer9_moe_output -> layer10_mha
    layer10_mha -> layer10_ln1
    layer10_ln1 -> layer10_add1
    layer9_moe_output -> layer10_add1
    layer10_add1 -> layer10_gate
    layer10_gate -> layer10_route_exp0
    layer10_route_exp0 -> layer10_expert0
    layer10_expert0 -> layer10_agg_exp0
    layer10_agg_exp0 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp1
    layer10_route_exp1 -> layer10_expert1
    layer10_expert1 -> layer10_agg_exp1
    layer10_agg_exp1 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp2
    layer10_route_exp2 -> layer10_expert2
    layer10_expert2 -> layer10_agg_exp2
    layer10_agg_exp2 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp3
    layer10_route_exp3 -> layer10_expert3
    layer10_expert3 -> layer10_agg_exp3
    layer10_agg_exp3 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp4
    layer10_route_exp4 -> layer10_expert4
    layer10_expert4 -> layer10_agg_exp4
    layer10_agg_exp4 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp5
    layer10_route_exp5 -> layer10_expert5
    layer10_expert5 -> layer10_agg_exp5
    layer10_agg_exp5 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp6
    layer10_route_exp6 -> layer10_expert6
    layer10_expert6 -> layer10_agg_exp6
    layer10_agg_exp6 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp7
    layer10_route_exp7 -> layer10_expert7
    layer10_expert7 -> layer10_agg_exp7
    layer10_agg_exp7 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp8
    layer10_route_exp8 -> layer10_expert8
    layer10_expert8 -> layer10_agg_exp8
    layer10_agg_exp8 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp9
    layer10_route_exp9 -> layer10_expert9
    layer10_expert9 -> layer10_agg_exp9
    layer10_agg_exp9 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp10
    layer10_route_exp10 -> layer10_expert10
    layer10_expert10 -> layer10_agg_exp10
    layer10_agg_exp10 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp11
    layer10_route_exp11 -> layer10_expert11
    layer10_expert11 -> layer10_agg_exp11
    layer10_agg_exp11 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp12
    layer10_route_exp12 -> layer10_expert12
    layer10_expert12 -> layer10_agg_exp12
    layer10_agg_exp12 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp13
    layer10_route_exp13 -> layer10_expert13
    layer10_expert13 -> layer10_agg_exp13
    layer10_agg_exp13 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp14
    layer10_route_exp14 -> layer10_expert14
    layer10_expert14 -> layer10_agg_exp14
    layer10_agg_exp14 -> layer10_moe_agg
    layer10_gate -> layer10_route_exp15
    layer10_route_exp15 -> layer10_expert15
    layer10_expert15 -> layer10_agg_exp15
    layer10_agg_exp15 -> layer10_moe_agg
    layer10_moe_agg -> layer10_ln2
    layer10_ln2 -> layer10_moe_output
    layer10_add1 -> layer10_moe_output
    layer10_moe_output -> layer11_mha
    layer11_mha -> layer11_ln1
    layer11_ln1 -> layer11_add1
    layer10_moe_output -> layer11_add1
    layer11_add1 -> layer11_gate
    layer11_gate -> layer11_route_exp0
    layer11_route_exp0 -> layer11_expert0
    layer11_expert0 -> layer11_agg_exp0
    layer11_agg_exp0 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp1
    layer11_route_exp1 -> layer11_expert1
    layer11_expert1 -> layer11_agg_exp1
    layer11_agg_exp1 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp2
    layer11_route_exp2 -> layer11_expert2
    layer11_expert2 -> layer11_agg_exp2
    layer11_agg_exp2 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp3
    layer11_route_exp3 -> layer11_expert3
    layer11_expert3 -> layer11_agg_exp3
    layer11_agg_exp3 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp4
    layer11_route_exp4 -> layer11_expert4
    layer11_expert4 -> layer11_agg_exp4
    layer11_agg_exp4 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp5
    layer11_route_exp5 -> layer11_expert5
    layer11_expert5 -> layer11_agg_exp5
    layer11_agg_exp5 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp6
    layer11_route_exp6 -> layer11_expert6
    layer11_expert6 -> layer11_agg_exp6
    layer11_agg_exp6 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp7
    layer11_route_exp7 -> layer11_expert7
    layer11_expert7 -> layer11_agg_exp7
    layer11_agg_exp7 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp8
    layer11_route_exp8 -> layer11_expert8
    layer11_expert8 -> layer11_agg_exp8
    layer11_agg_exp8 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp9
    layer11_route_exp9 -> layer11_expert9
    layer11_expert9 -> layer11_agg_exp9
    layer11_agg_exp9 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp10
    layer11_route_exp10 -> layer11_expert10
    layer11_expert10 -> layer11_agg_exp10
    layer11_agg_exp10 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp11
    layer11_route_exp11 -> layer11_expert11
    layer11_expert11 -> layer11_agg_exp11
    layer11_agg_exp11 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp12
    layer11_route_exp12 -> layer11_expert12
    layer11_expert12 -> layer11_agg_exp12
    layer11_agg_exp12 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp13
    layer11_route_exp13 -> layer11_expert13
    layer11_expert13 -> layer11_agg_exp13
    layer11_agg_exp13 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp14
    layer11_route_exp14 -> layer11_expert14
    layer11_expert14 -> layer11_agg_exp14
    layer11_agg_exp14 -> layer11_moe_agg
    layer11_gate -> layer11_route_exp15
    layer11_route_exp15 -> layer11_expert15
    layer11_expert15 -> layer11_agg_exp15
    layer11_agg_exp15 -> layer11_moe_agg
    layer11_moe_agg -> layer11_ln2
    layer11_ln2 -> layer11_moe_output
    layer11_add1 -> layer11_moe_output
    layer11_moe_output -> layer12_mha
    layer12_mha -> layer12_ln1
    layer12_ln1 -> layer12_add1
    layer11_moe_output -> layer12_add1
    layer12_add1 -> layer12_gate
    layer12_gate -> layer12_route_exp0
    layer12_route_exp0 -> layer12_expert0
    layer12_expert0 -> layer12_agg_exp0
    layer12_agg_exp0 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp1
    layer12_route_exp1 -> layer12_expert1
    layer12_expert1 -> layer12_agg_exp1
    layer12_agg_exp1 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp2
    layer12_route_exp2 -> layer12_expert2
    layer12_expert2 -> layer12_agg_exp2
    layer12_agg_exp2 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp3
    layer12_route_exp3 -> layer12_expert3
    layer12_expert3 -> layer12_agg_exp3
    layer12_agg_exp3 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp4
    layer12_route_exp4 -> layer12_expert4
    layer12_expert4 -> layer12_agg_exp4
    layer12_agg_exp4 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp5
    layer12_route_exp5 -> layer12_expert5
    layer12_expert5 -> layer12_agg_exp5
    layer12_agg_exp5 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp6
    layer12_route_exp6 -> layer12_expert6
    layer12_expert6 -> layer12_agg_exp6
    layer12_agg_exp6 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp7
    layer12_route_exp7 -> layer12_expert7
    layer12_expert7 -> layer12_agg_exp7
    layer12_agg_exp7 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp8
    layer12_route_exp8 -> layer12_expert8
    layer12_expert8 -> layer12_agg_exp8
    layer12_agg_exp8 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp9
    layer12_route_exp9 -> layer12_expert9
    layer12_expert9 -> layer12_agg_exp9
    layer12_agg_exp9 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp10
    layer12_route_exp10 -> layer12_expert10
    layer12_expert10 -> layer12_agg_exp10
    layer12_agg_exp10 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp11
    layer12_route_exp11 -> layer12_expert11
    layer12_expert11 -> layer12_agg_exp11
    layer12_agg_exp11 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp12
    layer12_route_exp12 -> layer12_expert12
    layer12_expert12 -> layer12_agg_exp12
    layer12_agg_exp12 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp13
    layer12_route_exp13 -> layer12_expert13
    layer12_expert13 -> layer12_agg_exp13
    layer12_agg_exp13 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp14
    layer12_route_exp14 -> layer12_expert14
    layer12_expert14 -> layer12_agg_exp14
    layer12_agg_exp14 -> layer12_moe_agg
    layer12_gate -> layer12_route_exp15
    layer12_route_exp15 -> layer12_expert15
    layer12_expert15 -> layer12_agg_exp15
    layer12_agg_exp15 -> layer12_moe_agg
    layer12_moe_agg -> layer12_ln2
    layer12_ln2 -> layer12_moe_output
    layer12_add1 -> layer12_moe_output
    layer12_moe_output -> layer13_mha
    layer13_mha -> layer13_ln1
    layer13_ln1 -> layer13_add1
    layer12_moe_output -> layer13_add1
    layer13_add1 -> layer13_gate
    layer13_gate -> layer13_route_exp0
    layer13_route_exp0 -> layer13_expert0
    layer13_expert0 -> layer13_agg_exp0
    layer13_agg_exp0 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp1
    layer13_route_exp1 -> layer13_expert1
    layer13_expert1 -> layer13_agg_exp1
    layer13_agg_exp1 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp2
    layer13_route_exp2 -> layer13_expert2
    layer13_expert2 -> layer13_agg_exp2
    layer13_agg_exp2 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp3
    layer13_route_exp3 -> layer13_expert3
    layer13_expert3 -> layer13_agg_exp3
    layer13_agg_exp3 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp4
    layer13_route_exp4 -> layer13_expert4
    layer13_expert4 -> layer13_agg_exp4
    layer13_agg_exp4 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp5
    layer13_route_exp5 -> layer13_expert5
    layer13_expert5 -> layer13_agg_exp5
    layer13_agg_exp5 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp6
    layer13_route_exp6 -> layer13_expert6
    layer13_expert6 -> layer13_agg_exp6
    layer13_agg_exp6 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp7
    layer13_route_exp7 -> layer13_expert7
    layer13_expert7 -> layer13_agg_exp7
    layer13_agg_exp7 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp8
    layer13_route_exp8 -> layer13_expert8
    layer13_expert8 -> layer13_agg_exp8
    layer13_agg_exp8 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp9
    layer13_route_exp9 -> layer13_expert9
    layer13_expert9 -> layer13_agg_exp9
    layer13_agg_exp9 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp10
    layer13_route_exp10 -> layer13_expert10
    layer13_expert10 -> layer13_agg_exp10
    layer13_agg_exp10 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp11
    layer13_route_exp11 -> layer13_expert11
    layer13_expert11 -> layer13_agg_exp11
    layer13_agg_exp11 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp12
    layer13_route_exp12 -> layer13_expert12
    layer13_expert12 -> layer13_agg_exp12
    layer13_agg_exp12 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp13
    layer13_route_exp13 -> layer13_expert13
    layer13_expert13 -> layer13_agg_exp13
    layer13_agg_exp13 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp14
    layer13_route_exp14 -> layer13_expert14
    layer13_expert14 -> layer13_agg_exp14
    layer13_agg_exp14 -> layer13_moe_agg
    layer13_gate -> layer13_route_exp15
    layer13_route_exp15 -> layer13_expert15
    layer13_expert15 -> layer13_agg_exp15
    layer13_agg_exp15 -> layer13_moe_agg
    layer13_moe_agg -> layer13_ln2
    layer13_ln2 -> layer13_moe_output
    layer13_add1 -> layer13_moe_output
    layer13_moe_output -> layer14_mha
    layer14_mha -> layer14_ln1
    layer14_ln1 -> layer14_add1
    layer13_moe_output -> layer14_add1
    layer14_add1 -> layer14_gate
    layer14_gate -> layer14_route_exp0
    layer14_route_exp0 -> layer14_expert0
    layer14_expert0 -> layer14_agg_exp0
    layer14_agg_exp0 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp1
    layer14_route_exp1 -> layer14_expert1
    layer14_expert1 -> layer14_agg_exp1
    layer14_agg_exp1 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp2
    layer14_route_exp2 -> layer14_expert2
    layer14_expert2 -> layer14_agg_exp2
    layer14_agg_exp2 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp3
    layer14_route_exp3 -> layer14_expert3
    layer14_expert3 -> layer14_agg_exp3
    layer14_agg_exp3 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp4
    layer14_route_exp4 -> layer14_expert4
    layer14_expert4 -> layer14_agg_exp4
    layer14_agg_exp4 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp5
    layer14_route_exp5 -> layer14_expert5
    layer14_expert5 -> layer14_agg_exp5
    layer14_agg_exp5 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp6
    layer14_route_exp6 -> layer14_expert6
    layer14_expert6 -> layer14_agg_exp6
    layer14_agg_exp6 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp7
    layer14_route_exp7 -> layer14_expert7
    layer14_expert7 -> layer14_agg_exp7
    layer14_agg_exp7 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp8
    layer14_route_exp8 -> layer14_expert8
    layer14_expert8 -> layer14_agg_exp8
    layer14_agg_exp8 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp9
    layer14_route_exp9 -> layer14_expert9
    layer14_expert9 -> layer14_agg_exp9
    layer14_agg_exp9 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp10
    layer14_route_exp10 -> layer14_expert10
    layer14_expert10 -> layer14_agg_exp10
    layer14_agg_exp10 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp11
    layer14_route_exp11 -> layer14_expert11
    layer14_expert11 -> layer14_agg_exp11
    layer14_agg_exp11 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp12
    layer14_route_exp12 -> layer14_expert12
    layer14_expert12 -> layer14_agg_exp12
    layer14_agg_exp12 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp13
    layer14_route_exp13 -> layer14_expert13
    layer14_expert13 -> layer14_agg_exp13
    layer14_agg_exp13 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp14
    layer14_route_exp14 -> layer14_expert14
    layer14_expert14 -> layer14_agg_exp14
    layer14_agg_exp14 -> layer14_moe_agg
    layer14_gate -> layer14_route_exp15
    layer14_route_exp15 -> layer14_expert15
    layer14_expert15 -> layer14_agg_exp15
    layer14_agg_exp15 -> layer14_moe_agg
    layer14_moe_agg -> layer14_ln2
    layer14_ln2 -> layer14_moe_output
    layer14_add1 -> layer14_moe_output
    layer14_moe_output -> layer15_mha
    layer15_mha -> layer15_ln1
    layer15_ln1 -> layer15_add1
    layer14_moe_output -> layer15_add1
    layer15_add1 -> layer15_gate
    layer15_gate -> layer15_route_exp0
    layer15_route_exp0 -> layer15_expert0
    layer15_expert0 -> layer15_agg_exp0
    layer15_agg_exp0 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp1
    layer15_route_exp1 -> layer15_expert1
    layer15_expert1 -> layer15_agg_exp1
    layer15_agg_exp1 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp2
    layer15_route_exp2 -> layer15_expert2
    layer15_expert2 -> layer15_agg_exp2
    layer15_agg_exp2 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp3
    layer15_route_exp3 -> layer15_expert3
    layer15_expert3 -> layer15_agg_exp3
    layer15_agg_exp3 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp4
    layer15_route_exp4 -> layer15_expert4
    layer15_expert4 -> layer15_agg_exp4
    layer15_agg_exp4 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp5
    layer15_route_exp5 -> layer15_expert5
    layer15_expert5 -> layer15_agg_exp5
    layer15_agg_exp5 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp6
    layer15_route_exp6 -> layer15_expert6
    layer15_expert6 -> layer15_agg_exp6
    layer15_agg_exp6 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp7
    layer15_route_exp7 -> layer15_expert7
    layer15_expert7 -> layer15_agg_exp7
    layer15_agg_exp7 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp8
    layer15_route_exp8 -> layer15_expert8
    layer15_expert8 -> layer15_agg_exp8
    layer15_agg_exp8 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp9
    layer15_route_exp9 -> layer15_expert9
    layer15_expert9 -> layer15_agg_exp9
    layer15_agg_exp9 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp10
    layer15_route_exp10 -> layer15_expert10
    layer15_expert10 -> layer15_agg_exp10
    layer15_agg_exp10 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp11
    layer15_route_exp11 -> layer15_expert11
    layer15_expert11 -> layer15_agg_exp11
    layer15_agg_exp11 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp12
    layer15_route_exp12 -> layer15_expert12
    layer15_expert12 -> layer15_agg_exp12
    layer15_agg_exp12 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp13
    layer15_route_exp13 -> layer15_expert13
    layer15_expert13 -> layer15_agg_exp13
    layer15_agg_exp13 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp14
    layer15_route_exp14 -> layer15_expert14
    layer15_expert14 -> layer15_agg_exp14
    layer15_agg_exp14 -> layer15_moe_agg
    layer15_gate -> layer15_route_exp15
    layer15_route_exp15 -> layer15_expert15
    layer15_expert15 -> layer15_agg_exp15
    layer15_agg_exp15 -> layer15_moe_agg
    layer15_moe_agg -> layer15_ln2
    layer15_ln2 -> layer15_moe_output
    layer15_add1 -> layer15_moe_output
    layer15_moe_output -> output
}