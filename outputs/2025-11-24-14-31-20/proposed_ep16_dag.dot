// Large EP Proposed Method - EP=16, 1 Expert/GPU
digraph {
	rankdir=TB splines=ortho
	input [label="Model Input\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightblue shape=box style=filled]
	mha0 [label="MHA Layer 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightcyan shape=rectangle style=filled]
	input -> mha0
	ln1_0 [label="LayerNorm 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" shape=rectangle]
	add1_0 [label="MHA Residual Add 0\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=yellow shape=rectangle style=filled]
	mha0 -> add1_0
	gate0 [label="Gate Layer 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]" fillcolor=lightgreen shape=parallelogram style=filled]
	add1_0 -> gate0
	route0_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp0
	expert0_exp0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp0 -> expert0_exp0
	agg0_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp0 -> agg0_exp0
	route0_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp1
	expert0_exp1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp1 -> expert0_exp1
	agg0_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp1 -> agg0_exp1
	route0_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp2
	expert0_exp2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp2 -> expert0_exp2
	agg0_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp2 -> agg0_exp2
	route0_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp3
	expert0_exp3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp3 -> expert0_exp3
	agg0_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp3 -> agg0_exp3
	route0_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp4
	expert0_exp4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp4 -> expert0_exp4
	agg0_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp4 -> agg0_exp4
	route0_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp5
	expert0_exp5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp5 -> expert0_exp5
	agg0_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp5 -> agg0_exp5
	route0_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp6
	expert0_exp6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp6 -> expert0_exp6
	agg0_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp6 -> agg0_exp6
	route0_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp7
	expert0_exp7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp7 -> expert0_exp7
	agg0_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp7 -> agg0_exp7
	route0_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp8
	expert0_exp8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp8 -> expert0_exp8
	agg0_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp8 -> agg0_exp8
	route0_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp9
	expert0_exp9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp9 -> expert0_exp9
	agg0_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp9 -> agg0_exp9
	route0_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp10
	expert0_exp10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp10 -> expert0_exp10
	agg0_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp10 -> agg0_exp10
	route0_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp11
	expert0_exp11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp11 -> expert0_exp11
	agg0_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp11 -> agg0_exp11
	route0_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp12
	expert0_exp12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp12 -> expert0_exp12
	agg0_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp12 -> agg0_exp12
	route0_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp13
	expert0_exp13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp13 -> expert0_exp13
	agg0_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp13 -> agg0_exp13
	route0_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp14
	expert0_exp14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp14 -> expert0_exp14
	agg0_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp14 -> agg0_exp14
	route0_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate0 -> route0_exp15
	expert0_exp15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route0_exp15 -> expert0_exp15
	agg0_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert0_exp15 -> agg0_exp15
	moe_agg0 [label="MoE Final Aggregation 0\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=gold shape=ellipse style=filled]
	agg0_exp0 -> moe_agg0
	agg0_exp1 -> moe_agg0
	agg0_exp2 -> moe_agg0
	agg0_exp3 -> moe_agg0
	agg0_exp4 -> moe_agg0
	agg0_exp5 -> moe_agg0
	agg0_exp6 -> moe_agg0
	agg0_exp7 -> moe_agg0
	agg0_exp8 -> moe_agg0
	agg0_exp9 -> moe_agg0
	agg0_exp10 -> moe_agg0
	agg0_exp11 -> moe_agg0
	agg0_exp12 -> moe_agg0
	agg0_exp13 -> moe_agg0
	agg0_exp14 -> moe_agg0
	agg0_exp15 -> moe_agg0
	moe0 [label="MoE Residual Add 0\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=yellow shape=rectangle style=filled]
	moe_agg0 -> moe0
	add1_0 -> moe0
	mha7 [label="MHA Layer 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightcyan shape=rectangle style=filled]
	moe6 -> mha7
	ln1_7 [label="LayerNorm 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" shape=rectangle]
	add1_7 [label="MHA Residual Add 7\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=yellow shape=rectangle style=filled]
	mha7 -> add1_7
	gate7 [label="Gate Layer 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]" fillcolor=lightgreen shape=parallelogram style=filled]
	add1_7 -> gate7
	route7_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp0
	expert7_exp0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp0 -> expert7_exp0
	agg7_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp0 -> agg7_exp0
	route7_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp1
	expert7_exp1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp1 -> expert7_exp1
	agg7_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp1 -> agg7_exp1
	route7_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp2
	expert7_exp2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp2 -> expert7_exp2
	agg7_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp2 -> agg7_exp2
	route7_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp3
	expert7_exp3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp3 -> expert7_exp3
	agg7_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp3 -> agg7_exp3
	route7_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp4
	expert7_exp4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp4 -> expert7_exp4
	agg7_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp4 -> agg7_exp4
	route7_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp5
	expert7_exp5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp5 -> expert7_exp5
	agg7_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp5 -> agg7_exp5
	route7_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp6
	expert7_exp6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp6 -> expert7_exp6
	agg7_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp6 -> agg7_exp6
	route7_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp7
	expert7_exp7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp7 -> expert7_exp7
	agg7_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp7 -> agg7_exp7
	route7_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp8
	expert7_exp8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp8 -> expert7_exp8
	agg7_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp8 -> agg7_exp8
	route7_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp9
	expert7_exp9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp9 -> expert7_exp9
	agg7_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp9 -> agg7_exp9
	route7_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp10
	expert7_exp10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp10 -> expert7_exp10
	agg7_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp10 -> agg7_exp10
	route7_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp11
	expert7_exp11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp11 -> expert7_exp11
	agg7_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp11 -> agg7_exp11
	route7_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp12
	expert7_exp12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp12 -> expert7_exp12
	agg7_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp12 -> agg7_exp12
	route7_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp13
	expert7_exp13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp13 -> expert7_exp13
	agg7_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp13 -> agg7_exp13
	route7_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp14
	expert7_exp14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp14 -> expert7_exp14
	agg7_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp14 -> agg7_exp14
	route7_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate7 -> route7_exp15
	expert7_exp15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route7_exp15 -> expert7_exp15
	agg7_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert7_exp15 -> agg7_exp15
	moe_agg7 [label="MoE Final Aggregation 7\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=gold shape=ellipse style=filled]
	agg7_exp0 -> moe_agg7
	agg7_exp1 -> moe_agg7
	agg7_exp2 -> moe_agg7
	agg7_exp3 -> moe_agg7
	agg7_exp4 -> moe_agg7
	agg7_exp5 -> moe_agg7
	agg7_exp6 -> moe_agg7
	agg7_exp7 -> moe_agg7
	agg7_exp8 -> moe_agg7
	agg7_exp9 -> moe_agg7
	agg7_exp10 -> moe_agg7
	agg7_exp11 -> moe_agg7
	agg7_exp12 -> moe_agg7
	agg7_exp13 -> moe_agg7
	agg7_exp14 -> moe_agg7
	agg7_exp15 -> moe_agg7
	moe7 [label="MoE Residual Add 7\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=yellow shape=rectangle style=filled]
	moe_agg7 -> moe7
	add1_7 -> moe7
	mha15 [label="MHA Layer 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightcyan shape=rectangle style=filled]
	moe14 -> mha15
	ln1_15 [label="LayerNorm 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" shape=rectangle]
	add1_15 [label="MHA Residual Add 15\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=yellow shape=rectangle style=filled]
	mha15 -> add1_15
	gate15 [label="Gate Layer 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]" fillcolor=lightgreen shape=parallelogram style=filled]
	add1_15 -> gate15
	route15_exp0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp0
	expert15_exp0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp0 -> expert15_exp0
	agg15_exp0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp0 -> agg15_exp0
	route15_exp1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp1
	expert15_exp1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp1 -> expert15_exp1
	agg15_exp1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp1 -> agg15_exp1
	route15_exp2 [label="Route to Expert 2\nGPU 2\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp2
	expert15_exp2 [label="MLP Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp2 -> expert15_exp2
	agg15_exp2 [label="Aggregate from Expert 2\nGPU 2\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp2 -> agg15_exp2
	route15_exp3 [label="Route to Expert 3\nGPU 3\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp3
	expert15_exp3 [label="MLP Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp3 -> expert15_exp3
	agg15_exp3 [label="Aggregate from Expert 3\nGPU 3\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp3 -> agg15_exp3
	route15_exp4 [label="Route to Expert 4\nGPU 4\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp4
	expert15_exp4 [label="MLP Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp4 -> expert15_exp4
	agg15_exp4 [label="Aggregate from Expert 4\nGPU 4\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp4 -> agg15_exp4
	route15_exp5 [label="Route to Expert 5\nGPU 5\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp5
	expert15_exp5 [label="MLP Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp5 -> expert15_exp5
	agg15_exp5 [label="Aggregate from Expert 5\nGPU 5\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp5 -> agg15_exp5
	route15_exp6 [label="Route to Expert 6\nGPU 6\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp6
	expert15_exp6 [label="MLP Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp6 -> expert15_exp6
	agg15_exp6 [label="Aggregate from Expert 6\nGPU 6\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp6 -> agg15_exp6
	route15_exp7 [label="Route to Expert 7\nGPU 7\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp7
	expert15_exp7 [label="MLP Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp7 -> expert15_exp7
	agg15_exp7 [label="Aggregate from Expert 7\nGPU 7\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp7 -> agg15_exp7
	route15_exp8 [label="Route to Expert 8\nGPU 8\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp8
	expert15_exp8 [label="MLP Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp8 -> expert15_exp8
	agg15_exp8 [label="Aggregate from Expert 8\nGPU 8\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp8 -> agg15_exp8
	route15_exp9 [label="Route to Expert 9\nGPU 9\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp9
	expert15_exp9 [label="MLP Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp9 -> expert15_exp9
	agg15_exp9 [label="Aggregate from Expert 9\nGPU 9\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp9 -> agg15_exp9
	route15_exp10 [label="Route to Expert 10\nGPU 10\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp10
	expert15_exp10 [label="MLP Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp10 -> expert15_exp10
	agg15_exp10 [label="Aggregate from Expert 10\nGPU 10\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp10 -> agg15_exp10
	route15_exp11 [label="Route to Expert 11\nGPU 11\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp11
	expert15_exp11 [label="MLP Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp11 -> expert15_exp11
	agg15_exp11 [label="Aggregate from Expert 11\nGPU 11\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp11 -> agg15_exp11
	route15_exp12 [label="Route to Expert 12\nGPU 12\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp12
	expert15_exp12 [label="MLP Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp12 -> expert15_exp12
	agg15_exp12 [label="Aggregate from Expert 12\nGPU 12\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp12 -> agg15_exp12
	route15_exp13 [label="Route to Expert 13\nGPU 13\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp13
	expert15_exp13 [label="MLP Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp13 -> expert15_exp13
	agg15_exp13 [label="Aggregate from Expert 13\nGPU 13\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp13 -> agg15_exp13
	route15_exp14 [label="Route to Expert 14\nGPU 14\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp14
	expert15_exp14 [label="MLP Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp14 -> expert15_exp14
	agg15_exp14 [label="Aggregate from Expert 14\nGPU 14\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp14 -> agg15_exp14
	route15_exp15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=orange shape=ellipse style=dashed]
	gate15 -> route15_exp15
	expert15_exp15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]" fillcolor=lightcoral shape=rectangle style=filled]
	route15_exp15 -> expert15_exp15
	agg15_exp15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightpink shape=ellipse style=filled]
	expert15_exp15 -> agg15_exp15
	moe_agg15 [label="MoE Final Aggregation 15\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=gold shape=ellipse style=filled]
	agg15_exp0 -> moe_agg15
	agg15_exp1 -> moe_agg15
	agg15_exp2 -> moe_agg15
	agg15_exp3 -> moe_agg15
	agg15_exp4 -> moe_agg15
	agg15_exp5 -> moe_agg15
	agg15_exp6 -> moe_agg15
	agg15_exp7 -> moe_agg15
	agg15_exp8 -> moe_agg15
	agg15_exp9 -> moe_agg15
	agg15_exp10 -> moe_agg15
	agg15_exp11 -> moe_agg15
	agg15_exp12 -> moe_agg15
	agg15_exp13 -> moe_agg15
	agg15_exp14 -> moe_agg15
	agg15_exp15 -> moe_agg15
	moe15 [label="MoE Residual Add 15\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=yellow shape=rectangle style=filled]
	moe_agg15 -> moe15
	add1_15 -> moe15
	output [label="Model Output\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]" fillcolor=lightblue shape=box style=filled]
	moe15 -> output
}
