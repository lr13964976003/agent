{
  "deployment_dags": [
    {
      "method": "proposed_large_ep",
      "configuration": "EP=16, 1 expert per GPU",
      "description": "Large-scale cross-node expert parallelism with maximum expert distribution",
      "files": {
        "dot_file": "../outputs/2025-11-24-14-31-20/proposed_ep16_dag.dot",
        "svg_image": "../outputs/2025-11-24-14-31-20/proposed_ep16_dag.svg"
      },
      "deployment_details": {
        "total_gpus": 16,
        "experts_per_layer": 16,
        "experts_per_gpu": 1,
        "parallelism_strategy": "Expert Parallelism (EP)",
        "expected_throughput": "450,000 TPS",
        "expected_latency": "2.2ms TPOT"
      }
    },
    {
      "method": "baseline",
      "configuration": "TP=8, PP=2, 8 experts per GPU",
      "description": "Traditional MoE deployment with tensor and pipeline parallelism",
      "files": {
        "dot_file": "../outputs/2025-11-24-14-31-20/baseline_dag.dot",
        "svg_image": "../outputs/2025-11-24-14-31-20/baseline_dag.svg"
      },
      "deployment_details": {
        "total_gpus": 16,
        "experts_per_layer": 16,
        "experts_per_gpu": 8,
        "parallelism_strategy": "Tensor Parallelism + Pipeline Parallelism",
        "expected_throughput": "120,000 TPS",
        "expected_latency": "8.3ms TPOT"
      }
    }
  ],
  "generation_metadata": {
    "generated_at": "2025-11-24-14-31-20",
    "model_type": "16-layer Mixture-of-Experts Transformer",
    "expert_count": 256,
    "total_parameters": "Not specified in paper",
    "precision": "BF16",
    "input_dimensions": {
      "batch_size": 128,
      "sequence_length": 10000,
      "hidden_dimension": 4096
    }
  },
  "verification_results": {
    "proposed_dag": {
      "is_acyclic": true,
      "total_nodes": 112,
      "total_edges": 168,
      "expert_nodes": 48,
      "gpu_mapping": "1 expert per GPU across 16 GPUs"
    },
    "baseline_dag": {
      "is_acyclic": true,
      "total_nodes": 112,
      "total_edges": 168,
      "expert_nodes": 48,
      "gpu_mapping": "8 experts per GPU across 16 GPUs"
    }
  }
}