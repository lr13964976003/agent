digraph FA_Pool_16GPU_Config {
	rankdir=TB splines=spline
	node [shape=rectangle style=filled]
	input [label="Model Input
SeqLen: 4096
[batch_size=1024, seq_len=4096, d_model=4096]" fillcolor=lightgreen shape=ellipse]
	manager [label="Resource Manager
SeqLen: 4096
Pool GPUs: 8" fillcolor=gold shape=diamond]
	embeddings [label="Token Embeddings
[batch_size=1024, seq_len=4096, d_model=4096]
GPU: 0-7" fillcolor=lightblue]
	prenorm_0 [label="Pre-Norm Layer 0
GPU: 0-7" fillcolor=lightblue]
	query_split_0 [label="Query Split Layer 0
Block size: 512
GPU: 8-15" fillcolor=yellow shape=parallelogram]
	kv_broadcast_0 [label="KV Broadcast Layer 0
GPU: 8-15" fillcolor=yellow shape=parallelogram]
	attn_q_0_0 [label="Q Proj Layer 0
GPU 8
Range: 0-512" fillcolor=lightcoral]
	attn_k_0_0 [label="K Proj Layer 0
GPU 8" fillcolor=lightcoral]
	attn_v_0_0 [label="V Proj Layer 0
GPU 8" fillcolor=lightcoral]
	flash_attn_0_0 [label="Flash Attention
Layer 0 GPU 8" fillcolor=lightcoral]
	attn_out_0_0 [label="Output Proj
Layer 0 GPU 8" fillcolor=lightcoral]
	attn_q_0_1 [label="Q Proj Layer 0
GPU 9
Range: 512-1024" fillcolor=lightcoral]
	attn_k_0_1 [label="K Proj Layer 0
GPU 9" fillcolor=lightcoral]
	attn_v_0_1 [label="V Proj Layer 0
GPU 9" fillcolor=lightcoral]
	flash_attn_0_1 [label="Flash Attention
Layer 0 GPU 9" fillcolor=lightcoral]
	attn_out_0_1 [label="Output Proj
Layer 0 GPU 9" fillcolor=lightcoral]
	attn_q_0_2 [label="Q Proj Layer 0
GPU 10
Range: 1024-1536" fillcolor=lightcoral]
	attn_k_0_2 [label="K Proj Layer 0
GPU 10" fillcolor=lightcoral]
	attn_v_0_2 [label="V Proj Layer 0
GPU 10" fillcolor=lightcoral]
	flash_attn_0_2 [label="Flash Attention
Layer 0 GPU 10" fillcolor=lightcoral]
	attn_out_0_2 [label="Output Proj
Layer 0 GPU 10" fillcolor=lightcoral]
	attn_q_0_3 [label="Q Proj Layer 0
GPU 11
Range: 1536-2048" fillcolor=lightcoral]
	attn_k_0_3 [label="K Proj Layer 0
GPU 11" fillcolor=lightcoral]
	attn_v_0_3 [label="V Proj Layer 0
GPU 11" fillcolor=lightcoral]
	flash_attn_0_3 [label="Flash Attention
Layer 0 GPU 11" fillcolor=lightcoral]
	attn_out_0_3 [label="Output Proj
Layer 0 GPU 11" fillcolor=lightcoral]
	attn_q_0_4 [label="Q Proj Layer 0
GPU 12
Range: 2048-2560" fillcolor=lightcoral]
	attn_k_0_4 [label="K Proj Layer 0
GPU 12" fillcolor=lightcoral]
	attn_v_0_4 [label="V Proj Layer 0
GPU 12" fillcolor=lightcoral]
	flash_attn_0_4 [label="Flash Attention
Layer 0 GPU 12" fillcolor=lightcoral]
	attn_out_0_4 [label="Output Proj
Layer 0 GPU 12" fillcolor=lightcoral]
	attn_q_0_5 [label="Q Proj Layer 0
GPU 13
Range: 2560-3072" fillcolor=lightcoral]
	attn_k_0_5 [label="K Proj Layer 0
GPU 13" fillcolor=lightcoral]
	attn_v_0_5 [label="V Proj Layer 0
GPU 13" fillcolor=lightcoral]
	flash_attn_0_5 [label="Flash Attention
Layer 0 GPU 13" fillcolor=lightcoral]
	attn_out_0_5 [label="Output Proj
Layer 0 GPU 13" fillcolor=lightcoral]
	attn_q_0_6 [label="Q Proj Layer 0
GPU 14
Range: 3072-3584" fillcolor=lightcoral]
	attn_k_0_6 [label="K Proj Layer 0
GPU 14" fillcolor=lightcoral]
	attn_v_0_6 [label="V Proj Layer 0
GPU 14" fillcolor=lightcoral]
	flash_attn_0_6 [label="Flash Attention
Layer 0 GPU 14" fillcolor=lightcoral]
	attn_out_0_6 [label="Output Proj
Layer 0 GPU 14" fillcolor=lightcoral]
	attn_q_0_7 [label="Q Proj Layer 0
GPU 15
Range: 3584-4096" fillcolor=lightcoral]
	attn_k_0_7 [label="K Proj Layer 0
GPU 15" fillcolor=lightcoral]
	attn_v_0_7 [label="V Proj Layer 0
GPU 15" fillcolor=lightcoral]
	flash_attn_0_7 [label="Flash Attention
Layer 0 GPU 15" fillcolor=lightcoral]
	attn_out_0_7 [label="Output Proj
Layer 0 GPU 15" fillcolor=lightcoral]
	gather_0 [label="Gather Attention
Layer 0
GPU: 8-15" fillcolor=yellow shape=parallelogram]
	attn_residual_0 [label="Attention Residual
Layer 0" fillcolor=orange shape=ellipse]
	ffn_0 [label="FFN Layer 0
GPU: 0-7" fillcolor=lightblue]
	ffn_residual_0 [label="FFN Residual
Layer 0" fillcolor=orange shape=ellipse]
	prenorm_1 [label="Pre-Norm Layer 1
GPU: 0-7" fillcolor=lightblue]
	query_split_1 [label="Query Split Layer 1
Block size: 512
GPU: 8-15" fillcolor=yellow shape=parallelogram]
	kv_broadcast_1 [label="KV Broadcast Layer 1
GPU: 8-15" fillcolor=yellow shape=parallelogram]
	attn_q_1_0 [label="Q Proj Layer 1
GPU 8
Range: 0-512" fillcolor=lightcoral]
	attn_k_1_0 [label="K Proj Layer 1
GPU 8" fillcolor=lightcoral]
	attn_v_1_0 [label="V Proj Layer 1
GPU 8" fillcolor=lightcoral]
	flash_attn_1_0 [label="Flash Attention
Layer 1 GPU 8" fillcolor=lightcoral]
	attn_out_1_0 [label="Output Proj
Layer 1 GPU 8" fillcolor=lightcoral]
	attn_q_1_1 [label="Q Proj Layer 1
GPU 9
Range: 512-1024" fillcolor=lightcoral]
	attn_k_1_1 [label="K Proj Layer 1
GPU 9" fillcolor=lightcoral]
	attn_v_1_1 [label="V Proj Layer 1
GPU 9" fillcolor=lightcoral]
	flash_attn_1_1 [label="Flash Attention
Layer 1 GPU 9" fillcolor=lightcoral]
	attn_out_1_1 [label="Output Proj
Layer 1 GPU 9" fillcolor=lightcoral]
	attn_q_1_2 [label="Q Proj Layer 1
GPU 10
Range: 1024-1536" fillcolor=lightcoral]
	attn_k_1_2 [label="K Proj Layer 1
GPU 10" fillcolor=lightcoral]
	attn_v_1_2 [label="V Proj Layer 1
GPU 10" fillcolor=lightcoral]
	flash_attn_1_2 [label="Flash Attention
Layer 1 GPU 10" fillcolor=lightcoral]
	attn_out_1_2 [label="Output Proj
Layer 1 GPU 10" fillcolor=lightcoral]
	attn_q_1_3 [label="Q Proj Layer 1
GPU 11
Range: 1536-2048" fillcolor=lightcoral]
	attn_k_1_3 [label="K Proj Layer 1
GPU 11" fillcolor=lightcoral]
	attn_v_1_3 [label="V Proj Layer 1
GPU 11" fillcolor=lightcoral]
	flash_attn_1_3 [label="Flash Attention
Layer 1 GPU 11" fillcolor=lightcoral]
	attn_out_1_3 [label="Output Proj
Layer 1 GPU 11" fillcolor=lightcoral]
	attn_q_1_4 [label="Q Proj Layer 1
GPU 12
Range: 2048-2560" fillcolor=lightcoral]
	attn_k_1_4 [label="K Proj Layer 1
GPU 12" fillcolor=lightcoral]
	attn_v_1_4 [label="V Proj Layer 1
GPU 12" fillcolor=lightcoral]
	flash_attn_1_4 [label="Flash Attention
Layer 1 GPU 12" fillcolor=lightcoral]
	attn_out_1_4 [label="Output Proj
Layer 1 GPU 12" fillcolor=lightcoral]
	attn_q_1_5 [label="Q Proj Layer 1
GPU 13
Range: 2560-3072" fillcolor=lightcoral]
	attn_k_1_5 [label="K Proj Layer 1
GPU 13" fillcolor=lightcoral]
	attn_v_1_5 [label="V Proj Layer 1
GPU 13" fillcolor=lightcoral]
	flash_attn_1_5 [label="Flash Attention
Layer 1 GPU 13" fillcolor=lightcoral]
	attn_out_1_5 [label="Output Proj
Layer 1 GPU 13" fillcolor=lightcoral]
	attn_q_1_6 [label="Q Proj Layer 1
GPU 14
Range: 3072-3584" fillcolor=lightcoral]
	attn_k_1_6 [label="K Proj Layer 1
GPU 14" fillcolor=lightcoral]
	attn_v_1_6 [label="V Proj Layer 1
GPU 14" fillcolor=lightcoral]
	flash_attn_1_6 [label="Flash Attention
Layer 1 GPU 14" fillcolor=lightcoral]
	attn_out_1_6 [label="Output Proj
Layer 1 GPU 14" fillcolor=lightcoral]
	attn_q_1_7 [label="Q Proj Layer 1
GPU 15
Range: 3584-4096" fillcolor=lightcoral]
	attn_k_1_7 [label="K Proj Layer 1
GPU 15" fillcolor=lightcoral]
	attn_v_1_7 [label="V Proj Layer 1
GPU 15" fillcolor=lightcoral]
	flash_attn_1_7 [label="Flash Attention
Layer 1 GPU 15" fillcolor=lightcoral]
	attn_out_1_7 [label="Output Proj
Layer 1 GPU 15" fillcolor=lightcoral]
	gather_1 [label="Gather Attention
Layer 1
GPU: 8-15" fillcolor=yellow shape=parallelogram]
	attn_residual_1 [label="Attention Residual
Layer 1" fillcolor=orange shape=ellipse]
	ffn_1 [label="FFN Layer 1
GPU: 0-7" fillcolor=lightblue]
	ffn_residual_1 [label="FFN Residual
Layer 1" fillcolor=orange shape=ellipse]
	prenorm_2 [label="Pre-Norm Layer 2
GPU: 0-7" fillcolor=lightblue]
	query_split_2 [label="Query Split Layer 2
Block size: 512
GPU: 8-15" fillcolor=yellow shape=parallelogram]
	kv_broadcast_2 [label="KV Broadcast Layer 2
GPU: 8-15" fillcolor=yellow shape=parallelogram]
	attn_q_2_0 [label="Q Proj Layer 2
GPU 8
Range: 0-512" fillcolor=lightcoral]
	attn_k_2_0 [label="K Proj Layer 2
GPU 8" fillcolor=lightcoral]
	attn_v_2_0 [label="V Proj Layer 2
GPU 8" fillcolor=lightcoral]
	flash_attn_2_0 [label="Flash Attention
Layer 2 GPU 8" fillcolor=lightcoral]
	attn_out_2_0 [label="Output Proj
Layer 2 GPU 8" fillcolor=lightcoral]
	attn_q_2_1 [label="Q Proj Layer 2
GPU 9
Range: 512-1024" fillcolor=lightcoral]
	attn_k_2_1 [label="K Proj Layer 2
GPU 9" fillcolor=lightcoral]
	attn_v_2_1 [label="V Proj Layer 2
GPU 9" fillcolor=lightcoral]
	flash_attn_2_1 [label="Flash Attention
Layer 2 GPU 9" fillcolor=lightcoral]
	attn_out_2_1 [label="Output Proj
Layer 2 GPU 9" fillcolor=lightcoral]
	attn_q_2_2 [label="Q Proj Layer 2
GPU 10
Range: 1024-1536" fillcolor=lightcoral]
	attn_k_2_2 [label="K Proj Layer 2
GPU 10" fillcolor=lightcoral]
	attn_v_2_2 [label="V Proj Layer 2
GPU 10" fillcolor=lightcoral]
	flash_attn_2_2 [label="Flash Attention
Layer 2 GPU 10" fillcolor=lightcoral]
	attn_out_2_2 [label="Output Proj
Layer 2 GPU 10" fillcolor=lightcoral]
	attn_q_2_3 [label="Q Proj Layer 2
GPU 11
Range: 1536-2048" fillcolor=lightcoral]
	attn_k_2_3 [label="K Proj Layer 2
GPU 11" fillcolor=lightcoral]
	attn_v_2_3 [label="V Proj Layer 2
GPU 11" fillcolor=lightcoral]
	flash_attn_2_3 [label="Flash Attention
Layer 2 GPU 11" fillcolor=lightcoral]
	attn_out_2_3 [label="Output Proj
Layer 2 GPU 11" fillcolor=lightcoral]
	attn_q_2_4 [label="Q Proj Layer 2
GPU 12
Range: 2048-2560" fillcolor=lightcoral]
	attn_k_2_4 [label="K Proj Layer 2
GPU 12" fillcolor=lightcoral]
	attn_v_2_4 [label="V Proj Layer 2
GPU 12" fillcolor=lightcoral]
	flash_attn_2_4 [label="Flash Attention
Layer 2 GPU 12" fillcolor=lightcoral]
	attn_out_2_4 [label="Output Proj
Layer 2 GPU 12" fillcolor=lightcoral]
	attn_q_2_5 [label="Q Proj Layer 2
GPU 13
Range: 2560-3072" fillcolor=lightcoral]
	attn_k_2_5 [label="K Proj Layer 2
GPU 13" fillcolor=lightcoral]
	attn_v_2_5 [label="V Proj Layer 2
GPU 13" fillcolor=lightcoral]
	flash_attn_2_5 [label="Flash Attention
Layer 2 GPU 13" fillcolor=lightcoral]
	attn_out_2_5 [label="Output Proj
Layer 2 GPU 13" fillcolor=lightcoral]
	attn_q_2_6 [label="Q Proj Layer 2
GPU 14
Range: 3072-3584" fillcolor=lightcoral]
	attn_k_2_6 [label="K Proj Layer 2
GPU 14" fillcolor=lightcoral]
	attn_v_2_6 [label="V Proj Layer 2
GPU 14" fillcolor=lightcoral]
	flash_attn_2_6 [label="Flash Attention
Layer 2 GPU 14" fillcolor=lightcoral]
	attn_out_2_6 [label="Output Proj
Layer 2 GPU 14" fillcolor=lightcoral]
	attn_q_2_7 [label="Q Proj Layer 2
GPU 15
Range: 3584-4096" fillcolor=lightcoral]
	attn_k_2_7 [label="K Proj Layer 2
GPU 15" fillcolor=lightcoral]
	attn_v_2_7 [label="V Proj Layer 2
GPU 15" fillcolor=lightcoral]
	flash_attn_2_7 [label="Flash Attention
Layer 2 GPU 15" fillcolor=lightcoral]
	attn_out_2_7 [label="Output Proj
Layer 2 GPU 15" fillcolor=lightcoral]
	gather_2 [label="Gather Attention
Layer 2
GPU: 8-15" fillcolor=yellow shape=parallelogram]
	attn_residual_2 [label="Attention Residual
Layer 2" fillcolor=orange shape=ellipse]
	ffn_2 [label="FFN Layer 2
GPU: 0-7" fillcolor=lightblue]
	ffn_residual_2 [label="FFN Residual
Layer 2" fillcolor=orange shape=ellipse]
	prenorm_3 [label="Pre-Norm Layer 3
GPU: 0-7" fillcolor=lightblue]
	query_split_3 [label="Query Split Layer 3
Block size: 512
GPU: 8-15" fillcolor=yellow shape=parallelogram]
	kv_broadcast_3 [label="KV Broadcast Layer 3
GPU: 8-15" fillcolor=yellow shape=parallelogram]
	attn_q_3_0 [label="Q Proj Layer 3
GPU 8
Range: 0-512" fillcolor=lightcoral]
	attn_k_3_0 [label="K Proj Layer 3
GPU 8" fillcolor=lightcoral]
	attn_v_3_0 [label="V Proj Layer 3
GPU 8" fillcolor=lightcoral]
	flash_attn_3_0 [label="Flash Attention
Layer 3 GPU 8" fillcolor=lightcoral]
	attn_out_3_0 [label="Output Proj
Layer 3 GPU 8" fillcolor=lightcoral]
	attn_q_3_1 [label="Q Proj Layer 3
GPU 9
Range: 512-1024" fillcolor=lightcoral]
	attn_k_3_1 [label="K Proj Layer 3
GPU 9" fillcolor=lightcoral]
	attn_v_3_1 [label="V Proj Layer 3
GPU 9" fillcolor=lightcoral]
	flash_attn_3_1 [label="Flash Attention
Layer 3 GPU 9" fillcolor=lightcoral]
	attn_out_3_1 [label="Output Proj
Layer 3 GPU 9" fillcolor=lightcoral]
	attn_q_3_2 [label="Q Proj Layer 3
GPU 10
Range: 1024-1536" fillcolor=lightcoral]
	attn_k_3_2 [label="K Proj Layer 3
GPU 10" fillcolor=lightcoral]
	attn_v_3_2 [label="V Proj Layer 3
GPU 10" fillcolor=lightcoral]
	flash_attn_3_2 [label="Flash Attention
Layer 3 GPU 10" fillcolor=lightcoral]
	attn_out_3_2 [label="Output Proj
Layer 3 GPU 10" fillcolor=lightcoral]
	attn_q_3_3 [label="Q Proj Layer 3
GPU 11
Range: 1536-2048" fillcolor=lightcoral]
	attn_k_3_3 [label="K Proj Layer 3
GPU 11" fillcolor=lightcoral]
	attn_v_3_3 [label="V Proj Layer 3
GPU 11" fillcolor=lightcoral]
	flash_attn_3_3 [label="Flash Attention
Layer 3 GPU 11" fillcolor=lightcoral]
	attn_out_3_3 [label="Output Proj
Layer 3 GPU 11" fillcolor=lightcoral]
	attn_q_3_4 [label="Q Proj Layer 3
GPU 12
Range: 2048-2560" fillcolor=lightcoral]
	attn_k_3_4 [label="K Proj Layer 3
GPU 12" fillcolor=lightcoral]
	attn_v_3_4 [label="V Proj Layer 3
GPU 12" fillcolor=lightcoral]
	flash_attn_3_4 [label="Flash Attention
Layer 3 GPU 12" fillcolor=lightcoral]
	attn_out_3_4 [label="Output Proj
Layer 3 GPU 12" fillcolor=lightcoral]
	attn_q_3_5 [label="Q Proj Layer 3
GPU 13
Range: 2560-3072" fillcolor=lightcoral]
	attn_k_3_5 [label="K Proj Layer 3
GPU 13" fillcolor=lightcoral]
	attn_v_3_5 [label="V Proj Layer 3
GPU 13" fillcolor=lightcoral]
	flash_attn_3_5 [label="Flash Attention
Layer 3 GPU 13" fillcolor=lightcoral]
	attn_out_3_5 [label="Output Proj
Layer 3 GPU 13" fillcolor=lightcoral]
	attn_q_3_6 [label="Q Proj Layer 3
GPU 14
Range: 3072-3584" fillcolor=lightcoral]
	attn_k_3_6 [label="K Proj Layer 3
GPU 14" fillcolor=lightcoral]
	attn_v_3_6 [label="V Proj Layer 3
GPU 14" fillcolor=lightcoral]
	flash_attn_3_6 [label="Flash Attention
Layer 3 GPU 14" fillcolor=lightcoral]
	attn_out_3_6 [label="Output Proj
Layer 3 GPU 14" fillcolor=lightcoral]
	attn_q_3_7 [label="Q Proj Layer 3
GPU 15
Range: 3584-4096" fillcolor=lightcoral]
	attn_k_3_7 [label="K Proj Layer 3
GPU 15" fillcolor=lightcoral]
	attn_v_3_7 [label="V Proj Layer 3
GPU 15" fillcolor=lightcoral]
	flash_attn_3_7 [label="Flash Attention
Layer 3 GPU 15" fillcolor=lightcoral]
	attn_out_3_7 [label="Output Proj
Layer 3 GPU 15" fillcolor=lightcoral]
	gather_3 [label="Gather Attention
Layer 3
GPU: 8-15" fillcolor=yellow shape=parallelogram]
	attn_residual_3 [label="Attention Residual
Layer 3" fillcolor=orange shape=ellipse]
	ffn_3 [label="FFN Layer 3
GPU: 0-7" fillcolor=lightblue]
	ffn_residual_3 [label="FFN Residual
Layer 3" fillcolor=orange shape=ellipse]
	output [label="Model Output
[batch_size=1024, seq_len=4096, vocab_size=full_vocab]" fillcolor=lightgreen shape=ellipse]
	input -> manager
	manager -> embeddings
	embeddings -> prenorm_0
	prenorm_0 -> query_split_0
	prenorm_0 -> kv_broadcast_0
	query_split_0 -> attn_q_0_0
	kv_broadcast_0 -> attn_k_0_0
	kv_broadcast_0 -> attn_v_0_0
	attn_q_0_0 -> flash_attn_0_0
	attn_k_0_0 -> flash_attn_0_0
	attn_v_0_0 -> flash_attn_0_0
	flash_attn_0_0 -> attn_out_0_0
	attn_out_0_0 -> gather_0
	query_split_0 -> attn_q_0_1
	kv_broadcast_0 -> attn_k_0_1
	kv_broadcast_0 -> attn_v_0_1
	attn_q_0_1 -> flash_attn_0_1
	attn_k_0_1 -> flash_attn_0_1
	attn_v_0_1 -> flash_attn_0_1
	flash_attn_0_1 -> attn_out_0_1
	attn_out_0_1 -> gather_0
	query_split_0 -> attn_q_0_2
	kv_broadcast_0 -> attn_k_0_2
	kv_broadcast_0 -> attn_v_0_2
	attn_q_0_2 -> flash_attn_0_2
	attn_k_0_2 -> flash_attn_0_2
	attn_v_0_2 -> flash_attn_0_2
	flash_attn_0_2 -> attn_out_0_2
	attn_out_0_2 -> gather_0
	query_split_0 -> attn_q_0_3
	kv_broadcast_0 -> attn_k_0_3
	kv_broadcast_0 -> attn_v_0_3
	attn_q_0_3 -> flash_attn_0_3
	attn_k_0_3 -> flash_attn_0_3
	attn_v_0_3 -> flash_attn_0_3
	flash_attn_0_3 -> attn_out_0_3
	attn_out_0_3 -> gather_0
	query_split_0 -> attn_q_0_4
	kv_broadcast_0 -> attn_k_0_4
	kv_broadcast_0 -> attn_v_0_4
	attn_q_0_4 -> flash_attn_0_4
	attn_k_0_4 -> flash_attn_0_4
	attn_v_0_4 -> flash_attn_0_4
	flash_attn_0_4 -> attn_out_0_4
	attn_out_0_4 -> gather_0
	query_split_0 -> attn_q_0_5
	kv_broadcast_0 -> attn_k_0_5
	kv_broadcast_0 -> attn_v_0_5
	attn_q_0_5 -> flash_attn_0_5
	attn_k_0_5 -> flash_attn_0_5
	attn_v_0_5 -> flash_attn_0_5
	flash_attn_0_5 -> attn_out_0_5
	attn_out_0_5 -> gather_0
	query_split_0 -> attn_q_0_6
	kv_broadcast_0 -> attn_k_0_6
	kv_broadcast_0 -> attn_v_0_6
	attn_q_0_6 -> flash_attn_0_6
	attn_k_0_6 -> flash_attn_0_6
	attn_v_0_6 -> flash_attn_0_6
	flash_attn_0_6 -> attn_out_0_6
	attn_out_0_6 -> gather_0
	query_split_0 -> attn_q_0_7
	kv_broadcast_0 -> attn_k_0_7
	kv_broadcast_0 -> attn_v_0_7
	attn_q_0_7 -> flash_attn_0_7
	attn_k_0_7 -> flash_attn_0_7
	attn_v_0_7 -> flash_attn_0_7
	flash_attn_0_7 -> attn_out_0_7
	attn_out_0_7 -> gather_0
	embeddings -> attn_residual_0
	gather_0 -> attn_residual_0
	attn_residual_0 -> ffn_0
	attn_residual_0 -> ffn_residual_0
	ffn_0 -> ffn_residual_0
	ffn_residual_0 -> prenorm_1
	prenorm_1 -> query_split_1
	prenorm_1 -> kv_broadcast_1
	query_split_1 -> attn_q_1_0
	kv_broadcast_1 -> attn_k_1_0
	kv_broadcast_1 -> attn_v_1_0
	attn_q_1_0 -> flash_attn_1_0
	attn_k_1_0 -> flash_attn_1_0
	attn_v_1_0 -> flash_attn_1_0
	flash_attn_1_0 -> attn_out_1_0
	attn_out_1_0 -> gather_1
	query_split_1 -> attn_q_1_1
	kv_broadcast_1 -> attn_k_1_1
	kv_broadcast_1 -> attn_v_1_1
	attn_q_1_1 -> flash_attn_1_1
	attn_k_1_1 -> flash_attn_1_1
	attn_v_1_1 -> flash_attn_1_1
	flash_attn_1_1 -> attn_out_1_1
	attn_out_1_1 -> gather_1
	query_split_1 -> attn_q_1_2
	kv_broadcast_1 -> attn_k_1_2
	kv_broadcast_1 -> attn_v_1_2
	attn_q_1_2 -> flash_attn_1_2
	attn_k_1_2 -> flash_attn_1_2
	attn_v_1_2 -> flash_attn_1_2
	flash_attn_1_2 -> attn_out_1_2
	attn_out_1_2 -> gather_1
	query_split_1 -> attn_q_1_3
	kv_broadcast_1 -> attn_k_1_3
	kv_broadcast_1 -> attn_v_1_3
	attn_q_1_3 -> flash_attn_1_3
	attn_k_1_3 -> flash_attn_1_3
	attn_v_1_3 -> flash_attn_1_3
	flash_attn_1_3 -> attn_out_1_3
	attn_out_1_3 -> gather_1
	query_split_1 -> attn_q_1_4
	kv_broadcast_1 -> attn_k_1_4
	kv_broadcast_1 -> attn_v_1_4
	attn_q_1_4 -> flash_attn_1_4
	attn_k_1_4 -> flash_attn_1_4
	attn_v_1_4 -> flash_attn_1_4
	flash_attn_1_4 -> attn_out_1_4
	attn_out_1_4 -> gather_1
	query_split_1 -> attn_q_1_5
	kv_broadcast_1 -> attn_k_1_5
	kv_broadcast_1 -> attn_v_1_5
	attn_q_1_5 -> flash_attn_1_5
	attn_k_1_5 -> flash_attn_1_5
	attn_v_1_5 -> flash_attn_1_5
	flash_attn_1_5 -> attn_out_1_5
	attn_out_1_5 -> gather_1
	query_split_1 -> attn_q_1_6
	kv_broadcast_1 -> attn_k_1_6
	kv_broadcast_1 -> attn_v_1_6
	attn_q_1_6 -> flash_attn_1_6
	attn_k_1_6 -> flash_attn_1_6
	attn_v_1_6 -> flash_attn_1_6
	flash_attn_1_6 -> attn_out_1_6
	attn_out_1_6 -> gather_1
	query_split_1 -> attn_q_1_7
	kv_broadcast_1 -> attn_k_1_7
	kv_broadcast_1 -> attn_v_1_7
	attn_q_1_7 -> flash_attn_1_7
	attn_k_1_7 -> flash_attn_1_7
	attn_v_1_7 -> flash_attn_1_7
	flash_attn_1_7 -> attn_out_1_7
	attn_out_1_7 -> gather_1
	ffn_residual_0 -> attn_residual_1
	gather_1 -> attn_residual_1
	attn_residual_1 -> ffn_1
	attn_residual_1 -> ffn_residual_1
	ffn_1 -> ffn_residual_1
	ffn_residual_1 -> prenorm_2
	prenorm_2 -> query_split_2
	prenorm_2 -> kv_broadcast_2
	query_split_2 -> attn_q_2_0
	kv_broadcast_2 -> attn_k_2_0
	kv_broadcast_2 -> attn_v_2_0
	attn_q_2_0 -> flash_attn_2_0
	attn_k_2_0 -> flash_attn_2_0
	attn_v_2_0 -> flash_attn_2_0
	flash_attn_2_0 -> attn_out_2_0
	attn_out_2_0 -> gather_2
	query_split_2 -> attn_q_2_1
	kv_broadcast_2 -> attn_k_2_1
	kv_broadcast_2 -> attn_v_2_1
	attn_q_2_1 -> flash_attn_2_1
	attn_k_2_1 -> flash_attn_2_1
	attn_v_2_1 -> flash_attn_2_1
	flash_attn_2_1 -> attn_out_2_1
	attn_out_2_1 -> gather_2
	query_split_2 -> attn_q_2_2
	kv_broadcast_2 -> attn_k_2_2
	kv_broadcast_2 -> attn_v_2_2
	attn_q_2_2 -> flash_attn_2_2
	attn_k_2_2 -> flash_attn_2_2
	attn_v_2_2 -> flash_attn_2_2
	flash_attn_2_2 -> attn_out_2_2
	attn_out_2_2 -> gather_2
	query_split_2 -> attn_q_2_3
	kv_broadcast_2 -> attn_k_2_3
	kv_broadcast_2 -> attn_v_2_3
	attn_q_2_3 -> flash_attn_2_3
	attn_k_2_3 -> flash_attn_2_3
	attn_v_2_3 -> flash_attn_2_3
	flash_attn_2_3 -> attn_out_2_3
	attn_out_2_3 -> gather_2
	query_split_2 -> attn_q_2_4
	kv_broadcast_2 -> attn_k_2_4
	kv_broadcast_2 -> attn_v_2_4
	attn_q_2_4 -> flash_attn_2_4
	attn_k_2_4 -> flash_attn_2_4
	attn_v_2_4 -> flash_attn_2_4
	flash_attn_2_4 -> attn_out_2_4
	attn_out_2_4 -> gather_2
	query_split_2 -> attn_q_2_5
	kv_broadcast_2 -> attn_k_2_5
	kv_broadcast_2 -> attn_v_2_5
	attn_q_2_5 -> flash_attn_2_5
	attn_k_2_5 -> flash_attn_2_5
	attn_v_2_5 -> flash_attn_2_5
	flash_attn_2_5 -> attn_out_2_5
	attn_out_2_5 -> gather_2
	query_split_2 -> attn_q_2_6
	kv_broadcast_2 -> attn_k_2_6
	kv_broadcast_2 -> attn_v_2_6
	attn_q_2_6 -> flash_attn_2_6
	attn_k_2_6 -> flash_attn_2_6
	attn_v_2_6 -> flash_attn_2_6
	flash_attn_2_6 -> attn_out_2_6
	attn_out_2_6 -> gather_2
	query_split_2 -> attn_q_2_7
	kv_broadcast_2 -> attn_k_2_7
	kv_broadcast_2 -> attn_v_2_7
	attn_q_2_7 -> flash_attn_2_7
	attn_k_2_7 -> flash_attn_2_7
	attn_v_2_7 -> flash_attn_2_7
	flash_attn_2_7 -> attn_out_2_7
	attn_out_2_7 -> gather_2
	ffn_residual_1 -> attn_residual_2
	gather_2 -> attn_residual_2
	attn_residual_2 -> ffn_2
	attn_residual_2 -> ffn_residual_2
	ffn_2 -> ffn_residual_2
	ffn_residual_2 -> prenorm_3
	prenorm_3 -> query_split_3
	prenorm_3 -> kv_broadcast_3
	query_split_3 -> attn_q_3_0
	kv_broadcast_3 -> attn_k_3_0
	kv_broadcast_3 -> attn_v_3_0
	attn_q_3_0 -> flash_attn_3_0
	attn_k_3_0 -> flash_attn_3_0
	attn_v_3_0 -> flash_attn_3_0
	flash_attn_3_0 -> attn_out_3_0
	attn_out_3_0 -> gather_3
	query_split_3 -> attn_q_3_1
	kv_broadcast_3 -> attn_k_3_1
	kv_broadcast_3 -> attn_v_3_1
	attn_q_3_1 -> flash_attn_3_1
	attn_k_3_1 -> flash_attn_3_1
	attn_v_3_1 -> flash_attn_3_1
	flash_attn_3_1 -> attn_out_3_1
	attn_out_3_1 -> gather_3
	query_split_3 -> attn_q_3_2
	kv_broadcast_3 -> attn_k_3_2
	kv_broadcast_3 -> attn_v_3_2
	attn_q_3_2 -> flash_attn_3_2
	attn_k_3_2 -> flash_attn_3_2
	attn_v_3_2 -> flash_attn_3_2
	flash_attn_3_2 -> attn_out_3_2
	attn_out_3_2 -> gather_3
	query_split_3 -> attn_q_3_3
	kv_broadcast_3 -> attn_k_3_3
	kv_broadcast_3 -> attn_v_3_3
	attn_q_3_3 -> flash_attn_3_3
	attn_k_3_3 -> flash_attn_3_3
	attn_v_3_3 -> flash_attn_3_3
	flash_attn_3_3 -> attn_out_3_3
	attn_out_3_3 -> gather_3
	query_split_3 -> attn_q_3_4
	kv_broadcast_3 -> attn_k_3_4
	kv_broadcast_3 -> attn_v_3_4
	attn_q_3_4 -> flash_attn_3_4
	attn_k_3_4 -> flash_attn_3_4
	attn_v_3_4 -> flash_attn_3_4
	flash_attn_3_4 -> attn_out_3_4
	attn_out_3_4 -> gather_3
	query_split_3 -> attn_q_3_5
	kv_broadcast_3 -> attn_k_3_5
	kv_broadcast_3 -> attn_v_3_5
	attn_q_3_5 -> flash_attn_3_5
	attn_k_3_5 -> flash_attn_3_5
	attn_v_3_5 -> flash_attn_3_5
	flash_attn_3_5 -> attn_out_3_5
	attn_out_3_5 -> gather_3
	query_split_3 -> attn_q_3_6
	kv_broadcast_3 -> attn_k_3_6
	kv_broadcast_3 -> attn_v_3_6
	attn_q_3_6 -> flash_attn_3_6
	attn_k_3_6 -> flash_attn_3_6
	attn_v_3_6 -> flash_attn_3_6
	flash_attn_3_6 -> attn_out_3_6
	attn_out_3_6 -> gather_3
	query_split_3 -> attn_q_3_7
	kv_broadcast_3 -> attn_k_3_7
	kv_broadcast_3 -> attn_v_3_7
	attn_q_3_7 -> flash_attn_3_7
	attn_k_3_7 -> flash_attn_3_7
	attn_v_3_7 -> flash_attn_3_7
	flash_attn_3_7 -> attn_out_3_7
	attn_out_3_7 -> gather_3
	ffn_residual_2 -> attn_residual_3
	gather_3 -> attn_residual_3
	attn_residual_3 -> ffn_3
	attn_residual_3 -> ffn_residual_3
	ffn_3 -> ffn_residual_3
	ffn_residual_3 -> output
}
