// FA Pool Base Layer - Tensor Parallelism (8 GPUs)
digraph {
	nodesep=0.5 rankdir=TB splines=ortho
	node [fillcolor=lightblue shape=rectangle style=filled]
	input [label="Input\nInput: [batch_size=1024, seq_len=?, d_model=4096]\nOutput: [batch_size=1024, seq_len=?, d_model=4096]\nGPU: all GPUs" fillcolor=lightgreen shape=ellipse]
	embedding_0 [label="Embedding Partition 0\nInput: [batch_size=1024, seq_len=?, d_model=4096]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 0"]
	embedding_1 [label="Embedding Partition 1\nInput: [batch_size=1024, seq_len=?, d_model=4096]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 1"]
	embedding_2 [label="Embedding Partition 2\nInput: [batch_size=1024, seq_len=?, d_model=4096]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 2"]
	embedding_3 [label="Embedding Partition 3\nInput: [batch_size=1024, seq_len=?, d_model=4096]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 3"]
	embedding_4 [label="Embedding Partition 4\nInput: [batch_size=1024, seq_len=?, d_model=4096]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 4"]
	embedding_5 [label="Embedding Partition 5\nInput: [batch_size=1024, seq_len=?, d_model=4096]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 5"]
	embedding_6 [label="Embedding Partition 6\nInput: [batch_size=1024, seq_len=?, d_model=4096]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 6"]
	embedding_7 [label="Embedding Partition 7\nInput: [batch_size=1024, seq_len=?, d_model=4096]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 7"]
	pos_enc_0 [label="Positional Encoding 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 0"]
	pos_enc_1 [label="Positional Encoding 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 1"]
	pos_enc_2 [label="Positional Encoding 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 2"]
	pos_enc_3 [label="Positional Encoding 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 3"]
	pos_enc_4 [label="Positional Encoding 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 4"]
	pos_enc_5 [label="Positional Encoding 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 5"]
	pos_enc_6 [label="Positional Encoding 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 6"]
	pos_enc_7 [label="Positional Encoding 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 7"]
	layer_norm_0_0 [label="Layer Norm 0 Part 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 0"]
	layer_norm_0_1 [label="Layer Norm 0 Part 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 1"]
	layer_norm_0_2 [label="Layer Norm 0 Part 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 2"]
	layer_norm_0_3 [label="Layer Norm 0 Part 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 3"]
	layer_norm_0_4 [label="Layer Norm 0 Part 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 4"]
	layer_norm_0_5 [label="Layer Norm 0 Part 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 5"]
	layer_norm_0_6 [label="Layer Norm 0 Part 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 6"]
	layer_norm_0_7 [label="Layer Norm 0 Part 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 7"]
	layer_norm_1_0 [label="Layer Norm 1 Part 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 0"]
	layer_norm_1_1 [label="Layer Norm 1 Part 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 1"]
	layer_norm_1_2 [label="Layer Norm 1 Part 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 2"]
	layer_norm_1_3 [label="Layer Norm 1 Part 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 3"]
	layer_norm_1_4 [label="Layer Norm 1 Part 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 4"]
	layer_norm_1_5 [label="Layer Norm 1 Part 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 5"]
	layer_norm_1_6 [label="Layer Norm 1 Part 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 6"]
	layer_norm_1_7 [label="Layer Norm 1 Part 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 7"]
	layer_norm_2_0 [label="Layer Norm 2 Part 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 0"]
	layer_norm_2_1 [label="Layer Norm 2 Part 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 1"]
	layer_norm_2_2 [label="Layer Norm 2 Part 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 2"]
	layer_norm_2_3 [label="Layer Norm 2 Part 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 3"]
	layer_norm_2_4 [label="Layer Norm 2 Part 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 4"]
	layer_norm_2_5 [label="Layer Norm 2 Part 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 5"]
	layer_norm_2_6 [label="Layer Norm 2 Part 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 6"]
	layer_norm_2_7 [label="Layer Norm 2 Part 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 7"]
	layer_norm_3_0 [label="Layer Norm 3 Part 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 0"]
	layer_norm_3_1 [label="Layer Norm 3 Part 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 1"]
	layer_norm_3_2 [label="Layer Norm 3 Part 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 2"]
	layer_norm_3_3 [label="Layer Norm 3 Part 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 3"]
	layer_norm_3_4 [label="Layer Norm 3 Part 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 4"]
	layer_norm_3_5 [label="Layer Norm 3 Part 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 5"]
	layer_norm_3_6 [label="Layer Norm 3 Part 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 6"]
	layer_norm_3_7 [label="Layer Norm 3 Part 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 7"]
	ffn_gate_0_0 [label="FFN Gate 0 Part 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 0"]
	ffn_gate_0_1 [label="FFN Gate 0 Part 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 1"]
	ffn_gate_0_2 [label="FFN Gate 0 Part 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 2"]
	ffn_gate_0_3 [label="FFN Gate 0 Part 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 3"]
	ffn_gate_0_4 [label="FFN Gate 0 Part 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 4"]
	ffn_gate_0_5 [label="FFN Gate 0 Part 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 5"]
	ffn_gate_0_6 [label="FFN Gate 0 Part 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 6"]
	ffn_gate_0_7 [label="FFN Gate 0 Part 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 7"]
	ffn_up_0_0 [label="FFN Up 0 Part 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 0"]
	ffn_up_0_1 [label="FFN Up 0 Part 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 1"]
	ffn_up_0_2 [label="FFN Up 0 Part 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 2"]
	ffn_up_0_3 [label="FFN Up 0 Part 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 3"]
	ffn_up_0_4 [label="FFN Up 0 Part 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 4"]
	ffn_up_0_5 [label="FFN Up 0 Part 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 5"]
	ffn_up_0_6 [label="FFN Up 0 Part 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 6"]
	ffn_up_0_7 [label="FFN Up 0 Part 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 7"]
	gelu_0_0 [label="GELU 0 Part 0\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 0"]
	gelu_0_1 [label="GELU 0 Part 1\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 1"]
	gelu_0_2 [label="GELU 0 Part 2\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 2"]
	gelu_0_3 [label="GELU 0 Part 3\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 3"]
	gelu_0_4 [label="GELU 0 Part 4\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 4"]
	gelu_0_5 [label="GELU 0 Part 5\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 5"]
	gelu_0_6 [label="GELU 0 Part 6\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 6"]
	gelu_0_7 [label="GELU 0 Part 7\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 7"]
	ffn_down_0_0 [label="FFN Down 0 Part 0\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 0"]
	ffn_down_0_1 [label="FFN Down 0 Part 1\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 1"]
	ffn_down_0_2 [label="FFN Down 0 Part 2\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 2"]
	ffn_down_0_3 [label="FFN Down 0 Part 3\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 3"]
	ffn_down_0_4 [label="FFN Down 0 Part 4\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 4"]
	ffn_down_0_5 [label="FFN Down 0 Part 5\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 5"]
	ffn_down_0_6 [label="FFN Down 0 Part 6\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 6"]
	ffn_down_0_7 [label="FFN Down 0 Part 7\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 7"]
	ffn_all_reduce_0 [label="FFN All-Reduce 0\nInput: [batch_size=1024, seq_len=?, d_model=512] x 8\nOutput: [batch_size=1024, seq_len=?, d_model=4096]\nGPU: all GPUs" fillcolor=lightyellow shape=parallelogram]
	residual_ffn_0 [label="Residual Add FFN 0\nInput: [batch_size=1024, seq_len=?, d_model=4096] x 2\nOutput: [batch_size=1024, seq_len=?, d_model=4096]\nGPU: all GPUs" fillcolor=orange shape=ellipse]
	ffn_gate_1_0 [label="FFN Gate 1 Part 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 0"]
	ffn_gate_1_1 [label="FFN Gate 1 Part 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 1"]
	ffn_gate_1_2 [label="FFN Gate 1 Part 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 2"]
	ffn_gate_1_3 [label="FFN Gate 1 Part 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 3"]
	ffn_gate_1_4 [label="FFN Gate 1 Part 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 4"]
	ffn_gate_1_5 [label="FFN Gate 1 Part 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 5"]
	ffn_gate_1_6 [label="FFN Gate 1 Part 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 6"]
	ffn_gate_1_7 [label="FFN Gate 1 Part 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 7"]
	ffn_up_1_0 [label="FFN Up 1 Part 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 0"]
	ffn_up_1_1 [label="FFN Up 1 Part 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 1"]
	ffn_up_1_2 [label="FFN Up 1 Part 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 2"]
	ffn_up_1_3 [label="FFN Up 1 Part 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 3"]
	ffn_up_1_4 [label="FFN Up 1 Part 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 4"]
	ffn_up_1_5 [label="FFN Up 1 Part 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 5"]
	ffn_up_1_6 [label="FFN Up 1 Part 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 6"]
	ffn_up_1_7 [label="FFN Up 1 Part 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 7"]
	gelu_1_0 [label="GELU 1 Part 0\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 0"]
	gelu_1_1 [label="GELU 1 Part 1\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 1"]
	gelu_1_2 [label="GELU 1 Part 2\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 2"]
	gelu_1_3 [label="GELU 1 Part 3\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 3"]
	gelu_1_4 [label="GELU 1 Part 4\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 4"]
	gelu_1_5 [label="GELU 1 Part 5\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 5"]
	gelu_1_6 [label="GELU 1 Part 6\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 6"]
	gelu_1_7 [label="GELU 1 Part 7\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 7"]
	ffn_down_1_0 [label="FFN Down 1 Part 0\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 0"]
	ffn_down_1_1 [label="FFN Down 1 Part 1\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 1"]
	ffn_down_1_2 [label="FFN Down 1 Part 2\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 2"]
	ffn_down_1_3 [label="FFN Down 1 Part 3\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 3"]
	ffn_down_1_4 [label="FFN Down 1 Part 4\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 4"]
	ffn_down_1_5 [label="FFN Down 1 Part 5\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 5"]
	ffn_down_1_6 [label="FFN Down 1 Part 6\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 6"]
	ffn_down_1_7 [label="FFN Down 1 Part 7\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 7"]
	ffn_all_reduce_1 [label="FFN All-Reduce 1\nInput: [batch_size=1024, seq_len=?, d_model=512] x 8\nOutput: [batch_size=1024, seq_len=?, d_model=4096]\nGPU: all GPUs" fillcolor=lightyellow shape=parallelogram]
	residual_ffn_1 [label="Residual Add FFN 1\nInput: [batch_size=1024, seq_len=?, d_model=4096] x 2\nOutput: [batch_size=1024, seq_len=?, d_model=4096]\nGPU: all GPUs" fillcolor=orange shape=ellipse]
	ffn_gate_2_0 [label="FFN Gate 2 Part 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 0"]
	ffn_gate_2_1 [label="FFN Gate 2 Part 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 1"]
	ffn_gate_2_2 [label="FFN Gate 2 Part 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 2"]
	ffn_gate_2_3 [label="FFN Gate 2 Part 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 3"]
	ffn_gate_2_4 [label="FFN Gate 2 Part 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 4"]
	ffn_gate_2_5 [label="FFN Gate 2 Part 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 5"]
	ffn_gate_2_6 [label="FFN Gate 2 Part 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 6"]
	ffn_gate_2_7 [label="FFN Gate 2 Part 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 7"]
	ffn_up_2_0 [label="FFN Up 2 Part 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 0"]
	ffn_up_2_1 [label="FFN Up 2 Part 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 1"]
	ffn_up_2_2 [label="FFN Up 2 Part 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 2"]
	ffn_up_2_3 [label="FFN Up 2 Part 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 3"]
	ffn_up_2_4 [label="FFN Up 2 Part 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 4"]
	ffn_up_2_5 [label="FFN Up 2 Part 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 5"]
	ffn_up_2_6 [label="FFN Up 2 Part 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 6"]
	ffn_up_2_7 [label="FFN Up 2 Part 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 7"]
	gelu_2_0 [label="GELU 2 Part 0\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 0"]
	gelu_2_1 [label="GELU 2 Part 1\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 1"]
	gelu_2_2 [label="GELU 2 Part 2\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 2"]
	gelu_2_3 [label="GELU 2 Part 3\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 3"]
	gelu_2_4 [label="GELU 2 Part 4\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 4"]
	gelu_2_5 [label="GELU 2 Part 5\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 5"]
	gelu_2_6 [label="GELU 2 Part 6\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 6"]
	gelu_2_7 [label="GELU 2 Part 7\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 7"]
	ffn_down_2_0 [label="FFN Down 2 Part 0\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 0"]
	ffn_down_2_1 [label="FFN Down 2 Part 1\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 1"]
	ffn_down_2_2 [label="FFN Down 2 Part 2\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 2"]
	ffn_down_2_3 [label="FFN Down 2 Part 3\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 3"]
	ffn_down_2_4 [label="FFN Down 2 Part 4\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 4"]
	ffn_down_2_5 [label="FFN Down 2 Part 5\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 5"]
	ffn_down_2_6 [label="FFN Down 2 Part 6\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 6"]
	ffn_down_2_7 [label="FFN Down 2 Part 7\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 7"]
	ffn_all_reduce_2 [label="FFN All-Reduce 2\nInput: [batch_size=1024, seq_len=?, d_model=512] x 8\nOutput: [batch_size=1024, seq_len=?, d_model=4096]\nGPU: all GPUs" fillcolor=lightyellow shape=parallelogram]
	residual_ffn_2 [label="Residual Add FFN 2\nInput: [batch_size=1024, seq_len=?, d_model=4096] x 2\nOutput: [batch_size=1024, seq_len=?, d_model=4096]\nGPU: all GPUs" fillcolor=orange shape=ellipse]
	ffn_gate_3_0 [label="FFN Gate 3 Part 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 0"]
	ffn_gate_3_1 [label="FFN Gate 3 Part 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 1"]
	ffn_gate_3_2 [label="FFN Gate 3 Part 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 2"]
	ffn_gate_3_3 [label="FFN Gate 3 Part 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 3"]
	ffn_gate_3_4 [label="FFN Gate 3 Part 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 4"]
	ffn_gate_3_5 [label="FFN Gate 3 Part 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 5"]
	ffn_gate_3_6 [label="FFN Gate 3 Part 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 6"]
	ffn_gate_3_7 [label="FFN Gate 3 Part 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 7"]
	ffn_up_3_0 [label="FFN Up 3 Part 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 0"]
	ffn_up_3_1 [label="FFN Up 3 Part 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 1"]
	ffn_up_3_2 [label="FFN Up 3 Part 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 2"]
	ffn_up_3_3 [label="FFN Up 3 Part 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 3"]
	ffn_up_3_4 [label="FFN Up 3 Part 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 4"]
	ffn_up_3_5 [label="FFN Up 3 Part 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 5"]
	ffn_up_3_6 [label="FFN Up 3 Part 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 6"]
	ffn_up_3_7 [label="FFN Up 3 Part 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 7"]
	gelu_3_0 [label="GELU 3 Part 0\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 0"]
	gelu_3_1 [label="GELU 3 Part 1\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 1"]
	gelu_3_2 [label="GELU 3 Part 2\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 2"]
	gelu_3_3 [label="GELU 3 Part 3\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 3"]
	gelu_3_4 [label="GELU 3 Part 4\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 4"]
	gelu_3_5 [label="GELU 3 Part 5\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 5"]
	gelu_3_6 [label="GELU 3 Part 6\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 6"]
	gelu_3_7 [label="GELU 3 Part 7\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nGPU: 7"]
	ffn_down_3_0 [label="FFN Down 3 Part 0\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 0"]
	ffn_down_3_1 [label="FFN Down 3 Part 1\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 1"]
	ffn_down_3_2 [label="FFN Down 3 Part 2\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 2"]
	ffn_down_3_3 [label="FFN Down 3 Part 3\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 3"]
	ffn_down_3_4 [label="FFN Down 3 Part 4\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 4"]
	ffn_down_3_5 [label="FFN Down 3 Part 5\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 5"]
	ffn_down_3_6 [label="FFN Down 3 Part 6\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 6"]
	ffn_down_3_7 [label="FFN Down 3 Part 7\nInput: [batch_size=1024, seq_len=?, ffn_dim=1024]\nOutput: [batch_size=1024, seq_len=?, d_model=512]\nGPU: 7"]
	ffn_all_reduce_3 [label="FFN All-Reduce 3\nInput: [batch_size=1024, seq_len=?, d_model=512] x 8\nOutput: [batch_size=1024, seq_len=?, d_model=4096]\nGPU: all GPUs" fillcolor=lightyellow shape=parallelogram]
	residual_ffn_3 [label="Residual Add FFN 3\nInput: [batch_size=1024, seq_len=?, d_model=4096] x 2\nOutput: [batch_size=1024, seq_len=?, d_model=4096]\nGPU: all GPUs" fillcolor=orange shape=ellipse]
	output_proj_0 [label="Output Projection 0\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, vocab_size=dim/8]\nGPU: 0"]
	output_proj_1 [label="Output Projection 1\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, vocab_size=dim/8]\nGPU: 1"]
	output_proj_2 [label="Output Projection 2\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, vocab_size=dim/8]\nGPU: 2"]
	output_proj_3 [label="Output Projection 3\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, vocab_size=dim/8]\nGPU: 3"]
	output_proj_4 [label="Output Projection 4\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, vocab_size=dim/8]\nGPU: 4"]
	output_proj_5 [label="Output Projection 5\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, vocab_size=dim/8]\nGPU: 5"]
	output_proj_6 [label="Output Projection 6\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, vocab_size=dim/8]\nGPU: 6"]
	output_proj_7 [label="Output Projection 7\nInput: [batch_size=1024, seq_len=?, d_model=512]\nOutput: [batch_size=1024, seq_len=?, vocab_size=dim/8]\nGPU: 7"]
	output_all_reduce [label="Output All-Reduce\nInput: [batch_size=1024, seq_len=?, vocab_size=dim/8] x 8\nOutput: [batch_size=1024, seq_len=?, vocab_size=full_vocab]\nGPU: all GPUs" fillcolor=lightyellow shape=parallelogram]
	output [label="Output\nInput: [batch_size=1024, seq_len=?, vocab_size=full_vocab]\nOutput: [batch_size=1024, seq_len=?, vocab_size=full_vocab]\nGPU: all GPUs" fillcolor=lightgreen shape=ellipse]
	input -> embedding_0
	input -> embedding_1
	input -> embedding_2
	input -> embedding_3
	input -> embedding_4
	input -> embedding_5
	input -> embedding_6
	input -> embedding_7
	embedding_0 -> pos_enc_0
	embedding_1 -> pos_enc_1
	embedding_2 -> pos_enc_2
	embedding_3 -> pos_enc_3
	embedding_4 -> pos_enc_4
	embedding_5 -> pos_enc_5
	embedding_6 -> pos_enc_6
	embedding_7 -> pos_enc_7
	pos_enc_0 -> layer_norm_0_0
	layer_norm_0_0 -> ffn_gate_0_0
	layer_norm_0_0 -> ffn_up_0_0
	ffn_gate_0_0 -> gelu_0_0
	ffn_up_0_0 -> gelu_0_0
	gelu_0_0 -> ffn_down_0_0
	ffn_down_0_0 -> ffn_all_reduce_0
	pos_enc_1 -> layer_norm_0_1
	layer_norm_0_1 -> ffn_gate_0_1
	layer_norm_0_1 -> ffn_up_0_1
	ffn_gate_0_1 -> gelu_0_1
	ffn_up_0_1 -> gelu_0_1
	gelu_0_1 -> ffn_down_0_1
	ffn_down_0_1 -> ffn_all_reduce_0
	pos_enc_2 -> layer_norm_0_2
	layer_norm_0_2 -> ffn_gate_0_2
	layer_norm_0_2 -> ffn_up_0_2
	ffn_gate_0_2 -> gelu_0_2
	ffn_up_0_2 -> gelu_0_2
	gelu_0_2 -> ffn_down_0_2
	ffn_down_0_2 -> ffn_all_reduce_0
	pos_enc_3 -> layer_norm_0_3
	layer_norm_0_3 -> ffn_gate_0_3
	layer_norm_0_3 -> ffn_up_0_3
	ffn_gate_0_3 -> gelu_0_3
	ffn_up_0_3 -> gelu_0_3
	gelu_0_3 -> ffn_down_0_3
	ffn_down_0_3 -> ffn_all_reduce_0
	pos_enc_4 -> layer_norm_0_4
	layer_norm_0_4 -> ffn_gate_0_4
	layer_norm_0_4 -> ffn_up_0_4
	ffn_gate_0_4 -> gelu_0_4
	ffn_up_0_4 -> gelu_0_4
	gelu_0_4 -> ffn_down_0_4
	ffn_down_0_4 -> ffn_all_reduce_0
	pos_enc_5 -> layer_norm_0_5
	layer_norm_0_5 -> ffn_gate_0_5
	layer_norm_0_5 -> ffn_up_0_5
	ffn_gate_0_5 -> gelu_0_5
	ffn_up_0_5 -> gelu_0_5
	gelu_0_5 -> ffn_down_0_5
	ffn_down_0_5 -> ffn_all_reduce_0
	pos_enc_6 -> layer_norm_0_6
	layer_norm_0_6 -> ffn_gate_0_6
	layer_norm_0_6 -> ffn_up_0_6
	ffn_gate_0_6 -> gelu_0_6
	ffn_up_0_6 -> gelu_0_6
	gelu_0_6 -> ffn_down_0_6
	ffn_down_0_6 -> ffn_all_reduce_0
	pos_enc_7 -> layer_norm_0_7
	layer_norm_0_7 -> ffn_gate_0_7
	layer_norm_0_7 -> ffn_up_0_7
	ffn_gate_0_7 -> gelu_0_7
	ffn_up_0_7 -> gelu_0_7
	gelu_0_7 -> ffn_down_0_7
	ffn_down_0_7 -> ffn_all_reduce_0
	input -> residual_ffn_0
	ffn_all_reduce_0 -> residual_ffn_0
	layer_norm_1_0 -> ffn_gate_1_0
	layer_norm_1_0 -> ffn_up_1_0
	ffn_gate_1_0 -> gelu_1_0
	ffn_up_1_0 -> gelu_1_0
	gelu_1_0 -> ffn_down_1_0
	ffn_down_1_0 -> ffn_all_reduce_1
	layer_norm_1_1 -> ffn_gate_1_1
	layer_norm_1_1 -> ffn_up_1_1
	ffn_gate_1_1 -> gelu_1_1
	ffn_up_1_1 -> gelu_1_1
	gelu_1_1 -> ffn_down_1_1
	ffn_down_1_1 -> ffn_all_reduce_1
	layer_norm_1_2 -> ffn_gate_1_2
	layer_norm_1_2 -> ffn_up_1_2
	ffn_gate_1_2 -> gelu_1_2
	ffn_up_1_2 -> gelu_1_2
	gelu_1_2 -> ffn_down_1_2
	ffn_down_1_2 -> ffn_all_reduce_1
	layer_norm_1_3 -> ffn_gate_1_3
	layer_norm_1_3 -> ffn_up_1_3
	ffn_gate_1_3 -> gelu_1_3
	ffn_up_1_3 -> gelu_1_3
	gelu_1_3 -> ffn_down_1_3
	ffn_down_1_3 -> ffn_all_reduce_1
	layer_norm_1_4 -> ffn_gate_1_4
	layer_norm_1_4 -> ffn_up_1_4
	ffn_gate_1_4 -> gelu_1_4
	ffn_up_1_4 -> gelu_1_4
	gelu_1_4 -> ffn_down_1_4
	ffn_down_1_4 -> ffn_all_reduce_1
	layer_norm_1_5 -> ffn_gate_1_5
	layer_norm_1_5 -> ffn_up_1_5
	ffn_gate_1_5 -> gelu_1_5
	ffn_up_1_5 -> gelu_1_5
	gelu_1_5 -> ffn_down_1_5
	ffn_down_1_5 -> ffn_all_reduce_1
	layer_norm_1_6 -> ffn_gate_1_6
	layer_norm_1_6 -> ffn_up_1_6
	ffn_gate_1_6 -> gelu_1_6
	ffn_up_1_6 -> gelu_1_6
	gelu_1_6 -> ffn_down_1_6
	ffn_down_1_6 -> ffn_all_reduce_1
	layer_norm_1_7 -> ffn_gate_1_7
	layer_norm_1_7 -> ffn_up_1_7
	ffn_gate_1_7 -> gelu_1_7
	ffn_up_1_7 -> gelu_1_7
	gelu_1_7 -> ffn_down_1_7
	ffn_down_1_7 -> ffn_all_reduce_1
	residual_ffn_0 -> residual_ffn_1
	ffn_all_reduce_1 -> residual_ffn_1
	layer_norm_2_0 -> ffn_gate_2_0
	layer_norm_2_0 -> ffn_up_2_0
	ffn_gate_2_0 -> gelu_2_0
	ffn_up_2_0 -> gelu_2_0
	gelu_2_0 -> ffn_down_2_0
	ffn_down_2_0 -> ffn_all_reduce_2
	layer_norm_2_1 -> ffn_gate_2_1
	layer_norm_2_1 -> ffn_up_2_1
	ffn_gate_2_1 -> gelu_2_1
	ffn_up_2_1 -> gelu_2_1
	gelu_2_1 -> ffn_down_2_1
	ffn_down_2_1 -> ffn_all_reduce_2
	layer_norm_2_2 -> ffn_gate_2_2
	layer_norm_2_2 -> ffn_up_2_2
	ffn_gate_2_2 -> gelu_2_2
	ffn_up_2_2 -> gelu_2_2
	gelu_2_2 -> ffn_down_2_2
	ffn_down_2_2 -> ffn_all_reduce_2
	layer_norm_2_3 -> ffn_gate_2_3
	layer_norm_2_3 -> ffn_up_2_3
	ffn_gate_2_3 -> gelu_2_3
	ffn_up_2_3 -> gelu_2_3
	gelu_2_3 -> ffn_down_2_3
	ffn_down_2_3 -> ffn_all_reduce_2
	layer_norm_2_4 -> ffn_gate_2_4
	layer_norm_2_4 -> ffn_up_2_4
	ffn_gate_2_4 -> gelu_2_4
	ffn_up_2_4 -> gelu_2_4
	gelu_2_4 -> ffn_down_2_4
	ffn_down_2_4 -> ffn_all_reduce_2
	layer_norm_2_5 -> ffn_gate_2_5
	layer_norm_2_5 -> ffn_up_2_5
	ffn_gate_2_5 -> gelu_2_5
	ffn_up_2_5 -> gelu_2_5
	gelu_2_5 -> ffn_down_2_5
	ffn_down_2_5 -> ffn_all_reduce_2
	layer_norm_2_6 -> ffn_gate_2_6
	layer_norm_2_6 -> ffn_up_2_6
	ffn_gate_2_6 -> gelu_2_6
	ffn_up_2_6 -> gelu_2_6
	gelu_2_6 -> ffn_down_2_6
	ffn_down_2_6 -> ffn_all_reduce_2
	layer_norm_2_7 -> ffn_gate_2_7
	layer_norm_2_7 -> ffn_up_2_7
	ffn_gate_2_7 -> gelu_2_7
	ffn_up_2_7 -> gelu_2_7
	gelu_2_7 -> ffn_down_2_7
	ffn_down_2_7 -> ffn_all_reduce_2
	residual_ffn_1 -> residual_ffn_2
	ffn_all_reduce_2 -> residual_ffn_2
	layer_norm_3_0 -> ffn_gate_3_0
	layer_norm_3_0 -> ffn_up_3_0
	ffn_gate_3_0 -> gelu_3_0
	ffn_up_3_0 -> gelu_3_0
	gelu_3_0 -> ffn_down_3_0
	ffn_down_3_0 -> ffn_all_reduce_3
	layer_norm_3_1 -> ffn_gate_3_1
	layer_norm_3_1 -> ffn_up_3_1
	ffn_gate_3_1 -> gelu_3_1
	ffn_up_3_1 -> gelu_3_1
	gelu_3_1 -> ffn_down_3_1
	ffn_down_3_1 -> ffn_all_reduce_3
	layer_norm_3_2 -> ffn_gate_3_2
	layer_norm_3_2 -> ffn_up_3_2
	ffn_gate_3_2 -> gelu_3_2
	ffn_up_3_2 -> gelu_3_2
	gelu_3_2 -> ffn_down_3_2
	ffn_down_3_2 -> ffn_all_reduce_3
	layer_norm_3_3 -> ffn_gate_3_3
	layer_norm_3_3 -> ffn_up_3_3
	ffn_gate_3_3 -> gelu_3_3
	ffn_up_3_3 -> gelu_3_3
	gelu_3_3 -> ffn_down_3_3
	ffn_down_3_3 -> ffn_all_reduce_3
	layer_norm_3_4 -> ffn_gate_3_4
	layer_norm_3_4 -> ffn_up_3_4
	ffn_gate_3_4 -> gelu_3_4
	ffn_up_3_4 -> gelu_3_4
	gelu_3_4 -> ffn_down_3_4
	ffn_down_3_4 -> ffn_all_reduce_3
	layer_norm_3_5 -> ffn_gate_3_5
	layer_norm_3_5 -> ffn_up_3_5
	ffn_gate_3_5 -> gelu_3_5
	ffn_up_3_5 -> gelu_3_5
	gelu_3_5 -> ffn_down_3_5
	ffn_down_3_5 -> ffn_all_reduce_3
	layer_norm_3_6 -> ffn_gate_3_6
	layer_norm_3_6 -> ffn_up_3_6
	ffn_gate_3_6 -> gelu_3_6
	ffn_up_3_6 -> gelu_3_6
	gelu_3_6 -> ffn_down_3_6
	ffn_down_3_6 -> ffn_all_reduce_3
	layer_norm_3_7 -> ffn_gate_3_7
	layer_norm_3_7 -> ffn_up_3_7
	ffn_gate_3_7 -> gelu_3_7
	ffn_up_3_7 -> gelu_3_7
	gelu_3_7 -> ffn_down_3_7
	ffn_down_3_7 -> ffn_all_reduce_3
	residual_ffn_2 -> residual_ffn_3
	ffn_all_reduce_3 -> residual_ffn_3
	residual_ffn_3 -> output_proj_0
	output_proj_0 -> output_all_reduce
	residual_ffn_3 -> output_proj_1
	output_proj_1 -> output_all_reduce
	residual_ffn_3 -> output_proj_2
	output_proj_2 -> output_all_reduce
	residual_ffn_3 -> output_proj_3
	output_proj_3 -> output_all_reduce
	residual_ffn_3 -> output_proj_4
	output_proj_4 -> output_all_reduce
	residual_ffn_3 -> output_proj_5
	output_proj_5 -> output_all_reduce
	residual_ffn_3 -> output_proj_6
	output_proj_6 -> output_all_reduce
	residual_ffn_3 -> output_proj_7
	output_proj_7 -> output_all_reduce
	output_all_reduce -> output
}
