digraph FA_Pool_8GPU_Config {
	overlap=false rankdir=TB splines=spline
	node [shape=rectangle style=filled]
	input [label="Model Input
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=green fillcolor=lightgreen penwidth=2 shape=ellipse]
	embed_0 [label="Token Embedding
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=blue fillcolor=lightblue]
	embed_1 [label="Token Embedding
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=blue fillcolor=lightblue]
	embed_2 [label="Token Embedding
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=blue fillcolor=lightblue]
	embed_3 [label="Token Embedding
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=blue fillcolor=lightblue]
	embed_4 [label="Token Embedding
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=blue fillcolor=lightblue]
	embed_5 [label="Token Embedding
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=blue fillcolor=lightblue]
	embed_6 [label="Token Embedding
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=blue fillcolor=lightblue]
	embed_7 [label="Token Embedding
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=blue fillcolor=lightblue]
	pos_0 [label="Positional Encoding
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=blue fillcolor=lightblue]
	pos_1 [label="Positional Encoding
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=blue fillcolor=lightblue]
	pos_2 [label="Positional Encoding
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=blue fillcolor=lightblue]
	pos_3 [label="Positional Encoding
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=blue fillcolor=lightblue]
	pos_4 [label="Positional Encoding
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=blue fillcolor=lightblue]
	pos_5 [label="Positional Encoding
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=blue fillcolor=lightblue]
	pos_6 [label="Positional Encoding
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=blue fillcolor=lightblue]
	pos_7 [label="Positional Encoding
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=blue fillcolor=lightblue]
	prenorm_0_0 [label="Pre-Norm Layer 0
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=blue fillcolor=lightblue]
	prenorm_0_1 [label="Pre-Norm Layer 0
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=blue fillcolor=lightblue]
	prenorm_0_2 [label="Pre-Norm Layer 0
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=blue fillcolor=lightblue]
	prenorm_0_3 [label="Pre-Norm Layer 0
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=blue fillcolor=lightblue]
	prenorm_0_4 [label="Pre-Norm Layer 0
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=blue fillcolor=lightblue]
	prenorm_0_5 [label="Pre-Norm Layer 0
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=blue fillcolor=lightblue]
	prenorm_0_6 [label="Pre-Norm Layer 0
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=blue fillcolor=lightblue]
	prenorm_0_7 [label="Pre-Norm Layer 0
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=blue fillcolor=lightblue]
	prenorm_1_0 [label="Pre-Norm Layer 1
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=blue fillcolor=lightblue]
	prenorm_1_1 [label="Pre-Norm Layer 1
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=blue fillcolor=lightblue]
	prenorm_1_2 [label="Pre-Norm Layer 1
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=blue fillcolor=lightblue]
	prenorm_1_3 [label="Pre-Norm Layer 1
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=blue fillcolor=lightblue]
	prenorm_1_4 [label="Pre-Norm Layer 1
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=blue fillcolor=lightblue]
	prenorm_1_5 [label="Pre-Norm Layer 1
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=blue fillcolor=lightblue]
	prenorm_1_6 [label="Pre-Norm Layer 1
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=blue fillcolor=lightblue]
	prenorm_1_7 [label="Pre-Norm Layer 1
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=blue fillcolor=lightblue]
	prenorm_2_0 [label="Pre-Norm Layer 2
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=blue fillcolor=lightblue]
	prenorm_2_1 [label="Pre-Norm Layer 2
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=blue fillcolor=lightblue]
	prenorm_2_2 [label="Pre-Norm Layer 2
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=blue fillcolor=lightblue]
	prenorm_2_3 [label="Pre-Norm Layer 2
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=blue fillcolor=lightblue]
	prenorm_2_4 [label="Pre-Norm Layer 2
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=blue fillcolor=lightblue]
	prenorm_2_5 [label="Pre-Norm Layer 2
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=blue fillcolor=lightblue]
	prenorm_2_6 [label="Pre-Norm Layer 2
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=blue fillcolor=lightblue]
	prenorm_2_7 [label="Pre-Norm Layer 2
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=blue fillcolor=lightblue]
	prenorm_3_0 [label="Pre-Norm Layer 3
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=blue fillcolor=lightblue]
	prenorm_3_1 [label="Pre-Norm Layer 3
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=blue fillcolor=lightblue]
	prenorm_3_2 [label="Pre-Norm Layer 3
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=blue fillcolor=lightblue]
	prenorm_3_3 [label="Pre-Norm Layer 3
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=blue fillcolor=lightblue]
	prenorm_3_4 [label="Pre-Norm Layer 3
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=blue fillcolor=lightblue]
	prenorm_3_5 [label="Pre-Norm Layer 3
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=blue fillcolor=lightblue]
	prenorm_3_6 [label="Pre-Norm Layer 3
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=blue fillcolor=lightblue]
	prenorm_3_7 [label="Pre-Norm Layer 3
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=blue fillcolor=lightblue]
	attn_q_0_0_0 [label="Q Proj Layer 0
Head 0-3
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_0_0_0 [label="K Proj Layer 0
Head 0-3
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_0_0_0 [label="V Proj Layer 0
Head 0-3
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_0_1_0 [label="Q Proj Layer 0
Head 4-7
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_0_1_0 [label="K Proj Layer 0
Head 4-7
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_0_1_0 [label="V Proj Layer 0
Head 4-7
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_0_2_0 [label="Q Proj Layer 0
Head 8-11
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_0_2_0 [label="K Proj Layer 0
Head 8-11
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_0_2_0 [label="V Proj Layer 0
Head 8-11
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_0_3_0 [label="Q Proj Layer 0
Head 12-15
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_0_3_0 [label="K Proj Layer 0
Head 12-15
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_0_3_0 [label="V Proj Layer 0
Head 12-15
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_0_4_0 [label="Q Proj Layer 0
Head 16-19
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_0_4_0 [label="K Proj Layer 0
Head 16-19
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_0_4_0 [label="V Proj Layer 0
Head 16-19
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_0_5_0 [label="Q Proj Layer 0
Head 20-23
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_0_5_0 [label="K Proj Layer 0
Head 20-23
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_0_5_0 [label="V Proj Layer 0
Head 20-23
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_0_6_0 [label="Q Proj Layer 0
Head 24-27
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_0_6_0 [label="K Proj Layer 0
Head 24-27
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_0_6_0 [label="V Proj Layer 0
Head 24-27
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_0_7_0 [label="Q Proj Layer 0
Head 28-31
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_0_7_0 [label="K Proj Layer 0
Head 28-31
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_0_7_0 [label="V Proj Layer 0
Head 28-31
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_q_0_0_1 [label="Q Proj Layer 0
Head 1-4
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_0_0_1 [label="K Proj Layer 0
Head 1-4
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_0_0_1 [label="V Proj Layer 0
Head 1-4
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_0_1_1 [label="Q Proj Layer 0
Head 5-8
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_0_1_1 [label="K Proj Layer 0
Head 5-8
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_0_1_1 [label="V Proj Layer 0
Head 5-8
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_0_2_1 [label="Q Proj Layer 0
Head 9-12
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_0_2_1 [label="K Proj Layer 0
Head 9-12
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_0_2_1 [label="V Proj Layer 0
Head 9-12
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_0_3_1 [label="Q Proj Layer 0
Head 13-16
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_0_3_1 [label="K Proj Layer 0
Head 13-16
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_0_3_1 [label="V Proj Layer 0
Head 13-16
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_0_4_1 [label="Q Proj Layer 0
Head 17-20
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_0_4_1 [label="K Proj Layer 0
Head 17-20
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_0_4_1 [label="V Proj Layer 0
Head 17-20
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_0_5_1 [label="Q Proj Layer 0
Head 21-24
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_0_5_1 [label="K Proj Layer 0
Head 21-24
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_0_5_1 [label="V Proj Layer 0
Head 21-24
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_0_6_1 [label="Q Proj Layer 0
Head 25-28
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_0_6_1 [label="K Proj Layer 0
Head 25-28
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_0_6_1 [label="V Proj Layer 0
Head 25-28
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_0_7_1 [label="Q Proj Layer 0
Head 29-32
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_0_7_1 [label="K Proj Layer 0
Head 29-32
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_0_7_1 [label="V Proj Layer 0
Head 29-32
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_q_0_0_2 [label="Q Proj Layer 0
Head 2-5
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_0_0_2 [label="K Proj Layer 0
Head 2-5
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_0_0_2 [label="V Proj Layer 0
Head 2-5
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_0_1_2 [label="Q Proj Layer 0
Head 6-9
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_0_1_2 [label="K Proj Layer 0
Head 6-9
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_0_1_2 [label="V Proj Layer 0
Head 6-9
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_0_2_2 [label="Q Proj Layer 0
Head 10-13
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_0_2_2 [label="K Proj Layer 0
Head 10-13
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_0_2_2 [label="V Proj Layer 0
Head 10-13
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_0_3_2 [label="Q Proj Layer 0
Head 14-17
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_0_3_2 [label="K Proj Layer 0
Head 14-17
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_0_3_2 [label="V Proj Layer 0
Head 14-17
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_0_4_2 [label="Q Proj Layer 0
Head 18-21
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_0_4_2 [label="K Proj Layer 0
Head 18-21
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_0_4_2 [label="V Proj Layer 0
Head 18-21
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_0_5_2 [label="Q Proj Layer 0
Head 22-25
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_0_5_2 [label="K Proj Layer 0
Head 22-25
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_0_5_2 [label="V Proj Layer 0
Head 22-25
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_0_6_2 [label="Q Proj Layer 0
Head 26-29
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_0_6_2 [label="K Proj Layer 0
Head 26-29
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_0_6_2 [label="V Proj Layer 0
Head 26-29
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_0_7_2 [label="Q Proj Layer 0
Head 30-33
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_0_7_2 [label="K Proj Layer 0
Head 30-33
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_0_7_2 [label="V Proj Layer 0
Head 30-33
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_q_0_0_3 [label="Q Proj Layer 0
Head 3-6
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_0_0_3 [label="K Proj Layer 0
Head 3-6
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_0_0_3 [label="V Proj Layer 0
Head 3-6
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_0_1_3 [label="Q Proj Layer 0
Head 7-10
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_0_1_3 [label="K Proj Layer 0
Head 7-10
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_0_1_3 [label="V Proj Layer 0
Head 7-10
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_0_2_3 [label="Q Proj Layer 0
Head 11-14
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_0_2_3 [label="K Proj Layer 0
Head 11-14
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_0_2_3 [label="V Proj Layer 0
Head 11-14
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_0_3_3 [label="Q Proj Layer 0
Head 15-18
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_0_3_3 [label="K Proj Layer 0
Head 15-18
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_0_3_3 [label="V Proj Layer 0
Head 15-18
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_0_4_3 [label="Q Proj Layer 0
Head 19-22
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_0_4_3 [label="K Proj Layer 0
Head 19-22
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_0_4_3 [label="V Proj Layer 0
Head 19-22
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_0_5_3 [label="Q Proj Layer 0
Head 23-26
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_0_5_3 [label="K Proj Layer 0
Head 23-26
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_0_5_3 [label="V Proj Layer 0
Head 23-26
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_0_6_3 [label="Q Proj Layer 0
Head 27-30
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_0_6_3 [label="K Proj Layer 0
Head 27-30
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_0_6_3 [label="V Proj Layer 0
Head 27-30
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_0_7_3 [label="Q Proj Layer 0
Head 31-34
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_0_7_3 [label="K Proj Layer 0
Head 31-34
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_0_7_3 [label="V Proj Layer 0
Head 31-34
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	flash_attn_0_0 [label="Flash Attention Layer 0
GPU 0
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	flash_attn_0_1 [label="Flash Attention Layer 0
GPU 1
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	flash_attn_0_2 [label="Flash Attention Layer 0
GPU 2
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	flash_attn_0_3 [label="Flash Attention Layer 0
GPU 3
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	flash_attn_0_4 [label="Flash Attention Layer 0
GPU 4
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	flash_attn_0_5 [label="Flash Attention Layer 0
GPU 5
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	flash_attn_0_6 [label="Flash Attention Layer 0
GPU 6
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	flash_attn_0_7 [label="Flash Attention Layer 0
GPU 7
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_out_0_0 [label="Attention Output Layer 0
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=red fillcolor=lightcoral]
	attn_out_0_1 [label="Attention Output Layer 0
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=red fillcolor=lightcoral]
	attn_out_0_2 [label="Attention Output Layer 0
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=red fillcolor=lightcoral]
	attn_out_0_3 [label="Attention Output Layer 0
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=red fillcolor=lightcoral]
	attn_out_0_4 [label="Attention Output Layer 0
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=red fillcolor=lightcoral]
	attn_out_0_5 [label="Attention Output Layer 0
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=red fillcolor=lightcoral]
	attn_out_0_6 [label="Attention Output Layer 0
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=red fillcolor=lightcoral]
	attn_out_0_7 [label="Attention Output Layer 0
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=red fillcolor=lightcoral]
	attn_allreduce_0 [label="Attention All-Reduce
Layer 0
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=yellow penwidth=2 shape=parallelogram]
	attn_residual_0 [label="Attention Residual
Layer 0
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=orange shape=ellipse]
	attn_q_1_0_0 [label="Q Proj Layer 1
Head 0-3
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_1_0_0 [label="K Proj Layer 1
Head 0-3
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_1_0_0 [label="V Proj Layer 1
Head 0-3
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_1_1_0 [label="Q Proj Layer 1
Head 4-7
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_1_1_0 [label="K Proj Layer 1
Head 4-7
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_1_1_0 [label="V Proj Layer 1
Head 4-7
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_1_2_0 [label="Q Proj Layer 1
Head 8-11
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_1_2_0 [label="K Proj Layer 1
Head 8-11
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_1_2_0 [label="V Proj Layer 1
Head 8-11
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_1_3_0 [label="Q Proj Layer 1
Head 12-15
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_1_3_0 [label="K Proj Layer 1
Head 12-15
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_1_3_0 [label="V Proj Layer 1
Head 12-15
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_1_4_0 [label="Q Proj Layer 1
Head 16-19
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_1_4_0 [label="K Proj Layer 1
Head 16-19
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_1_4_0 [label="V Proj Layer 1
Head 16-19
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_1_5_0 [label="Q Proj Layer 1
Head 20-23
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_1_5_0 [label="K Proj Layer 1
Head 20-23
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_1_5_0 [label="V Proj Layer 1
Head 20-23
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_1_6_0 [label="Q Proj Layer 1
Head 24-27
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_1_6_0 [label="K Proj Layer 1
Head 24-27
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_1_6_0 [label="V Proj Layer 1
Head 24-27
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_1_7_0 [label="Q Proj Layer 1
Head 28-31
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_1_7_0 [label="K Proj Layer 1
Head 28-31
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_1_7_0 [label="V Proj Layer 1
Head 28-31
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_q_1_0_1 [label="Q Proj Layer 1
Head 1-4
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_1_0_1 [label="K Proj Layer 1
Head 1-4
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_1_0_1 [label="V Proj Layer 1
Head 1-4
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_1_1_1 [label="Q Proj Layer 1
Head 5-8
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_1_1_1 [label="K Proj Layer 1
Head 5-8
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_1_1_1 [label="V Proj Layer 1
Head 5-8
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_1_2_1 [label="Q Proj Layer 1
Head 9-12
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_1_2_1 [label="K Proj Layer 1
Head 9-12
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_1_2_1 [label="V Proj Layer 1
Head 9-12
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_1_3_1 [label="Q Proj Layer 1
Head 13-16
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_1_3_1 [label="K Proj Layer 1
Head 13-16
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_1_3_1 [label="V Proj Layer 1
Head 13-16
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_1_4_1 [label="Q Proj Layer 1
Head 17-20
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_1_4_1 [label="K Proj Layer 1
Head 17-20
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_1_4_1 [label="V Proj Layer 1
Head 17-20
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_1_5_1 [label="Q Proj Layer 1
Head 21-24
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_1_5_1 [label="K Proj Layer 1
Head 21-24
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_1_5_1 [label="V Proj Layer 1
Head 21-24
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_1_6_1 [label="Q Proj Layer 1
Head 25-28
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_1_6_1 [label="K Proj Layer 1
Head 25-28
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_1_6_1 [label="V Proj Layer 1
Head 25-28
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_1_7_1 [label="Q Proj Layer 1
Head 29-32
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_1_7_1 [label="K Proj Layer 1
Head 29-32
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_1_7_1 [label="V Proj Layer 1
Head 29-32
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_q_1_0_2 [label="Q Proj Layer 1
Head 2-5
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_1_0_2 [label="K Proj Layer 1
Head 2-5
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_1_0_2 [label="V Proj Layer 1
Head 2-5
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_1_1_2 [label="Q Proj Layer 1
Head 6-9
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_1_1_2 [label="K Proj Layer 1
Head 6-9
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_1_1_2 [label="V Proj Layer 1
Head 6-9
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_1_2_2 [label="Q Proj Layer 1
Head 10-13
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_1_2_2 [label="K Proj Layer 1
Head 10-13
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_1_2_2 [label="V Proj Layer 1
Head 10-13
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_1_3_2 [label="Q Proj Layer 1
Head 14-17
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_1_3_2 [label="K Proj Layer 1
Head 14-17
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_1_3_2 [label="V Proj Layer 1
Head 14-17
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_1_4_2 [label="Q Proj Layer 1
Head 18-21
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_1_4_2 [label="K Proj Layer 1
Head 18-21
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_1_4_2 [label="V Proj Layer 1
Head 18-21
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_1_5_2 [label="Q Proj Layer 1
Head 22-25
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_1_5_2 [label="K Proj Layer 1
Head 22-25
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_1_5_2 [label="V Proj Layer 1
Head 22-25
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_1_6_2 [label="Q Proj Layer 1
Head 26-29
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_1_6_2 [label="K Proj Layer 1
Head 26-29
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_1_6_2 [label="V Proj Layer 1
Head 26-29
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_1_7_2 [label="Q Proj Layer 1
Head 30-33
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_1_7_2 [label="K Proj Layer 1
Head 30-33
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_1_7_2 [label="V Proj Layer 1
Head 30-33
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_q_1_0_3 [label="Q Proj Layer 1
Head 3-6
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_1_0_3 [label="K Proj Layer 1
Head 3-6
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_1_0_3 [label="V Proj Layer 1
Head 3-6
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_1_1_3 [label="Q Proj Layer 1
Head 7-10
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_1_1_3 [label="K Proj Layer 1
Head 7-10
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_1_1_3 [label="V Proj Layer 1
Head 7-10
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_1_2_3 [label="Q Proj Layer 1
Head 11-14
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_1_2_3 [label="K Proj Layer 1
Head 11-14
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_1_2_3 [label="V Proj Layer 1
Head 11-14
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_1_3_3 [label="Q Proj Layer 1
Head 15-18
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_1_3_3 [label="K Proj Layer 1
Head 15-18
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_1_3_3 [label="V Proj Layer 1
Head 15-18
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_1_4_3 [label="Q Proj Layer 1
Head 19-22
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_1_4_3 [label="K Proj Layer 1
Head 19-22
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_1_4_3 [label="V Proj Layer 1
Head 19-22
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_1_5_3 [label="Q Proj Layer 1
Head 23-26
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_1_5_3 [label="K Proj Layer 1
Head 23-26
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_1_5_3 [label="V Proj Layer 1
Head 23-26
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_1_6_3 [label="Q Proj Layer 1
Head 27-30
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_1_6_3 [label="K Proj Layer 1
Head 27-30
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_1_6_3 [label="V Proj Layer 1
Head 27-30
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_1_7_3 [label="Q Proj Layer 1
Head 31-34
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_1_7_3 [label="K Proj Layer 1
Head 31-34
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_1_7_3 [label="V Proj Layer 1
Head 31-34
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	flash_attn_1_0 [label="Flash Attention Layer 1
GPU 0
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	flash_attn_1_1 [label="Flash Attention Layer 1
GPU 1
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	flash_attn_1_2 [label="Flash Attention Layer 1
GPU 2
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	flash_attn_1_3 [label="Flash Attention Layer 1
GPU 3
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	flash_attn_1_4 [label="Flash Attention Layer 1
GPU 4
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	flash_attn_1_5 [label="Flash Attention Layer 1
GPU 5
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	flash_attn_1_6 [label="Flash Attention Layer 1
GPU 6
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	flash_attn_1_7 [label="Flash Attention Layer 1
GPU 7
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_out_1_0 [label="Attention Output Layer 1
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=red fillcolor=lightcoral]
	attn_out_1_1 [label="Attention Output Layer 1
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=red fillcolor=lightcoral]
	attn_out_1_2 [label="Attention Output Layer 1
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=red fillcolor=lightcoral]
	attn_out_1_3 [label="Attention Output Layer 1
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=red fillcolor=lightcoral]
	attn_out_1_4 [label="Attention Output Layer 1
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=red fillcolor=lightcoral]
	attn_out_1_5 [label="Attention Output Layer 1
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=red fillcolor=lightcoral]
	attn_out_1_6 [label="Attention Output Layer 1
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=red fillcolor=lightcoral]
	attn_out_1_7 [label="Attention Output Layer 1
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=red fillcolor=lightcoral]
	attn_allreduce_1 [label="Attention All-Reduce
Layer 1
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=yellow penwidth=2 shape=parallelogram]
	attn_residual_1 [label="Attention Residual
Layer 1
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=orange shape=ellipse]
	attn_q_2_0_0 [label="Q Proj Layer 2
Head 0-3
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_2_0_0 [label="K Proj Layer 2
Head 0-3
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_2_0_0 [label="V Proj Layer 2
Head 0-3
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_2_1_0 [label="Q Proj Layer 2
Head 4-7
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_2_1_0 [label="K Proj Layer 2
Head 4-7
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_2_1_0 [label="V Proj Layer 2
Head 4-7
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_2_2_0 [label="Q Proj Layer 2
Head 8-11
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_2_2_0 [label="K Proj Layer 2
Head 8-11
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_2_2_0 [label="V Proj Layer 2
Head 8-11
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_2_3_0 [label="Q Proj Layer 2
Head 12-15
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_2_3_0 [label="K Proj Layer 2
Head 12-15
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_2_3_0 [label="V Proj Layer 2
Head 12-15
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_2_4_0 [label="Q Proj Layer 2
Head 16-19
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_2_4_0 [label="K Proj Layer 2
Head 16-19
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_2_4_0 [label="V Proj Layer 2
Head 16-19
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_2_5_0 [label="Q Proj Layer 2
Head 20-23
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_2_5_0 [label="K Proj Layer 2
Head 20-23
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_2_5_0 [label="V Proj Layer 2
Head 20-23
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_2_6_0 [label="Q Proj Layer 2
Head 24-27
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_2_6_0 [label="K Proj Layer 2
Head 24-27
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_2_6_0 [label="V Proj Layer 2
Head 24-27
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_2_7_0 [label="Q Proj Layer 2
Head 28-31
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_2_7_0 [label="K Proj Layer 2
Head 28-31
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_2_7_0 [label="V Proj Layer 2
Head 28-31
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_q_2_0_1 [label="Q Proj Layer 2
Head 1-4
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_2_0_1 [label="K Proj Layer 2
Head 1-4
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_2_0_1 [label="V Proj Layer 2
Head 1-4
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_2_1_1 [label="Q Proj Layer 2
Head 5-8
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_2_1_1 [label="K Proj Layer 2
Head 5-8
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_2_1_1 [label="V Proj Layer 2
Head 5-8
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_2_2_1 [label="Q Proj Layer 2
Head 9-12
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_2_2_1 [label="K Proj Layer 2
Head 9-12
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_2_2_1 [label="V Proj Layer 2
Head 9-12
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_2_3_1 [label="Q Proj Layer 2
Head 13-16
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_2_3_1 [label="K Proj Layer 2
Head 13-16
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_2_3_1 [label="V Proj Layer 2
Head 13-16
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_2_4_1 [label="Q Proj Layer 2
Head 17-20
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_2_4_1 [label="K Proj Layer 2
Head 17-20
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_2_4_1 [label="V Proj Layer 2
Head 17-20
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_2_5_1 [label="Q Proj Layer 2
Head 21-24
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_2_5_1 [label="K Proj Layer 2
Head 21-24
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_2_5_1 [label="V Proj Layer 2
Head 21-24
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_2_6_1 [label="Q Proj Layer 2
Head 25-28
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_2_6_1 [label="K Proj Layer 2
Head 25-28
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_2_6_1 [label="V Proj Layer 2
Head 25-28
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_2_7_1 [label="Q Proj Layer 2
Head 29-32
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_2_7_1 [label="K Proj Layer 2
Head 29-32
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_2_7_1 [label="V Proj Layer 2
Head 29-32
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_q_2_0_2 [label="Q Proj Layer 2
Head 2-5
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_2_0_2 [label="K Proj Layer 2
Head 2-5
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_2_0_2 [label="V Proj Layer 2
Head 2-5
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_2_1_2 [label="Q Proj Layer 2
Head 6-9
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_2_1_2 [label="K Proj Layer 2
Head 6-9
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_2_1_2 [label="V Proj Layer 2
Head 6-9
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_2_2_2 [label="Q Proj Layer 2
Head 10-13
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_2_2_2 [label="K Proj Layer 2
Head 10-13
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_2_2_2 [label="V Proj Layer 2
Head 10-13
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_2_3_2 [label="Q Proj Layer 2
Head 14-17
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_2_3_2 [label="K Proj Layer 2
Head 14-17
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_2_3_2 [label="V Proj Layer 2
Head 14-17
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_2_4_2 [label="Q Proj Layer 2
Head 18-21
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_2_4_2 [label="K Proj Layer 2
Head 18-21
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_2_4_2 [label="V Proj Layer 2
Head 18-21
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_2_5_2 [label="Q Proj Layer 2
Head 22-25
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_2_5_2 [label="K Proj Layer 2
Head 22-25
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_2_5_2 [label="V Proj Layer 2
Head 22-25
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_2_6_2 [label="Q Proj Layer 2
Head 26-29
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_2_6_2 [label="K Proj Layer 2
Head 26-29
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_2_6_2 [label="V Proj Layer 2
Head 26-29
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_2_7_2 [label="Q Proj Layer 2
Head 30-33
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_2_7_2 [label="K Proj Layer 2
Head 30-33
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_2_7_2 [label="V Proj Layer 2
Head 30-33
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_q_2_0_3 [label="Q Proj Layer 2
Head 3-6
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_2_0_3 [label="K Proj Layer 2
Head 3-6
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_2_0_3 [label="V Proj Layer 2
Head 3-6
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_2_1_3 [label="Q Proj Layer 2
Head 7-10
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_2_1_3 [label="K Proj Layer 2
Head 7-10
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_2_1_3 [label="V Proj Layer 2
Head 7-10
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_2_2_3 [label="Q Proj Layer 2
Head 11-14
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_2_2_3 [label="K Proj Layer 2
Head 11-14
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_2_2_3 [label="V Proj Layer 2
Head 11-14
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_2_3_3 [label="Q Proj Layer 2
Head 15-18
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_2_3_3 [label="K Proj Layer 2
Head 15-18
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_2_3_3 [label="V Proj Layer 2
Head 15-18
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_2_4_3 [label="Q Proj Layer 2
Head 19-22
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_2_4_3 [label="K Proj Layer 2
Head 19-22
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_2_4_3 [label="V Proj Layer 2
Head 19-22
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_2_5_3 [label="Q Proj Layer 2
Head 23-26
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_2_5_3 [label="K Proj Layer 2
Head 23-26
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_2_5_3 [label="V Proj Layer 2
Head 23-26
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_2_6_3 [label="Q Proj Layer 2
Head 27-30
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_2_6_3 [label="K Proj Layer 2
Head 27-30
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_2_6_3 [label="V Proj Layer 2
Head 27-30
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_2_7_3 [label="Q Proj Layer 2
Head 31-34
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_2_7_3 [label="K Proj Layer 2
Head 31-34
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_2_7_3 [label="V Proj Layer 2
Head 31-34
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	flash_attn_2_0 [label="Flash Attention Layer 2
GPU 0
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	flash_attn_2_1 [label="Flash Attention Layer 2
GPU 1
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	flash_attn_2_2 [label="Flash Attention Layer 2
GPU 2
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	flash_attn_2_3 [label="Flash Attention Layer 2
GPU 3
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	flash_attn_2_4 [label="Flash Attention Layer 2
GPU 4
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	flash_attn_2_5 [label="Flash Attention Layer 2
GPU 5
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	flash_attn_2_6 [label="Flash Attention Layer 2
GPU 6
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	flash_attn_2_7 [label="Flash Attention Layer 2
GPU 7
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_out_2_0 [label="Attention Output Layer 2
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=red fillcolor=lightcoral]
	attn_out_2_1 [label="Attention Output Layer 2
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=red fillcolor=lightcoral]
	attn_out_2_2 [label="Attention Output Layer 2
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=red fillcolor=lightcoral]
	attn_out_2_3 [label="Attention Output Layer 2
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=red fillcolor=lightcoral]
	attn_out_2_4 [label="Attention Output Layer 2
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=red fillcolor=lightcoral]
	attn_out_2_5 [label="Attention Output Layer 2
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=red fillcolor=lightcoral]
	attn_out_2_6 [label="Attention Output Layer 2
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=red fillcolor=lightcoral]
	attn_out_2_7 [label="Attention Output Layer 2
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=red fillcolor=lightcoral]
	attn_allreduce_2 [label="Attention All-Reduce
Layer 2
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=yellow penwidth=2 shape=parallelogram]
	attn_residual_2 [label="Attention Residual
Layer 2
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=orange shape=ellipse]
	attn_q_3_0_0 [label="Q Proj Layer 3
Head 0-3
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_3_0_0 [label="K Proj Layer 3
Head 0-3
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_3_0_0 [label="V Proj Layer 3
Head 0-3
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_3_1_0 [label="Q Proj Layer 3
Head 4-7
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_3_1_0 [label="K Proj Layer 3
Head 4-7
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_3_1_0 [label="V Proj Layer 3
Head 4-7
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_3_2_0 [label="Q Proj Layer 3
Head 8-11
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_3_2_0 [label="K Proj Layer 3
Head 8-11
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_3_2_0 [label="V Proj Layer 3
Head 8-11
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_3_3_0 [label="Q Proj Layer 3
Head 12-15
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_3_3_0 [label="K Proj Layer 3
Head 12-15
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_3_3_0 [label="V Proj Layer 3
Head 12-15
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_3_4_0 [label="Q Proj Layer 3
Head 16-19
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_3_4_0 [label="K Proj Layer 3
Head 16-19
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_3_4_0 [label="V Proj Layer 3
Head 16-19
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_3_5_0 [label="Q Proj Layer 3
Head 20-23
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_3_5_0 [label="K Proj Layer 3
Head 20-23
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_3_5_0 [label="V Proj Layer 3
Head 20-23
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_3_6_0 [label="Q Proj Layer 3
Head 24-27
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_3_6_0 [label="K Proj Layer 3
Head 24-27
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_3_6_0 [label="V Proj Layer 3
Head 24-27
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_3_7_0 [label="Q Proj Layer 3
Head 28-31
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_3_7_0 [label="K Proj Layer 3
Head 28-31
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_3_7_0 [label="V Proj Layer 3
Head 28-31
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_q_3_0_1 [label="Q Proj Layer 3
Head 1-4
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_3_0_1 [label="K Proj Layer 3
Head 1-4
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_3_0_1 [label="V Proj Layer 3
Head 1-4
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_3_1_1 [label="Q Proj Layer 3
Head 5-8
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_3_1_1 [label="K Proj Layer 3
Head 5-8
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_3_1_1 [label="V Proj Layer 3
Head 5-8
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_3_2_1 [label="Q Proj Layer 3
Head 9-12
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_3_2_1 [label="K Proj Layer 3
Head 9-12
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_3_2_1 [label="V Proj Layer 3
Head 9-12
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_3_3_1 [label="Q Proj Layer 3
Head 13-16
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_3_3_1 [label="K Proj Layer 3
Head 13-16
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_3_3_1 [label="V Proj Layer 3
Head 13-16
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_3_4_1 [label="Q Proj Layer 3
Head 17-20
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_3_4_1 [label="K Proj Layer 3
Head 17-20
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_3_4_1 [label="V Proj Layer 3
Head 17-20
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_3_5_1 [label="Q Proj Layer 3
Head 21-24
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_3_5_1 [label="K Proj Layer 3
Head 21-24
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_3_5_1 [label="V Proj Layer 3
Head 21-24
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_3_6_1 [label="Q Proj Layer 3
Head 25-28
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_3_6_1 [label="K Proj Layer 3
Head 25-28
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_3_6_1 [label="V Proj Layer 3
Head 25-28
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_3_7_1 [label="Q Proj Layer 3
Head 29-32
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_3_7_1 [label="K Proj Layer 3
Head 29-32
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_3_7_1 [label="V Proj Layer 3
Head 29-32
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_q_3_0_2 [label="Q Proj Layer 3
Head 2-5
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_3_0_2 [label="K Proj Layer 3
Head 2-5
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_3_0_2 [label="V Proj Layer 3
Head 2-5
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_3_1_2 [label="Q Proj Layer 3
Head 6-9
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_3_1_2 [label="K Proj Layer 3
Head 6-9
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_3_1_2 [label="V Proj Layer 3
Head 6-9
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_3_2_2 [label="Q Proj Layer 3
Head 10-13
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_3_2_2 [label="K Proj Layer 3
Head 10-13
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_3_2_2 [label="V Proj Layer 3
Head 10-13
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_3_3_2 [label="Q Proj Layer 3
Head 14-17
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_3_3_2 [label="K Proj Layer 3
Head 14-17
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_3_3_2 [label="V Proj Layer 3
Head 14-17
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_3_4_2 [label="Q Proj Layer 3
Head 18-21
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_3_4_2 [label="K Proj Layer 3
Head 18-21
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_3_4_2 [label="V Proj Layer 3
Head 18-21
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_3_5_2 [label="Q Proj Layer 3
Head 22-25
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_3_5_2 [label="K Proj Layer 3
Head 22-25
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_3_5_2 [label="V Proj Layer 3
Head 22-25
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_3_6_2 [label="Q Proj Layer 3
Head 26-29
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_3_6_2 [label="K Proj Layer 3
Head 26-29
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_3_6_2 [label="V Proj Layer 3
Head 26-29
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_3_7_2 [label="Q Proj Layer 3
Head 30-33
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_3_7_2 [label="K Proj Layer 3
Head 30-33
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_3_7_2 [label="V Proj Layer 3
Head 30-33
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_q_3_0_3 [label="Q Proj Layer 3
Head 3-6
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_k_3_0_3 [label="K Proj Layer 3
Head 3-6
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_v_3_0_3 [label="V Proj Layer 3
Head 3-6
[batch_size=1024, seq_len=?, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	attn_q_3_1_3 [label="Q Proj Layer 3
Head 7-10
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_k_3_1_3 [label="K Proj Layer 3
Head 7-10
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_v_3_1_3 [label="V Proj Layer 3
Head 7-10
[batch_size=1024, seq_len=?, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	attn_q_3_2_3 [label="Q Proj Layer 3
Head 11-14
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_k_3_2_3 [label="K Proj Layer 3
Head 11-14
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_v_3_2_3 [label="V Proj Layer 3
Head 11-14
[batch_size=1024, seq_len=?, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	attn_q_3_3_3 [label="Q Proj Layer 3
Head 15-18
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_k_3_3_3 [label="K Proj Layer 3
Head 15-18
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_v_3_3_3 [label="V Proj Layer 3
Head 15-18
[batch_size=1024, seq_len=?, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	attn_q_3_4_3 [label="Q Proj Layer 3
Head 19-22
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_k_3_4_3 [label="K Proj Layer 3
Head 19-22
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_v_3_4_3 [label="V Proj Layer 3
Head 19-22
[batch_size=1024, seq_len=?, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	attn_q_3_5_3 [label="Q Proj Layer 3
Head 23-26
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_k_3_5_3 [label="K Proj Layer 3
Head 23-26
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_v_3_5_3 [label="V Proj Layer 3
Head 23-26
[batch_size=1024, seq_len=?, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	attn_q_3_6_3 [label="Q Proj Layer 3
Head 27-30
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_k_3_6_3 [label="K Proj Layer 3
Head 27-30
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_v_3_6_3 [label="V Proj Layer 3
Head 27-30
[batch_size=1024, seq_len=?, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	attn_q_3_7_3 [label="Q Proj Layer 3
Head 31-34
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_k_3_7_3 [label="K Proj Layer 3
Head 31-34
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_v_3_7_3 [label="V Proj Layer 3
Head 31-34
[batch_size=1024, seq_len=?, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	flash_attn_3_0 [label="Flash Attention Layer 3
GPU 0
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 0" color=red fillcolor=lightcoral]
	flash_attn_3_1 [label="Flash Attention Layer 3
GPU 1
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 1" color=red fillcolor=lightcoral]
	flash_attn_3_2 [label="Flash Attention Layer 3
GPU 2
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 2" color=red fillcolor=lightcoral]
	flash_attn_3_3 [label="Flash Attention Layer 3
GPU 3
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 3" color=red fillcolor=lightcoral]
	flash_attn_3_4 [label="Flash Attention Layer 3
GPU 4
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 4" color=red fillcolor=lightcoral]
	flash_attn_3_5 [label="Flash Attention Layer 3
GPU 5
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 5" color=red fillcolor=lightcoral]
	flash_attn_3_6 [label="Flash Attention Layer 3
GPU 6
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 6" color=red fillcolor=lightcoral]
	flash_attn_3_7 [label="Flash Attention Layer 3
GPU 7
[batch_size=1024, seq_len=?, heads=4, d_k=128]
GPU: 7" color=red fillcolor=lightcoral]
	attn_out_3_0 [label="Attention Output Layer 3
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=red fillcolor=lightcoral]
	attn_out_3_1 [label="Attention Output Layer 3
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=red fillcolor=lightcoral]
	attn_out_3_2 [label="Attention Output Layer 3
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=red fillcolor=lightcoral]
	attn_out_3_3 [label="Attention Output Layer 3
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=red fillcolor=lightcoral]
	attn_out_3_4 [label="Attention Output Layer 3
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=red fillcolor=lightcoral]
	attn_out_3_5 [label="Attention Output Layer 3
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=red fillcolor=lightcoral]
	attn_out_3_6 [label="Attention Output Layer 3
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=red fillcolor=lightcoral]
	attn_out_3_7 [label="Attention Output Layer 3
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=red fillcolor=lightcoral]
	attn_allreduce_3 [label="Attention All-Reduce
Layer 3
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=yellow penwidth=2 shape=parallelogram]
	attn_residual_3 [label="Attention Residual
Layer 3
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=orange shape=ellipse]
	ffn_gate_0_0 [label="FFN Gate Layer 0
Shard 0
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 0" color=blue fillcolor=lightblue]
	ffn_gate_0_1 [label="FFN Gate Layer 0
Shard 1
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 1" color=blue fillcolor=lightblue]
	ffn_gate_0_2 [label="FFN Gate Layer 0
Shard 2
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 2" color=blue fillcolor=lightblue]
	ffn_gate_0_3 [label="FFN Gate Layer 0
Shard 3
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 3" color=blue fillcolor=lightblue]
	ffn_gate_0_4 [label="FFN Gate Layer 0
Shard 4
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 4" color=blue fillcolor=lightblue]
	ffn_gate_0_5 [label="FFN Gate Layer 0
Shard 5
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 5" color=blue fillcolor=lightblue]
	ffn_gate_0_6 [label="FFN Gate Layer 0
Shard 6
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 6" color=blue fillcolor=lightblue]
	ffn_gate_0_7 [label="FFN Gate Layer 0
Shard 7
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 7" color=blue fillcolor=lightblue]
	ffn_up_0_0 [label="FFN Up Layer 0
Shard 0
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 0" color=blue fillcolor=lightblue]
	ffn_up_0_1 [label="FFN Up Layer 0
Shard 1
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 1" color=blue fillcolor=lightblue]
	ffn_up_0_2 [label="FFN Up Layer 0
Shard 2
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 2" color=blue fillcolor=lightblue]
	ffn_up_0_3 [label="FFN Up Layer 0
Shard 3
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 3" color=blue fillcolor=lightblue]
	ffn_up_0_4 [label="FFN Up Layer 0
Shard 4
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 4" color=blue fillcolor=lightblue]
	ffn_up_0_5 [label="FFN Up Layer 0
Shard 5
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 5" color=blue fillcolor=lightblue]
	ffn_up_0_6 [label="FFN Up Layer 0
Shard 6
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 6" color=blue fillcolor=lightblue]
	ffn_up_0_7 [label="FFN Up Layer 0
Shard 7
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 7" color=blue fillcolor=lightblue]
	gelu_0_0 [label="GELU Layer 0
Shard 0
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 0" color=blue fillcolor=lightblue]
	gelu_0_1 [label="GELU Layer 0
Shard 1
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 1" color=blue fillcolor=lightblue]
	gelu_0_2 [label="GELU Layer 0
Shard 2
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 2" color=blue fillcolor=lightblue]
	gelu_0_3 [label="GELU Layer 0
Shard 3
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 3" color=blue fillcolor=lightblue]
	gelu_0_4 [label="GELU Layer 0
Shard 4
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 4" color=blue fillcolor=lightblue]
	gelu_0_5 [label="GELU Layer 0
Shard 5
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 5" color=blue fillcolor=lightblue]
	gelu_0_6 [label="GELU Layer 0
Shard 6
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 6" color=blue fillcolor=lightblue]
	gelu_0_7 [label="GELU Layer 0
Shard 7
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 7" color=blue fillcolor=lightblue]
	ffn_down_0_0 [label="FFN Down Layer 0
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=blue fillcolor=lightblue]
	ffn_down_0_1 [label="FFN Down Layer 0
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=blue fillcolor=lightblue]
	ffn_down_0_2 [label="FFN Down Layer 0
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=blue fillcolor=lightblue]
	ffn_down_0_3 [label="FFN Down Layer 0
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=blue fillcolor=lightblue]
	ffn_down_0_4 [label="FFN Down Layer 0
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=blue fillcolor=lightblue]
	ffn_down_0_5 [label="FFN Down Layer 0
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=blue fillcolor=lightblue]
	ffn_down_0_6 [label="FFN Down Layer 0
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=blue fillcolor=lightblue]
	ffn_down_0_7 [label="FFN Down Layer 0
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=blue fillcolor=lightblue]
	ffn_allreduce_0 [label="FFN All-Reduce
Layer 0
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=yellow penwidth=2 shape=parallelogram]
	ffn_residual_0 [label="FFN Residual
Layer 0
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=orange shape=ellipse]
	ffn_gate_1_0 [label="FFN Gate Layer 1
Shard 0
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 0" color=blue fillcolor=lightblue]
	ffn_gate_1_1 [label="FFN Gate Layer 1
Shard 1
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 1" color=blue fillcolor=lightblue]
	ffn_gate_1_2 [label="FFN Gate Layer 1
Shard 2
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 2" color=blue fillcolor=lightblue]
	ffn_gate_1_3 [label="FFN Gate Layer 1
Shard 3
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 3" color=blue fillcolor=lightblue]
	ffn_gate_1_4 [label="FFN Gate Layer 1
Shard 4
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 4" color=blue fillcolor=lightblue]
	ffn_gate_1_5 [label="FFN Gate Layer 1
Shard 5
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 5" color=blue fillcolor=lightblue]
	ffn_gate_1_6 [label="FFN Gate Layer 1
Shard 6
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 6" color=blue fillcolor=lightblue]
	ffn_gate_1_7 [label="FFN Gate Layer 1
Shard 7
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 7" color=blue fillcolor=lightblue]
	ffn_up_1_0 [label="FFN Up Layer 1
Shard 0
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 0" color=blue fillcolor=lightblue]
	ffn_up_1_1 [label="FFN Up Layer 1
Shard 1
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 1" color=blue fillcolor=lightblue]
	ffn_up_1_2 [label="FFN Up Layer 1
Shard 2
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 2" color=blue fillcolor=lightblue]
	ffn_up_1_3 [label="FFN Up Layer 1
Shard 3
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 3" color=blue fillcolor=lightblue]
	ffn_up_1_4 [label="FFN Up Layer 1
Shard 4
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 4" color=blue fillcolor=lightblue]
	ffn_up_1_5 [label="FFN Up Layer 1
Shard 5
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 5" color=blue fillcolor=lightblue]
	ffn_up_1_6 [label="FFN Up Layer 1
Shard 6
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 6" color=blue fillcolor=lightblue]
	ffn_up_1_7 [label="FFN Up Layer 1
Shard 7
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 7" color=blue fillcolor=lightblue]
	gelu_1_0 [label="GELU Layer 1
Shard 0
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 0" color=blue fillcolor=lightblue]
	gelu_1_1 [label="GELU Layer 1
Shard 1
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 1" color=blue fillcolor=lightblue]
	gelu_1_2 [label="GELU Layer 1
Shard 2
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 2" color=blue fillcolor=lightblue]
	gelu_1_3 [label="GELU Layer 1
Shard 3
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 3" color=blue fillcolor=lightblue]
	gelu_1_4 [label="GELU Layer 1
Shard 4
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 4" color=blue fillcolor=lightblue]
	gelu_1_5 [label="GELU Layer 1
Shard 5
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 5" color=blue fillcolor=lightblue]
	gelu_1_6 [label="GELU Layer 1
Shard 6
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 6" color=blue fillcolor=lightblue]
	gelu_1_7 [label="GELU Layer 1
Shard 7
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 7" color=blue fillcolor=lightblue]
	ffn_down_1_0 [label="FFN Down Layer 1
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=blue fillcolor=lightblue]
	ffn_down_1_1 [label="FFN Down Layer 1
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=blue fillcolor=lightblue]
	ffn_down_1_2 [label="FFN Down Layer 1
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=blue fillcolor=lightblue]
	ffn_down_1_3 [label="FFN Down Layer 1
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=blue fillcolor=lightblue]
	ffn_down_1_4 [label="FFN Down Layer 1
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=blue fillcolor=lightblue]
	ffn_down_1_5 [label="FFN Down Layer 1
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=blue fillcolor=lightblue]
	ffn_down_1_6 [label="FFN Down Layer 1
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=blue fillcolor=lightblue]
	ffn_down_1_7 [label="FFN Down Layer 1
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=blue fillcolor=lightblue]
	ffn_allreduce_1 [label="FFN All-Reduce
Layer 1
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=yellow penwidth=2 shape=parallelogram]
	ffn_residual_1 [label="FFN Residual
Layer 1
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=orange shape=ellipse]
	ffn_gate_2_0 [label="FFN Gate Layer 2
Shard 0
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 0" color=blue fillcolor=lightblue]
	ffn_gate_2_1 [label="FFN Gate Layer 2
Shard 1
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 1" color=blue fillcolor=lightblue]
	ffn_gate_2_2 [label="FFN Gate Layer 2
Shard 2
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 2" color=blue fillcolor=lightblue]
	ffn_gate_2_3 [label="FFN Gate Layer 2
Shard 3
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 3" color=blue fillcolor=lightblue]
	ffn_gate_2_4 [label="FFN Gate Layer 2
Shard 4
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 4" color=blue fillcolor=lightblue]
	ffn_gate_2_5 [label="FFN Gate Layer 2
Shard 5
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 5" color=blue fillcolor=lightblue]
	ffn_gate_2_6 [label="FFN Gate Layer 2
Shard 6
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 6" color=blue fillcolor=lightblue]
	ffn_gate_2_7 [label="FFN Gate Layer 2
Shard 7
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 7" color=blue fillcolor=lightblue]
	ffn_up_2_0 [label="FFN Up Layer 2
Shard 0
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 0" color=blue fillcolor=lightblue]
	ffn_up_2_1 [label="FFN Up Layer 2
Shard 1
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 1" color=blue fillcolor=lightblue]
	ffn_up_2_2 [label="FFN Up Layer 2
Shard 2
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 2" color=blue fillcolor=lightblue]
	ffn_up_2_3 [label="FFN Up Layer 2
Shard 3
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 3" color=blue fillcolor=lightblue]
	ffn_up_2_4 [label="FFN Up Layer 2
Shard 4
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 4" color=blue fillcolor=lightblue]
	ffn_up_2_5 [label="FFN Up Layer 2
Shard 5
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 5" color=blue fillcolor=lightblue]
	ffn_up_2_6 [label="FFN Up Layer 2
Shard 6
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 6" color=blue fillcolor=lightblue]
	ffn_up_2_7 [label="FFN Up Layer 2
Shard 7
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 7" color=blue fillcolor=lightblue]
	gelu_2_0 [label="GELU Layer 2
Shard 0
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 0" color=blue fillcolor=lightblue]
	gelu_2_1 [label="GELU Layer 2
Shard 1
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 1" color=blue fillcolor=lightblue]
	gelu_2_2 [label="GELU Layer 2
Shard 2
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 2" color=blue fillcolor=lightblue]
	gelu_2_3 [label="GELU Layer 2
Shard 3
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 3" color=blue fillcolor=lightblue]
	gelu_2_4 [label="GELU Layer 2
Shard 4
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 4" color=blue fillcolor=lightblue]
	gelu_2_5 [label="GELU Layer 2
Shard 5
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 5" color=blue fillcolor=lightblue]
	gelu_2_6 [label="GELU Layer 2
Shard 6
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 6" color=blue fillcolor=lightblue]
	gelu_2_7 [label="GELU Layer 2
Shard 7
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 7" color=blue fillcolor=lightblue]
	ffn_down_2_0 [label="FFN Down Layer 2
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=blue fillcolor=lightblue]
	ffn_down_2_1 [label="FFN Down Layer 2
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=blue fillcolor=lightblue]
	ffn_down_2_2 [label="FFN Down Layer 2
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=blue fillcolor=lightblue]
	ffn_down_2_3 [label="FFN Down Layer 2
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=blue fillcolor=lightblue]
	ffn_down_2_4 [label="FFN Down Layer 2
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=blue fillcolor=lightblue]
	ffn_down_2_5 [label="FFN Down Layer 2
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=blue fillcolor=lightblue]
	ffn_down_2_6 [label="FFN Down Layer 2
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=blue fillcolor=lightblue]
	ffn_down_2_7 [label="FFN Down Layer 2
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=blue fillcolor=lightblue]
	ffn_allreduce_2 [label="FFN All-Reduce
Layer 2
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=yellow penwidth=2 shape=parallelogram]
	ffn_residual_2 [label="FFN Residual
Layer 2
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=orange shape=ellipse]
	ffn_gate_3_0 [label="FFN Gate Layer 3
Shard 0
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 0" color=blue fillcolor=lightblue]
	ffn_gate_3_1 [label="FFN Gate Layer 3
Shard 1
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 1" color=blue fillcolor=lightblue]
	ffn_gate_3_2 [label="FFN Gate Layer 3
Shard 2
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 2" color=blue fillcolor=lightblue]
	ffn_gate_3_3 [label="FFN Gate Layer 3
Shard 3
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 3" color=blue fillcolor=lightblue]
	ffn_gate_3_4 [label="FFN Gate Layer 3
Shard 4
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 4" color=blue fillcolor=lightblue]
	ffn_gate_3_5 [label="FFN Gate Layer 3
Shard 5
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 5" color=blue fillcolor=lightblue]
	ffn_gate_3_6 [label="FFN Gate Layer 3
Shard 6
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 6" color=blue fillcolor=lightblue]
	ffn_gate_3_7 [label="FFN Gate Layer 3
Shard 7
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 7" color=blue fillcolor=lightblue]
	ffn_up_3_0 [label="FFN Up Layer 3
Shard 0
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 0" color=blue fillcolor=lightblue]
	ffn_up_3_1 [label="FFN Up Layer 3
Shard 1
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 1" color=blue fillcolor=lightblue]
	ffn_up_3_2 [label="FFN Up Layer 3
Shard 2
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 2" color=blue fillcolor=lightblue]
	ffn_up_3_3 [label="FFN Up Layer 3
Shard 3
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 3" color=blue fillcolor=lightblue]
	ffn_up_3_4 [label="FFN Up Layer 3
Shard 4
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 4" color=blue fillcolor=lightblue]
	ffn_up_3_5 [label="FFN Up Layer 3
Shard 5
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 5" color=blue fillcolor=lightblue]
	ffn_up_3_6 [label="FFN Up Layer 3
Shard 6
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 6" color=blue fillcolor=lightblue]
	ffn_up_3_7 [label="FFN Up Layer 3
Shard 7
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 7" color=blue fillcolor=lightblue]
	gelu_3_0 [label="GELU Layer 3
Shard 0
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 0" color=blue fillcolor=lightblue]
	gelu_3_1 [label="GELU Layer 3
Shard 1
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 1" color=blue fillcolor=lightblue]
	gelu_3_2 [label="GELU Layer 3
Shard 2
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 2" color=blue fillcolor=lightblue]
	gelu_3_3 [label="GELU Layer 3
Shard 3
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 3" color=blue fillcolor=lightblue]
	gelu_3_4 [label="GELU Layer 3
Shard 4
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 4" color=blue fillcolor=lightblue]
	gelu_3_5 [label="GELU Layer 3
Shard 5
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 5" color=blue fillcolor=lightblue]
	gelu_3_6 [label="GELU Layer 3
Shard 6
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 6" color=blue fillcolor=lightblue]
	gelu_3_7 [label="GELU Layer 3
Shard 7
[batch_size=1024, seq_len=?, ffn_dim=2048]
GPU: 7" color=blue fillcolor=lightblue]
	ffn_down_3_0 [label="FFN Down Layer 3
Shard 0
[batch_size=1024, seq_len=?, d_model=512]
GPU: 0" color=blue fillcolor=lightblue]
	ffn_down_3_1 [label="FFN Down Layer 3
Shard 1
[batch_size=1024, seq_len=?, d_model=512]
GPU: 1" color=blue fillcolor=lightblue]
	ffn_down_3_2 [label="FFN Down Layer 3
Shard 2
[batch_size=1024, seq_len=?, d_model=512]
GPU: 2" color=blue fillcolor=lightblue]
	ffn_down_3_3 [label="FFN Down Layer 3
Shard 3
[batch_size=1024, seq_len=?, d_model=512]
GPU: 3" color=blue fillcolor=lightblue]
	ffn_down_3_4 [label="FFN Down Layer 3
Shard 4
[batch_size=1024, seq_len=?, d_model=512]
GPU: 4" color=blue fillcolor=lightblue]
	ffn_down_3_5 [label="FFN Down Layer 3
Shard 5
[batch_size=1024, seq_len=?, d_model=512]
GPU: 5" color=blue fillcolor=lightblue]
	ffn_down_3_6 [label="FFN Down Layer 3
Shard 6
[batch_size=1024, seq_len=?, d_model=512]
GPU: 6" color=blue fillcolor=lightblue]
	ffn_down_3_7 [label="FFN Down Layer 3
Shard 7
[batch_size=1024, seq_len=?, d_model=512]
GPU: 7" color=blue fillcolor=lightblue]
	ffn_allreduce_3 [label="FFN All-Reduce
Layer 3
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=yellow penwidth=2 shape=parallelogram]
	ffn_residual_3 [label="FFN Residual
Layer 3
[batch_size=1024, seq_len=?, d_model=4096]
GPU: 0-7" color=orange fillcolor=orange shape=ellipse]
	output [label="Model Output
[batch_size=1024, seq_len=?, vocab_size=full_vocab]
GPU: 0-7" color=green fillcolor=lightgreen penwidth=2 shape=ellipse]
	input -> embed_0
	input -> embed_1
	input -> embed_2
	input -> embed_3
	input -> embed_4
	input -> embed_5
	input -> embed_6
	input -> embed_7
	embed_0 -> pos_0
	embed_1 -> pos_1
	embed_2 -> pos_2
	embed_3 -> pos_3
	embed_4 -> pos_4
	embed_5 -> pos_5
	embed_6 -> pos_6
	embed_7 -> pos_7
	pos_0 -> prenorm_0_0
	pos_1 -> prenorm_0_1
	pos_2 -> prenorm_0_2
	pos_3 -> prenorm_0_3
	pos_4 -> prenorm_0_4
	pos_5 -> prenorm_0_5
	pos_6 -> prenorm_0_6
	pos_7 -> prenorm_0_7
	prenorm_0_0 -> attn_q_0_0_0
	prenorm_0_0 -> attn_k_0_0_0
	prenorm_0_0 -> attn_v_0_0_0
	attn_q_0_0_0 -> flash_attn_0_0
	attn_k_0_0_0 -> flash_attn_0_0
	attn_v_0_0_0 -> flash_attn_0_0
	attn_q_0_0_1 -> flash_attn_0_0
	attn_k_0_0_1 -> flash_attn_0_0
	attn_v_0_0_1 -> flash_attn_0_0
	attn_q_0_0_2 -> flash_attn_0_0
	attn_k_0_0_2 -> flash_attn_0_0
	attn_v_0_0_2 -> flash_attn_0_0
	attn_q_0_0_3 -> flash_attn_0_0
	attn_k_0_0_3 -> flash_attn_0_0
	attn_v_0_0_3 -> flash_attn_0_0
	flash_attn_0_0 -> attn_out_0_0
	attn_out_0_0 -> attn_allreduce_0
	prenorm_0_1 -> attn_q_0_1_0
	prenorm_0_1 -> attn_k_0_1_0
	prenorm_0_1 -> attn_v_0_1_0
	attn_q_0_1_0 -> flash_attn_0_1
	attn_k_0_1_0 -> flash_attn_0_1
	attn_v_0_1_0 -> flash_attn_0_1
	attn_q_0_1_1 -> flash_attn_0_1
	attn_k_0_1_1 -> flash_attn_0_1
	attn_v_0_1_1 -> flash_attn_0_1
	attn_q_0_1_2 -> flash_attn_0_1
	attn_k_0_1_2 -> flash_attn_0_1
	attn_v_0_1_2 -> flash_attn_0_1
	attn_q_0_1_3 -> flash_attn_0_1
	attn_k_0_1_3 -> flash_attn_0_1
	attn_v_0_1_3 -> flash_attn_0_1
	flash_attn_0_1 -> attn_out_0_1
	attn_out_0_1 -> attn_allreduce_0
	prenorm_0_2 -> attn_q_0_2_0
	prenorm_0_2 -> attn_k_0_2_0
	prenorm_0_2 -> attn_v_0_2_0
	attn_q_0_2_0 -> flash_attn_0_2
	attn_k_0_2_0 -> flash_attn_0_2
	attn_v_0_2_0 -> flash_attn_0_2
	attn_q_0_2_1 -> flash_attn_0_2
	attn_k_0_2_1 -> flash_attn_0_2
	attn_v_0_2_1 -> flash_attn_0_2
	attn_q_0_2_2 -> flash_attn_0_2
	attn_k_0_2_2 -> flash_attn_0_2
	attn_v_0_2_2 -> flash_attn_0_2
	attn_q_0_2_3 -> flash_attn_0_2
	attn_k_0_2_3 -> flash_attn_0_2
	attn_v_0_2_3 -> flash_attn_0_2
	flash_attn_0_2 -> attn_out_0_2
	attn_out_0_2 -> attn_allreduce_0
	prenorm_0_3 -> attn_q_0_3_0
	prenorm_0_3 -> attn_k_0_3_0
	prenorm_0_3 -> attn_v_0_3_0
	attn_q_0_3_0 -> flash_attn_0_3
	attn_k_0_3_0 -> flash_attn_0_3
	attn_v_0_3_0 -> flash_attn_0_3
	attn_q_0_3_1 -> flash_attn_0_3
	attn_k_0_3_1 -> flash_attn_0_3
	attn_v_0_3_1 -> flash_attn_0_3
	attn_q_0_3_2 -> flash_attn_0_3
	attn_k_0_3_2 -> flash_attn_0_3
	attn_v_0_3_2 -> flash_attn_0_3
	attn_q_0_3_3 -> flash_attn_0_3
	attn_k_0_3_3 -> flash_attn_0_3
	attn_v_0_3_3 -> flash_attn_0_3
	flash_attn_0_3 -> attn_out_0_3
	attn_out_0_3 -> attn_allreduce_0
	prenorm_0_4 -> attn_q_0_4_0
	prenorm_0_4 -> attn_k_0_4_0
	prenorm_0_4 -> attn_v_0_4_0
	attn_q_0_4_0 -> flash_attn_0_4
	attn_k_0_4_0 -> flash_attn_0_4
	attn_v_0_4_0 -> flash_attn_0_4
	attn_q_0_4_1 -> flash_attn_0_4
	attn_k_0_4_1 -> flash_attn_0_4
	attn_v_0_4_1 -> flash_attn_0_4
	attn_q_0_4_2 -> flash_attn_0_4
	attn_k_0_4_2 -> flash_attn_0_4
	attn_v_0_4_2 -> flash_attn_0_4
	attn_q_0_4_3 -> flash_attn_0_4
	attn_k_0_4_3 -> flash_attn_0_4
	attn_v_0_4_3 -> flash_attn_0_4
	flash_attn_0_4 -> attn_out_0_4
	attn_out_0_4 -> attn_allreduce_0
	prenorm_0_5 -> attn_q_0_5_0
	prenorm_0_5 -> attn_k_0_5_0
	prenorm_0_5 -> attn_v_0_5_0
	attn_q_0_5_0 -> flash_attn_0_5
	attn_k_0_5_0 -> flash_attn_0_5
	attn_v_0_5_0 -> flash_attn_0_5
	attn_q_0_5_1 -> flash_attn_0_5
	attn_k_0_5_1 -> flash_attn_0_5
	attn_v_0_5_1 -> flash_attn_0_5
	attn_q_0_5_2 -> flash_attn_0_5
	attn_k_0_5_2 -> flash_attn_0_5
	attn_v_0_5_2 -> flash_attn_0_5
	attn_q_0_5_3 -> flash_attn_0_5
	attn_k_0_5_3 -> flash_attn_0_5
	attn_v_0_5_3 -> flash_attn_0_5
	flash_attn_0_5 -> attn_out_0_5
	attn_out_0_5 -> attn_allreduce_0
	prenorm_0_6 -> attn_q_0_6_0
	prenorm_0_6 -> attn_k_0_6_0
	prenorm_0_6 -> attn_v_0_6_0
	attn_q_0_6_0 -> flash_attn_0_6
	attn_k_0_6_0 -> flash_attn_0_6
	attn_v_0_6_0 -> flash_attn_0_6
	attn_q_0_6_1 -> flash_attn_0_6
	attn_k_0_6_1 -> flash_attn_0_6
	attn_v_0_6_1 -> flash_attn_0_6
	attn_q_0_6_2 -> flash_attn_0_6
	attn_k_0_6_2 -> flash_attn_0_6
	attn_v_0_6_2 -> flash_attn_0_6
	attn_q_0_6_3 -> flash_attn_0_6
	attn_k_0_6_3 -> flash_attn_0_6
	attn_v_0_6_3 -> flash_attn_0_6
	flash_attn_0_6 -> attn_out_0_6
	attn_out_0_6 -> attn_allreduce_0
	prenorm_0_7 -> attn_q_0_7_0
	prenorm_0_7 -> attn_k_0_7_0
	prenorm_0_7 -> attn_v_0_7_0
	attn_q_0_7_0 -> flash_attn_0_7
	attn_k_0_7_0 -> flash_attn_0_7
	attn_v_0_7_0 -> flash_attn_0_7
	attn_q_0_7_1 -> flash_attn_0_7
	attn_k_0_7_1 -> flash_attn_0_7
	attn_v_0_7_1 -> flash_attn_0_7
	attn_q_0_7_2 -> flash_attn_0_7
	attn_k_0_7_2 -> flash_attn_0_7
	attn_v_0_7_2 -> flash_attn_0_7
	attn_q_0_7_3 -> flash_attn_0_7
	attn_k_0_7_3 -> flash_attn_0_7
	attn_v_0_7_3 -> flash_attn_0_7
	flash_attn_0_7 -> attn_out_0_7
	attn_out_0_7 -> attn_allreduce_0
	input -> attn_residual_0
	attn_allreduce_0 -> attn_residual_0
	attn_residual_0 -> ffn_gate_0_0
	attn_residual_0 -> ffn_up_0_0
	ffn_gate_0_0 -> gelu_0_0
	ffn_up_0_0 -> gelu_0_0
	gelu_0_0 -> ffn_down_0_0
	ffn_down_0_0 -> ffn_allreduce_0
	attn_residual_0 -> ffn_gate_0_1
	attn_residual_0 -> ffn_up_0_1
	ffn_gate_0_1 -> gelu_0_1
	ffn_up_0_1 -> gelu_0_1
	gelu_0_1 -> ffn_down_0_1
	ffn_down_0_1 -> ffn_allreduce_0
	attn_residual_0 -> ffn_gate_0_2
	attn_residual_0 -> ffn_up_0_2
	ffn_gate_0_2 -> gelu_0_2
	ffn_up_0_2 -> gelu_0_2
	gelu_0_2 -> ffn_down_0_2
	ffn_down_0_2 -> ffn_allreduce_0
	attn_residual_0 -> ffn_gate_0_3
	attn_residual_0 -> ffn_up_0_3
	ffn_gate_0_3 -> gelu_0_3
	ffn_up_0_3 -> gelu_0_3
	gelu_0_3 -> ffn_down_0_3
	ffn_down_0_3 -> ffn_allreduce_0
	attn_residual_0 -> ffn_gate_0_4
	attn_residual_0 -> ffn_up_0_4
	ffn_gate_0_4 -> gelu_0_4
	ffn_up_0_4 -> gelu_0_4
	gelu_0_4 -> ffn_down_0_4
	ffn_down_0_4 -> ffn_allreduce_0
	attn_residual_0 -> ffn_gate_0_5
	attn_residual_0 -> ffn_up_0_5
	ffn_gate_0_5 -> gelu_0_5
	ffn_up_0_5 -> gelu_0_5
	gelu_0_5 -> ffn_down_0_5
	ffn_down_0_5 -> ffn_allreduce_0
	attn_residual_0 -> ffn_gate_0_6
	attn_residual_0 -> ffn_up_0_6
	ffn_gate_0_6 -> gelu_0_6
	ffn_up_0_6 -> gelu_0_6
	gelu_0_6 -> ffn_down_0_6
	ffn_down_0_6 -> ffn_allreduce_0
	attn_residual_0 -> ffn_gate_0_7
	attn_residual_0 -> ffn_up_0_7
	ffn_gate_0_7 -> gelu_0_7
	ffn_up_0_7 -> gelu_0_7
	gelu_0_7 -> ffn_down_0_7
	ffn_down_0_7 -> ffn_allreduce_0
	attn_residual_0 -> ffn_residual_0
	ffn_allreduce_0 -> ffn_residual_0
	ffn_residual_0 -> prenorm_1_0
	ffn_residual_0 -> prenorm_1_1
	ffn_residual_0 -> prenorm_1_2
	ffn_residual_0 -> prenorm_1_3
	ffn_residual_0 -> prenorm_1_4
	ffn_residual_0 -> prenorm_1_5
	ffn_residual_0 -> prenorm_1_6
	ffn_residual_0 -> prenorm_1_7
	prenorm_1_0 -> attn_q_1_0_0
	prenorm_1_0 -> attn_k_1_0_0
	prenorm_1_0 -> attn_v_1_0_0
	attn_q_1_0_0 -> flash_attn_1_0
	attn_k_1_0_0 -> flash_attn_1_0
	attn_v_1_0_0 -> flash_attn_1_0
	attn_q_1_0_1 -> flash_attn_1_0
	attn_k_1_0_1 -> flash_attn_1_0
	attn_v_1_0_1 -> flash_attn_1_0
	attn_q_1_0_2 -> flash_attn_1_0
	attn_k_1_0_2 -> flash_attn_1_0
	attn_v_1_0_2 -> flash_attn_1_0
	attn_q_1_0_3 -> flash_attn_1_0
	attn_k_1_0_3 -> flash_attn_1_0
	attn_v_1_0_3 -> flash_attn_1_0
	flash_attn_1_0 -> attn_out_1_0
	attn_out_1_0 -> attn_allreduce_1
	prenorm_1_1 -> attn_q_1_1_0
	prenorm_1_1 -> attn_k_1_1_0
	prenorm_1_1 -> attn_v_1_1_0
	attn_q_1_1_0 -> flash_attn_1_1
	attn_k_1_1_0 -> flash_attn_1_1
	attn_v_1_1_0 -> flash_attn_1_1
	attn_q_1_1_1 -> flash_attn_1_1
	attn_k_1_1_1 -> flash_attn_1_1
	attn_v_1_1_1 -> flash_attn_1_1
	attn_q_1_1_2 -> flash_attn_1_1
	attn_k_1_1_2 -> flash_attn_1_1
	attn_v_1_1_2 -> flash_attn_1_1
	attn_q_1_1_3 -> flash_attn_1_1
	attn_k_1_1_3 -> flash_attn_1_1
	attn_v_1_1_3 -> flash_attn_1_1
	flash_attn_1_1 -> attn_out_1_1
	attn_out_1_1 -> attn_allreduce_1
	prenorm_1_2 -> attn_q_1_2_0
	prenorm_1_2 -> attn_k_1_2_0
	prenorm_1_2 -> attn_v_1_2_0
	attn_q_1_2_0 -> flash_attn_1_2
	attn_k_1_2_0 -> flash_attn_1_2
	attn_v_1_2_0 -> flash_attn_1_2
	attn_q_1_2_1 -> flash_attn_1_2
	attn_k_1_2_1 -> flash_attn_1_2
	attn_v_1_2_1 -> flash_attn_1_2
	attn_q_1_2_2 -> flash_attn_1_2
	attn_k_1_2_2 -> flash_attn_1_2
	attn_v_1_2_2 -> flash_attn_1_2
	attn_q_1_2_3 -> flash_attn_1_2
	attn_k_1_2_3 -> flash_attn_1_2
	attn_v_1_2_3 -> flash_attn_1_2
	flash_attn_1_2 -> attn_out_1_2
	attn_out_1_2 -> attn_allreduce_1
	prenorm_1_3 -> attn_q_1_3_0
	prenorm_1_3 -> attn_k_1_3_0
	prenorm_1_3 -> attn_v_1_3_0
	attn_q_1_3_0 -> flash_attn_1_3
	attn_k_1_3_0 -> flash_attn_1_3
	attn_v_1_3_0 -> flash_attn_1_3
	attn_q_1_3_1 -> flash_attn_1_3
	attn_k_1_3_1 -> flash_attn_1_3
	attn_v_1_3_1 -> flash_attn_1_3
	attn_q_1_3_2 -> flash_attn_1_3
	attn_k_1_3_2 -> flash_attn_1_3
	attn_v_1_3_2 -> flash_attn_1_3
	attn_q_1_3_3 -> flash_attn_1_3
	attn_k_1_3_3 -> flash_attn_1_3
	attn_v_1_3_3 -> flash_attn_1_3
	flash_attn_1_3 -> attn_out_1_3
	attn_out_1_3 -> attn_allreduce_1
	prenorm_1_4 -> attn_q_1_4_0
	prenorm_1_4 -> attn_k_1_4_0
	prenorm_1_4 -> attn_v_1_4_0
	attn_q_1_4_0 -> flash_attn_1_4
	attn_k_1_4_0 -> flash_attn_1_4
	attn_v_1_4_0 -> flash_attn_1_4
	attn_q_1_4_1 -> flash_attn_1_4
	attn_k_1_4_1 -> flash_attn_1_4
	attn_v_1_4_1 -> flash_attn_1_4
	attn_q_1_4_2 -> flash_attn_1_4
	attn_k_1_4_2 -> flash_attn_1_4
	attn_v_1_4_2 -> flash_attn_1_4
	attn_q_1_4_3 -> flash_attn_1_4
	attn_k_1_4_3 -> flash_attn_1_4
	attn_v_1_4_3 -> flash_attn_1_4
	flash_attn_1_4 -> attn_out_1_4
	attn_out_1_4 -> attn_allreduce_1
	prenorm_1_5 -> attn_q_1_5_0
	prenorm_1_5 -> attn_k_1_5_0
	prenorm_1_5 -> attn_v_1_5_0
	attn_q_1_5_0 -> flash_attn_1_5
	attn_k_1_5_0 -> flash_attn_1_5
	attn_v_1_5_0 -> flash_attn_1_5
	attn_q_1_5_1 -> flash_attn_1_5
	attn_k_1_5_1 -> flash_attn_1_5
	attn_v_1_5_1 -> flash_attn_1_5
	attn_q_1_5_2 -> flash_attn_1_5
	attn_k_1_5_2 -> flash_attn_1_5
	attn_v_1_5_2 -> flash_attn_1_5
	attn_q_1_5_3 -> flash_attn_1_5
	attn_k_1_5_3 -> flash_attn_1_5
	attn_v_1_5_3 -> flash_attn_1_5
	flash_attn_1_5 -> attn_out_1_5
	attn_out_1_5 -> attn_allreduce_1
	prenorm_1_6 -> attn_q_1_6_0
	prenorm_1_6 -> attn_k_1_6_0
	prenorm_1_6 -> attn_v_1_6_0
	attn_q_1_6_0 -> flash_attn_1_6
	attn_k_1_6_0 -> flash_attn_1_6
	attn_v_1_6_0 -> flash_attn_1_6
	attn_q_1_6_1 -> flash_attn_1_6
	attn_k_1_6_1 -> flash_attn_1_6
	attn_v_1_6_1 -> flash_attn_1_6
	attn_q_1_6_2 -> flash_attn_1_6
	attn_k_1_6_2 -> flash_attn_1_6
	attn_v_1_6_2 -> flash_attn_1_6
	attn_q_1_6_3 -> flash_attn_1_6
	attn_k_1_6_3 -> flash_attn_1_6
	attn_v_1_6_3 -> flash_attn_1_6
	flash_attn_1_6 -> attn_out_1_6
	attn_out_1_6 -> attn_allreduce_1
	prenorm_1_7 -> attn_q_1_7_0
	prenorm_1_7 -> attn_k_1_7_0
	prenorm_1_7 -> attn_v_1_7_0
	attn_q_1_7_0 -> flash_attn_1_7
	attn_k_1_7_0 -> flash_attn_1_7
	attn_v_1_7_0 -> flash_attn_1_7
	attn_q_1_7_1 -> flash_attn_1_7
	attn_k_1_7_1 -> flash_attn_1_7
	attn_v_1_7_1 -> flash_attn_1_7
	attn_q_1_7_2 -> flash_attn_1_7
	attn_k_1_7_2 -> flash_attn_1_7
	attn_v_1_7_2 -> flash_attn_1_7
	attn_q_1_7_3 -> flash_attn_1_7
	attn_k_1_7_3 -> flash_attn_1_7
	attn_v_1_7_3 -> flash_attn_1_7
	flash_attn_1_7 -> attn_out_1_7
	attn_out_1_7 -> attn_allreduce_1
	ffn_residual_0 -> attn_residual_1
	attn_allreduce_1 -> attn_residual_1
	attn_residual_1 -> ffn_gate_1_0
	attn_residual_1 -> ffn_up_1_0
	ffn_gate_1_0 -> gelu_1_0
	ffn_up_1_0 -> gelu_1_0
	gelu_1_0 -> ffn_down_1_0
	ffn_down_1_0 -> ffn_allreduce_1
	attn_residual_1 -> ffn_gate_1_1
	attn_residual_1 -> ffn_up_1_1
	ffn_gate_1_1 -> gelu_1_1
	ffn_up_1_1 -> gelu_1_1
	gelu_1_1 -> ffn_down_1_1
	ffn_down_1_1 -> ffn_allreduce_1
	attn_residual_1 -> ffn_gate_1_2
	attn_residual_1 -> ffn_up_1_2
	ffn_gate_1_2 -> gelu_1_2
	ffn_up_1_2 -> gelu_1_2
	gelu_1_2 -> ffn_down_1_2
	ffn_down_1_2 -> ffn_allreduce_1
	attn_residual_1 -> ffn_gate_1_3
	attn_residual_1 -> ffn_up_1_3
	ffn_gate_1_3 -> gelu_1_3
	ffn_up_1_3 -> gelu_1_3
	gelu_1_3 -> ffn_down_1_3
	ffn_down_1_3 -> ffn_allreduce_1
	attn_residual_1 -> ffn_gate_1_4
	attn_residual_1 -> ffn_up_1_4
	ffn_gate_1_4 -> gelu_1_4
	ffn_up_1_4 -> gelu_1_4
	gelu_1_4 -> ffn_down_1_4
	ffn_down_1_4 -> ffn_allreduce_1
	attn_residual_1 -> ffn_gate_1_5
	attn_residual_1 -> ffn_up_1_5
	ffn_gate_1_5 -> gelu_1_5
	ffn_up_1_5 -> gelu_1_5
	gelu_1_5 -> ffn_down_1_5
	ffn_down_1_5 -> ffn_allreduce_1
	attn_residual_1 -> ffn_gate_1_6
	attn_residual_1 -> ffn_up_1_6
	ffn_gate_1_6 -> gelu_1_6
	ffn_up_1_6 -> gelu_1_6
	gelu_1_6 -> ffn_down_1_6
	ffn_down_1_6 -> ffn_allreduce_1
	attn_residual_1 -> ffn_gate_1_7
	attn_residual_1 -> ffn_up_1_7
	ffn_gate_1_7 -> gelu_1_7
	ffn_up_1_7 -> gelu_1_7
	gelu_1_7 -> ffn_down_1_7
	ffn_down_1_7 -> ffn_allreduce_1
	attn_residual_1 -> ffn_residual_1
	ffn_allreduce_1 -> ffn_residual_1
	ffn_residual_1 -> prenorm_2_0
	ffn_residual_1 -> prenorm_2_1
	ffn_residual_1 -> prenorm_2_2
	ffn_residual_1 -> prenorm_2_3
	ffn_residual_1 -> prenorm_2_4
	ffn_residual_1 -> prenorm_2_5
	ffn_residual_1 -> prenorm_2_6
	ffn_residual_1 -> prenorm_2_7
	prenorm_2_0 -> attn_q_2_0_0
	prenorm_2_0 -> attn_k_2_0_0
	prenorm_2_0 -> attn_v_2_0_0
	attn_q_2_0_0 -> flash_attn_2_0
	attn_k_2_0_0 -> flash_attn_2_0
	attn_v_2_0_0 -> flash_attn_2_0
	attn_q_2_0_1 -> flash_attn_2_0
	attn_k_2_0_1 -> flash_attn_2_0
	attn_v_2_0_1 -> flash_attn_2_0
	attn_q_2_0_2 -> flash_attn_2_0
	attn_k_2_0_2 -> flash_attn_2_0
	attn_v_2_0_2 -> flash_attn_2_0
	attn_q_2_0_3 -> flash_attn_2_0
	attn_k_2_0_3 -> flash_attn_2_0
	attn_v_2_0_3 -> flash_attn_2_0
	flash_attn_2_0 -> attn_out_2_0
	attn_out_2_0 -> attn_allreduce_2
	prenorm_2_1 -> attn_q_2_1_0
	prenorm_2_1 -> attn_k_2_1_0
	prenorm_2_1 -> attn_v_2_1_0
	attn_q_2_1_0 -> flash_attn_2_1
	attn_k_2_1_0 -> flash_attn_2_1
	attn_v_2_1_0 -> flash_attn_2_1
	attn_q_2_1_1 -> flash_attn_2_1
	attn_k_2_1_1 -> flash_attn_2_1
	attn_v_2_1_1 -> flash_attn_2_1
	attn_q_2_1_2 -> flash_attn_2_1
	attn_k_2_1_2 -> flash_attn_2_1
	attn_v_2_1_2 -> flash_attn_2_1
	attn_q_2_1_3 -> flash_attn_2_1
	attn_k_2_1_3 -> flash_attn_2_1
	attn_v_2_1_3 -> flash_attn_2_1
	flash_attn_2_1 -> attn_out_2_1
	attn_out_2_1 -> attn_allreduce_2
	prenorm_2_2 -> attn_q_2_2_0
	prenorm_2_2 -> attn_k_2_2_0
	prenorm_2_2 -> attn_v_2_2_0
	attn_q_2_2_0 -> flash_attn_2_2
	attn_k_2_2_0 -> flash_attn_2_2
	attn_v_2_2_0 -> flash_attn_2_2
	attn_q_2_2_1 -> flash_attn_2_2
	attn_k_2_2_1 -> flash_attn_2_2
	attn_v_2_2_1 -> flash_attn_2_2
	attn_q_2_2_2 -> flash_attn_2_2
	attn_k_2_2_2 -> flash_attn_2_2
	attn_v_2_2_2 -> flash_attn_2_2
	attn_q_2_2_3 -> flash_attn_2_2
	attn_k_2_2_3 -> flash_attn_2_2
	attn_v_2_2_3 -> flash_attn_2_2
	flash_attn_2_2 -> attn_out_2_2
	attn_out_2_2 -> attn_allreduce_2
	prenorm_2_3 -> attn_q_2_3_0
	prenorm_2_3 -> attn_k_2_3_0
	prenorm_2_3 -> attn_v_2_3_0
	attn_q_2_3_0 -> flash_attn_2_3
	attn_k_2_3_0 -> flash_attn_2_3
	attn_v_2_3_0 -> flash_attn_2_3
	attn_q_2_3_1 -> flash_attn_2_3
	attn_k_2_3_1 -> flash_attn_2_3
	attn_v_2_3_1 -> flash_attn_2_3
	attn_q_2_3_2 -> flash_attn_2_3
	attn_k_2_3_2 -> flash_attn_2_3
	attn_v_2_3_2 -> flash_attn_2_3
	attn_q_2_3_3 -> flash_attn_2_3
	attn_k_2_3_3 -> flash_attn_2_3
	attn_v_2_3_3 -> flash_attn_2_3
	flash_attn_2_3 -> attn_out_2_3
	attn_out_2_3 -> attn_allreduce_2
	prenorm_2_4 -> attn_q_2_4_0
	prenorm_2_4 -> attn_k_2_4_0
	prenorm_2_4 -> attn_v_2_4_0
	attn_q_2_4_0 -> flash_attn_2_4
	attn_k_2_4_0 -> flash_attn_2_4
	attn_v_2_4_0 -> flash_attn_2_4
	attn_q_2_4_1 -> flash_attn_2_4
	attn_k_2_4_1 -> flash_attn_2_4
	attn_v_2_4_1 -> flash_attn_2_4
	attn_q_2_4_2 -> flash_attn_2_4
	attn_k_2_4_2 -> flash_attn_2_4
	attn_v_2_4_2 -> flash_attn_2_4
	attn_q_2_4_3 -> flash_attn_2_4
	attn_k_2_4_3 -> flash_attn_2_4
	attn_v_2_4_3 -> flash_attn_2_4
	flash_attn_2_4 -> attn_out_2_4
	attn_out_2_4 -> attn_allreduce_2
	prenorm_2_5 -> attn_q_2_5_0
	prenorm_2_5 -> attn_k_2_5_0
	prenorm_2_5 -> attn_v_2_5_0
	attn_q_2_5_0 -> flash_attn_2_5
	attn_k_2_5_0 -> flash_attn_2_5
	attn_v_2_5_0 -> flash_attn_2_5
	attn_q_2_5_1 -> flash_attn_2_5
	attn_k_2_5_1 -> flash_attn_2_5
	attn_v_2_5_1 -> flash_attn_2_5
	attn_q_2_5_2 -> flash_attn_2_5
	attn_k_2_5_2 -> flash_attn_2_5
	attn_v_2_5_2 -> flash_attn_2_5
	attn_q_2_5_3 -> flash_attn_2_5
	attn_k_2_5_3 -> flash_attn_2_5
	attn_v_2_5_3 -> flash_attn_2_5
	flash_attn_2_5 -> attn_out_2_5
	attn_out_2_5 -> attn_allreduce_2
	prenorm_2_6 -> attn_q_2_6_0
	prenorm_2_6 -> attn_k_2_6_0
	prenorm_2_6 -> attn_v_2_6_0
	attn_q_2_6_0 -> flash_attn_2_6
	attn_k_2_6_0 -> flash_attn_2_6
	attn_v_2_6_0 -> flash_attn_2_6
	attn_q_2_6_1 -> flash_attn_2_6
	attn_k_2_6_1 -> flash_attn_2_6
	attn_v_2_6_1 -> flash_attn_2_6
	attn_q_2_6_2 -> flash_attn_2_6
	attn_k_2_6_2 -> flash_attn_2_6
	attn_v_2_6_2 -> flash_attn_2_6
	attn_q_2_6_3 -> flash_attn_2_6
	attn_k_2_6_3 -> flash_attn_2_6
	attn_v_2_6_3 -> flash_attn_2_6
	flash_attn_2_6 -> attn_out_2_6
	attn_out_2_6 -> attn_allreduce_2
	prenorm_2_7 -> attn_q_2_7_0
	prenorm_2_7 -> attn_k_2_7_0
	prenorm_2_7 -> attn_v_2_7_0
	attn_q_2_7_0 -> flash_attn_2_7
	attn_k_2_7_0 -> flash_attn_2_7
	attn_v_2_7_0 -> flash_attn_2_7
	attn_q_2_7_1 -> flash_attn_2_7
	attn_k_2_7_1 -> flash_attn_2_7
	attn_v_2_7_1 -> flash_attn_2_7
	attn_q_2_7_2 -> flash_attn_2_7
	attn_k_2_7_2 -> flash_attn_2_7
	attn_v_2_7_2 -> flash_attn_2_7
	attn_q_2_7_3 -> flash_attn_2_7
	attn_k_2_7_3 -> flash_attn_2_7
	attn_v_2_7_3 -> flash_attn_2_7
	flash_attn_2_7 -> attn_out_2_7
	attn_out_2_7 -> attn_allreduce_2
	ffn_residual_1 -> attn_residual_2
	attn_allreduce_2 -> attn_residual_2
	attn_residual_2 -> ffn_gate_2_0
	attn_residual_2 -> ffn_up_2_0
	ffn_gate_2_0 -> gelu_2_0
	ffn_up_2_0 -> gelu_2_0
	gelu_2_0 -> ffn_down_2_0
	ffn_down_2_0 -> ffn_allreduce_2
	attn_residual_2 -> ffn_gate_2_1
	attn_residual_2 -> ffn_up_2_1
	ffn_gate_2_1 -> gelu_2_1
	ffn_up_2_1 -> gelu_2_1
	gelu_2_1 -> ffn_down_2_1
	ffn_down_2_1 -> ffn_allreduce_2
	attn_residual_2 -> ffn_gate_2_2
	attn_residual_2 -> ffn_up_2_2
	ffn_gate_2_2 -> gelu_2_2
	ffn_up_2_2 -> gelu_2_2
	gelu_2_2 -> ffn_down_2_2
	ffn_down_2_2 -> ffn_allreduce_2
	attn_residual_2 -> ffn_gate_2_3
	attn_residual_2 -> ffn_up_2_3
	ffn_gate_2_3 -> gelu_2_3
	ffn_up_2_3 -> gelu_2_3
	gelu_2_3 -> ffn_down_2_3
	ffn_down_2_3 -> ffn_allreduce_2
	attn_residual_2 -> ffn_gate_2_4
	attn_residual_2 -> ffn_up_2_4
	ffn_gate_2_4 -> gelu_2_4
	ffn_up_2_4 -> gelu_2_4
	gelu_2_4 -> ffn_down_2_4
	ffn_down_2_4 -> ffn_allreduce_2
	attn_residual_2 -> ffn_gate_2_5
	attn_residual_2 -> ffn_up_2_5
	ffn_gate_2_5 -> gelu_2_5
	ffn_up_2_5 -> gelu_2_5
	gelu_2_5 -> ffn_down_2_5
	ffn_down_2_5 -> ffn_allreduce_2
	attn_residual_2 -> ffn_gate_2_6
	attn_residual_2 -> ffn_up_2_6
	ffn_gate_2_6 -> gelu_2_6
	ffn_up_2_6 -> gelu_2_6
	gelu_2_6 -> ffn_down_2_6
	ffn_down_2_6 -> ffn_allreduce_2
	attn_residual_2 -> ffn_gate_2_7
	attn_residual_2 -> ffn_up_2_7
	ffn_gate_2_7 -> gelu_2_7
	ffn_up_2_7 -> gelu_2_7
	gelu_2_7 -> ffn_down_2_7
	ffn_down_2_7 -> ffn_allreduce_2
	attn_residual_2 -> ffn_residual_2
	ffn_allreduce_2 -> ffn_residual_2
	ffn_residual_2 -> prenorm_3_0
	ffn_residual_2 -> prenorm_3_1
	ffn_residual_2 -> prenorm_3_2
	ffn_residual_2 -> prenorm_3_3
	ffn_residual_2 -> prenorm_3_4
	ffn_residual_2 -> prenorm_3_5
	ffn_residual_2 -> prenorm_3_6
	ffn_residual_2 -> prenorm_3_7
	prenorm_3_0 -> attn_q_3_0_0
	prenorm_3_0 -> attn_k_3_0_0
	prenorm_3_0 -> attn_v_3_0_0
	attn_q_3_0_0 -> flash_attn_3_0
	attn_k_3_0_0 -> flash_attn_3_0
	attn_v_3_0_0 -> flash_attn_3_0
	attn_q_3_0_1 -> flash_attn_3_0
	attn_k_3_0_1 -> flash_attn_3_0
	attn_v_3_0_1 -> flash_attn_3_0
	attn_q_3_0_2 -> flash_attn_3_0
	attn_k_3_0_2 -> flash_attn_3_0
	attn_v_3_0_2 -> flash_attn_3_0
	attn_q_3_0_3 -> flash_attn_3_0
	attn_k_3_0_3 -> flash_attn_3_0
	attn_v_3_0_3 -> flash_attn_3_0
	flash_attn_3_0 -> attn_out_3_0
	attn_out_3_0 -> attn_allreduce_3
	prenorm_3_1 -> attn_q_3_1_0
	prenorm_3_1 -> attn_k_3_1_0
	prenorm_3_1 -> attn_v_3_1_0
	attn_q_3_1_0 -> flash_attn_3_1
	attn_k_3_1_0 -> flash_attn_3_1
	attn_v_3_1_0 -> flash_attn_3_1
	attn_q_3_1_1 -> flash_attn_3_1
	attn_k_3_1_1 -> flash_attn_3_1
	attn_v_3_1_1 -> flash_attn_3_1
	attn_q_3_1_2 -> flash_attn_3_1
	attn_k_3_1_2 -> flash_attn_3_1
	attn_v_3_1_2 -> flash_attn_3_1
	attn_q_3_1_3 -> flash_attn_3_1
	attn_k_3_1_3 -> flash_attn_3_1
	attn_v_3_1_3 -> flash_attn_3_1
	flash_attn_3_1 -> attn_out_3_1
	attn_out_3_1 -> attn_allreduce_3
	prenorm_3_2 -> attn_q_3_2_0
	prenorm_3_2 -> attn_k_3_2_0
	prenorm_3_2 -> attn_v_3_2_0
	attn_q_3_2_0 -> flash_attn_3_2
	attn_k_3_2_0 -> flash_attn_3_2
	attn_v_3_2_0 -> flash_attn_3_2
	attn_q_3_2_1 -> flash_attn_3_2
	attn_k_3_2_1 -> flash_attn_3_2
	attn_v_3_2_1 -> flash_attn_3_2
	attn_q_3_2_2 -> flash_attn_3_2
	attn_k_3_2_2 -> flash_attn_3_2
	attn_v_3_2_2 -> flash_attn_3_2
	attn_q_3_2_3 -> flash_attn_3_2
	attn_k_3_2_3 -> flash_attn_3_2
	attn_v_3_2_3 -> flash_attn_3_2
	flash_attn_3_2 -> attn_out_3_2
	attn_out_3_2 -> attn_allreduce_3
	prenorm_3_3 -> attn_q_3_3_0
	prenorm_3_3 -> attn_k_3_3_0
	prenorm_3_3 -> attn_v_3_3_0
	attn_q_3_3_0 -> flash_attn_3_3
	attn_k_3_3_0 -> flash_attn_3_3
	attn_v_3_3_0 -> flash_attn_3_3
	attn_q_3_3_1 -> flash_attn_3_3
	attn_k_3_3_1 -> flash_attn_3_3
	attn_v_3_3_1 -> flash_attn_3_3
	attn_q_3_3_2 -> flash_attn_3_3
	attn_k_3_3_2 -> flash_attn_3_3
	attn_v_3_3_2 -> flash_attn_3_3
	attn_q_3_3_3 -> flash_attn_3_3
	attn_k_3_3_3 -> flash_attn_3_3
	attn_v_3_3_3 -> flash_attn_3_3
	flash_attn_3_3 -> attn_out_3_3
	attn_out_3_3 -> attn_allreduce_3
	prenorm_3_4 -> attn_q_3_4_0
	prenorm_3_4 -> attn_k_3_4_0
	prenorm_3_4 -> attn_v_3_4_0
	attn_q_3_4_0 -> flash_attn_3_4
	attn_k_3_4_0 -> flash_attn_3_4
	attn_v_3_4_0 -> flash_attn_3_4
	attn_q_3_4_1 -> flash_attn_3_4
	attn_k_3_4_1 -> flash_attn_3_4
	attn_v_3_4_1 -> flash_attn_3_4
	attn_q_3_4_2 -> flash_attn_3_4
	attn_k_3_4_2 -> flash_attn_3_4
	attn_v_3_4_2 -> flash_attn_3_4
	attn_q_3_4_3 -> flash_attn_3_4
	attn_k_3_4_3 -> flash_attn_3_4
	attn_v_3_4_3 -> flash_attn_3_4
	flash_attn_3_4 -> attn_out_3_4
	attn_out_3_4 -> attn_allreduce_3
	prenorm_3_5 -> attn_q_3_5_0
	prenorm_3_5 -> attn_k_3_5_0
	prenorm_3_5 -> attn_v_3_5_0
	attn_q_3_5_0 -> flash_attn_3_5
	attn_k_3_5_0 -> flash_attn_3_5
	attn_v_3_5_0 -> flash_attn_3_5
	attn_q_3_5_1 -> flash_attn_3_5
	attn_k_3_5_1 -> flash_attn_3_5
	attn_v_3_5_1 -> flash_attn_3_5
	attn_q_3_5_2 -> flash_attn_3_5
	attn_k_3_5_2 -> flash_attn_3_5
	attn_v_3_5_2 -> flash_attn_3_5
	attn_q_3_5_3 -> flash_attn_3_5
	attn_k_3_5_3 -> flash_attn_3_5
	attn_v_3_5_3 -> flash_attn_3_5
	flash_attn_3_5 -> attn_out_3_5
	attn_out_3_5 -> attn_allreduce_3
	prenorm_3_6 -> attn_q_3_6_0
	prenorm_3_6 -> attn_k_3_6_0
	prenorm_3_6 -> attn_v_3_6_0
	attn_q_3_6_0 -> flash_attn_3_6
	attn_k_3_6_0 -> flash_attn_3_6
	attn_v_3_6_0 -> flash_attn_3_6
	attn_q_3_6_1 -> flash_attn_3_6
	attn_k_3_6_1 -> flash_attn_3_6
	attn_v_3_6_1 -> flash_attn_3_6
	attn_q_3_6_2 -> flash_attn_3_6
	attn_k_3_6_2 -> flash_attn_3_6
	attn_v_3_6_2 -> flash_attn_3_6
	attn_q_3_6_3 -> flash_attn_3_6
	attn_k_3_6_3 -> flash_attn_3_6
	attn_v_3_6_3 -> flash_attn_3_6
	flash_attn_3_6 -> attn_out_3_6
	attn_out_3_6 -> attn_allreduce_3
	prenorm_3_7 -> attn_q_3_7_0
	prenorm_3_7 -> attn_k_3_7_0
	prenorm_3_7 -> attn_v_3_7_0
	attn_q_3_7_0 -> flash_attn_3_7
	attn_k_3_7_0 -> flash_attn_3_7
	attn_v_3_7_0 -> flash_attn_3_7
	attn_q_3_7_1 -> flash_attn_3_7
	attn_k_3_7_1 -> flash_attn_3_7
	attn_v_3_7_1 -> flash_attn_3_7
	attn_q_3_7_2 -> flash_attn_3_7
	attn_k_3_7_2 -> flash_attn_3_7
	attn_v_3_7_2 -> flash_attn_3_7
	attn_q_3_7_3 -> flash_attn_3_7
	attn_k_3_7_3 -> flash_attn_3_7
	attn_v_3_7_3 -> flash_attn_3_7
	flash_attn_3_7 -> attn_out_3_7
	attn_out_3_7 -> attn_allreduce_3
	ffn_residual_2 -> attn_residual_3
	attn_allreduce_3 -> attn_residual_3
	attn_residual_3 -> ffn_gate_3_0
	attn_residual_3 -> ffn_up_3_0
	ffn_gate_3_0 -> gelu_3_0
	ffn_up_3_0 -> gelu_3_0
	gelu_3_0 -> ffn_down_3_0
	ffn_down_3_0 -> ffn_allreduce_3
	attn_residual_3 -> ffn_gate_3_1
	attn_residual_3 -> ffn_up_3_1
	ffn_gate_3_1 -> gelu_3_1
	ffn_up_3_1 -> gelu_3_1
	gelu_3_1 -> ffn_down_3_1
	ffn_down_3_1 -> ffn_allreduce_3
	attn_residual_3 -> ffn_gate_3_2
	attn_residual_3 -> ffn_up_3_2
	ffn_gate_3_2 -> gelu_3_2
	ffn_up_3_2 -> gelu_3_2
	gelu_3_2 -> ffn_down_3_2
	ffn_down_3_2 -> ffn_allreduce_3
	attn_residual_3 -> ffn_gate_3_3
	attn_residual_3 -> ffn_up_3_3
	ffn_gate_3_3 -> gelu_3_3
	ffn_up_3_3 -> gelu_3_3
	gelu_3_3 -> ffn_down_3_3
	ffn_down_3_3 -> ffn_allreduce_3
	attn_residual_3 -> ffn_gate_3_4
	attn_residual_3 -> ffn_up_3_4
	ffn_gate_3_4 -> gelu_3_4
	ffn_up_3_4 -> gelu_3_4
	gelu_3_4 -> ffn_down_3_4
	ffn_down_3_4 -> ffn_allreduce_3
	attn_residual_3 -> ffn_gate_3_5
	attn_residual_3 -> ffn_up_3_5
	ffn_gate_3_5 -> gelu_3_5
	ffn_up_3_5 -> gelu_3_5
	gelu_3_5 -> ffn_down_3_5
	ffn_down_3_5 -> ffn_allreduce_3
	attn_residual_3 -> ffn_gate_3_6
	attn_residual_3 -> ffn_up_3_6
	ffn_gate_3_6 -> gelu_3_6
	ffn_up_3_6 -> gelu_3_6
	gelu_3_6 -> ffn_down_3_6
	ffn_down_3_6 -> ffn_allreduce_3
	attn_residual_3 -> ffn_gate_3_7
	attn_residual_3 -> ffn_up_3_7
	ffn_gate_3_7 -> gelu_3_7
	ffn_up_3_7 -> gelu_3_7
	gelu_3_7 -> ffn_down_3_7
	ffn_down_3_7 -> ffn_allreduce_3
	attn_residual_3 -> ffn_residual_3
	ffn_allreduce_3 -> ffn_residual_3
	ffn_residual_3 -> output
}
