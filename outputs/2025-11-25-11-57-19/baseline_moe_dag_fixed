// Fixed Baseline MoE with TP=8, PP=2, colocated experts
// Fixed connectivity issue between layer 7 and layer 8
// This DAG resolves the broken flow from layer_7_add_norm2 to comm_stage_0_1
digraph baseline_moe_dag {
	nodesep=0.8 rankdir=TB ranksep=2.0 splines=ortho
	node [fillcolor=lightblue shape=rectangle style=filled]
	edge [arrowhead=normal]
	input [label="Total Input\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: All" fillcolor=lightgreen shape=ellipse]
	input -> layer_0_qkv
	layer_0_qkv -> layer_0_attn
	layer_0_attn -> layer_0_proj
	layer_0_proj -> layer_0_add_norm1
	input -> layer_0_add_norm1 [label=residual style=dashed]
	layer_0_add_norm1 -> layer_0_gate
	layer_0_add_norm1 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_0 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_0
	layer_0_expert_0 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_1 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_1
	layer_0_expert_1 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_2 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_2
	layer_0_expert_2 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_3 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_3
	layer_0_expert_3 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_4 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_4
	layer_0_expert_4 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_5 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_5
	layer_0_expert_5 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_6 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_6
	layer_0_expert_6 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_7 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_7
	layer_0_expert_7 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_8 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_8
	layer_0_expert_8 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_9 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_9
	layer_0_expert_9 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_10 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_10
	layer_0_expert_10 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_11 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_11
	layer_0_expert_11 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_12 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_12
	layer_0_expert_12 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_13 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_13
	layer_0_expert_13 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_14 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_14
	layer_0_expert_14 -> layer_0_moe_agg
	layer_0_gate -> layer_0_expert_15 [label=routing style=dashed]
	layer_0_add_norm1 -> layer_0_expert_15
	layer_0_expert_15 -> layer_0_moe_agg
	layer_0_moe_agg -> layer_0_add_norm2
	layer_0_add_norm1 -> layer_0_add_norm2 [label=residual style=dashed]
	layer_0_add_norm2 -> layer_1_qkv
	layer_1_qkv -> layer_1_attn
	layer_1_attn -> layer_1_proj
	layer_1_proj -> layer_1_add_norm1
	layer_0_add_norm2 -> layer_1_add_norm1 [label=residual style=dashed]
	layer_1_add_norm1 -> layer_1_gate
	layer_1_add_norm1 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_0 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_0
	layer_1_expert_0 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_1 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_1
	layer_1_expert_1 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_2 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_2
	layer_1_expert_2 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_3 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_3
	layer_1_expert_3 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_4 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_4
	layer_1_expert_4 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_5 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_5
	layer_1_expert_5 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_6 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_6
	layer_1_expert_6 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_7 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_7
	layer_1_expert_7 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_8 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_8
	layer_1_expert_8 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_9 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_9
	layer_1_expert_9 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_10 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_10
	layer_1_expert_10 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_11 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_11
	layer_1_expert_11 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_12 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_12
	layer_1_expert_12 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_13 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_13
	layer_1_expert_13 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_14 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_14
	layer_1_expert_14 -> layer_1_moe_agg
	layer_1_gate -> layer_1_expert_15 [label=routing style=dashed]
	layer_1_add_norm1 -> layer_1_expert_15
	layer_1_expert_15 -> layer_1_moe_agg
	layer_1_moe_agg -> layer_1_add_norm2
	layer_1_add_norm1 -> layer_1_add_norm2 [label=residual style=dashed]
	layer_1_add_norm2 -> layer_2_qkv
	layer_2_qkv -> layer_2_attn
	layer_2_attn -> layer_2_proj
	layer_2_proj -> layer_2_add_norm1
	layer_1_add_norm2 -> layer_2_add_norm1 [label=residual style=dashed]
	layer_2_add_norm1 -> layer_2_gate
	layer_2_add_norm1 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_0 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_0
	layer_2_expert_0 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_1 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_1
	layer_2_expert_1 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_2 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_2
	layer_2_expert_2 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_3 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_3
	layer_2_expert_3 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_4 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_4
	layer_2_expert_4 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_5 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_5
	layer_2_expert_5 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_6 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_6
	layer_2_expert_6 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_7 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_7
	layer_2_expert_7 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_8 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_8
	layer_2_expert_8 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_9 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_9
	layer_2_expert_9 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_10 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_10
	layer_2_expert_10 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_11 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_11
	layer_2_expert_11 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_12 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_12
	layer_2_expert_12 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_13 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_13
	layer_2_expert_13 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_14 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_14
	layer_2_expert_14 -> layer_2_moe_agg
	layer_2_gate -> layer_2_expert_15 [label=routing style=dashed]
	layer_2_add_norm1 -> layer_2_expert_15
	layer_2_expert_15 -> layer_2_moe_agg
	layer_2_moe_agg -> layer_2_add_norm2
	layer_2_add_norm1 -> layer_2_add_norm2 [label=residual style=dashed]
	layer_2_add_norm2 -> layer_3_qkv
	layer_3_qkv -> layer_3_attn
	layer_3_attn -> layer_3_proj
	layer_3_proj -> layer_3_add_norm1
	layer_2_add_norm2 -> layer_3_add_norm1 [label=residual style=dashed]
	layer_3_add_norm1 -> layer_3_gate
	layer_3_add_norm1 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_0 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_0
	layer_3_expert_0 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_1 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_1
	layer_3_expert_1 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_2 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_2
	layer_3_expert_2 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_3 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_3
	layer_3_expert_3 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_4 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_4
	layer_3_expert_4 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_5 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_5
	layer_3_expert_5 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_6 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_6
	layer_3_expert_6 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_7 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_7
	layer_3_expert_7 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_8 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_8
	layer_3_expert_8 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_9 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_9
	layer_3_expert_9 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_10 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_10
	layer_3_expert_10 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_11 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_11
	layer_3_expert_11 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_12 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_12
	layer_3_expert_12 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_13 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_13
	layer_3_expert_13 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_14 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_14
	layer_3_expert_14 -> layer_3_moe_agg
	layer_3_gate -> layer_3_expert_15 [label=routing style=dashed]
	layer_3_add_norm1 -> layer_3_expert_15
	layer_3_expert_15 -> layer_3_moe_agg
	layer_3_moe_agg -> layer_3_add_norm2
	layer_3_add_norm1 -> layer_3_add_norm2 [label=residual style=dashed]
	layer_3_add_norm2 -> layer_4_qkv
	layer_4_qkv -> layer_4_attn
	layer_4_attn -> layer_4_proj
	layer_4_proj -> layer_4_add_norm1
	layer_3_add_norm2 -> layer_4_add_norm1 [label=residual style=dashed]
	layer_4_add_norm1 -> layer_4_gate
	layer_4_add_norm1 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_0 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_0
	layer_4_expert_0 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_1 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_1
	layer_4_expert_1 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_2 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_2
	layer_4_expert_2 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_3 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_3
	layer_4_expert_3 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_4 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_4
	layer_4_expert_4 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_5 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_5
	layer_4_expert_5 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_6 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_6
	layer_4_expert_6 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_7 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_7
	layer_4_expert_7 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_8 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_8
	layer_4_expert_8 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_9 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_9
	layer_4_expert_9 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_10 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_10
	layer_4_expert_10 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_11 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_11
	layer_4_expert_11 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_12 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_12
	layer_4_expert_12 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_13 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_13
	layer_4_expert_13 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_14 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_14
	layer_4_expert_14 -> layer_4_moe_agg
	layer_4_gate -> layer_4_expert_15 [label=routing style=dashed]
	layer_4_add_norm1 -> layer_4_expert_15
	layer_4_expert_15 -> layer_4_moe_agg
	layer_4_moe_agg -> layer_4_add_norm2
	layer_4_add_norm1 -> layer_4_add_norm2 [label=residual style=dashed]
	layer_4_add_norm2 -> layer_5_qkv
	layer_5_qkv -> layer_5_attn
	layer_5_attn -> layer_5_proj
	layer_5_proj -> layer_5_add_norm1
	layer_4_add_norm2 -> layer_5_add_norm1 [label=residual style=dashed]
	layer_5_add_norm1 -> layer_5_gate
	layer_5_add_norm1 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_0 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_0
	layer_5_expert_0 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_1 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_1
	layer_5_expert_1 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_2 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_2
	layer_5_expert_2 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_3 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_3
	layer_5_expert_3 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_4 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_4
	layer_5_expert_4 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_5 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_5
	layer_5_expert_5 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_6 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_6
	layer_5_expert_6 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_7 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_7
	layer_5_expert_7 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_8 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_8
	layer_5_expert_8 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_9 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_9
	layer_5_expert_9 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_10 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_10
	layer_5_expert_10 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_11 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_11
	layer_5_expert_11 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_12 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_12
	layer_5_expert_12 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_13 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_13
	layer_5_expert_13 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_14 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_14
	layer_5_expert_14 -> layer_5_moe_agg
	layer_5_gate -> layer_5_expert_15 [label=routing style=dashed]
	layer_5_add_norm1 -> layer_5_expert_15
	layer_5_expert_15 -> layer_5_moe_agg
	layer_5_moe_agg -> layer_5_add_norm2
	layer_5_add_norm1 -> layer_5_add_norm2 [label=residual style=dashed]
	layer_5_add_norm2 -> layer_6_qkv
	layer_6_qkv -> layer_6_attn
	layer_6_attn -> layer_6_proj
	layer_6_proj -> layer_6_add_norm1
	layer_5_add_norm2 -> layer_6_add_norm1 [label=residual style=dashed]
	layer_6_add_norm1 -> layer_6_gate
	layer_6_add_norm1 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_0 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_0
	layer_6_expert_0 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_1 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_1
	layer_6_expert_1 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_2 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_2
	layer_6_expert_2 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_3 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_3
	layer_6_expert_3 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_4 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_4
	layer_6_expert_4 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_5 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_5
	layer_6_expert_5 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_6 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_6
	layer_6_expert_6 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_7 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_7
	layer_6_expert_7 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_8 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_8
	layer_6_expert_8 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_9 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_9
	layer_6_expert_9 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_10 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_10
	layer_6_expert_10 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_11 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_11
	layer_6_expert_11 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_12 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_12
	layer_6_expert_12 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_13 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_13
	layer_6_expert_13 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_14 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_14
	layer_6_expert_14 -> layer_6_moe_agg
	layer_6_gate -> layer_6_expert_15 [label=routing style=dashed]
	layer_6_add_norm1 -> layer_6_expert_15
	layer_6_expert_15 -> layer_6_moe_agg
	layer_6_moe_agg -> layer_6_add_norm2
	layer_6_add_norm1 -> layer_6_add_norm2 [label=residual style=dashed]
	layer_6_add_norm2 -> layer_7_qkv
	layer_7_qkv -> layer_7_attn
	layer_7_attn -> layer_7_proj
	layer_7_proj -> layer_7_add_norm1
	layer_6_add_norm2 -> layer_7_add_norm1 [label=residual style=dashed]
	layer_7_add_norm1 -> layer_7_gate
	layer_7_add_norm1 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_0 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_0
	layer_7_expert_0 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_1 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_1
	layer_7_expert_1 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_2 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_2
	layer_7_expert_2 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_3 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_3
	layer_7_expert_3 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_4 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_4
	layer_7_expert_4 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_5 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_5
	layer_7_expert_5 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_6 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_6
	layer_7_expert_6 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_7 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_7
	layer_7_expert_7 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_8 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_8
	layer_7_expert_8 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_9 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_9
	layer_7_expert_9 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_10 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_10
	layer_7_expert_10 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_11 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_11
	layer_7_expert_11 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_12 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_12
	layer_7_expert_12 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_13 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_13
	layer_7_expert_13 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_14 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_14
	layer_7_expert_14 -> layer_7_moe_agg
	layer_7_gate -> layer_7_expert_15 [label=routing style=dashed]
	layer_7_add_norm1 -> layer_7_expert_15
	layer_7_expert_15 -> layer_7_moe_agg
	layer_7_moe_agg -> layer_7_add_norm2
	layer_7_add_norm1 -> layer_7_add_norm2 [label=residual style=dashed]
	layer_7_add_norm2 -> comm_stage_0_1
	subgraph cluster_stage0 {
		color=red label="Pipeline Stage 0\nLayers 0-7\nGPUs: 0-7" style=dashed
		layer_0_mha [label="MHA Layer 0\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_0_qkv [label="QKV Linear\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_0_attn [label="Attention\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_0_proj [label="Projection\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_0_add_norm1 [label="Residual+Norm\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_0_gate [label="Gating\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, topk=2]\nGPU: 0-7" fillcolor=yellow shape=parallelogram]
		layer_0_expert_0 [label="Expert 0\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0"]
		layer_0_expert_1 [label="Expert 1\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0"]
		layer_0_expert_2 [label="Expert 2\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 1"]
		layer_0_expert_3 [label="Expert 3\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 1"]
		layer_0_expert_4 [label="Expert 4\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 2"]
		layer_0_expert_5 [label="Expert 5\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 2"]
		layer_0_expert_6 [label="Expert 6\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 3"]
		layer_0_expert_7 [label="Expert 7\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 3"]
		layer_0_expert_8 [label="Expert 8\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 4"]
		layer_0_expert_9 [label="Expert 9\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 4"]
		layer_0_expert_10 [label="Expert 10\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 5"]
		layer_0_expert_11 [label="Expert 11\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 5"]
		layer_0_expert_12 [label="Expert 12\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 6"]
		layer_0_expert_13 [label="Expert 13\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 6"]
		layer_0_expert_14 [label="Expert 14\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 7"]
		layer_0_expert_15 [label="Expert 15\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 7"]
		layer_0_moe_agg [label="MoE Aggregation\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_0_add_norm2 [label="Residual+Norm\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_1_mha [label="MHA Layer 1\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_1_qkv [label="QKV Linear\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_1_attn [label="Attention\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_1_proj [label="Projection\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_1_add_norm1 [label="Residual+Norm\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_1_gate [label="Gating\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, topk=2]\nGPU: 0-7" fillcolor=yellow shape=parallelogram]
		layer_1_expert_0 [label="Expert 0\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0"]
		layer_1_expert_1 [label="Expert 1\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0"]
		layer_1_expert_2 [label="Expert 2\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 1"]
		layer_1_expert_3 [label="Expert 3\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 1"]
		layer_1_expert_4 [label="Expert 4\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 2"]
		layer_1_expert_5 [label="Expert 5\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 2"]
		layer_1_expert_6 [label="Expert 6\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 3"]
		layer_1_expert_7 [label="Expert 7\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 3"]
		layer_1_expert_8 [label="Expert 8\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 4"]
		layer_1_expert_9 [label="Expert 9\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 4"]
		layer_1_expert_10 [label="Expert 10\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 5"]
		layer_1_expert_11 [label="Expert 11\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 5"]
		layer_1_expert_12 [label="Expert 12\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 6"]
		layer_1_expert_13 [label="Expert 13\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 6"]
		layer_1_expert_14 [label="Expert 14\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 7"]
		layer_1_expert_15 [label="Expert 15\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 7"]
		layer_1_moe_agg [label="MoE Aggregation\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_1_add_norm2 [label="Residual+Norm\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_2_mha [label="MHA Layer 2\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_2_qkv [label="QKV Linear\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_2_attn [label="Attention\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_2_proj [label="Projection\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_2_add_norm1 [label="Residual+Norm\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_2_gate [label="Gating\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, topk=2]\nGPU: 0-7" fillcolor=yellow shape=parallelogram]
		layer_2_expert_0 [label="Expert 0\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0"]
		layer_2_expert_1 [label="Expert 1\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0"]
		layer_2_expert_2 [label="Expert 2\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 1"]
		layer_2_expert_3 [label="Expert 3\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 1"]
		layer_2_expert_4 [label="Expert 4\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 2"]
		layer_2_expert_5 [label="Expert 5\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 2"]
		layer_2_expert_6 [label="Expert 6\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 3"]
		layer_2_expert_7 [label="Expert 7\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 3"]
		layer_2_expert_8 [label="Expert 8\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 4"]
		layer_2_expert_9 [label="Expert 9\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 4"]
		layer_2_expert_10 [label="Expert 10\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 5"]
		layer_2_expert_11 [label="Expert 11\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 5"]
		layer_2_expert_12 [label="Expert 12\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 6"]
		layer_2_expert_13 [label="Expert 13\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 6"]
		layer_2_expert_14 [label="Expert 14\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 7"]
		layer_2_expert_15 [label="Expert 15\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 7"]
		layer_2_moe_agg [label="MoE Aggregation\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_2_add_norm2 [label="Residual+Norm\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_3_mha [label="MHA Layer 3\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_3_qkv [label="QKV Linear\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_3_attn [label="Attention\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_3_proj [label="Projection\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_3_add_norm1 [label="Residual+Norm\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_3_gate [label="Gating\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, topk=2]\nGPU: 0-7" fillcolor=yellow shape=parallelogram]
		layer_3_expert_0 [label="Expert 0\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0"]
		layer_3_expert_1 [label="Expert 1\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0"]
		layer_3_expert_2 [label="Expert 2\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 1"]
		layer_3_expert_3 [label="Expert 3\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 1"]
		layer_3_expert_4 [label="Expert 4\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 2"]
		layer_3_expert_5 [label="Expert 5\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 2"]
		layer_3_expert_6 [label="Expert 6\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 3"]
		layer_3_expert_7 [label="Expert 7\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 3"]
		layer_3_expert_8 [label="Expert 8\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 4"]
		layer_3_expert_9 [label="Expert 9\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 4"]
		layer_3_expert_10 [label="Expert 10\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 5"]
		layer_3_expert_11 [label="Expert 11\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 5"]
		layer_3_expert_12 [label="Expert 12\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 6"]
		layer_3_expert_13 [label="Expert 13\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 6"]
		layer_3_expert_14 [label="Expert 14\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 7"]
		layer_3_expert_15 [label="Expert 15\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 7"]
		layer_3_moe_agg [label="MoE Aggregation\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_3_add_norm2 [label="Residual+Norm\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_4_mha [label="MHA Layer 4\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_4_qkv [label="QKV Linear\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_4_attn [label="Attention\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_4_proj [label="Projection\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_4_add_norm1 [label="Residual+Norm\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_4_gate [label="Gating\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, topk=2]\nGPU: 0-7" fillcolor=yellow shape=parallelogram]
		layer_4_expert_0 [label="Expert 0\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0"]
		layer_4_expert_1 [label="Expert 1\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0"]
		layer_4_expert_2 [label="Expert 2\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 1"]
		layer_4_expert_3 [label="Expert 3\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 1"]
		layer_4_expert_4 [label="Expert 4\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 2"]
		layer_4_expert_5 [label="Expert 5\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 2"]
		layer_4_expert_6 [label="Expert 6\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 3"]
		layer_4_expert_7 [label="Expert 7\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 3"]
		layer_4_expert_8 [label="Expert 8\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 4"]
		layer_4_expert_9 [label="Expert 9\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 4"]
		layer_4_expert_10 [label="Expert 10\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 5"]
		layer_4_expert_11 [label="Expert 11\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 5"]
		layer_4_expert_12 [label="Expert 12\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 6"]
		layer_4_expert_13 [label="Expert 13\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 6"]
		layer_4_expert_14 [label="Expert 14\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 7"]
		layer_4_expert_15 [label="Expert 15\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 7"]
		layer_4_moe_agg [label="MoE Aggregation\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_4_add_norm2 [label="Residual+Norm\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_5_mha [label="MHA Layer 5\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_5_qkv [label="QKV Linear\nInput: [batch=128, seq=10000, hidden=4096]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_5_attn [label="Attention\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, heads=32, d_k=128]\nGPU: 0-7 (TP=8)"]
		layer_5_proj [label="Projection\nInput: [batch=128, seq=10000, heads=32, d_k=128]\nOutput: [batch=128, seq=10000, hidden=4096]\nGPU: 0-7 (TP=8)"]
		layer_5_add_norm1 [label="Residual+Norm\nInput: [batch=128, seq=10000, hidden
