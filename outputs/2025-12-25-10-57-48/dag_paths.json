{
  "dot_file": "../outputs/2025-12-25-10-57-48/llama70b_parallel_dag.dot",
  "svg_file": "../outputs/2025-12-25-10-57-48/llama70b_parallel_dag.svg",
  "graphviz_code": "// Llama3-70B Parallel Strategy Deployment DAG\ndigraph {\n\trankdir=TB size=\"20,30\"\n\tnode [fontname=Arial fontsize=10]\n\tedge [fontname=Arial fontsize=8]\n\tnode [fillcolor=lightblue shape=ellipse style=filled]\n\tnode [fillcolor=lightgreen shape=rectangle style=filled]\n\tnode [fillcolor=lightyellow shape=parallelogram style=filled]\n\tinput [label=\"Input\\nInput: [batch_size=64, seq_len=8192, hidden_size=8192]\\nOutput: [batch_size=64, seq_len=8192, hidden_size=8192]\" fillcolor=lightpink shape=diamond style=filled]\n\tsubgraph cluster_stage0 {\n\t\tfillcolor=lightgray label=\"Stage 0: GPUs 0,1 (Layers 0-19)\" style=\"rounded, filled\"\n\t\tsubgraph cluster_gpu0 {\n\t\t\tlabel=\"GPU 0\" style=rounded\n\t\t\tgpu0_layer0_attn_qkv [label=\"Layer 0 Attention QKV Linear\\nColumn Parallel\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 3072]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu0_layer0_attn_gate [label=\"Layer 0 Attention Gate\\nRouting\\nInput: [64, 8192, 3072]\\nOutput: [64, 8192, 1536]\" fillcolor=lightyellow shape=parallelogram style=filled]\n\t\t\tgpu0_layer0_attn_out [label=\"Layer 0 Attention Output Linear\\nRow Parallel\\nInput: [64, 8192, 1536]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu0_layer0_mlp_gate [label=\"Layer 0 MLP Gate\\nRouting\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 28672]\" fillcolor=lightyellow shape=parallelogram style=filled]\n\t\t\tgpu0_layer0_mlp_up [label=\"Layer 0 MLP Up Linear\\nColumn Parallel\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 28672]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu0_layer0_mlp_down [label=\"Layer 0 MLP Down Linear\\nRow Parallel\\nInput: [64, 8192, 28672]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t}\n\t\tsubgraph cluster_gpu1 {\n\t\t\tlabel=\"GPU 1\" style=rounded\n\t\t\tgpu1_layer0_attn_qkv [label=\"Layer 0 Attention QKV Linear\\nColumn Parallel\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 3072]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu1_layer0_attn_gate [label=\"Layer 0 Attention Gate\\nRouting\\nInput: [64, 8192, 3072]\\nOutput: [64, 8192, 1536]\" fillcolor=lightyellow shape=parallelogram style=filled]\n\t\t\tgpu1_layer0_attn_out [label=\"Layer 0 Attention Output Linear\\nRow Parallel\\nInput: [64, 8192, 1536]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu1_layer0_mlp_gate [label=\"Layer 0 MLP Gate\\nRouting\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 28672]\" fillcolor=lightyellow shape=parallelogram style=filled]\n\t\t\tgpu1_layer0_mlp_up [label=\"Layer 0 MLP Up Linear\\nColumn Parallel\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 28672]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu1_layer0_mlp_down [label=\"Layer 0 MLP Down Linear\\nRow Parallel\\nInput: [64, 8192, 28672]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t}\n\t\tstage0_tp_allreduce1 [label=\"TP All-Reduce\\nAttention Output\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 4096]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tstage0_tp_allreduce2 [label=\"TP All-Reduce\\nMLP Output\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 4096]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tstage0_output [label=\"Stage 0 Output\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 8192]\" fillcolor=lightcoral shape=diamond style=filled]\n\t}\n\tsubgraph cluster_stage1 {\n\t\tfillcolor=lightgray label=\"Stage 1: GPUs 2,3 (Layers 20-39)\" style=\"rounded, filled\"\n\t\tsubgraph cluster_gpu2 {\n\t\t\tlabel=\"GPU 2\" style=rounded\n\t\t\tgpu2_layer20_attn_qkv [label=\"Layer 20 Attention QKV Linear\\nColumn Parallel\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 3072]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu2_layer20_attn_out [label=\"Layer 20 Attention Output Linear\\nRow Parallel\\nInput: [64, 8192, 1536]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu2_layer20_mlp_up [label=\"Layer 20 MLP Up Linear\\nColumn Parallel\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 28672]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu2_layer20_mlp_down [label=\"Layer 20 MLP Down Linear\\nRow Parallel\\nInput: [64, 8192, 28672]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t}\n\t\tsubgraph cluster_gpu3 {\n\t\t\tlabel=\"GPU 3\" style=rounded\n\t\t\tgpu3_layer20_attn_qkv [label=\"Layer 20 Attention QKV Linear\\nColumn Parallel\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 3072]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu3_layer20_attn_out [label=\"Layer 20 Attention Output Linear\\nRow Parallel\\nInput: [64, 8192, 1536]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu3_layer20_mlp_up [label=\"Layer 20 MLP Up Linear\\nColumn Parallel\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 28672]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu3_layer20_mlp_down [label=\"Layer 20 MLP Down Linear\\nRow Parallel\\nInput: [64, 8192, 28672]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t}\n\t\tstage1_tp_allreduce1 [label=\"TP All-Reduce\\nAttention Output\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 4096]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tstage1_tp_allreduce2 [label=\"TP All-Reduce\\nMLP Output\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 4096]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tp2p_send_0_1 [label=\"Pipeline Send\\nStage0\u2192Stage1\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 8192]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tp2p_recv_0_1 [label=\"Pipeline Receive\\nStage0\u2192Stage1\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 8192]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tstage1_output [label=\"Stage 1 Output\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 8192]\" fillcolor=lightcoral shape=diamond style=filled]\n\t}\n\tsubgraph cluster_stage2 {\n\t\tfillcolor=lightgray label=\"Stage 2: GPUs 4,5 (Layers 40-59)\" style=\"rounded, filled\"\n\t\tsubgraph cluster_gpu4 {\n\t\t\tlabel=\"GPU 4\" style=rounded\n\t\t\tgpu4_layer40_attn_qkv [label=\"Layer 40 Attention QKV Linear\\nColumn Parallel\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 3072]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu4_layer40_attn_out [label=\"Layer 40 Attention Output Linear\\nRow Parallel\\nInput: [64, 8192, 1536]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu4_layer40_mlp_up [label=\"Layer 40 MLP Up Linear\\nColumn Parallel\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 28672]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu4_layer40_mlp_down [label=\"Layer 40 MLP Down Linear\\nRow Parallel\\nInput: [64, 8192, 28672]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t}\n\t\tsubgraph cluster_gpu5 {\n\t\t\tlabel=\"GPU 5\" style=rounded\n\t\t\tgpu5_layer40_attn_qkv [label=\"Layer 40 Attention QKV Linear\\nColumn Parallel\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 3072]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu5_layer40_attn_out [label=\"Layer 40 Attention Output Linear\\nRow Parallel\\nInput: [64, 8192, 1536]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu5_layer40_mlp_up [label=\"Layer 40 MLP Up Linear\\nColumn Parallel\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 28672]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu5_layer40_mlp_down [label=\"Layer 40 MLP Down Linear\\nRow Parallel\\nInput: [64, 8192, 28672]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t}\n\t\tstage2_tp_allreduce1 [label=\"TP All-Reduce\\nAttention Output\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 4096]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tstage2_tp_allreduce2 [label=\"TP All-Reduce\\nMLP Output\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 4096]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tp2p_send_1_2 [label=\"Pipeline Send\\nStage1\u2192Stage2\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 8192]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tp2p_recv_1_2 [label=\"Pipeline Receive\\nStage1\u2192Stage2\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 8192]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tstage2_output [label=\"Stage 2 Output\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 8192]\" fillcolor=lightcoral shape=diamond style=filled]\n\t}\n\tsubgraph cluster_stage3 {\n\t\tfillcolor=lightgray label=\"Stage 3: GPUs 6,7 (Layers 60-79)\" style=\"rounded, filled\"\n\t\tsubgraph cluster_gpu6 {\n\t\t\tlabel=\"GPU 6\" style=rounded\n\t\t\tgpu6_layer60_attn_qkv [label=\"Layer 60 Attention QKV Linear\\nColumn Parallel\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 3072]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu6_layer60_attn_out [label=\"Layer 60 Attention Output Linear\\nRow Parallel\\nInput: [64, 8192, 1536]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu6_layer60_mlp_up [label=\"Layer 60 MLP Up Linear\\nColumn Parallel\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 28672]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu6_layer60_mlp_down [label=\"Layer 60 MLP Down Linear\\nRow Parallel\\nInput: [64, 8192, 28672]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu6_lm_head [label=\"LM Head\\nColumn Parallel\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 128256]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t}\n\t\tsubgraph cluster_gpu7 {\n\t\t\tlabel=\"GPU 7\" style=rounded\n\t\t\tgpu7_layer60_attn_qkv [label=\"Layer 60 Attention QKV Linear\\nColumn Parallel\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 3072]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu7_layer60_attn_out [label=\"Layer 60 Attention Output Linear\\nRow Parallel\\nInput: [64, 8192, 1536]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu7_layer60_mlp_up [label=\"Layer 60 MLP Up Linear\\nColumn Parallel\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 28672]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu7_layer60_mlp_down [label=\"Layer 60 MLP Down Linear\\nRow Parallel\\nInput: [64, 8192, 28672]\\nOutput: [64, 8192, 4096]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t\tgpu7_lm_head [label=\"LM Head\\nColumn Parallel\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 128256]\" fillcolor=lightgreen shape=rectangle style=filled]\n\t\t}\n\t\tstage3_tp_allreduce1 [label=\"TP All-Reduce\\nAttention Output\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 4096]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tstage3_tp_allreduce2 [label=\"TP All-Reduce\\nMLP Output\\nInput: [64, 8192, 4096]\\nOutput: [64, 8192, 4096]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tp2p_send_2_3 [label=\"Pipeline Send\\nStage2\u2192Stage3\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 8192]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tp2p_recv_2_3 [label=\"Pipeline Receive\\nStage2\u2192Stage3\\nInput: [64, 8192, 8192]\\nOutput: [64, 8192, 8192]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\tfinal_allreduce [label=\"Final All-Reduce\\nLM Head Output\\nInput: [64, 8192, 128256]\\nOutput: [64, 8192, 128256]\" fillcolor=lightblue shape=ellipse style=filled]\n\t\toutput [label=\"Final Output\\nInput: [64, 8192, 128256]\\nOutput: [64, 8192, 128256]\" fillcolor=lightpink shape=diamond style=filled]\n\t}\n\tinput -> gpu0_layer0_attn_qkv\n\tinput -> gpu1_layer0_attn_qkv\n\tgpu0_layer0_attn_qkv -> gpu0_layer0_attn_gate\n\tgpu1_layer0_attn_qkv -> gpu1_layer0_attn_gate\n\tgpu0_layer0_attn_gate -> gpu0_layer0_attn_out\n\tgpu1_layer0_attn_gate -> gpu1_layer0_attn_out\n\tgpu0_layer0_attn_out -> stage0_tp_allreduce1\n\tgpu1_layer0_attn_out -> stage0_tp_allreduce1\n\tstage0_tp_allreduce1 -> gpu0_layer0_mlp_gate\n\tstage0_tp_allreduce1 -> gpu0_layer0_mlp_up\n\tstage0_tp_allreduce1 -> gpu1_layer0_mlp_gate\n\tstage0_tp_allreduce1 -> gpu1_layer0_mlp_up\n\tgpu0_layer0_mlp_gate -> gpu0_layer0_mlp_down\n\tgpu1_layer0_mlp_gate -> gpu1_layer0_mlp_down\n\tgpu0_layer0_mlp_up -> gpu0_layer0_mlp_down\n\tgpu1_layer0_mlp_up -> gpu1_layer0_mlp_down\n\tgpu0_layer0_mlp_down -> stage0_tp_allreduce2\n\tgpu1_layer0_mlp_down -> stage0_tp_allreduce2\n\tstage0_tp_allreduce2 -> stage0_output\n\tstage0_output -> p2p_send_0_1\n\tp2p_send_0_1 -> p2p_recv_0_1\n\tp2p_recv_0_1 -> gpu2_layer20_attn_qkv\n\tp2p_recv_0_1 -> gpu3_layer20_attn_qkv\n\tgpu2_layer20_attn_qkv -> gpu2_layer20_attn_out\n\tgpu3_layer20_attn_qkv -> gpu3_layer20_attn_out\n\tgpu2_layer20_attn_out -> stage1_tp_allreduce1\n\tgpu3_layer20_attn_out -> stage1_tp_allreduce1\n\tstage1_tp_allreduce1 -> gpu2_layer20_mlp_up\n\tstage1_tp_allreduce1 -> gpu3_layer20_mlp_up\n\tgpu2_layer20_mlp_up -> gpu2_layer20_mlp_down\n\tgpu3_layer20_mlp_up -> gpu3_layer20_mlp_down\n\tgpu2_layer20_mlp_down -> stage1_tp_allreduce2\n\tgpu3_layer20_mlp_down -> stage1_tp_allreduce2\n\tstage1_tp_allreduce2 -> stage1_output\n\tstage1_output -> p2p_send_1_2\n\tp2p_send_1_2 -> p2p_recv_1_2\n\tp2p_recv_1_2 -> gpu4_layer40_attn_qkv\n\tp2p_recv_1_2 -> gpu5_layer40_attn_qkv\n\tgpu4_layer40_attn_qkv -> gpu4_layer40_attn_out\n\tgpu5_layer40_attn_qkv -> gpu5_layer40_attn_out\n\tgpu4_layer40_attn_out -> stage2_tp_allreduce1\n\tgpu5_layer40_attn_out -> stage2_tp_allreduce1\n\tstage2_tp_allreduce1 -> gpu4_layer40_mlp_up\n\tstage2_tp_allreduce1 -> gpu5_layer40_mlp_up\n\tgpu4_layer40_mlp_up -> gpu4_layer40_mlp_down\n\tgpu5_layer40_mlp_up -> gpu5_layer40_mlp_down\n\tgpu4_layer40_mlp_down -> stage2_tp_allreduce2\n\tgpu5_layer40_mlp_down -> stage2_tp_allreduce2\n\tstage2_tp_allreduce2 -> stage2_output\n\tstage2_output -> p2p_send_2_3\n\tp2p_send_2_3 -> p2p_recv_2_3\n\tp2p_recv_2_3 -> gpu6_layer60_attn_qkv\n\tp2p_recv_2_3 -> gpu7_layer60_attn_qkv\n\tgpu6_layer60_attn_qkv -> gpu6_layer60_attn_out\n\tgpu7_layer60_attn_qkv -> gpu7_layer60_attn_out\n\tgpu6_layer60_attn_out -> stage3_tp_allreduce1\n\tgpu7_layer60_attn_out -> stage3_tp_allreduce1\n\tstage3_tp_allreduce1 -> gpu6_layer60_mlp_up\n\tstage3_tp_allreduce1 -> gpu7_layer60_mlp_up\n\tgpu6_layer60_mlp_up -> gpu6_layer60_mlp_down\n\tgpu7_layer60_mlp_up -> gpu7_layer60_mlp_down\n\tgpu6_layer60_mlp_down -> stage3_tp_allreduce2\n\tgpu7_layer60_mlp_down -> stage3_tp_allreduce2\n\tstage3_tp_allreduce2 -> gpu6_lm_head\n\tstage3_tp_allreduce2 -> gpu7_lm_head\n\tgpu6_lm_head -> final_allreduce\n\tgpu7_lm_head -> final_allreduce\n\tfinal_allreduce -> output\n}\n"
}