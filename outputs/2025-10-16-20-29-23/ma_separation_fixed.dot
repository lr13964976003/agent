digraph ma_separation {
    rankdir=TB;
    compound=true;
    ranksep=2.0;
    nodesep=0.8;
    
    subgraph cluster_attention {
        label="Attention Computation (GPUs 0-11)";
        style=rounded;
        color=blue;
        bgcolor=lightblue;
        
        input [shape=ellipse, style=filled, fillcolor=lightgreen, label="Model Input\nInput: [batch_size=1024, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs"];
        
        embed [label="Token Embedding\nInput: [batch_size=1024, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        // ======= LAYER 0 =======
        ln1_l0 [label="LayerNorm L0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        qkv_l0_gpu0 [label="QKV Projection\nGPU 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 0"];
        qkv_l0_gpu1 [label="QKV Projection\nGPU 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 1"];
        qkv_l0_gpu2 [label="QKV Projection\nGPU 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 2"];
        qkv_l0_gpu3 [label="QKV Projection\nGPU 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 3"];
        qkv_l0_gpu4 [label="QKV Projection\nGPU 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 4"];
        qkv_l0_gpu5 [label="QKV Projection\nGPU 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 5"];
        qkv_l0_gpu6 [label="QKV Projection\nGPU 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 6"];
        qkv_l0_gpu7 [label="QKV Projection\nGPU 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 7"];
        qkv_l0_gpu8 [label="QKV Projection\nGPU 8\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 8"];
        qkv_l0_gpu9 [label="QKV Projection\nGPU 9\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 9"];
        qkv_l0_gpu10 [label="QKV Projection\nGPU 10\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 10"];
        qkv_l0_gpu11 [label="QKV Projection\nGPU 11\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 11"];
        
        scores_l0_gpu0 [label="Attention Scores\nGPU 0\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 0"];
        scores_l0_gpu1 [label="Attention Scores\nGPU 1\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 1"];
        scores_l0_gpu2 [label="Attention Scores\nGPU 2\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 2"];
        scores_l0_gpu3 [label="Attention Scores\nGPU 3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 3"];
        scores_l0_gpu4 [label="Attention Scores\nGPU 4\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 4"];
        scores_l0_gpu5 [label="Attention Scores\nGPU 5\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 5"];
        scores_l0_gpu6 [label="Attention Scores\nGPU 6\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 6"];
        scores_l0_gpu7 [label="Attention Scores\nGPU 7\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 7"];
        scores_l0_gpu8 [label="Attention Scores\nGPU 8\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 8"];
        scores_l0_gpu9 [label="Attention Scores\nGPU 9\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 9"];
        scores_l0_gpu10 [label="Attention Scores\nGPU 10\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 10"];
        scores_l0_gpu11 [label="Attention Scores\nGPU 11\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 11"];
        
        softmax_l0_gpu0 [label="Softmax\nGPU 0\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 0"];
        softmax_l0_gpu1 [label="Softmax\nGPU 1\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 1"];
        softmax_l0_gpu2 [label="Softmax\nGPU 2\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 2"];
        softmax_l0_gpu3 [label="Softmax\nGPU 3\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 3"];
        softmax_l0_gpu4 [label="Softmax\nGPU 4\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 4"];
        softmax_l0_gpu5 [label="Softmax\nGPU 5\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 5"];
        softmax_l0_gpu6 [label="Softmax\nGPU 6\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 6"];
        softmax_l0_gpu7 [label="Softmax\nGPU 7\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 7"];
        softmax_l0_gpu8 [label="Softmax\nGPU 8\nInput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 8"];
        softmax_l0_gpu9 [label="Softmax\nGPU 9\nInput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 9"];
        softmax_l0_gpu10 [label="Softmax\nGPU 10\nInput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 10"];
        softmax_l0_gpu11 [label="Softmax\nGPU 11\nInput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 11"];
        
        attn_out_l0_gpu0 [label="Attention Output\nGPU 0\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 0"];
        attn_out_l0_gpu1 [label="Attention Output\nGPU 1\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 1"];
        attn_out_l0_gpu2 [label="Attention Output\nGPU 2\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 2"];
        attn_out_l0_gpu3 [label="Attention Output\nGPU 3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 3"];
        attn_out_l0_gpu4 [label="Attention Output\nGPU 4\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 4"];
        attn_out_l0_gpu5 [label="Attention Output\nGPU 5\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 5"];
        attn_out_l0_gpu6 [label="Attention Output\nGPU 6\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 6"];
        attn_out_l0_gpu7 [label="Attention Output\nGPU 7\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 7"];
        attn_out_l0_gpu8 [label="Attention Output\nGPU 8\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 8"];
        attn_out_l0_gpu9 [label="Attention Output\nGPU 9\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 9"];
        attn_out_l0_gpu10 [label="Attention Output\nGPU 10\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 10"];
        attn_out_l0_gpu11 [label="Attention Output\nGPU 11\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 11"];
        
        o_proj_l0_gpu0 [label="Output Projection\nGPU 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 0"];
        o_proj_l0_gpu1 [label="Output Projection\nGPU 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 1"];
        o_proj_l0_gpu2 [label="Output Projection\nGPU 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 2"];
        o_proj_l0_gpu3 [label="Output Projection\nGPU 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 3"];
        o_proj_l0_gpu4 [label="Output Projection\nGPU 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 4"];
        o_proj_l0_gpu5 [label="Output Projection\nGPU 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 5"];
        o_proj_l0_gpu6 [label="Output Projection\nGPU 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 6"];
        o_proj_l0_gpu7 [label="Output Projection\nGPU 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 7"];
        o_proj_l0_gpu8 [label="Output Projection\nGPU 8\nInput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 8"];
        o_proj_l0_gpu9 [label="Output Projection\nGPU 9\nInput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 9"];
        o_proj_l0_gpu10 [label="Output Projection\nGPU 10\nInput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 10"];
        o_proj_l0_gpu11 [label="Output Projection\nGPU 11\nInput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 11"];
        
        all_reduce_attn_l0 [shape=ellipse, style=filled, fillcolor=yellow, label="Attention All-Reduce\nHierarchical\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        res1_l0 [shape=ellipse, style=filled, fillcolor=pink, label="Residual Add L0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        broadcast_l0 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Broadcast to MoE\nLayer 0\nFrom: GPUs 0-11\nTo: GPUs 12-15\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
        
        // ======= LAYER 1 =======
        ln1_l1 [label="LayerNorm L1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        qkv_l1_gpu0 [label="QKV Projection L1\nGPU 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 0"];
        qkv_l1_gpu1 [label="QKV Projection L1\nGPU 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 1"];
        qkv_l1_gpu2 [label="QKV Projection L1\nGPU 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 2"];
        qkv_l1_gpu3 [label="QKV Projection L1\nGPU 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 3"];
        qkv_l1_gpu4 [label="QKV Projection L1\nGPU 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 4"];
        qkv_l1_gpu5 [label="QKV Projection L1\nGPU 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 5"];
        qkv_l1_gpu6 [label="QKV Projection L1\nGPU 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 6"];
        qkv_l1_gpu7 [label="QKV Projection L1\nGPU 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 7"];
        qkv_l1_gpu8 [label="QKV Projection L1\nGPU 8\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 8"];
        qkv_l1_gpu9 [label="QKV Projection L1\nGPU 9\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 9"];
        qkv_l1_gpu10 [label="QKV Projection L1\nGPU 10\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 10"];
        qkv_l1_gpu11 [label="QKV Projection L1\nGPU 11\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 11"];
        
        // Complete layer 1 attention nodes... [similar pattern as layer 0]
        scores_l1_gpu0 [label="Attention Scores L1\nGPU 0\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 0"];
        scores_l1_gpu1 [label="Attention Scores L1\nGPU 1\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 1"];
        scores_l1_gpu2 [label="Attention Scores L1\nGPU 2\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 2"];
        scores_l1_gpu3 [label="Attention Scores L1\nGPU 3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 3"];
        scores_l1_gpu4 [label="Attention Scores L1\nGPU 4\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 4"];
        scores_l1_gpu5 [label="Attention Scores L1\nGPU 5\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 5"];
        scores_l1_gpu6 [label="Attention Scores L1\nGPU 6\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 6"];
        scores_l1_gpu7 [label="Attention Scores L1\nGPU 7\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 7"];
        scores_l1_gpu8 [label="Attention Scores L1\nGPU 8\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 8"];
        scores_l1_gpu9 [label="Attention Scores L1\nGPU 9\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 9"];
        scores_l1_gpu10 [label="Attention Scores L1\nGPU 10\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 10"];
        scores_l1_gpu11 [label="Attention Scores L1\nGPU 11\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 11"];
        
        softmax_l1_gpu0 [label="Softmax L1\nGPU 0\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 0"];
        softmax_l1_gpu1 [label="Softmax L1\nGPU 1\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 1"];
        softmax_l1_gpu2 [label="Softmax L1\nGPU 2\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 2"];
        softmax_l1_gpu3 [label="Softmax L1\nGPU 3\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 3"];
        softmax_l1_gpu4 [label="Softmax L1\nGPU 4\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 4"];
        softmax_l1_gpu5 [label="Softmax L1\nGPU 5\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 5"];
        softmax_l1_gpu6 [label="Softmax L1\nGPU 6\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 6"];
        softmax_l1_gpu7 [label="Softmax L1\nGPU 7\nInput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 7"];
        softmax_l1_gpu8 [label="Softmax L1\nGPU 8\nInput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 8"];
        softmax_l1_gpu9 [label="Softmax L1\nGPU 9\nInput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 9"];
        softmax_l1_gpu10 [label="Softmax L1\nGPU 10\nInput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 10"];
        softmax_l1_gpu11 [label="Softmax L1\nGPU 11\nInput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 11"];
        
        attn_out_l1_gpu0 [label="Attention Output L1\nGPU 0\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 0"];
        attn_out_l1_gpu1 [label="Attention Output L1\nGPU 1\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 1"];
        attn_out_l1_gpu2 [label="Attention Output L1\nGPU 2\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 2"];
        attn_out_l1_gpu3 [label="Attention Output L1\nGPU 3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 3"];
        attn_out_l1_gpu4 [label="Attention Output L1\nGPU 4\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 4"];
        attn_out_l1_gpu5 [label="Attention Output L1\nGPU 5\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 5"];
        attn_out_l1_gpu6 [label="Attention Output L1\nGPU 6\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 6"];
        attn_out_l1_gpu7 [label="Attention Output L1\nGPU 7\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 7"];
        attn_out_l1_gpu8 [label="Attention Output L1\nGPU 8\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 8"];
        attn_out_l1_gpu9 [label="Attention Output L1\nGPU 9\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 9"];
        attn_out_l1_gpu10 [label="Attention Output L1\nGPU 10\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 10"];
        attn_out_l1_gpu11 [label="Attention Output L1\nGPU 11\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 11"];
        
        o_proj_l1_gpu0 [label="Output Projection L1\nGPU 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 0"];
        o_proj_l1_gpu1 [label="Output Projection L1\nGPU 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 1"];
        o_proj_l1_gpu2 [label="Output Projection L1\nGPU 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 2"];
        o_proj_l1_gpu3 [label="Output Projection L1\nGPU 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 3"];
        o_proj_l1_gpu4 [label="Output Projection L1\nGPU 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 4"];
        o_proj_l1_gpu5 [label="Output Projection L1\nGPU 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 5"];
        o_proj_l1_gpu6 [label="Output Projection L1\nGPU 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 6"];
        o_proj_l1_gpu7 [label="Output Projection L1\nGPU 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 7"];
        o_proj_l1_gpu8 [label="Output Projection L1\nGPU 8\nInput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 8"];
        o_proj_l1_gpu9 [label="Output Projection L1\nGPU 9\nInput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 9"];
        o_proj_l1_gpu10 [label="Output Projection L1\nGPU 10\nInput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 10"];
        o_proj_l1_gpu11 [label="Output Projection L1\nGPU 11\nInput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 11"];
        
        all_reduce_attn_l1 [shape=ellipse, style=filled, fillcolor=yellow, label="Attention All-Reduce L1\nHierarchical\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        res1_l1 [shape=ellipse, style=filled, fillcolor=pink, label="Residual Add L1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        broadcast_l1 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Broadcast to MoE\nLayer 1\nFrom: GPUs 0-11\nTo: GPUs 12-15\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
        
        // ======= LAYER 2 =======
        ln1_l2 [label="LayerNorm L2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        qkv_l2_gpu0 [label="QKV Projection L2\nGPU 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 0"];
        qkv_l2_gpu1 [label="QKV Projection L2\nGPU 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 1"];
        qkv_l2_gpu2 [label="QKV Projection L2\nGPU 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 2"];
        qkv_l2_gpu3 [label="QKV Projection L2\nGPU 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 3"];
        qkv_l2_gpu4 [label="QKV Projection L2\nGPU 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 4"];
        qkv_l2_gpu5 [label="QKV Projection L2\nGPU 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 5"];
        qkv_l2_gpu6 [label="QKV Projection L2\nGPU 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 6"];
        qkv_l2_gpu7 [label="QKV Projection L2\nGPU 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 7"];
        qkv_l2_gpu8 [label="QKV Projection L2\nGPU 8\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 8"];
        qkv_l2_gpu9 [label="QKV Projection L2\nGPU 9\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 9"];
        qkv_l2_gpu10 [label="QKV Projection L2\nGPU 10\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 10"];
        qkv_l2_gpu11 [label="QKV Projection L2\nGPU 11\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 11"];
        
        all_reduce_attn_l2 [shape=ellipse, style=filled, fillcolor=yellow, label="Attention All-Reduce L2\nHierarchical\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        res1_l2 [shape=ellipse, style=filled, fillcolor=pink, label="Residual Add L2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        broadcast_l2 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Broadcast to MoE\nLayer 2\nFrom: GPUs 0-11\nTo: GPUs 12-15\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
        
        // ======= LAYER 3 =======
        ln1_l3 [label="LayerNorm L3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        qkv_l3_gpu0 [label="QKV Projection L3\nGPU 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 0"];
        qkv_l3_gpu1 [label="QKV Projection L3\nGPU 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 1"];
        qkv_l3_gpu2 [label="QKV Projection L3\nGPU 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 2"];
        qkv_l3_gpu3 [label="QKV Projection L3\nGPU 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 3"];
        qkv_l3_gpu4 [label="QKV Projection L3\nGPU 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 4"];
        qkv_l3_gpu5 [label="QKV Projection L3\nGPU 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 5"];
        qkv_l3_gpu6 [label="QKV Projection L3\nGPU 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 6"];
        qkv_l3_gpu7 [label="QKV Projection L3\nGPU 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 7"];
        qkv_l3_gpu8 [label="QKV Projection L3\nGPU 8\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 8"];
        qkv_l3_gpu9 [label="QKV Projection L3\nGPU 9\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 9"];
        qkv_l3_gpu10 [label="QKV Projection L3\nGPU 10\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 10"];
        qkv_l3_gpu11 [label="QKV Projection L3\nGPU 11\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 11"];
        
        all_reduce_attn_l3 [shape=ellipse, style=filled, fillcolor=yellow, label="Attention All-Reduce L3\nHierarchical\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        res1_l3 [shape=ellipse, style=filled, fillcolor=pink, label="Residual Add L3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        final_output [shape=ellipse, style=filled, fillcolor=lightgreen, label="Model Output\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, vocab_size=50265]\nGPU: 0-11"];
    }
    
    subgraph cluster_moe {
        label="MoE Computation (GPUs 12-15)";
        style=rounded;
        color=red;
        bgcolor=lightcoral;
        
        // Layer 0 MoE
        recv_moe_l0 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Receive from Attention\nLayer 0\nFrom: GPUs 0-11\nTo: GPUs 12-15\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
        
        ln_moe_l0_gpu12 [label="LayerNorm MoE L0\nGPU 12\nInput/Output: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12"];
        ln_moe_l0_gpu13 [label="LayerNorm MoE L0\nGPU 13\nInput/Output: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 13"];
        ln_moe_l0_gpu14 [label="LayerNorm MoE L0\nGPU 14\nInput/Output: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 14"];
        ln_moe_l0_gpu15 [label="LayerNorm MoE L0\nGPU 15\nInput/Output: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15"];
        
        gate_l0_gpu12 [shape=parallelogram, style=filled, fillcolor=orange, label="Gate Network L0\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 12"];
        gate_l0_gpu13 [shape=parallelogram, style=filled, fillcolor=orange, label="Gate Network L0\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 13"];
        gate_l0_gpu14 [shape=parallelogram, style=filled, fillcolor=orange, label="Gate Network L0\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 14"];
        gate_l0_gpu15 [shape=parallelogram, style=filled, fillcolor=orange, label="Gate Network L0\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 15"];
        
        // Experts for Layer 0
        expert_l0_0_gpu12 [label="Expert 0 L0\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12"];
        expert_l0_1_gpu12 [label="Expert 1 L0\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12"];
        expert_l0_2_gpu12 [label="Expert 2 L0\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12"];
        expert_l0_3_gpu12 [label="Expert 3 L0\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12"];
        
        expert_l0_4_gpu13 [label="Expert 4 L0\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 13"];
        expert_l0_5_gpu13 [label="Expert 5 L0\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 13"];
        expert_l0_6_gpu13 [label="Expert 6 L0\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 13"];
        expert_l0_7_gpu13 [label="Expert 7 L0\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 13"];
        
        expert_l0_8_gpu14 [label="Expert 8 L0\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 14"];
        expert_l0_9_gpu14 [label="Expert 9 L0\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 14"];
        expert_l0_10_gpu14 [label="Expert 10 L0\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 14"];
        expert_l0_11_gpu14 [label="Expert 11 L0\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 14"];
        
        expert_l0_12_gpu15 [label="Expert 12 L0\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15"];
        expert_l0_13_gpu15 [label="Expert 13 L0\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15"];
        expert_l0_14_gpu15 [label="Expert 14 L0\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15"];
        expert_l0_15_gpu15 [label="Expert 15 L0\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15"];
        
        expert_agg_l0_gpu12 [shape=ellipse, style=filled, fillcolor=lightyellow, label="Expert Aggregation L0\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096, top_k=2]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12"];
        expert_agg_l0_gpu13 [shape=ellipse, style=filled, fillcolor=lightyellow, label="Expert Aggregation L0\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096, top_k=2]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 13"];
        expert_agg_l0_gpu14 [shape=ellipse, style=filled, fillcolor=lightyellow, label="Expert Aggregation L0\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096, top_k=2]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 14"];
        expert_agg_l0_gpu15 [shape=ellipse, style=filled, fillcolor=lightyellow, label="Expert Aggregation L0\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096, top_k=2]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15"];
        
        all_to_all_l0_gpu12 [shape=ellipse, style=filled, fillcolor=yellow, label="All-to-All L0\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12"];
        all_to_all_l0_gpu13 [shape=ellipse, style=filled, fillcolor=yellow, label="All-to-All L0\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 13"];
        all_to_all_l0_gpu14 [shape=ellipse, style=filled, fillcolor=yellow, label="All-to-All L0\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 14"];
        all_to_all_l0_gpu15 [shape=ellipse, style=filled, fillcolor=yellow, label="All-to-All L0\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15"];
        
        return_l0 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Return to Attention\nLayer 0\nFrom: GPUs 12-15\nTo: GPUs 0-11\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
        
        // Similar structures for Layers 1-3 MoE
        recv_moe_l1 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Receive from Attention\nLayer 1\nFrom: GPUs 0-11\nTo: GPUs 12-15\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
        recv_moe_l2 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Receive from Attention\nLayer 2\nFrom: GPUs 0-11\nTo: GPUs 12-15\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
        recv_moe_l3 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Receive from Attention\nLayer 3\nFrom: GPUs 0-11\nTo: GPUs 12-15\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
        
        return_l1 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Return to Attention\nLayer 1\nFrom: GPUs 12-15\nTo: GPUs 0-11\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
        return_l2 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Return to Attention\nLayer 2\nFrom: GPUs 12-15\nTo: GPUs 0-11\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
        return_l3 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Return to Attention\nLayer 3\nFrom: GPUs 12-15\nTo: GPUs 0-11\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
    }
    
    // ======= LAYER 0 FLOWS =======
    input -> embed;
    embed -> ln1_l0;
    ln1_l0 -> qkv_l0_gpu0;
    ln1_l0 -> qkv_l0_gpu1;
    ln1_l0 -> qkv_l0_gpu2;
    ln1_l0 -> qkv_l0_gpu3;
    ln1_l0 -> qkv_l0_gpu4;
    ln1_l0 -> qkv_l0_gpu5;
    ln1_l0 -> qkv_l0_gpu6;
    ln1_l0 -> qkv_l0_gpu7;
    ln1_l0 -> qkv_l0_gpu8;
    ln1_l0 -> qkv_l0_gpu9;
    ln1_l0 -> qkv_l0_gpu10;
    ln1_l0 -> qkv_l0_gpu11;
    
    q