{
  "models": {
    "baseline_tp_8_pp_2": {
      "parallel_strategy": "hybrid_tensor_pipeline",
      "parameters": {
        "tensor_parallel_degree": 8,
        "pipeline_parallel_degree": 2,
        "total_gpus": 16
      },
      "module_division": {
        "pipeline_stages": [
          {
            "stage_id": 0,
            "layers": [0, 1],
            "gpus": [0, 1, 2, 3, 4, 5, 6, 7],
            "tensor_parallel": true
          },
          {
            "stage_id": 1,
            "layers": [2, 3],
            "gpus": [8, 9, 10, 11, 12, 13, 14, 15],
            "tensor_parallel": true
          }
        ]
      },
      "device_mapping": {
        "layer_0": {
          "attention": {
            "gpus": [0, 1, 2, 3, 4, 5, 6, 7],
            "head_distribution": [4, 4, 4, 4, 4, 4, 4, 4],
            "parameters": ["q_proj", "k_proj", "v_proj", "o_proj"]
          },
          "mlp": {
            "gpus": [0, 1, 2, 3, 4, 5, 6, 7],
            "tensor_parallel_split": "column_then_row"
          }
        },
        "layer_1": {
          "attention": {
            "gpus": [0, 1, 2, 3, 4, 5, 6, 7],
            "head_distribution": [4, 4, 4, 4, 4, 4, 4, 4]
          },
          "mlp": {
            "gpus": [0, 1, 2, 3, 4, 5, 6, 7]
          }
        },
        "layer_2": {
          "attention": {
            "gpus": [8, 9, 10, 11, 12, 13, 14, 15],
            "head_distribution": [4, 4, 4, 4, 4, 4, 4, 4]
          },
          "mlp": {
            "gpus": [8, 9, 10, 11, 12, 13, 14, 15]
          }
        },
        "layer_3": {
          "attention": {
            "gpus": [8, 9, 10, 11, 12, 13, 14, 15],
            "head_distribution": [4, 4, 4, 4, 4, 4, 4, 4]
          },
          "mlp": {
            "gpus": [8, 9, 10, 11, 12, 13, 14, 15]
          }
        }
      }
    },
    "ma_separation": {
      "parallel_strategy": "ma_separation",
      "parameters": {
        "attention_gpus": 12,
        "moe_gpus": 4,
        "total_gpus": 16,
        "gpu_allocation_ratio": "3:1"
      },
      "module_division": {
        "attention_modules": {
          "gpus": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
          "replication_factor": 2,
          "modules": [
            "qkv_projection",
            "attention_scores",
            "attention_output",
            "output_projection"
          ]
        },
        "moe_modules": {
          "gpus": [12, 13, 14, 15],
          "modules": [
            "gate_network",
            "expert_0", "expert_1", "expert_2", "expert_3",
            "expert_computation",
            "expert_aggregation"
          ]
        }
      },
      "device_mapping": {
        "attention_computation": {
          "gpu_0": {
            "heads": [0, 1, 2],
            "sequence_split": "full",
            "memory_allocated": "10.3 GB"
          },
          "gpu_1": {
            "heads": [3, 4, 5],
            "sequence_split": "full",
            "memory_allocated": "10.3 GB"
          },
          "gpu_2": {
            "heads": [6, 7, 8],
            "sequence_split": "full",
            "memory_allocated": "10.3 GB"
          },
          "gpu_3": {
            "heads": [9, 10, 11],
            "sequence_split": "full",
            "memory_allocated": "10.3 GB"
          },
          "gpu_4": {
            "heads": [12, 13, 14],
            "sequence_split": "full",
            "memory_allocated": "10.3 GB"
          },
          "gpu_5": {
            "heads": [15, 16, 17],
            "sequence_split": "full",
            "memory_allocated": "10.3 GB"
          },
          "gpu_6": {
            "heads": [18, 19, 20],
            "sequence_split": "full",
            "memory_allocated": "10.3 GB"
          },
          "gpu_7": {
            "heads": [21, 22, 23],
            "sequence_split": "full",
            "memory_allocated": "10.3 GB"
          },
          "gpu_8": {
            "heads": [24, 25],
            "sequence_split": "full",
            "memory_allocated": "6.9 GB"
          },
          "gpu_9": {
            "heads": [26, 27],
            "sequence_split": "full",
            "memory_allocated": "6.9 GB"
          },
          "gpu_10": {
            "heads": [28, 29],
            "sequence_split": "full",
            "memory_allocated": "6.9 GB"
          },
          "gpu_11": {
            "heads": [30, 31],
            "sequence_split": "full",
            "memory_allocated": "6.9 GB"
          }
        },
        "moe_computation": {
          "gpu_12": {
            "experts": [0, 1, 2, 3],
            "parameters_per_expert": "67.1 MB",
            "total_expert_memory": "268.4 MB"
          },
          "gpu_13": {
            "experts": [4, 5, 6, 7],
            "parameters_per_expert": "67.1 MB",
            "total_expert_memory": "268.4 MB"
          },
          "gpu_14": {
            "experts": [8, 9, 10, 11],
            "parameters_per_expert": "67.1 MB",
            "total_expert_memory": "268.4 MB"
          },
          "gpu_15": {
            "experts": [12, 13, 14, 15],
            "parameters_per_expert": "67.1 MB",
            "total_expert_memory": "268.4 MB"
          }
        }
      },
      "communication_patterns": {
        "attention_all_reduce": {
          "type": "hierarchical_all_reduce",
          "intra_node": {
            "groups": [
              [0, 1, 2, 3],
              [4, 5, 6, 7],
              [8, 9, 10, 11]
            ],
            "bandwidth": "600 GB/s"
          },
          "inter_node": {
            "nodes": [0, 1, 2],
            "bandwidth": "200 Gb/s"
          }
        },
        "moe_all_to_all": {
          "type": "all_to_all",
          "gpus": [12, 13, 14, 15],
          "bandwidth": "600 GB/s"
        },
        "attention_to_moe": {
          "type": "broadcast",
          "from": "attention_gpus",
          "to": "moe_gpus",
          "data_size": "33.6 MB"  # batch_size * seq_len * hidden_dim
        }
      },
      "synchronization": {
        "time_prediction_model": {
          "type": "3_layer_mlp",
          "input_features": 5,
          "hidden_layers": [64, 32],
          "output_features": 2,
          "threshold": 0.05
        },
        "cuda_events": {
          "attention_events": ["event_0", "event_1", ..., "event_11"],
          "moe_events": ["event_12", "event_13", "event_14", "event_15"],
          "sync_interval": 100
        }
      },
      "memory_requirements": {
        "per_gpu": {
          "model_parameters": 23.1,
          "activations": 18.7,
          "gradients": 23.1,
          "optimizer_states": 46.2,
          "communication_buffers": 12.6,
          "total": 123.7
        },
        "units": "GB"
      }
    }
  },
  "model_configurations": {
    "common": {
      "layers": 4,
      "hidden_dimension": 4096,
      "attention_heads": 32,
      "head_dimension": 128,
      "sequence_length": 2048,
      "batch_size": 1024,
      "total_tokens_per_batch": 2097152,
      "vocab_size": 50265
    },
    "moe_specific": {
      "experts_per_layer": 16,
      "expert_hidden_dimension": 16384,
      "top_k": 2,
      "capacity_factor": 1.0,
      "load_balancing_loss": 0.01,
      "router_z_loss": 0.001,
      "expert_dropout": 0.1
    }
  },
  "hardware_requirements": {
    "total_gpus": 16,
    "gpu_type": "NVIDIA A100 80GB",
    "cpu": "AMD EPYC 7763 64-Core",
    "system_memory": "1TB DDR4",
    "interconnect": {
      "intra_node": "NVLink 3.0 (600 GB/s)",
      "inter_node": "InfiniBand HDR (200 Gb/s)"
    },
    "node_topology": {
      "node_0": [0, 1, 2, 3],
      "node_1": [4, 5, 6, 7],
      "node_2": [8, 9, 10, 11],
      "node_3": [12, 13, 14, 15]
    }
  },
  "software_stack": {
    "pytorch": "2.0",
    "cuda": "11.8",
    "nccl": "2.15",
    "python": ">=3.8",
    "optimizations": [
      "mixed_precision_fp16_bf16",
      "gradient_checkpointing",
      "fused_operations",
      "dynamic_tensor_parallelism"
    ]
  },
  "performance_benchmarks": {
    "baseline_tp_8_pp_2": {
      "tpot_ms": 2.76,
      "tps": 8696,
      "gpu_utilization": 71.2,
      "memory_efficiency": 74.1
    },
    "ma_separation": {
      "tpot_ms": 1.82,
      "tps": 13289,
      "gpu_utilization": 89.7,
      "memory_efficiency": 85.4,
      "improvements": {
        "tpot_reduction": 34.2,
        "tps_increase": 52.8,
        "gpu_utilization_increase": 25.9,
        "memory_efficiency_increase": 15.2
      }
    }
  }
}