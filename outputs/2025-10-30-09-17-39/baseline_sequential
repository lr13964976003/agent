// Baseline Sequential Execution
digraph Baseline_Sequential {
	graph [compound=true nodesep=0.1 rankdir=TB ranksep=0.8]
	subgraph cluster_llama_7b {
		color=black fillcolor=lightyellow label="LLAMA_7B Sequential Execution" style="rounded,filled"
		llama_7b_input [label="Input\n[batch=6, seq=2048, vocab=32000]" fillcolor=lightgreen shape=ellipse style=filled]
		llama_7b_embedding [label="Embedding\n[batch=6, seq=2048, vocab=32000] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightblue shape=rectangle style=filled]
		llama_7b_output_proj [label="Output Projection\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, vocab=32000]" fillcolor=lightblue shape=rectangle style=filled]
		llama_7b_final_output [label="Final Output\n[batch=6, seq=2048, vocab=32000]" fillcolor=lightgreen shape=doubleoctagon style=filled]
		llama_7b_layer_1_qkv_proj [label="L1 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_1_attn_calc [label="L1 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_1_attn_out [label="L1 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_1_attn_res [label="L1 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_1_ffn_up [label="L1 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_1_ffn_gate [label="L1 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_1_ffn_down [label="L1 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_1_ffn_res [label="L1 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_1_norm1 [label="L1 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_1_norm2 [label="L1 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_embedding -> llama_7b_layer_1_norm1
		llama_7b_layer_1_norm1 -> llama_7b_layer_1_qkv_proj
		llama_7b_layer_1_qkv_proj -> llama_7b_layer_1_attn_calc
		llama_7b_layer_1_attn_calc -> llama_7b_layer_1_attn_out
		llama_7b_layer_1_attn_out -> llama_7b_layer_1_attn_res
		llama_7b_layer_1_attn_res -> llama_7b_layer_1_norm2
		llama_7b_layer_1_norm2 -> llama_7b_layer_1_ffn_up
		llama_7b_layer_1_ffn_up -> llama_7b_layer_1_ffn_gate
		llama_7b_layer_1_ffn_gate -> llama_7b_layer_1_ffn_down
		llama_7b_layer_1_ffn_down -> llama_7b_layer_1_ffn_res
		llama_7b_layer_2_qkv_proj [label="L2 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_2_attn_calc [label="L2 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_2_attn_out [label="L2 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_2_attn_res [label="L2 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_2_ffn_up [label="L2 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_2_ffn_gate [label="L2 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_2_ffn_down [label="L2 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_2_ffn_res [label="L2 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_2_norm1 [label="L2 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_2_norm2 [label="L2 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_1_ffn_res -> llama_7b_layer_2_norm1
		llama_7b_layer_2_norm1 -> llama_7b_layer_2_qkv_proj
		llama_7b_layer_2_qkv_proj -> llama_7b_layer_2_attn_calc
		llama_7b_layer_2_attn_calc -> llama_7b_layer_2_attn_out
		llama_7b_layer_2_attn_out -> llama_7b_layer_2_attn_res
		llama_7b_layer_2_attn_res -> llama_7b_layer_2_norm2
		llama_7b_layer_2_norm2 -> llama_7b_layer_2_ffn_up
		llama_7b_layer_2_ffn_up -> llama_7b_layer_2_ffn_gate
		llama_7b_layer_2_ffn_gate -> llama_7b_layer_2_ffn_down
		llama_7b_layer_2_ffn_down -> llama_7b_layer_2_ffn_res
		llama_7b_layer_3_qkv_proj [label="L3 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_3_attn_calc [label="L3 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_3_attn_out [label="L3 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_3_attn_res [label="L3 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_3_ffn_up [label="L3 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_3_ffn_gate [label="L3 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_3_ffn_down [label="L3 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_3_ffn_res [label="L3 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_3_norm1 [label="L3 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_3_norm2 [label="L3 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_2_ffn_res -> llama_7b_layer_3_norm1
		llama_7b_layer_3_norm1 -> llama_7b_layer_3_qkv_proj
		llama_7b_layer_3_qkv_proj -> llama_7b_layer_3_attn_calc
		llama_7b_layer_3_attn_calc -> llama_7b_layer_3_attn_out
		llama_7b_layer_3_attn_out -> llama_7b_layer_3_attn_res
		llama_7b_layer_3_attn_res -> llama_7b_layer_3_norm2
		llama_7b_layer_3_norm2 -> llama_7b_layer_3_ffn_up
		llama_7b_layer_3_ffn_up -> llama_7b_layer_3_ffn_gate
		llama_7b_layer_3_ffn_gate -> llama_7b_layer_3_ffn_down
		llama_7b_layer_3_ffn_down -> llama_7b_layer_3_ffn_res
		llama_7b_layer_4_qkv_proj [label="L4 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_4_attn_calc [label="L4 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_4_attn_out [label="L4 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_4_attn_res [label="L4 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_4_ffn_up [label="L4 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_4_ffn_gate [label="L4 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_4_ffn_down [label="L4 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_4_ffn_res [label="L4 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_4_norm1 [label="L4 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_4_norm2 [label="L4 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_3_ffn_res -> llama_7b_layer_4_norm1
		llama_7b_layer_4_norm1 -> llama_7b_layer_4_qkv_proj
		llama_7b_layer_4_qkv_proj -> llama_7b_layer_4_attn_calc
		llama_7b_layer_4_attn_calc -> llama_7b_layer_4_attn_out
		llama_7b_layer_4_attn_out -> llama_7b_layer_4_attn_res
		llama_7b_layer_4_attn_res -> llama_7b_layer_4_norm2
		llama_7b_layer_4_norm2 -> llama_7b_layer_4_ffn_up
		llama_7b_layer_4_ffn_up -> llama_7b_layer_4_ffn_gate
		llama_7b_layer_4_ffn_gate -> llama_7b_layer_4_ffn_down
		llama_7b_layer_4_ffn_down -> llama_7b_layer_4_ffn_res
		llama_7b_layer_5_qkv_proj [label="L5 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_5_attn_calc [label="L5 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_5_attn_out [label="L5 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_5_attn_res [label="L5 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_5_ffn_up [label="L5 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_5_ffn_gate [label="L5 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_5_ffn_down [label="L5 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_5_ffn_res [label="L5 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_5_norm1 [label="L5 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_5_norm2 [label="L5 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_4_ffn_res -> llama_7b_layer_5_norm1
		llama_7b_layer_5_norm1 -> llama_7b_layer_5_qkv_proj
		llama_7b_layer_5_qkv_proj -> llama_7b_layer_5_attn_calc
		llama_7b_layer_5_attn_calc -> llama_7b_layer_5_attn_out
		llama_7b_layer_5_attn_out -> llama_7b_layer_5_attn_res
		llama_7b_layer_5_attn_res -> llama_7b_layer_5_norm2
		llama_7b_layer_5_norm2 -> llama_7b_layer_5_ffn_up
		llama_7b_layer_5_ffn_up -> llama_7b_layer_5_ffn_gate
		llama_7b_layer_5_ffn_gate -> llama_7b_layer_5_ffn_down
		llama_7b_layer_5_ffn_down -> llama_7b_layer_5_ffn_res
		llama_7b_layer_6_qkv_proj [label="L6 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_6_attn_calc [label="L6 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_6_attn_out [label="L6 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_6_attn_res [label="L6 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_6_ffn_up [label="L6 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_6_ffn_gate [label="L6 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_6_ffn_down [label="L6 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_6_ffn_res [label="L6 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_6_norm1 [label="L6 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_6_norm2 [label="L6 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_5_ffn_res -> llama_7b_layer_6_norm1
		llama_7b_layer_6_norm1 -> llama_7b_layer_6_qkv_proj
		llama_7b_layer_6_qkv_proj -> llama_7b_layer_6_attn_calc
		llama_7b_layer_6_attn_calc -> llama_7b_layer_6_attn_out
		llama_7b_layer_6_attn_out -> llama_7b_layer_6_attn_res
		llama_7b_layer_6_attn_res -> llama_7b_layer_6_norm2
		llama_7b_layer_6_norm2 -> llama_7b_layer_6_ffn_up
		llama_7b_layer_6_ffn_up -> llama_7b_layer_6_ffn_gate
		llama_7b_layer_6_ffn_gate -> llama_7b_layer_6_ffn_down
		llama_7b_layer_6_ffn_down -> llama_7b_layer_6_ffn_res
		llama_7b_layer_7_qkv_proj [label="L7 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_7_attn_calc [label="L7 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_7_attn_out [label="L7 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_7_attn_res [label="L7 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_7_ffn_up [label="L7 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_7_ffn_gate [label="L7 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_7_ffn_down [label="L7 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_7_ffn_res [label="L7 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_7_norm1 [label="L7 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_7_norm2 [label="L7 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_6_ffn_res -> llama_7b_layer_7_norm1
		llama_7b_layer_7_norm1 -> llama_7b_layer_7_qkv_proj
		llama_7b_layer_7_qkv_proj -> llama_7b_layer_7_attn_calc
		llama_7b_layer_7_attn_calc -> llama_7b_layer_7_attn_out
		llama_7b_layer_7_attn_out -> llama_7b_layer_7_attn_res
		llama_7b_layer_7_attn_res -> llama_7b_layer_7_norm2
		llama_7b_layer_7_norm2 -> llama_7b_layer_7_ffn_up
		llama_7b_layer_7_ffn_up -> llama_7b_layer_7_ffn_gate
		llama_7b_layer_7_ffn_gate -> llama_7b_layer_7_ffn_down
		llama_7b_layer_7_ffn_down -> llama_7b_layer_7_ffn_res
		llama_7b_layer_8_qkv_proj [label="L8 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_8_attn_calc [label="L8 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_8_attn_out [label="L8 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_8_attn_res [label="L8 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_8_ffn_up [label="L8 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_8_ffn_gate [label="L8 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_8_ffn_down [label="L8 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_8_ffn_res [label="L8 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_8_norm1 [label="L8 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_8_norm2 [label="L8 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_7_ffn_res -> llama_7b_layer_8_norm1
		llama_7b_layer_8_norm1 -> llama_7b_layer_8_qkv_proj
		llama_7b_layer_8_qkv_proj -> llama_7b_layer_8_attn_calc
		llama_7b_layer_8_attn_calc -> llama_7b_layer_8_attn_out
		llama_7b_layer_8_attn_out -> llama_7b_layer_8_attn_res
		llama_7b_layer_8_attn_res -> llama_7b_layer_8_norm2
		llama_7b_layer_8_norm2 -> llama_7b_layer_8_ffn_up
		llama_7b_layer_8_ffn_up -> llama_7b_layer_8_ffn_gate
		llama_7b_layer_8_ffn_gate -> llama_7b_layer_8_ffn_down
		llama_7b_layer_8_ffn_down -> llama_7b_layer_8_ffn_res
		llama_7b_layer_9_qkv_proj [label="L9 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_9_attn_calc [label="L9 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_9_attn_out [label="L9 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_9_attn_res [label="L9 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_9_ffn_up [label="L9 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_9_ffn_gate [label="L9 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_9_ffn_down [label="L9 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_9_ffn_res [label="L9 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_9_norm1 [label="L9 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_9_norm2 [label="L9 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_8_ffn_res -> llama_7b_layer_9_norm1
		llama_7b_layer_9_norm1 -> llama_7b_layer_9_qkv_proj
		llama_7b_layer_9_qkv_proj -> llama_7b_layer_9_attn_calc
		llama_7b_layer_9_attn_calc -> llama_7b_layer_9_attn_out
		llama_7b_layer_9_attn_out -> llama_7b_layer_9_attn_res
		llama_7b_layer_9_attn_res -> llama_7b_layer_9_norm2
		llama_7b_layer_9_norm2 -> llama_7b_layer_9_ffn_up
		llama_7b_layer_9_ffn_up -> llama_7b_layer_9_ffn_gate
		llama_7b_layer_9_ffn_gate -> llama_7b_layer_9_ffn_down
		llama_7b_layer_9_ffn_down -> llama_7b_layer_9_ffn_res
		llama_7b_layer_10_qkv_proj [label="L10 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_10_attn_calc [label="L10 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_10_attn_out [label="L10 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_10_attn_res [label="L10 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_10_ffn_up [label="L10 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_10_ffn_gate [label="L10 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_10_ffn_down [label="L10 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_10_ffn_res [label="L10 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_10_norm1 [label="L10 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_10_norm2 [label="L10 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_9_ffn_res -> llama_7b_layer_10_norm1
		llama_7b_layer_10_norm1 -> llama_7b_layer_10_qkv_proj
		llama_7b_layer_10_qkv_proj -> llama_7b_layer_10_attn_calc
		llama_7b_layer_10_attn_calc -> llama_7b_layer_10_attn_out
		llama_7b_layer_10_attn_out -> llama_7b_layer_10_attn_res
		llama_7b_layer_10_attn_res -> llama_7b_layer_10_norm2
		llama_7b_layer_10_norm2 -> llama_7b_layer_10_ffn_up
		llama_7b_layer_10_ffn_up -> llama_7b_layer_10_ffn_gate
		llama_7b_layer_10_ffn_gate -> llama_7b_layer_10_ffn_down
		llama_7b_layer_10_ffn_down -> llama_7b_layer_10_ffn_res
		llama_7b_layer_11_qkv_proj [label="L11 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_11_attn_calc [label="L11 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_11_attn_out [label="L11 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_11_attn_res [label="L11 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_11_ffn_up [label="L11 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_11_ffn_gate [label="L11 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_11_ffn_down [label="L11 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_11_ffn_res [label="L11 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_11_norm1 [label="L11 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_11_norm2 [label="L11 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_10_ffn_res -> llama_7b_layer_11_norm1
		llama_7b_layer_11_norm1 -> llama_7b_layer_11_qkv_proj
		llama_7b_layer_11_qkv_proj -> llama_7b_layer_11_attn_calc
		llama_7b_layer_11_attn_calc -> llama_7b_layer_11_attn_out
		llama_7b_layer_11_attn_out -> llama_7b_layer_11_attn_res
		llama_7b_layer_11_attn_res -> llama_7b_layer_11_norm2
		llama_7b_layer_11_norm2 -> llama_7b_layer_11_ffn_up
		llama_7b_layer_11_ffn_up -> llama_7b_layer_11_ffn_gate
		llama_7b_layer_11_ffn_gate -> llama_7b_layer_11_ffn_down
		llama_7b_layer_11_ffn_down -> llama_7b_layer_11_ffn_res
		llama_7b_layer_12_qkv_proj [label="L12 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_12_attn_calc [label="L12 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_12_attn_out [label="L12 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_12_attn_res [label="L12 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_12_ffn_up [label="L12 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_12_ffn_gate [label="L12 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_12_ffn_down [label="L12 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_12_ffn_res [label="L12 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_12_norm1 [label="L12 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_12_norm2 [label="L12 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_11_ffn_res -> llama_7b_layer_12_norm1
		llama_7b_layer_12_norm1 -> llama_7b_layer_12_qkv_proj
		llama_7b_layer_12_qkv_proj -> llama_7b_layer_12_attn_calc
		llama_7b_layer_12_attn_calc -> llama_7b_layer_12_attn_out
		llama_7b_layer_12_attn_out -> llama_7b_layer_12_attn_res
		llama_7b_layer_12_attn_res -> llama_7b_layer_12_norm2
		llama_7b_layer_12_norm2 -> llama_7b_layer_12_ffn_up
		llama_7b_layer_12_ffn_up -> llama_7b_layer_12_ffn_gate
		llama_7b_layer_12_ffn_gate -> llama_7b_layer_12_ffn_down
		llama_7b_layer_12_ffn_down -> llama_7b_layer_12_ffn_res
		llama_7b_layer_13_qkv_proj [label="L13 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_13_attn_calc [label="L13 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_13_attn_out [label="L13 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_13_attn_res [label="L13 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_13_ffn_up [label="L13 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_13_ffn_gate [label="L13 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_13_ffn_down [label="L13 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_13_ffn_res [label="L13 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_13_norm1 [label="L13 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_13_norm2 [label="L13 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_12_ffn_res -> llama_7b_layer_13_norm1
		llama_7b_layer_13_norm1 -> llama_7b_layer_13_qkv_proj
		llama_7b_layer_13_qkv_proj -> llama_7b_layer_13_attn_calc
		llama_7b_layer_13_attn_calc -> llama_7b_layer_13_attn_out
		llama_7b_layer_13_attn_out -> llama_7b_layer_13_attn_res
		llama_7b_layer_13_attn_res -> llama_7b_layer_13_norm2
		llama_7b_layer_13_norm2 -> llama_7b_layer_13_ffn_up
		llama_7b_layer_13_ffn_up -> llama_7b_layer_13_ffn_gate
		llama_7b_layer_13_ffn_gate -> llama_7b_layer_13_ffn_down
		llama_7b_layer_13_ffn_down -> llama_7b_layer_13_ffn_res
		llama_7b_layer_14_qkv_proj [label="L14 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_14_attn_calc [label="L14 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_14_attn_out [label="L14 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_14_attn_res [label="L14 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_14_ffn_up [label="L14 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_14_ffn_gate [label="L14 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_14_ffn_down [label="L14 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_14_ffn_res [label="L14 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_14_norm1 [label="L14 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_14_norm2 [label="L14 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_13_ffn_res -> llama_7b_layer_14_norm1
		llama_7b_layer_14_norm1 -> llama_7b_layer_14_qkv_proj
		llama_7b_layer_14_qkv_proj -> llama_7b_layer_14_attn_calc
		llama_7b_layer_14_attn_calc -> llama_7b_layer_14_attn_out
		llama_7b_layer_14_attn_out -> llama_7b_layer_14_attn_res
		llama_7b_layer_14_attn_res -> llama_7b_layer_14_norm2
		llama_7b_layer_14_norm2 -> llama_7b_layer_14_ffn_up
		llama_7b_layer_14_ffn_up -> llama_7b_layer_14_ffn_gate
		llama_7b_layer_14_ffn_gate -> llama_7b_layer_14_ffn_down
		llama_7b_layer_14_ffn_down -> llama_7b_layer_14_ffn_res
		llama_7b_layer_15_qkv_proj [label="L15 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_15_attn_calc [label="L15 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_15_attn_out [label="L15 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_15_attn_res [label="L15 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_15_ffn_up [label="L15 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_15_ffn_gate [label="L15 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_15_ffn_down [label="L15 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_15_ffn_res [label="L15 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_15_norm1 [label="L15 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_15_norm2 [label="L15 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_14_ffn_res -> llama_7b_layer_15_norm1
		llama_7b_layer_15_norm1 -> llama_7b_layer_15_qkv_proj
		llama_7b_layer_15_qkv_proj -> llama_7b_layer_15_attn_calc
		llama_7b_layer_15_attn_calc -> llama_7b_layer_15_attn_out
		llama_7b_layer_15_attn_out -> llama_7b_layer_15_attn_res
		llama_7b_layer_15_attn_res -> llama_7b_layer_15_norm2
		llama_7b_layer_15_norm2 -> llama_7b_layer_15_ffn_up
		llama_7b_layer_15_ffn_up -> llama_7b_layer_15_ffn_gate
		llama_7b_layer_15_ffn_gate -> llama_7b_layer_15_ffn_down
		llama_7b_layer_15_ffn_down -> llama_7b_layer_15_ffn_res
		llama_7b_layer_16_qkv_proj [label="L16 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_16_attn_calc [label="L16 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_16_attn_out [label="L16 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_16_attn_res [label="L16 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_16_ffn_up [label="L16 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_16_ffn_gate [label="L16 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_16_ffn_down [label="L16 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_16_ffn_res [label="L16 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_16_norm1 [label="L16 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_16_norm2 [label="L16 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_15_ffn_res -> llama_7b_layer_16_norm1
		llama_7b_layer_16_norm1 -> llama_7b_layer_16_qkv_proj
		llama_7b_layer_16_qkv_proj -> llama_7b_layer_16_attn_calc
		llama_7b_layer_16_attn_calc -> llama_7b_layer_16_attn_out
		llama_7b_layer_16_attn_out -> llama_7b_layer_16_attn_res
		llama_7b_layer_16_attn_res -> llama_7b_layer_16_norm2
		llama_7b_layer_16_norm2 -> llama_7b_layer_16_ffn_up
		llama_7b_layer_16_ffn_up -> llama_7b_layer_16_ffn_gate
		llama_7b_layer_16_ffn_gate -> llama_7b_layer_16_ffn_down
		llama_7b_layer_16_ffn_down -> llama_7b_layer_16_ffn_res
		llama_7b_layer_17_qkv_proj [label="L17 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_17_attn_calc [label="L17 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_17_attn_out [label="L17 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_17_attn_res [label="L17 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_17_ffn_up [label="L17 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_17_ffn_gate [label="L17 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_17_ffn_down [label="L17 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_17_ffn_res [label="L17 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_17_norm1 [label="L17 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_17_norm2 [label="L17 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_16_ffn_res -> llama_7b_layer_17_norm1
		llama_7b_layer_17_norm1 -> llama_7b_layer_17_qkv_proj
		llama_7b_layer_17_qkv_proj -> llama_7b_layer_17_attn_calc
		llama_7b_layer_17_attn_calc -> llama_7b_layer_17_attn_out
		llama_7b_layer_17_attn_out -> llama_7b_layer_17_attn_res
		llama_7b_layer_17_attn_res -> llama_7b_layer_17_norm2
		llama_7b_layer_17_norm2 -> llama_7b_layer_17_ffn_up
		llama_7b_layer_17_ffn_up -> llama_7b_layer_17_ffn_gate
		llama_7b_layer_17_ffn_gate -> llama_7b_layer_17_ffn_down
		llama_7b_layer_17_ffn_down -> llama_7b_layer_17_ffn_res
		llama_7b_layer_18_qkv_proj [label="L18 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_18_attn_calc [label="L18 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_18_attn_out [label="L18 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_18_attn_res [label="L18 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_18_ffn_up [label="L18 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_18_ffn_gate [label="L18 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_18_ffn_down [label="L18 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_18_ffn_res [label="L18 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_18_norm1 [label="L18 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_18_norm2 [label="L18 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_17_ffn_res -> llama_7b_layer_18_norm1
		llama_7b_layer_18_norm1 -> llama_7b_layer_18_qkv_proj
		llama_7b_layer_18_qkv_proj -> llama_7b_layer_18_attn_calc
		llama_7b_layer_18_attn_calc -> llama_7b_layer_18_attn_out
		llama_7b_layer_18_attn_out -> llama_7b_layer_18_attn_res
		llama_7b_layer_18_attn_res -> llama_7b_layer_18_norm2
		llama_7b_layer_18_norm2 -> llama_7b_layer_18_ffn_up
		llama_7b_layer_18_ffn_up -> llama_7b_layer_18_ffn_gate
		llama_7b_layer_18_ffn_gate -> llama_7b_layer_18_ffn_down
		llama_7b_layer_18_ffn_down -> llama_7b_layer_18_ffn_res
		llama_7b_layer_19_qkv_proj [label="L19 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_19_attn_calc [label="L19 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_19_attn_out [label="L19 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_19_attn_res [label="L19 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_19_ffn_up [label="L19 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_19_ffn_gate [label="L19 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_19_ffn_down [label="L19 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_19_ffn_res [label="L19 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_19_norm1 [label="L19 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_19_norm2 [label="L19 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_18_ffn_res -> llama_7b_layer_19_norm1
		llama_7b_layer_19_norm1 -> llama_7b_layer_19_qkv_proj
		llama_7b_layer_19_qkv_proj -> llama_7b_layer_19_attn_calc
		llama_7b_layer_19_attn_calc -> llama_7b_layer_19_attn_out
		llama_7b_layer_19_attn_out -> llama_7b_layer_19_attn_res
		llama_7b_layer_19_attn_res -> llama_7b_layer_19_norm2
		llama_7b_layer_19_norm2 -> llama_7b_layer_19_ffn_up
		llama_7b_layer_19_ffn_up -> llama_7b_layer_19_ffn_gate
		llama_7b_layer_19_ffn_gate -> llama_7b_layer_19_ffn_down
		llama_7b_layer_19_ffn_down -> llama_7b_layer_19_ffn_res
		llama_7b_layer_20_qkv_proj [label="L20 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_20_attn_calc [label="L20 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_20_attn_out [label="L20 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_20_attn_res [label="L20 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_20_ffn_up [label="L20 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_20_ffn_gate [label="L20 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_20_ffn_down [label="L20 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_20_ffn_res [label="L20 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_20_norm1 [label="L20 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_20_norm2 [label="L20 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_19_ffn_res -> llama_7b_layer_20_norm1
		llama_7b_layer_20_norm1 -> llama_7b_layer_20_qkv_proj
		llama_7b_layer_20_qkv_proj -> llama_7b_layer_20_attn_calc
		llama_7b_layer_20_attn_calc -> llama_7b_layer_20_attn_out
		llama_7b_layer_20_attn_out -> llama_7b_layer_20_attn_res
		llama_7b_layer_20_attn_res -> llama_7b_layer_20_norm2
		llama_7b_layer_20_norm2 -> llama_7b_layer_20_ffn_up
		llama_7b_layer_20_ffn_up -> llama_7b_layer_20_ffn_gate
		llama_7b_layer_20_ffn_gate -> llama_7b_layer_20_ffn_down
		llama_7b_layer_20_ffn_down -> llama_7b_layer_20_ffn_res
		llama_7b_layer_21_qkv_proj [label="L21 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_21_attn_calc [label="L21 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_21_attn_out [label="L21 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_21_attn_res [label="L21 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_21_ffn_up [label="L21 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_21_ffn_gate [label="L21 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_21_ffn_down [label="L21 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_21_ffn_res [label="L21 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_21_norm1 [label="L21 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_21_norm2 [label="L21 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_20_ffn_res -> llama_7b_layer_21_norm1
		llama_7b_layer_21_norm1 -> llama_7b_layer_21_qkv_proj
		llama_7b_layer_21_qkv_proj -> llama_7b_layer_21_attn_calc
		llama_7b_layer_21_attn_calc -> llama_7b_layer_21_attn_out
		llama_7b_layer_21_attn_out -> llama_7b_layer_21_attn_res
		llama_7b_layer_21_attn_res -> llama_7b_layer_21_norm2
		llama_7b_layer_21_norm2 -> llama_7b_layer_21_ffn_up
		llama_7b_layer_21_ffn_up -> llama_7b_layer_21_ffn_gate
		llama_7b_layer_21_ffn_gate -> llama_7b_layer_21_ffn_down
		llama_7b_layer_21_ffn_down -> llama_7b_layer_21_ffn_res
		llama_7b_layer_22_qkv_proj [label="L22 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_22_attn_calc [label="L22 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_22_attn_out [label="L22 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_22_attn_res [label="L22 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_22_ffn_up [label="L22 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_22_ffn_gate [label="L22 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_22_ffn_down [label="L22 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_22_ffn_res [label="L22 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_22_norm1 [label="L22 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_22_norm2 [label="L22 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_21_ffn_res -> llama_7b_layer_22_norm1
		llama_7b_layer_22_norm1 -> llama_7b_layer_22_qkv_proj
		llama_7b_layer_22_qkv_proj -> llama_7b_layer_22_attn_calc
		llama_7b_layer_22_attn_calc -> llama_7b_layer_22_attn_out
		llama_7b_layer_22_attn_out -> llama_7b_layer_22_attn_res
		llama_7b_layer_22_attn_res -> llama_7b_layer_22_norm2
		llama_7b_layer_22_norm2 -> llama_7b_layer_22_ffn_up
		llama_7b_layer_22_ffn_up -> llama_7b_layer_22_ffn_gate
		llama_7b_layer_22_ffn_gate -> llama_7b_layer_22_ffn_down
		llama_7b_layer_22_ffn_down -> llama_7b_layer_22_ffn_res
		llama_7b_layer_23_qkv_proj [label="L23 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_23_attn_calc [label="L23 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_23_attn_out [label="L23 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_23_attn_res [label="L23 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_23_ffn_up [label="L23 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_23_ffn_gate [label="L23 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_23_ffn_down [label="L23 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_23_ffn_res [label="L23 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_23_norm1 [label="L23 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_23_norm2 [label="L23 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_22_ffn_res -> llama_7b_layer_23_norm1
		llama_7b_layer_23_norm1 -> llama_7b_layer_23_qkv_proj
		llama_7b_layer_23_qkv_proj -> llama_7b_layer_23_attn_calc
		llama_7b_layer_23_attn_calc -> llama_7b_layer_23_attn_out
		llama_7b_layer_23_attn_out -> llama_7b_layer_23_attn_res
		llama_7b_layer_23_attn_res -> llama_7b_layer_23_norm2
		llama_7b_layer_23_norm2 -> llama_7b_layer_23_ffn_up
		llama_7b_layer_23_ffn_up -> llama_7b_layer_23_ffn_gate
		llama_7b_layer_23_ffn_gate -> llama_7b_layer_23_ffn_down
		llama_7b_layer_23_ffn_down -> llama_7b_layer_23_ffn_res
		llama_7b_layer_24_qkv_proj [label="L24 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_24_attn_calc [label="L24 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_24_attn_out [label="L24 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_24_attn_res [label="L24 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_24_ffn_up [label="L24 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_24_ffn_gate [label="L24 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_24_ffn_down [label="L24 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_24_ffn_res [label="L24 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_24_norm1 [label="L24 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_24_norm2 [label="L24 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_23_ffn_res -> llama_7b_layer_24_norm1
		llama_7b_layer_24_norm1 -> llama_7b_layer_24_qkv_proj
		llama_7b_layer_24_qkv_proj -> llama_7b_layer_24_attn_calc
		llama_7b_layer_24_attn_calc -> llama_7b_layer_24_attn_out
		llama_7b_layer_24_attn_out -> llama_7b_layer_24_attn_res
		llama_7b_layer_24_attn_res -> llama_7b_layer_24_norm2
		llama_7b_layer_24_norm2 -> llama_7b_layer_24_ffn_up
		llama_7b_layer_24_ffn_up -> llama_7b_layer_24_ffn_gate
		llama_7b_layer_24_ffn_gate -> llama_7b_layer_24_ffn_down
		llama_7b_layer_24_ffn_down -> llama_7b_layer_24_ffn_res
		llama_7b_layer_25_qkv_proj [label="L25 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_25_attn_calc [label="L25 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_25_attn_out [label="L25 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_25_attn_res [label="L25 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_25_ffn_up [label="L25 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_25_ffn_gate [label="L25 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_25_ffn_down [label="L25 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_25_ffn_res [label="L25 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_25_norm1 [label="L25 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_25_norm2 [label="L25 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_24_ffn_res -> llama_7b_layer_25_norm1
		llama_7b_layer_25_norm1 -> llama_7b_layer_25_qkv_proj
		llama_7b_layer_25_qkv_proj -> llama_7b_layer_25_attn_calc
		llama_7b_layer_25_attn_calc -> llama_7b_layer_25_attn_out
		llama_7b_layer_25_attn_out -> llama_7b_layer_25_attn_res
		llama_7b_layer_25_attn_res -> llama_7b_layer_25_norm2
		llama_7b_layer_25_norm2 -> llama_7b_layer_25_ffn_up
		llama_7b_layer_25_ffn_up -> llama_7b_layer_25_ffn_gate
		llama_7b_layer_25_ffn_gate -> llama_7b_layer_25_ffn_down
		llama_7b_layer_25_ffn_down -> llama_7b_layer_25_ffn_res
		llama_7b_layer_26_qkv_proj [label="L26 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_26_attn_calc [label="L26 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_26_attn_out [label="L26 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_26_attn_res [label="L26 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_26_ffn_up [label="L26 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_26_ffn_gate [label="L26 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_26_ffn_down [label="L26 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_26_ffn_res [label="L26 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_26_norm1 [label="L26 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_26_norm2 [label="L26 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_25_ffn_res -> llama_7b_layer_26_norm1
		llama_7b_layer_26_norm1 -> llama_7b_layer_26_qkv_proj
		llama_7b_layer_26_qkv_proj -> llama_7b_layer_26_attn_calc
		llama_7b_layer_26_attn_calc -> llama_7b_layer_26_attn_out
		llama_7b_layer_26_attn_out -> llama_7b_layer_26_attn_res
		llama_7b_layer_26_attn_res -> llama_7b_layer_26_norm2
		llama_7b_layer_26_norm2 -> llama_7b_layer_26_ffn_up
		llama_7b_layer_26_ffn_up -> llama_7b_layer_26_ffn_gate
		llama_7b_layer_26_ffn_gate -> llama_7b_layer_26_ffn_down
		llama_7b_layer_26_ffn_down -> llama_7b_layer_26_ffn_res
		llama_7b_layer_27_qkv_proj [label="L27 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_27_attn_calc [label="L27 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_27_attn_out [label="L27 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_27_attn_res [label="L27 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_27_ffn_up [label="L27 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_27_ffn_gate [label="L27 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_27_ffn_down [label="L27 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_27_ffn_res [label="L27 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_27_norm1 [label="L27 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_27_norm2 [label="L27 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_26_ffn_res -> llama_7b_layer_27_norm1
		llama_7b_layer_27_norm1 -> llama_7b_layer_27_qkv_proj
		llama_7b_layer_27_qkv_proj -> llama_7b_layer_27_attn_calc
		llama_7b_layer_27_attn_calc -> llama_7b_layer_27_attn_out
		llama_7b_layer_27_attn_out -> llama_7b_layer_27_attn_res
		llama_7b_layer_27_attn_res -> llama_7b_layer_27_norm2
		llama_7b_layer_27_norm2 -> llama_7b_layer_27_ffn_up
		llama_7b_layer_27_ffn_up -> llama_7b_layer_27_ffn_gate
		llama_7b_layer_27_ffn_gate -> llama_7b_layer_27_ffn_down
		llama_7b_layer_27_ffn_down -> llama_7b_layer_27_ffn_res
		llama_7b_layer_28_qkv_proj [label="L28 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_28_attn_calc [label="L28 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_28_attn_out [label="L28 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_28_attn_res [label="L28 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_28_ffn_up [label="L28 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_28_ffn_gate [label="L28 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_28_ffn_down [label="L28 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_28_ffn_res [label="L28 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_28_norm1 [label="L28 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_28_norm2 [label="L28 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_27_ffn_res -> llama_7b_layer_28_norm1
		llama_7b_layer_28_norm1 -> llama_7b_layer_28_qkv_proj
		llama_7b_layer_28_qkv_proj -> llama_7b_layer_28_attn_calc
		llama_7b_layer_28_attn_calc -> llama_7b_layer_28_attn_out
		llama_7b_layer_28_attn_out -> llama_7b_layer_28_attn_res
		llama_7b_layer_28_attn_res -> llama_7b_layer_28_norm2
		llama_7b_layer_28_norm2 -> llama_7b_layer_28_ffn_up
		llama_7b_layer_28_ffn_up -> llama_7b_layer_28_ffn_gate
		llama_7b_layer_28_ffn_gate -> llama_7b_layer_28_ffn_down
		llama_7b_layer_28_ffn_down -> llama_7b_layer_28_ffn_res
		llama_7b_layer_29_qkv_proj [label="L29 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_29_attn_calc [label="L29 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_29_attn_out [label="L29 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_29_attn_res [label="L29 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_29_ffn_up [label="L29 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_29_ffn_gate [label="L29 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_29_ffn_down [label="L29 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_29_ffn_res [label="L29 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_29_norm1 [label="L29 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_29_norm2 [label="L29 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_28_ffn_res -> llama_7b_layer_29_norm1
		llama_7b_layer_29_norm1 -> llama_7b_layer_29_qkv_proj
		llama_7b_layer_29_qkv_proj -> llama_7b_layer_29_attn_calc
		llama_7b_layer_29_attn_calc -> llama_7b_layer_29_attn_out
		llama_7b_layer_29_attn_out -> llama_7b_layer_29_attn_res
		llama_7b_layer_29_attn_res -> llama_7b_layer_29_norm2
		llama_7b_layer_29_norm2 -> llama_7b_layer_29_ffn_up
		llama_7b_layer_29_ffn_up -> llama_7b_layer_29_ffn_gate
		llama_7b_layer_29_ffn_gate -> llama_7b_layer_29_ffn_down
		llama_7b_layer_29_ffn_down -> llama_7b_layer_29_ffn_res
		llama_7b_layer_30_qkv_proj [label="L30 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_30_attn_calc [label="L30 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_30_attn_out [label="L30 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_30_attn_res [label="L30 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_30_ffn_up [label="L30 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_30_ffn_gate [label="L30 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_30_ffn_down [label="L30 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_30_ffn_res [label="L30 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_30_norm1 [label="L30 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_30_norm2 [label="L30 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_29_ffn_res -> llama_7b_layer_30_norm1
		llama_7b_layer_30_norm1 -> llama_7b_layer_30_qkv_proj
		llama_7b_layer_30_qkv_proj -> llama_7b_layer_30_attn_calc
		llama_7b_layer_30_attn_calc -> llama_7b_layer_30_attn_out
		llama_7b_layer_30_attn_out -> llama_7b_layer_30_attn_res
		llama_7b_layer_30_attn_res -> llama_7b_layer_30_norm2
		llama_7b_layer_30_norm2 -> llama_7b_layer_30_ffn_up
		llama_7b_layer_30_ffn_up -> llama_7b_layer_30_ffn_gate
		llama_7b_layer_30_ffn_gate -> llama_7b_layer_30_ffn_down
		llama_7b_layer_30_ffn_down -> llama_7b_layer_30_ffn_res
		llama_7b_layer_31_qkv_proj [label="L31 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_31_attn_calc [label="L31 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_31_attn_out [label="L31 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_31_attn_res [label="L31 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_31_ffn_up [label="L31 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_31_ffn_gate [label="L31 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_31_ffn_down [label="L31 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_31_ffn_res [label="L31 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_31_norm1 [label="L31 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_31_norm2 [label="L31 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_30_ffn_res -> llama_7b_layer_31_norm1
		llama_7b_layer_31_norm1 -> llama_7b_layer_31_qkv_proj
		llama_7b_layer_31_qkv_proj -> llama_7b_layer_31_attn_calc
		llama_7b_layer_31_attn_calc -> llama_7b_layer_31_attn_out
		llama_7b_layer_31_attn_out -> llama_7b_layer_31_attn_res
		llama_7b_layer_31_attn_res -> llama_7b_layer_31_norm2
		llama_7b_layer_31_norm2 -> llama_7b_layer_31_ffn_up
		llama_7b_layer_31_ffn_up -> llama_7b_layer_31_ffn_gate
		llama_7b_layer_31_ffn_gate -> llama_7b_layer_31_ffn_down
		llama_7b_layer_31_ffn_down -> llama_7b_layer_31_ffn_res
		llama_7b_layer_32_qkv_proj [label="L32 QKV Proj\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, heads=32, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_32_attn_calc [label="L32 Attention\n[batch=6, seq=2048, heads=32, head_dim=128] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_32_attn_out [label="L32 Attn Output\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcoral shape=rectangle style=filled]
		llama_7b_layer_32_attn_res [label="L32 Attn+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_32_ffn_up [label="L32 FFN Up\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_32_ffn_gate [label="L32 FFN Gate\n[batch=6, seq=2048, hidden=4096] → [batch=6, seq=2048, ffn=11008]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_32_ffn_down [label="L32 FFN Down\n[batch=6, seq=2048, ffn=11008] → [batch=6, seq=2048, hidden=4096]" fillcolor=lightcyan shape=rectangle style=filled]
		llama_7b_layer_32_ffn_res [label="L32 FFN+Residual\n[batch=6, seq=2048, hidden=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
		llama_7b_layer_32_norm1 [label="L32 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_32_norm2 [label="L32 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		llama_7b_layer_31_ffn_res -> llama_7b_layer_32_norm1
		llama_7b_layer_32_norm1 -> llama_7b_layer_32_qkv_proj
		llama_7b_layer_32_qkv_proj -> llama_7b_layer_32_attn_calc
		llama_7b_layer_32_attn_calc -> llama_7b_layer_32_attn_out
		llama_7b_layer_32_attn_out -> llama_7b_layer_32_attn_res
		llama_7b_layer_32_attn_res -> llama_7b_layer_32_norm2
		llama_7b_layer_32_norm2 -> llama_7b_layer_32_ffn_up
		llama_7b_layer_32_ffn_up -> llama_7b_layer_32_ffn_gate
		llama_7b_layer_32_ffn_gate -> llama_7b_layer_32_ffn_down
		llama_7b_layer_32_ffn_down -> llama_7b_layer_32_ffn_res
		llama_7b_layer_32_ffn_res -> llama_7b_output_proj
	}
	subgraph cluster_gpt3_2b {
		color=black fillcolor=lightyellow label="GPT3_2B Sequential Execution" style="rounded,filled"
		gpt3_2b_input [label="Input\n[batch=12, seq=2048, vocab=50257]" fillcolor=lightgreen shape=ellipse style=filled]
		gpt3_2b_embedding [label="Embedding\n[batch=12, seq=2048, vocab=50257] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightblue shape=rectangle style=filled]
		gpt3_2b_output_proj [label="Output Projection\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, vocab=50257]" fillcolor=lightblue shape=rectangle style=filled]
		gpt3_2b_final_output [label="Final Output\n[batch=12, seq=2048, vocab=50257]" fillcolor=lightgreen shape=doubleoctagon style=filled]
		gpt3_2b_layer_1_qkv_proj [label="L1 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_1_attn_calc [label="L1 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_1_attn_out [label="L1 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_1_attn_res [label="L1 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_1_ffn_up [label="L1 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_1_ffn_gate [label="L1 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_1_ffn_down [label="L1 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_1_ffn_res [label="L1 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_1_norm1 [label="L1 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_1_norm2 [label="L1 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_embedding -> gpt3_2b_layer_1_norm1
		gpt3_2b_layer_1_norm1 -> gpt3_2b_layer_1_qkv_proj
		gpt3_2b_layer_1_qkv_proj -> gpt3_2b_layer_1_attn_calc
		gpt3_2b_layer_1_attn_calc -> gpt3_2b_layer_1_attn_out
		gpt3_2b_layer_1_attn_out -> gpt3_2b_layer_1_attn_res
		gpt3_2b_layer_1_attn_res -> gpt3_2b_layer_1_norm2
		gpt3_2b_layer_1_norm2 -> gpt3_2b_layer_1_ffn_up
		gpt3_2b_layer_1_ffn_up -> gpt3_2b_layer_1_ffn_gate
		gpt3_2b_layer_1_ffn_gate -> gpt3_2b_layer_1_ffn_down
		gpt3_2b_layer_1_ffn_down -> gpt3_2b_layer_1_ffn_res
		gpt3_2b_layer_2_qkv_proj [label="L2 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_2_attn_calc [label="L2 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_2_attn_out [label="L2 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_2_attn_res [label="L2 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_2_ffn_up [label="L2 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_2_ffn_gate [label="L2 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_2_ffn_down [label="L2 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_2_ffn_res [label="L2 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_2_norm1 [label="L2 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_2_norm2 [label="L2 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_1_ffn_res -> gpt3_2b_layer_2_norm1
		gpt3_2b_layer_2_norm1 -> gpt3_2b_layer_2_qkv_proj
		gpt3_2b_layer_2_qkv_proj -> gpt3_2b_layer_2_attn_calc
		gpt3_2b_layer_2_attn_calc -> gpt3_2b_layer_2_attn_out
		gpt3_2b_layer_2_attn_out -> gpt3_2b_layer_2_attn_res
		gpt3_2b_layer_2_attn_res -> gpt3_2b_layer_2_norm2
		gpt3_2b_layer_2_norm2 -> gpt3_2b_layer_2_ffn_up
		gpt3_2b_layer_2_ffn_up -> gpt3_2b_layer_2_ffn_gate
		gpt3_2b_layer_2_ffn_gate -> gpt3_2b_layer_2_ffn_down
		gpt3_2b_layer_2_ffn_down -> gpt3_2b_layer_2_ffn_res
		gpt3_2b_layer_3_qkv_proj [label="L3 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_3_attn_calc [label="L3 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_3_attn_out [label="L3 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_3_attn_res [label="L3 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_3_ffn_up [label="L3 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_3_ffn_gate [label="L3 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_3_ffn_down [label="L3 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_3_ffn_res [label="L3 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_3_norm1 [label="L3 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_3_norm2 [label="L3 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_2_ffn_res -> gpt3_2b_layer_3_norm1
		gpt3_2b_layer_3_norm1 -> gpt3_2b_layer_3_qkv_proj
		gpt3_2b_layer_3_qkv_proj -> gpt3_2b_layer_3_attn_calc
		gpt3_2b_layer_3_attn_calc -> gpt3_2b_layer_3_attn_out
		gpt3_2b_layer_3_attn_out -> gpt3_2b_layer_3_attn_res
		gpt3_2b_layer_3_attn_res -> gpt3_2b_layer_3_norm2
		gpt3_2b_layer_3_norm2 -> gpt3_2b_layer_3_ffn_up
		gpt3_2b_layer_3_ffn_up -> gpt3_2b_layer_3_ffn_gate
		gpt3_2b_layer_3_ffn_gate -> gpt3_2b_layer_3_ffn_down
		gpt3_2b_layer_3_ffn_down -> gpt3_2b_layer_3_ffn_res
		gpt3_2b_layer_4_qkv_proj [label="L4 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_4_attn_calc [label="L4 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_4_attn_out [label="L4 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_4_attn_res [label="L4 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_4_ffn_up [label="L4 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_4_ffn_gate [label="L4 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_4_ffn_down [label="L4 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_4_ffn_res [label="L4 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_4_norm1 [label="L4 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_4_norm2 [label="L4 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_3_ffn_res -> gpt3_2b_layer_4_norm1
		gpt3_2b_layer_4_norm1 -> gpt3_2b_layer_4_qkv_proj
		gpt3_2b_layer_4_qkv_proj -> gpt3_2b_layer_4_attn_calc
		gpt3_2b_layer_4_attn_calc -> gpt3_2b_layer_4_attn_out
		gpt3_2b_layer_4_attn_out -> gpt3_2b_layer_4_attn_res
		gpt3_2b_layer_4_attn_res -> gpt3_2b_layer_4_norm2
		gpt3_2b_layer_4_norm2 -> gpt3_2b_layer_4_ffn_up
		gpt3_2b_layer_4_ffn_up -> gpt3_2b_layer_4_ffn_gate
		gpt3_2b_layer_4_ffn_gate -> gpt3_2b_layer_4_ffn_down
		gpt3_2b_layer_4_ffn_down -> gpt3_2b_layer_4_ffn_res
		gpt3_2b_layer_5_qkv_proj [label="L5 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_5_attn_calc [label="L5 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_5_attn_out [label="L5 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_5_attn_res [label="L5 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_5_ffn_up [label="L5 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_5_ffn_gate [label="L5 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_5_ffn_down [label="L5 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_5_ffn_res [label="L5 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_5_norm1 [label="L5 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_5_norm2 [label="L5 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_4_ffn_res -> gpt3_2b_layer_5_norm1
		gpt3_2b_layer_5_norm1 -> gpt3_2b_layer_5_qkv_proj
		gpt3_2b_layer_5_qkv_proj -> gpt3_2b_layer_5_attn_calc
		gpt3_2b_layer_5_attn_calc -> gpt3_2b_layer_5_attn_out
		gpt3_2b_layer_5_attn_out -> gpt3_2b_layer_5_attn_res
		gpt3_2b_layer_5_attn_res -> gpt3_2b_layer_5_norm2
		gpt3_2b_layer_5_norm2 -> gpt3_2b_layer_5_ffn_up
		gpt3_2b_layer_5_ffn_up -> gpt3_2b_layer_5_ffn_gate
		gpt3_2b_layer_5_ffn_gate -> gpt3_2b_layer_5_ffn_down
		gpt3_2b_layer_5_ffn_down -> gpt3_2b_layer_5_ffn_res
		gpt3_2b_layer_6_qkv_proj [label="L6 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_6_attn_calc [label="L6 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_6_attn_out [label="L6 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_6_attn_res [label="L6 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_6_ffn_up [label="L6 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_6_ffn_gate [label="L6 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_6_ffn_down [label="L6 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_6_ffn_res [label="L6 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_6_norm1 [label="L6 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_6_norm2 [label="L6 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_5_ffn_res -> gpt3_2b_layer_6_norm1
		gpt3_2b_layer_6_norm1 -> gpt3_2b_layer_6_qkv_proj
		gpt3_2b_layer_6_qkv_proj -> gpt3_2b_layer_6_attn_calc
		gpt3_2b_layer_6_attn_calc -> gpt3_2b_layer_6_attn_out
		gpt3_2b_layer_6_attn_out -> gpt3_2b_layer_6_attn_res
		gpt3_2b_layer_6_attn_res -> gpt3_2b_layer_6_norm2
		gpt3_2b_layer_6_norm2 -> gpt3_2b_layer_6_ffn_up
		gpt3_2b_layer_6_ffn_up -> gpt3_2b_layer_6_ffn_gate
		gpt3_2b_layer_6_ffn_gate -> gpt3_2b_layer_6_ffn_down
		gpt3_2b_layer_6_ffn_down -> gpt3_2b_layer_6_ffn_res
		gpt3_2b_layer_7_qkv_proj [label="L7 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_7_attn_calc [label="L7 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_7_attn_out [label="L7 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_7_attn_res [label="L7 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_7_ffn_up [label="L7 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_7_ffn_gate [label="L7 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_7_ffn_down [label="L7 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_7_ffn_res [label="L7 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_7_norm1 [label="L7 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_7_norm2 [label="L7 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_6_ffn_res -> gpt3_2b_layer_7_norm1
		gpt3_2b_layer_7_norm1 -> gpt3_2b_layer_7_qkv_proj
		gpt3_2b_layer_7_qkv_proj -> gpt3_2b_layer_7_attn_calc
		gpt3_2b_layer_7_attn_calc -> gpt3_2b_layer_7_attn_out
		gpt3_2b_layer_7_attn_out -> gpt3_2b_layer_7_attn_res
		gpt3_2b_layer_7_attn_res -> gpt3_2b_layer_7_norm2
		gpt3_2b_layer_7_norm2 -> gpt3_2b_layer_7_ffn_up
		gpt3_2b_layer_7_ffn_up -> gpt3_2b_layer_7_ffn_gate
		gpt3_2b_layer_7_ffn_gate -> gpt3_2b_layer_7_ffn_down
		gpt3_2b_layer_7_ffn_down -> gpt3_2b_layer_7_ffn_res
		gpt3_2b_layer_8_qkv_proj [label="L8 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_8_attn_calc [label="L8 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_8_attn_out [label="L8 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_8_attn_res [label="L8 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_8_ffn_up [label="L8 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_8_ffn_gate [label="L8 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_8_ffn_down [label="L8 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_8_ffn_res [label="L8 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_8_norm1 [label="L8 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_8_norm2 [label="L8 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_7_ffn_res -> gpt3_2b_layer_8_norm1
		gpt3_2b_layer_8_norm1 -> gpt3_2b_layer_8_qkv_proj
		gpt3_2b_layer_8_qkv_proj -> gpt3_2b_layer_8_attn_calc
		gpt3_2b_layer_8_attn_calc -> gpt3_2b_layer_8_attn_out
		gpt3_2b_layer_8_attn_out -> gpt3_2b_layer_8_attn_res
		gpt3_2b_layer_8_attn_res -> gpt3_2b_layer_8_norm2
		gpt3_2b_layer_8_norm2 -> gpt3_2b_layer_8_ffn_up
		gpt3_2b_layer_8_ffn_up -> gpt3_2b_layer_8_ffn_gate
		gpt3_2b_layer_8_ffn_gate -> gpt3_2b_layer_8_ffn_down
		gpt3_2b_layer_8_ffn_down -> gpt3_2b_layer_8_ffn_res
		gpt3_2b_layer_9_qkv_proj [label="L9 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_9_attn_calc [label="L9 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_9_attn_out [label="L9 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_9_attn_res [label="L9 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_9_ffn_up [label="L9 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_9_ffn_gate [label="L9 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_9_ffn_down [label="L9 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_9_ffn_res [label="L9 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_9_norm1 [label="L9 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_9_norm2 [label="L9 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_8_ffn_res -> gpt3_2b_layer_9_norm1
		gpt3_2b_layer_9_norm1 -> gpt3_2b_layer_9_qkv_proj
		gpt3_2b_layer_9_qkv_proj -> gpt3_2b_layer_9_attn_calc
		gpt3_2b_layer_9_attn_calc -> gpt3_2b_layer_9_attn_out
		gpt3_2b_layer_9_attn_out -> gpt3_2b_layer_9_attn_res
		gpt3_2b_layer_9_attn_res -> gpt3_2b_layer_9_norm2
		gpt3_2b_layer_9_norm2 -> gpt3_2b_layer_9_ffn_up
		gpt3_2b_layer_9_ffn_up -> gpt3_2b_layer_9_ffn_gate
		gpt3_2b_layer_9_ffn_gate -> gpt3_2b_layer_9_ffn_down
		gpt3_2b_layer_9_ffn_down -> gpt3_2b_layer_9_ffn_res
		gpt3_2b_layer_10_qkv_proj [label="L10 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_10_attn_calc [label="L10 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_10_attn_out [label="L10 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_10_attn_res [label="L10 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_10_ffn_up [label="L10 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_10_ffn_gate [label="L10 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_10_ffn_down [label="L10 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_10_ffn_res [label="L10 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_10_norm1 [label="L10 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_10_norm2 [label="L10 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_9_ffn_res -> gpt3_2b_layer_10_norm1
		gpt3_2b_layer_10_norm1 -> gpt3_2b_layer_10_qkv_proj
		gpt3_2b_layer_10_qkv_proj -> gpt3_2b_layer_10_attn_calc
		gpt3_2b_layer_10_attn_calc -> gpt3_2b_layer_10_attn_out
		gpt3_2b_layer_10_attn_out -> gpt3_2b_layer_10_attn_res
		gpt3_2b_layer_10_attn_res -> gpt3_2b_layer_10_norm2
		gpt3_2b_layer_10_norm2 -> gpt3_2b_layer_10_ffn_up
		gpt3_2b_layer_10_ffn_up -> gpt3_2b_layer_10_ffn_gate
		gpt3_2b_layer_10_ffn_gate -> gpt3_2b_layer_10_ffn_down
		gpt3_2b_layer_10_ffn_down -> gpt3_2b_layer_10_ffn_res
		gpt3_2b_layer_11_qkv_proj [label="L11 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_11_attn_calc [label="L11 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_11_attn_out [label="L11 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_11_attn_res [label="L11 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_11_ffn_up [label="L11 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_11_ffn_gate [label="L11 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_11_ffn_down [label="L11 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_11_ffn_res [label="L11 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_11_norm1 [label="L11 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_11_norm2 [label="L11 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_10_ffn_res -> gpt3_2b_layer_11_norm1
		gpt3_2b_layer_11_norm1 -> gpt3_2b_layer_11_qkv_proj
		gpt3_2b_layer_11_qkv_proj -> gpt3_2b_layer_11_attn_calc
		gpt3_2b_layer_11_attn_calc -> gpt3_2b_layer_11_attn_out
		gpt3_2b_layer_11_attn_out -> gpt3_2b_layer_11_attn_res
		gpt3_2b_layer_11_attn_res -> gpt3_2b_layer_11_norm2
		gpt3_2b_layer_11_norm2 -> gpt3_2b_layer_11_ffn_up
		gpt3_2b_layer_11_ffn_up -> gpt3_2b_layer_11_ffn_gate
		gpt3_2b_layer_11_ffn_gate -> gpt3_2b_layer_11_ffn_down
		gpt3_2b_layer_11_ffn_down -> gpt3_2b_layer_11_ffn_res
		gpt3_2b_layer_12_qkv_proj [label="L12 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_12_attn_calc [label="L12 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_12_attn_out [label="L12 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_12_attn_res [label="L12 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_12_ffn_up [label="L12 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_12_ffn_gate [label="L12 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_12_ffn_down [label="L12 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_12_ffn_res [label="L12 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_12_norm1 [label="L12 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_12_norm2 [label="L12 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_11_ffn_res -> gpt3_2b_layer_12_norm1
		gpt3_2b_layer_12_norm1 -> gpt3_2b_layer_12_qkv_proj
		gpt3_2b_layer_12_qkv_proj -> gpt3_2b_layer_12_attn_calc
		gpt3_2b_layer_12_attn_calc -> gpt3_2b_layer_12_attn_out
		gpt3_2b_layer_12_attn_out -> gpt3_2b_layer_12_attn_res
		gpt3_2b_layer_12_attn_res -> gpt3_2b_layer_12_norm2
		gpt3_2b_layer_12_norm2 -> gpt3_2b_layer_12_ffn_up
		gpt3_2b_layer_12_ffn_up -> gpt3_2b_layer_12_ffn_gate
		gpt3_2b_layer_12_ffn_gate -> gpt3_2b_layer_12_ffn_down
		gpt3_2b_layer_12_ffn_down -> gpt3_2b_layer_12_ffn_res
		gpt3_2b_layer_13_qkv_proj [label="L13 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_13_attn_calc [label="L13 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_13_attn_out [label="L13 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_13_attn_res [label="L13 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_13_ffn_up [label="L13 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_13_ffn_gate [label="L13 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_13_ffn_down [label="L13 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_13_ffn_res [label="L13 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_13_norm1 [label="L13 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_13_norm2 [label="L13 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_12_ffn_res -> gpt3_2b_layer_13_norm1
		gpt3_2b_layer_13_norm1 -> gpt3_2b_layer_13_qkv_proj
		gpt3_2b_layer_13_qkv_proj -> gpt3_2b_layer_13_attn_calc
		gpt3_2b_layer_13_attn_calc -> gpt3_2b_layer_13_attn_out
		gpt3_2b_layer_13_attn_out -> gpt3_2b_layer_13_attn_res
		gpt3_2b_layer_13_attn_res -> gpt3_2b_layer_13_norm2
		gpt3_2b_layer_13_norm2 -> gpt3_2b_layer_13_ffn_up
		gpt3_2b_layer_13_ffn_up -> gpt3_2b_layer_13_ffn_gate
		gpt3_2b_layer_13_ffn_gate -> gpt3_2b_layer_13_ffn_down
		gpt3_2b_layer_13_ffn_down -> gpt3_2b_layer_13_ffn_res
		gpt3_2b_layer_14_qkv_proj [label="L14 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_14_attn_calc [label="L14 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_14_attn_out [label="L14 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_14_attn_res [label="L14 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_14_ffn_up [label="L14 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_14_ffn_gate [label="L14 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_14_ffn_down [label="L14 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_14_ffn_res [label="L14 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_14_norm1 [label="L14 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_14_norm2 [label="L14 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_13_ffn_res -> gpt3_2b_layer_14_norm1
		gpt3_2b_layer_14_norm1 -> gpt3_2b_layer_14_qkv_proj
		gpt3_2b_layer_14_qkv_proj -> gpt3_2b_layer_14_attn_calc
		gpt3_2b_layer_14_attn_calc -> gpt3_2b_layer_14_attn_out
		gpt3_2b_layer_14_attn_out -> gpt3_2b_layer_14_attn_res
		gpt3_2b_layer_14_attn_res -> gpt3_2b_layer_14_norm2
		gpt3_2b_layer_14_norm2 -> gpt3_2b_layer_14_ffn_up
		gpt3_2b_layer_14_ffn_up -> gpt3_2b_layer_14_ffn_gate
		gpt3_2b_layer_14_ffn_gate -> gpt3_2b_layer_14_ffn_down
		gpt3_2b_layer_14_ffn_down -> gpt3_2b_layer_14_ffn_res
		gpt3_2b_layer_15_qkv_proj [label="L15 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_15_attn_calc [label="L15 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_15_attn_out [label="L15 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_15_attn_res [label="L15 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_15_ffn_up [label="L15 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_15_ffn_gate [label="L15 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_15_ffn_down [label="L15 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_15_ffn_res [label="L15 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_15_norm1 [label="L15 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_15_norm2 [label="L15 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_14_ffn_res -> gpt3_2b_layer_15_norm1
		gpt3_2b_layer_15_norm1 -> gpt3_2b_layer_15_qkv_proj
		gpt3_2b_layer_15_qkv_proj -> gpt3_2b_layer_15_attn_calc
		gpt3_2b_layer_15_attn_calc -> gpt3_2b_layer_15_attn_out
		gpt3_2b_layer_15_attn_out -> gpt3_2b_layer_15_attn_res
		gpt3_2b_layer_15_attn_res -> gpt3_2b_layer_15_norm2
		gpt3_2b_layer_15_norm2 -> gpt3_2b_layer_15_ffn_up
		gpt3_2b_layer_15_ffn_up -> gpt3_2b_layer_15_ffn_gate
		gpt3_2b_layer_15_ffn_gate -> gpt3_2b_layer_15_ffn_down
		gpt3_2b_layer_15_ffn_down -> gpt3_2b_layer_15_ffn_res
		gpt3_2b_layer_16_qkv_proj [label="L16 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_16_attn_calc [label="L16 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_16_attn_out [label="L16 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_16_attn_res [label="L16 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_16_ffn_up [label="L16 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_16_ffn_gate [label="L16 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_16_ffn_down [label="L16 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_16_ffn_res [label="L16 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_16_norm1 [label="L16 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_16_norm2 [label="L16 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_15_ffn_res -> gpt3_2b_layer_16_norm1
		gpt3_2b_layer_16_norm1 -> gpt3_2b_layer_16_qkv_proj
		gpt3_2b_layer_16_qkv_proj -> gpt3_2b_layer_16_attn_calc
		gpt3_2b_layer_16_attn_calc -> gpt3_2b_layer_16_attn_out
		gpt3_2b_layer_16_attn_out -> gpt3_2b_layer_16_attn_res
		gpt3_2b_layer_16_attn_res -> gpt3_2b_layer_16_norm2
		gpt3_2b_layer_16_norm2 -> gpt3_2b_layer_16_ffn_up
		gpt3_2b_layer_16_ffn_up -> gpt3_2b_layer_16_ffn_gate
		gpt3_2b_layer_16_ffn_gate -> gpt3_2b_layer_16_ffn_down
		gpt3_2b_layer_16_ffn_down -> gpt3_2b_layer_16_ffn_res
		gpt3_2b_layer_17_qkv_proj [label="L17 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_17_attn_calc [label="L17 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_17_attn_out [label="L17 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_17_attn_res [label="L17 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_17_ffn_up [label="L17 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_17_ffn_gate [label="L17 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_17_ffn_down [label="L17 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_17_ffn_res [label="L17 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_17_norm1 [label="L17 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_17_norm2 [label="L17 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_16_ffn_res -> gpt3_2b_layer_17_norm1
		gpt3_2b_layer_17_norm1 -> gpt3_2b_layer_17_qkv_proj
		gpt3_2b_layer_17_qkv_proj -> gpt3_2b_layer_17_attn_calc
		gpt3_2b_layer_17_attn_calc -> gpt3_2b_layer_17_attn_out
		gpt3_2b_layer_17_attn_out -> gpt3_2b_layer_17_attn_res
		gpt3_2b_layer_17_attn_res -> gpt3_2b_layer_17_norm2
		gpt3_2b_layer_17_norm2 -> gpt3_2b_layer_17_ffn_up
		gpt3_2b_layer_17_ffn_up -> gpt3_2b_layer_17_ffn_gate
		gpt3_2b_layer_17_ffn_gate -> gpt3_2b_layer_17_ffn_down
		gpt3_2b_layer_17_ffn_down -> gpt3_2b_layer_17_ffn_res
		gpt3_2b_layer_18_qkv_proj [label="L18 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_18_attn_calc [label="L18 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_18_attn_out [label="L18 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_18_attn_res [label="L18 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_18_ffn_up [label="L18 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_18_ffn_gate [label="L18 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_18_ffn_down [label="L18 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_18_ffn_res [label="L18 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_18_norm1 [label="L18 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_18_norm2 [label="L18 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_17_ffn_res -> gpt3_2b_layer_18_norm1
		gpt3_2b_layer_18_norm1 -> gpt3_2b_layer_18_qkv_proj
		gpt3_2b_layer_18_qkv_proj -> gpt3_2b_layer_18_attn_calc
		gpt3_2b_layer_18_attn_calc -> gpt3_2b_layer_18_attn_out
		gpt3_2b_layer_18_attn_out -> gpt3_2b_layer_18_attn_res
		gpt3_2b_layer_18_attn_res -> gpt3_2b_layer_18_norm2
		gpt3_2b_layer_18_norm2 -> gpt3_2b_layer_18_ffn_up
		gpt3_2b_layer_18_ffn_up -> gpt3_2b_layer_18_ffn_gate
		gpt3_2b_layer_18_ffn_gate -> gpt3_2b_layer_18_ffn_down
		gpt3_2b_layer_18_ffn_down -> gpt3_2b_layer_18_ffn_res
		gpt3_2b_layer_19_qkv_proj [label="L19 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_19_attn_calc [label="L19 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_19_attn_out [label="L19 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_19_attn_res [label="L19 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_19_ffn_up [label="L19 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_19_ffn_gate [label="L19 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_19_ffn_down [label="L19 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_19_ffn_res [label="L19 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_19_norm1 [label="L19 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_19_norm2 [label="L19 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_18_ffn_res -> gpt3_2b_layer_19_norm1
		gpt3_2b_layer_19_norm1 -> gpt3_2b_layer_19_qkv_proj
		gpt3_2b_layer_19_qkv_proj -> gpt3_2b_layer_19_attn_calc
		gpt3_2b_layer_19_attn_calc -> gpt3_2b_layer_19_attn_out
		gpt3_2b_layer_19_attn_out -> gpt3_2b_layer_19_attn_res
		gpt3_2b_layer_19_attn_res -> gpt3_2b_layer_19_norm2
		gpt3_2b_layer_19_norm2 -> gpt3_2b_layer_19_ffn_up
		gpt3_2b_layer_19_ffn_up -> gpt3_2b_layer_19_ffn_gate
		gpt3_2b_layer_19_ffn_gate -> gpt3_2b_layer_19_ffn_down
		gpt3_2b_layer_19_ffn_down -> gpt3_2b_layer_19_ffn_res
		gpt3_2b_layer_20_qkv_proj [label="L20 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_20_attn_calc [label="L20 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_20_attn_out [label="L20 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_20_attn_res [label="L20 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_20_ffn_up [label="L20 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_20_ffn_gate [label="L20 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_20_ffn_down [label="L20 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_20_ffn_res [label="L20 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_20_norm1 [label="L20 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_20_norm2 [label="L20 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_19_ffn_res -> gpt3_2b_layer_20_norm1
		gpt3_2b_layer_20_norm1 -> gpt3_2b_layer_20_qkv_proj
		gpt3_2b_layer_20_qkv_proj -> gpt3_2b_layer_20_attn_calc
		gpt3_2b_layer_20_attn_calc -> gpt3_2b_layer_20_attn_out
		gpt3_2b_layer_20_attn_out -> gpt3_2b_layer_20_attn_res
		gpt3_2b_layer_20_attn_res -> gpt3_2b_layer_20_norm2
		gpt3_2b_layer_20_norm2 -> gpt3_2b_layer_20_ffn_up
		gpt3_2b_layer_20_ffn_up -> gpt3_2b_layer_20_ffn_gate
		gpt3_2b_layer_20_ffn_gate -> gpt3_2b_layer_20_ffn_down
		gpt3_2b_layer_20_ffn_down -> gpt3_2b_layer_20_ffn_res
		gpt3_2b_layer_21_qkv_proj [label="L21 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_21_attn_calc [label="L21 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_21_attn_out [label="L21 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_21_attn_res [label="L21 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_21_ffn_up [label="L21 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_21_ffn_gate [label="L21 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_21_ffn_down [label="L21 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_21_ffn_res [label="L21 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_21_norm1 [label="L21 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_21_norm2 [label="L21 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_20_ffn_res -> gpt3_2b_layer_21_norm1
		gpt3_2b_layer_21_norm1 -> gpt3_2b_layer_21_qkv_proj
		gpt3_2b_layer_21_qkv_proj -> gpt3_2b_layer_21_attn_calc
		gpt3_2b_layer_21_attn_calc -> gpt3_2b_layer_21_attn_out
		gpt3_2b_layer_21_attn_out -> gpt3_2b_layer_21_attn_res
		gpt3_2b_layer_21_attn_res -> gpt3_2b_layer_21_norm2
		gpt3_2b_layer_21_norm2 -> gpt3_2b_layer_21_ffn_up
		gpt3_2b_layer_21_ffn_up -> gpt3_2b_layer_21_ffn_gate
		gpt3_2b_layer_21_ffn_gate -> gpt3_2b_layer_21_ffn_down
		gpt3_2b_layer_21_ffn_down -> gpt3_2b_layer_21_ffn_res
		gpt3_2b_layer_22_qkv_proj [label="L22 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_22_attn_calc [label="L22 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_22_attn_out [label="L22 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_22_attn_res [label="L22 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_22_ffn_up [label="L22 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_22_ffn_gate [label="L22 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_22_ffn_down [label="L22 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_22_ffn_res [label="L22 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_22_norm1 [label="L22 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_22_norm2 [label="L22 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_21_ffn_res -> gpt3_2b_layer_22_norm1
		gpt3_2b_layer_22_norm1 -> gpt3_2b_layer_22_qkv_proj
		gpt3_2b_layer_22_qkv_proj -> gpt3_2b_layer_22_attn_calc
		gpt3_2b_layer_22_attn_calc -> gpt3_2b_layer_22_attn_out
		gpt3_2b_layer_22_attn_out -> gpt3_2b_layer_22_attn_res
		gpt3_2b_layer_22_attn_res -> gpt3_2b_layer_22_norm2
		gpt3_2b_layer_22_norm2 -> gpt3_2b_layer_22_ffn_up
		gpt3_2b_layer_22_ffn_up -> gpt3_2b_layer_22_ffn_gate
		gpt3_2b_layer_22_ffn_gate -> gpt3_2b_layer_22_ffn_down
		gpt3_2b_layer_22_ffn_down -> gpt3_2b_layer_22_ffn_res
		gpt3_2b_layer_23_qkv_proj [label="L23 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_23_attn_calc [label="L23 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_23_attn_out [label="L23 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_23_attn_res [label="L23 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_23_ffn_up [label="L23 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_23_ffn_gate [label="L23 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_23_ffn_down [label="L23 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_23_ffn_res [label="L23 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_23_norm1 [label="L23 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_23_norm2 [label="L23 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_22_ffn_res -> gpt3_2b_layer_23_norm1
		gpt3_2b_layer_23_norm1 -> gpt3_2b_layer_23_qkv_proj
		gpt3_2b_layer_23_qkv_proj -> gpt3_2b_layer_23_attn_calc
		gpt3_2b_layer_23_attn_calc -> gpt3_2b_layer_23_attn_out
		gpt3_2b_layer_23_attn_out -> gpt3_2b_layer_23_attn_res
		gpt3_2b_layer_23_attn_res -> gpt3_2b_layer_23_norm2
		gpt3_2b_layer_23_norm2 -> gpt3_2b_layer_23_ffn_up
		gpt3_2b_layer_23_ffn_up -> gpt3_2b_layer_23_ffn_gate
		gpt3_2b_layer_23_ffn_gate -> gpt3_2b_layer_23_ffn_down
		gpt3_2b_layer_23_ffn_down -> gpt3_2b_layer_23_ffn_res
		gpt3_2b_layer_24_qkv_proj [label="L24 QKV Proj\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, heads=16, head_dim=128]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_24_attn_calc [label="L24 Attention\n[batch=12, seq=2048, heads=16, head_dim=128] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_24_attn_out [label="L24 Attn Output\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
		gpt3_2b_layer_24_attn_res [label="L24 Attn+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_24_ffn_up [label="L24 FFN Up\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_24_ffn_gate [label="L24 FFN Gate\n[batch=12, seq=2048, hidden=2048] → [batch=12, seq=2048, ffn=8192]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_24_ffn_down [label="L24 FFN Down\n[batch=12, seq=2048, ffn=8192] → [batch=12, seq=2048, hidden=2048]" fillcolor=lightcyan shape=rectangle style=filled]
		gpt3_2b_layer_24_ffn_res [label="L24 FFN+Residual\n[batch=12, seq=2048, hidden=2048]" fillcolor=lightyellow shape=parallelogram style=filled]
		gpt3_2b_layer_24_norm1 [label="L24 LayerNorm 1" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_24_norm2 [label="L24 LayerNorm 2" fillcolor=lightgray shape=ellipse style=filled]
		gpt3_2b_layer_23_ffn_res -> gpt3_2b_layer_24_norm1
		gpt3_2b_layer_24_norm1 -> gpt3_2b_layer_24_qkv_proj
		gpt3_2b_layer_24_qkv_proj -> gpt3_2b_layer_24_attn_calc
		gpt3_2b_layer_24_attn_calc -> gpt3_2b_layer_24_attn_out
		gpt3_2b_layer_24_attn_out -> gpt3_2b_layer_24_attn_res
		gpt3_2b_layer_24_attn_res -> gpt3_2b_layer_24_norm2
		gpt3_2b_layer_24_norm2 -> gpt3_2b_layer_24_ffn_up
		gpt3_2b_layer_24_ffn_up -> gpt3_2b_layer_24_ffn_gate
		gpt3_2b_layer_24_ffn_gate -> gpt3_2b_layer_24_ffn_down
		gpt3_2b_layer_24_ffn_down -> gpt3_2b_layer_24_ffn_res
		gpt3_2b_layer_24_ffn_res -> gpt3_2b_output_proj
	}
	llama_7b_input -> llama_7b_embedding
	llama_7b_output_proj -> llama_7b_final_output
	gpt3_2b_input -> gpt3_2b_embedding
	gpt3_2b_output_proj -> gpt3_2b_final_output
}
