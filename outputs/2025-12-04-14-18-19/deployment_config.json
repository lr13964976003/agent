{
  "deployment_strategy": {
    "tensor_parallel_size": 8,
    "pipeline_parallel_size": 4,
    "expert_parallel_size": 16,
    "data_parallel_size": 4,
    "total_gpus": 512,
    "parallel_dimensions": {
      "tp": 8,
      "pp": 4,
      "ep": 16,
      "dp": 4
    }
  },
  "model_configuration": {
    "total_parameters": "30B",
    "layers": 16,
    "experts_per_layer": 64,
    "hidden_size": 1024,
    "ffn_hidden_size": 2048,
    "num_heads": 16,
    "head_dim": 64,
    "precision": "FP16",
    "batch_size": 128,
    "sequence_length": "128-10240"
  },
  "hardware_specifications": {
    "gpu_memory_gb": 64,
    "gpu_compute_tflops": 400,
    "memory_bandwidth_tb": 1.8,
    "mfu_utilization": 0.6,
    "bandwidth_utilization": 0.8
  },
  "performance_metrics": {
    "expected_latency_seconds": 0.016,
    "expected_throughput_sequences_per_second": 8000.0,
    "memory_efficiency": 1.0,
    "compute_efficiency": 0.6
  },
  "module_division": {
    "layers_per_pipeline_stage": 4,
    "experts_per_gpu": 4,
    "hidden_dimensions_per_tensor_group": 128,
    "attention_heads_per_tensor_group": 2,
    "total_pipeline_stages": 4,
    "total_tensor_groups": 8,
    "total_expert_groups": 16
  },
  "load_balancing": {
    "expert_distribution": "uniform",
    "layer_distribution": "uniform",
    "tensor_distribution": "uniform",
    "data_distribution": "uniform"
  },
  "optimization_features": {
    "expert_parallelism": true,
    "pipeline_parallelism": true,
    "tensor_parallelism": true,
    "data_parallelism": true,
    "gradient_checkpointing": true,
    "mixed_precision": true,
    "communication_overlapping": true
  }
}