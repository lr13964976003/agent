{
  "deployment_strategy": {
    "tensor_parallel_size": 8,
    "pipeline_parallel_size": 4,
    "expert_parallel_size": 8,
    "data_parallel_size": 2,
    "total_gpus": 512,
    "parallel_dimensions": {
      "tp": 8,
      "pp": 4,
      "ep": 8,
      "dp": 2
    }
  },
  "model_configuration": {
    "total_parameters": "30B",
    "layers": 16,
    "experts_per_layer": 64,
    "hidden_size": 1024,
    "ffn_hidden_size": 2048,
    "num_heads": 16,
    "head_dim": 64,
    "precision": "FP16",
    "batch_size": 128,
    "sequence_length": "128-10240"
  },
  "hardware_specifications": {
    "gpu_memory_gb": 64,
    "gpu_compute_tflops": 400,
    "memory_bandwidth_tb": 1.8,
    "mfu_utilization": 0.6,
    "bandwidth_utilization": 0.8
  },
  "performance_metrics": {
    "expected_latency_seconds": 0.064,
    "expected_throughput_sequences_per_second": 2000.0,
    "memory_efficiency": 1.0,
    "compute_efficiency": 0.6
  },
  "module_division": {
    "layers_per_pipeline_stage": 4,
    "experts_per_gpu": 8,
    "hidden_dimensions_per_tensor_group": 128,
    "attention_heads_per_tensor_group": 2,
    "total_pipeline_stages": 4,
    "total_tensor_groups": 8,
    "total_expert_groups": 8
  },
  "load_balancing": {
    "expert_distribution": "uniform",
    "layer_distribution": "uniform",
    "tensor_distribution": "uniform",
    "data_distribution": "uniform"
  },
  "optimization_features": {
    "expert_parallelism": true,
    "pipeline_parallelism": true,
    "tensor_parallelism": true,
    "data_parallelism": true,
    "gradient_checkpointing": true,
    "mixed_precision": true,
    "communication_overlapping": true
  },
  "mathematical_verification": {
    "gpu_count_calculation": "8 × 4 × 8 × 2 = 512",
    "expert_division": "64 ÷ 8 = 8",
    "layer_division": "16 ÷ 4 = 4",
    "hidden_division": "1024 ÷ 8 = 128",
    "head_division": "16 ÷ 8 = 2",
    "memory_per_gpu": "~32.15GB",
    "total_memory_required": "~257GB"
  }
}