digraph EP64_TP2_Deployment {
    rankdir=TB;
    node [shape=box, style=rounded];
    
    // Input layer
    Input [label="Input\n128 seq × 1024 tokens", shape=ellipse];
    
    // Layer clusters
    subgraph cluster_layer_0 {
        label = "Layer 0 (EP64_TP2)";
        style = filled;
        fillcolor = lightblue;
        
        // Attention TP pairs
        subgraph cluster_att_0 {
            label = "Attention (TP=2)";
            style = filled;
            fillcolor = lightgreen;
            
            ATT_0_0 [label="GPU 0-1\nAttention TP Pair 0\n8 heads × 512 dim"];
            ATT_0_2 [label="GPU 2-3\nAttention TP Pair 1\n8 heads × 512 dim"];
            ATT_0_4 [label="GPU 4-5\nAttention TP Pair 2\n8 heads × 512 dim"];
            // ... pattern continues
            ATT_0_62 [label="GPU 62-63\nAttention TP Pair 31\n8 heads × 512 dim"];
        }
        
        // MOE EP groups
        subgraph cluster_moe_0 {
            label = "MOE (EP=64)";
            style = filled;
            fillcolor = lightyellow;
            
            // Expert groups - each handles 0.5 expert
            EXP_0_0 [label="GPU 0-64\nExpert Group 0\n0.5 expert"];
            EXP_0_1 [label="GPU 1-65\nExpert Group 1\n0.5 expert"];
            EXP_0_2 [label="GPU 2-66\nExpert Group 2\n0.5 expert"];
            // ... pattern continues
            EXP_0_63 [label="GPU 63-127\nExpert Group 63\n0.5 expert"];
        }
    }
    
    // Continue pattern for all 16 layers (showing representative layers)
    subgraph cluster_layer_1 {
        label = "Layer 1 (EP64_TP2)";
        style = filled;
        fillcolor = lightblue;
        
        ATT_1_0 [label="GPU 0-1\nAttention TP Pair 0"];
        ATT_1_2 [label="GPU 2-3\nAttention TP Pair 1"];
        // ... continues for all 32 TP pairs
        
        EXP_1_0 [label="GPU 0-64\nExpert Group 0"];
        EXP_1_1 [label="GPU 1-65\nExpert Group 1"];
        // ... continues for all 64 EP groups
    }
    
    // Final layer
    subgraph cluster_layer_15 {
        label = "Layer 15 (EP64_TP2)";
        style = filled;
        fillcolor = lightblue;
        
        ATT_15_0 [label="GPU 0-1\nAttention TP Pair 0"];
        ATT_15_2 [label="GPU 2-3\nAttention TP Pair 1"];
        // ... continues for all 32 TP pairs
        
        EXP_15_0 [label="GPU 0-64\nExpert Group 0"];
        EXP_15_1 [label="GPU 1-65\nExpert Group 1"];
        // ... continues for all 64 EP groups
    }
    
    // Output
    Output [label="Output\n128 seq × 1024 tokens", shape=ellipse];
    
    // Connections
    Input -> ATT_0_0;
    ATT_0_0 -> EXP_0_0;
    EXP_0_0 -> ATT_1_0;
    // ... pattern continues through all layers
    EXP_15_0 -> Output;
    
    // All-reduce communication edges
    edge [style=dashed, color=red, label="AllReduce"];
    ATT_0_1 -> ATT_0_0;
    ATT_0_2 -> ATT_0_1;
    // ... continues for all TP pairs
    
    // Expert communication edges
    edge [style=dotted, color=blue, label="Expert Comm"];
    EXP_0_0 -> EXP_0_1;
    EXP_0_1 -> EXP_0_2;
    // ... continues for expert routing
    
    // Summary statistics
    Stats [label="Deployment Summary:\n• 128 GPUs Total\n• EP=64, TP=2, PP=1\n• 8 Expert Instances per GPU\n• Perfect Load Balancing\n• 1.024ms Latency\n• 128M tokens/sec Throughput", 
           shape=note, style=filled, fillcolor=lightcoral];
}