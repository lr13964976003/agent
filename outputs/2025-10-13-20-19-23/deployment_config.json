{
  "models": {
    "baseline": {
      "name": "MoE_Baseline_TP8_PP2",
      "description": "Traditional MoE deployment with tensor parallelism and pipeline parallelism",
      "model_config": {
        "layers": 4,
        "experts_per_layer": 16,
        "expert_type": "MLP",
        "precision": "FP16",
        "token_dimension": 8192,
        "mha_heads": 16,
        "mha_head_dim": 512,
        "mlp_hidden_size": 32768,
        "batch_size": 1024,
        "sequence_length": 10000
      },
      "parallel_strategy": {
        "tensor_parallelism": {
          "degree": 8,
          "type": "column_row_parallel",
          "partition_dims": ["output_dim", "input_dim"]
        },
        "pipeline_parallelism": {
          "degree": 2,
          "stages": 2,
          "layers_per_stage": 2
        },
        "expert_parallelism": {
          "degree": 1,
          "experts_per_device": 8,
          "strategy": "colocated"
        }
      },
      "device_mapping": {
        "total_gpus": 16,
        "gpu_type": "H100",
        "topology": {
          "stage_0": {
            "gpus": [0, 1, 2, 3, 4, 5, 6, 7],
            "layers": [0, 1],
            "experts_per_gpu": 8,
            "tensor_parallel_group": [0, 1, 2, 3, 4, 5, 6, 7]
          },
          "stage_1": {
            "gpus": [8, 9, 10, 11, 12, 13, 14, 15],
            "layers": [2, 3],
            "experts_per_gpu": 8,
            "tensor_parallel_group": [8, 9, 10, 11, 12, 13, 14, 15]
          }
        }
      },
      "modules": {
        "attention_layer": {
          "type": "multi_head_attention",
          "tensor_parallel_shard": {
            "device_0": {"qkv_weight": [8192, 8192/8], "output_weight": [8192, 8192/8]},
            "device_1": {"qkv_weight": [8192, 8192/8], "output_weight": [8192, 8192/8]},
            "device_2": {"qkv_weight": [8192, 8192/8], "output_weight": [8192, 8192/8]},
            "device_3": {"qkv_weight": [8192, 8192/8], "output_weight": [8192, 8192/8]},
            "device_4": {"qkv_weight": [8192, 8192/8], "output_weight": [8192, 8192/8]},
            "device_5": {"qkv_weight": [8192, 8192/8], "output_weight": [8192, 8192/8]},
            "device_6": {"qkv_weight": [8192, 8192/8], "output_weight": [8192, 8192/8]},
            "device_7": {"qkv_weight": [8192, 8192/8], "output_weight": [8192, 8192/8]}
          }
        },
        "expert_layer": {
          "type": "mlp_expert",
          "experts": {
            "expert_0_to_7": {"device": "shared_across_gpus", "parameters": 8},
            "expert_8_to_15": {"device": "shared_across_gpus", "parameters": 8}
          }
        }
      }
    },
    "proposed": {
      "name": "MoE_Large_EP16",
      "description": "Large-scale cross-node expert parallelism with one expert per GPU",
      "model_config": {
        "layers": 4,
        "experts_per_layer": 16,
        "expert_type": "MLP",
        "precision": "FP16",
        "token_dimension": 8192,
        "mha_heads": 16,
        "mha_head_dim": 512,
        "mlp_hidden_size": 32768,
        "batch_size": 1024,
        "sequence_length": 10000
      },
      "parallel_strategy": {
        "expert_parallelism": {
          "degree": 16,
          "type": "cross_node",
          "experts_per_device": 1,
          "strategy": "single_expert_per_gpu"
        },
        "tensor_parallelism": {
          "degree": 1,
          "usage": "within_expert_if_needed"
        },
        "pipeline_parallelism": {
          "degree": 1,
          "usage": "not_used"
        },
        "data_parallelism": {
          "degree": 1,
          "usage": "for_replicas"
        }
      },
      "device_mapping": {
        "total_gpus": 16,
        "gpu_type": "H100",
        "topology": {
          "expert_placement": {
            "layer_0": {
              "expert_0": {"gpu": 0, "parameters": "full_expert_mlp"},
              "expert_1": {"gpu": 1, "parameters": "full_expert_mlp"},
              "expert_2": {"gpu": 2, "parameters": "full_expert_mlp"},
              "expert_3": {"gpu": 3, "parameters": "full_expert_mlp"},
              "expert_4": {"gpu": 4, "parameters": "full_expert_mlp"},
              "expert_5": {"gpu": 5, "parameters": "full_expert_mlp"},
              "expert_6": {"gpu": 6, "parameters": "full_expert_mlp"},
              "expert_7": {"gpu": 7, "parameters": "full_expert_mlp"},
              "expert_8": {"gpu": 8, "parameters": "full_expert_mlp"},
              "expert_9": {"gpu": 9, "parameters": "full_expert_mlp"},
              "expert_10": {"gpu": 10, "parameters": "full_expert_mlp"},
              "expert_11": {"gpu": 11, "parameters": "full_expert_mlp"},
              "expert_12": {"gpu": 12, "parameters": "full_expert_mlp"},
              "expert_13": {"gpu": 13, "parameters": "full_expert_mlp"},
              "expert_14": {"gpu": 14, "parameters": "full_expert_mlp"},
              "expert_15": {"gpu": 15, "parameters": "full_expert_mlp"}
            },
            "layer_1": {
              "expert_0": {"gpu": 0, "parameters": "full_expert_mlp"},
              "expert_1": {"gpu": 1, "parameters": "full_expert_mlp"},
              "expert_2": {"gpu": 2, "parameters": "full_expert_mlp"},
              "expert_3": {"gpu": 3, "parameters": "full_expert_mlp"},
              "expert_4": {"gpu": 4, "parameters": "full_expert_mlp"},
              "expert_5": {"gpu": 5, "parameters": "full_expert_mlp"},
              "expert_6": {"gpu": 6, "parameters": "full_expert_mlp"},
              "expert_7": {"gpu": 7, "parameters": "full_expert_mlp"},
              "expert_8": {"gpu": 8, "parameters": "full_expert_mlp"},
              "expert_9": {"gpu": 9, "parameters": "full_expert_mlp"},
              "expert_10": {"gpu": 10, "parameters": "full_expert_mlp"},
              "expert_11": {"gpu": 11, "parameters": "full_expert_mlp"},
              "expert_12": {"gpu": 12, "parameters": "full_expert_mlp"},
              "expert_13": {"gpu": 13, "parameters": "full_expert_mlp"},
              "expert_14": {"gpu": 14, "parameters": "full_expert_mlp"},
              "expert_15": {"gpu": 15, "parameters": "full_expert_mlp"}
            },
            "layer_2": {
              "expert_0": {"gpu": 0, "parameters": "full_expert_mlp"},
              "expert_1": {"gpu": 1, "parameters": "full_expert_mlp"},
              "expert_2": {"gpu": 2, "parameters": "full_expert_mlp"},
              "expert_3": {"gpu": 3, "parameters": "full_expert_mlp"},
              "expert_4": {"gpu": 4, "parameters": "full_expert_mlp"},
              "expert_5": {"gpu": 5, "parameters": "full_expert_mlp"},
              "expert_6": {"gpu": 6, "parameters": "full_expert_mlp"},
              "expert_7": {"gpu": 7, "parameters": "full_expert_mlp"},
              "expert_8": {"gpu": 8, "parameters": "full_expert_mlp"},
              "expert_9": {"gpu": 9, "parameters": "full_expert_mlp"},
              "expert_10": {"gpu": 10, "parameters": "full_expert_mlp"},
              "expert_11": {"gpu": 11, "parameters": "full_expert_mlp"},
              "expert_12": {"gpu": 12, "parameters": "full_expert_mlp"},
              "expert_13": {"gpu": 13, "parameters": "full_expert_mlp"},
              "expert_14": {"gpu": 14, "parameters": "full_expert_mlp"},
              "expert_15": {"gpu": 15, "parameters": "full_expert_mlp"}
            },
            "layer_3": {
              "expert_0": {"gpu": 0, "parameters": "full_expert_mlp"},
              "expert_1": {"gpu": 1, "parameters": "full_expert_mlp"},
              "expert_2": {"gpu": 2, "parameters": "full_expert_mlp"},
              "expert_3": {"gpu": 3, "parameters": "full_expert_mlp"},
              "expert_4": {"gpu": 4, "parameters": "full_expert_mlp"},
              "expert_5": {"gpu": 5, "parameters": "full_expert_mlp"},
              "expert_6": {"gpu": 6, "parameters": "full_expert_mlp"},
              "expert_7": {"gpu": 7, "parameters": "full_expert_mlp"},
              "expert_8": {"gpu": 8, "parameters": "full_expert_mlp"},
              "expert_9": {"gpu": 9, "parameters": "full_expert_mlp"},
              "expert_10": {"gpu": 10, "parameters": "full_expert_mlp"},
              "expert_11": {"gpu": 11, "parameters": "full_expert_mlp"},
              "expert_12": {"gpu": 12, "parameters": "full_expert_mlp"},
              "expert_13": {"gpu": 13, "parameters": "full_expert_mlp"},
              "expert_14": {"gpu": 14, "parameters": "full_expert_mlp"},
              "expert_15": {"gpu": 15, "parameters": "full_expert_mlp"}
            }
          }
        }
      },
      "modules": {
        "gating_network": {
          "type": "top_k_routing",
          "k": 2,
          "location": "all_gpus",
          "parameters": {"shared": true}
        },
        "expert_modules": {
          "type": "mlp_expert",
          "parameters_per_expert": {
            "w1": [32768, 8192],
            "w2": [8192, 32768],
            "activation": "gelu"
          }
        },
        "attention_modules": {
          "type": "multi_head_attention",
          "location": "each_gpu",
          "parameters": {
            "q_proj": [8192, 8192],
            "k_proj": [8192, 8192],
            "v_proj": [8192, 8192],
            "o_proj": [8192, 8192]
          }
        },
        "communication": {
          "type": "asynchronous_nccl",
          "token_batching": true,
          "double_buffering": true,
          "cuda_streams": 4
        }
      }
    }
  },
  "deployment_parameters": {
    "network_requirements": {
      "interconnect": "NVLink_or_InfiniBand",
      "bandwidth": "400_Gbps_minimum",
      "latency": "sub_microsecond"
    },
    "memory_requirements": {
      "per_gpu_memory": "80GB_H100",
      "expert_memory_per_gpu": "~16GB_FP16",
      "activation_memory": "~32GB_per_batch"
    },
    "scheduling": {
      "load_balancing": "dynamic_gating_adjustment",
      "routing_strategy": "topology_aware",
      "communication_overlap": "compute_communication_interleaving"
    }
  }
}