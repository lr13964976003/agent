{
  "generated_dags": {
    "baseline_deployment": {
      "dot_file": "../outputs/2025-11-14-14-42-00/baseline_deployment.dot",
      "svg_file": "../outputs/2025-11-14-14-42-00/baseline_deployment.svg",
      "description": "Tensor Parallelism + Pipeline Parallelism deployment strategy using 16 GPUs with TP=8, PP=2"
    },
    "proposed_deployment": {
      "dot_file": "../outputs/2025-11-14-14-42-00/proposed_deployment.dot", 
      "svg_file": "../outputs/2025-11-14-14-42-00/proposed_deployment.svg",
      "description": "Layer-wise Cache-Optimized Deployment with 4 GPU groups Ã— 4 GPUs each, featuring weight streaming and activation chunking"
    }
  },
  "analysis_summary": {
    "model_architecture": "4-layer dense neural network with 30B parameters",
    "total_gpus": 16,
    "baseline_strategy": {
      "type": "hybrid parallelism",
      "tensor_parallel_size": 8,
      "pipeline_parallel_size": 2,
      "layers_per_stage": 2,
      "memory_distribution": "tensor sharding across 8 GPUs"
    },
    "proposed_strategy": {
      "type": "layer-wise cache-optimized",
      "gpu_groups": 4,
      "gpus_per_group": 4,
      "layers_per_group": 1,
      "cache_utilization": "48MB/50MB per GPU group",
      "weight_streaming": "tile-based (32MB tiles)",
      "activation_chunking": "64-token chunks"
    }
  },
  "dag_validation": {
    "baseline": {
      "nodes_total": 23,
      "edges_total": 22,
      "has_cycle": false,
      "input_output_complete": true,
      "gpu_assignments": {
        "stage_0": [0, 1, 2, 3, 4, 5, 6, 7],
        "stage_1": [8, 9, 10, 11, 12, 13, 14, 15]
      }
    },
    "proposed": {
      "nodes_total": 21,
      "edges_total": 20,
      "has_cycle": false,
      "input_output_complete": true,
      "gpu_assignments": {
        "group_0": [0, 1, 2, 3],
        "group_1": [4, 5, 6, 7],
        "group_2": [8, 9, 10, 11],
        "group_3": [12, 13, 14, 15]
      }
    }
  }
}