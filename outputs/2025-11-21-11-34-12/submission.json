{
  "baseline_dag": {
    "dot_file": "../outputs/2025-11-21-11-34-12/baseline_dag.dot",
    "svg_file": "../outputs/2025-11-21-11-34-12/baseline_dag.svg",
    "description": "Tensor Parallel + Pipeline Parallel baseline using 16 GPUs (TP=8, PP=2)"
  },
  "proposed_dag": {
    "dot_file": "../outputs/2025-11-21-11-34-12/proposed_dag.dot",
    "svg_file": "../outputs/2025-11-21-11-34-12/proposed_dag.svg", 
    "description": "Two-Level Attention Partitioning method using 16 GPUs (m×n=16 with 4 head groups × 4 dimension slices)"
  },
  "summary": {
    "total_gpus": 16,
    "baseline_strategy": "Hybrid Tensor+Pipeline Parallelism",
    "proposed_strategy": "Two-Level Attention Partitioning",
    "files_generated": [
      "baseline_dag.dot",
      "baseline_dag.svg", 
      "proposed_dag.dot",
      "proposed_dag.svg",
      "baseline_dag.py",
      "proposed_dag.py"
    ]
  }
}