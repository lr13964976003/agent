// DeepSeek-R1 with Helix Parallelism (16 GPUs)
digraph DeepSeek_R1_Helix {
	compound=true rankdir=TB size="20,30"
	node [fontsize=10 height=0.8 width=2.5]
	input [label="Input\nInput: [batch_size=B, seq_len=1, hidden_dim=16384]" fillcolor=lightblue shape=ellipse style=filled]
	input -> qkv_0
	qkv_0 -> attn_0
	attn_0 -> hopb
	input -> qkv_1
	qkv_1 -> attn_1
	attn_1 -> hopb
	input -> qkv_2
	qkv_2 -> attn_2
	attn_2 -> hopb
	input -> qkv_3
	qkv_3 -> attn_3
	attn_3 -> hopb
	input -> qkv_4
	qkv_4 -> attn_4
	attn_4 -> hopb
	input -> qkv_5
	qkv_5 -> attn_5
	attn_5 -> hopb
	input -> qkv_6
	qkv_6 -> attn_6
	attn_6 -> hopb
	input -> qkv_7
	qkv_7 -> attn_7
	attn_7 -> hopb
	input -> qkv_8
	qkv_8 -> attn_8
	attn_8 -> hopb
	input -> qkv_9
	qkv_9 -> attn_9
	attn_9 -> hopb
	input -> qkv_10
	qkv_10 -> attn_10
	attn_10 -> hopb
	input -> qkv_11
	qkv_11 -> attn_11
	attn_11 -> hopb
	input -> qkv_12
	qkv_12 -> attn_12
	attn_12 -> hopb
	input -> qkv_13
	qkv_13 -> attn_13
	attn_13 -> hopb
	input -> qkv_14
	qkv_14 -> attn_14
	attn_14 -> hopb
	input -> qkv_15
	qkv_15 -> attn_15
	attn_15 -> hopb
	subgraph cluster_attention {
		label="Attention Phase (KV Parallelism)" style=dashed
		qkv_0 [label="QKV Projection\nGPU 0\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_1 [label="QKV Projection\nGPU 1\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_2 [label="QKV Projection\nGPU 2\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_3 [label="QKV Projection\nGPU 3\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_4 [label="QKV Projection\nGPU 4\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_5 [label="QKV Projection\nGPU 5\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_6 [label="QKV Projection\nGPU 6\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_7 [label="QKV Projection\nGPU 7\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_8 [label="QKV Projection\nGPU 8\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_9 [label="QKV Projection\nGPU 9\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_10 [label="QKV Projection\nGPU 10\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_11 [label="QKV Projection\nGPU 11\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_12 [label="QKV Projection\nGPU 12\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_13 [label="QKV Projection\nGPU 13\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_14 [label="QKV Projection\nGPU 14\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_15 [label="QKV Projection\nGPU 15\nInput: [B, 1, 16384]\nOutput: [B, 1, 128, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		attn_0 [label="Attention\nGPU 0\nKV slice: [tokens 0:16S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_1 [label="Attention\nGPU 1\nKV slice: [tokens 16:32S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_2 [label="Attention\nGPU 2\nKV slice: [tokens 32:48S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_3 [label="Attention\nGPU 3\nKV slice: [tokens 48:64S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_4 [label="Attention\nGPU 4\nKV slice: [tokens 64:80S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_5 [label="Attention\nGPU 5\nKV slice: [tokens 80:96S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_6 [label="Attention\nGPU 6\nKV slice: [tokens 96:112S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_7 [label="Attention\nGPU 7\nKV slice: [tokens 112:128S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_8 [label="Attention\nGPU 8\nKV slice: [tokens 128:144S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_9 [label="Attention\nGPU 9\nKV slice: [tokens 144:160S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_10 [label="Attention\nGPU 10\nKV slice: [tokens 160:176S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_11 [label="Attention\nGPU 11\nKV slice: [tokens 176:192S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_12 [label="Attention\nGPU 12\nKV slice: [tokens 192:208S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_13 [label="Attention\nGPU 13\nKV slice: [tokens 208:224S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_14 [label="Attention\nGPU 14\nKV slice: [tokens 224:240S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_15 [label="Attention\nGPU 15\nKV slice: [tokens 240:256S/16]\nInput: [B, 1, 128, 128]\nOutput: [B, 1, 128, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		hopb [label="HOP-B All-to-All\nCommunication\nVolume: [B, 16384]\nOverlap with next token" fillcolor=lightcoral shape=parallelogram style=dashed]
	}
	hopb -> gate_0 [lhead=cluster_ffn]
	gate_0 -> expert_0_tp_0
	expert_0_tp_0 -> ffn_0_tp_0
	ffn_0_tp_0 -> allreduce_0
	allreduce_0 -> ep_allgather
	gate_1 -> expert_0_tp_1
	expert_0_tp_1 -> ffn_0_tp_1
	ffn_0_tp_1 -> allreduce_0
	allreduce_0 -> ep_allgather
	gate_2 -> expert_1_tp_0
	expert_1_tp_0 -> ffn_1_tp_0
	ffn_1_tp_0 -> allreduce_1
	allreduce_1 -> ep_allgather
	gate_3 -> expert_1_tp_1
	expert_1_tp_1 -> ffn_1_tp_1
	ffn_1_tp_1 -> allreduce_1
	allreduce_1 -> ep_allgather
	gate_4 -> expert_2_tp_0
	expert_2_tp_0 -> ffn_2_tp_0
	ffn_2_tp_0 -> allreduce_2
	allreduce_2 -> ep_allgather
	gate_5 -> expert_2_tp_1
	expert_2_tp_1 -> ffn_2_tp_1
	ffn_2_tp_1 -> allreduce_2
	allreduce_2 -> ep_allgather
	gate_6 -> expert_3_tp_0
	expert_3_tp_0 -> ffn_3_tp_0
	ffn_3_tp_0 -> allreduce_3
	allreduce_3 -> ep_allgather
	gate_7 -> expert_3_tp_1
	expert_3_tp_1 -> ffn_3_tp_1
	ffn_3_tp_1 -> allreduce_3
	allreduce_3 -> ep_allgather
	gate_8 -> expert_4_tp_0
	expert_4_tp_0 -> ffn_4_tp_0
	ffn_4_tp_0 -> allreduce_4
	allreduce_4 -> ep_allgather
	gate_9 -> expert_4_tp_1
	expert_4_tp_1 -> ffn_4_tp_1
	ffn_4_tp_1 -> allreduce_4
	allreduce_4 -> ep_allgather
	gate_10 -> expert_5_tp_0
	expert_5_tp_0 -> ffn_5_tp_0
	ffn_5_tp_0 -> allreduce_5
	allreduce_5 -> ep_allgather
	gate_11 -> expert_5_tp_1
	expert_5_tp_1 -> ffn_5_tp_1
	ffn_5_tp_1 -> allreduce_5
	allreduce_5 -> ep_allgather
	gate_12 -> expert_6_tp_0
	expert_6_tp_0 -> ffn_6_tp_0
	ffn_6_tp_0 -> allreduce_6
	allreduce_6 -> ep_allgather
	gate_13 -> expert_6_tp_1
	expert_6_tp_1 -> ffn_6_tp_1
	ffn_6_tp_1 -> allreduce_6
	allreduce_6 -> ep_allgather
	gate_14 -> expert_7_tp_0
	expert_7_tp_0 -> ffn_7_tp_0
	ffn_7_tp_0 -> allreduce_7
	allreduce_7 -> ep_allgather
	gate_15 -> expert_7_tp_1
	expert_7_tp_1 -> ffn_7_tp_1
	ffn_7_tp_1 -> allreduce_7
	allreduce_7 -> ep_allgather
	subgraph cluster_ffn {
		label="FFN Phase (TPÃ—EP)" style=dashed
		gate_0 [label="Gate\nGPU 0\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_1 [label="Gate\nGPU 1\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_2 [label="Gate\nGPU 2\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_3 [label="Gate\nGPU 3\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_4 [label="Gate\nGPU 4\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_5 [label="Gate\nGPU 5\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_6 [label="Gate\nGPU 6\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_7 [label="Gate\nGPU 7\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_8 [label="Gate\nGPU 8\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_9 [label="Gate\nGPU 9\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_10 [label="Gate\nGPU 10\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_11 [label="Gate\nGPU 11\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_12 [label="Gate\nGPU 12\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_13 [label="Gate\nGPU 13\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_14 [label="Gate\nGPU 14\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_15 [label="Gate\nGPU 15\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		expert_0_tp_0 [label="Expert 0\nTP 0\nGPU 0\nExperts: [0:2]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_0_tp_1 [label="Expert 0\nTP 1\nGPU 1\nExperts: [0:2]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_1_tp_0 [label="Expert 1\nTP 0\nGPU 2\nExperts: [2:4]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_1_tp_1 [label="Expert 1\nTP 1\nGPU 3\nExperts: [2:4]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_2_tp_0 [label="Expert 2\nTP 0\nGPU 4\nExperts: [4:6]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_2_tp_1 [label="Expert 2\nTP 1\nGPU 5\nExperts: [4:6]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_3_tp_0 [label="Expert 3\nTP 0\nGPU 6\nExperts: [6:8]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_3_tp_1 [label="Expert 3\nTP 1\nGPU 7\nExperts: [6:8]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_4_tp_0 [label="Expert 4\nTP 0\nGPU 8\nExperts: [8:10]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_4_tp_1 [label="Expert 4\nTP 1\nGPU 9\nExperts: [8:10]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_5_tp_0 [label="Expert 5\nTP 0\nGPU 10\nExperts: [10:12]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_5_tp_1 [label="Expert 5\nTP 1\nGPU 11\nExperts: [10:12]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_6_tp_0 [label="Expert 6\nTP 0\nGPU 12\nExperts: [12:14]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_6_tp_1 [label="Expert 6\nTP 1\nGPU 13\nExperts: [12:14]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_7_tp_0 [label="Expert 7\nTP 0\nGPU 14\nExperts: [14:16]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_7_tp_1 [label="Expert 7\nTP 1\nGPU 15\nExperts: [14:16]\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		ffn_0_tp_0 [label="FFN\nGPU 0\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_0_tp_1 [label="FFN\nGPU 1\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_1_tp_0 [label="FFN\nGPU 2\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_1_tp_1 [label="FFN\nGPU 3\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_2_tp_0 [label="FFN\nGPU 4\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_2_tp_1 [label="FFN\nGPU 5\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_3_tp_0 [label="FFN\nGPU 6\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_3_tp_1 [label="FFN\nGPU 7\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_4_tp_0 [label="FFN\nGPU 8\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_4_tp_1 [label="FFN\nGPU 9\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_5_tp_0 [label="FFN\nGPU 10\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_5_tp_1 [label="FFN\nGPU 11\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_6_tp_0 [label="FFN\nGPU 12\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_6_tp_1 [label="FFN\nGPU 13\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_7_tp_0 [label="FFN\nGPU 14\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_7_tp_1 [label="FFN\nGPU 15\nUP: [B/8, 2048, 16384/2]\nGate: [B/8, 2048, 16384/2]\nDown: [B/8, 16384/2, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		allreduce_0 [label="TP All-Reduce\nExpert 0\nTPF=2\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=gold shape=parallelogram style=filled]
		allreduce_1 [label="TP All-Reduce\nExpert 1\nTPF=2\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=gold shape=parallelogram style=filled]
		allreduce_2 [label="TP All-Reduce\nExpert 2\nTPF=2\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=gold shape=parallelogram style=filled]
		allreduce_3 [label="TP All-Reduce\nExpert 3\nTPF=2\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=gold shape=parallelogram style=filled]
		allreduce_4 [label="TP All-Reduce\nExpert 4\nTPF=2\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=gold shape=parallelogram style=filled]
		allreduce_5 [label="TP All-Reduce\nExpert 5\nTPF=2\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=gold shape=parallelogram style=filled]
		allreduce_6 [label="TP All-Reduce\nExpert 6\nTPF=2\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=gold shape=parallelogram style=filled]
		allreduce_7 [label="TP All-Reduce\nExpert 7\nTPF=2\nInput: [B/8, 1, 2048]\nOutput: [B/8, 1, 2048]" fillcolor=gold shape=parallelogram style=filled]
		ep_allgather [label="EP All-Gather\n8 experts\nInput: [B/{8}, 1, 2048]\nOutput: [B, 1, 16384]" fillcolor=gold shape=parallelogram style=filled]
	}
	output [label="Output\nInput: [B, 1, 16384]\nOutput: [B, 1, 16384]" fillcolor=lightgreen shape=ellipse style=filled]
	ep_allgather -> output
}
