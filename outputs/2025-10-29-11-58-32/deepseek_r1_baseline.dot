// DeepSeek-R1 Baseline Tensor Parallelism (8 GPUs)
digraph DeepSeek_R1_TP_Baseline {
	compound=true rankdir=TB size="15,20"
	node [fontsize=10 height=0.8 width=2.5]
	input [label="Input\nInput: [batch_size=B, seq_len=1, hidden_dim=16384]" fillcolor=lightblue shape=ellipse style=filled]
	subgraph cluster_attention {
		label="Attention Phase (TP=8)" style=dashed
		qkv_0 [label="QKV Projection\nGPU 0\nInput: [B, 1, 16384]\nOutput: [B, 1, 16, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_1 [label="QKV Projection\nGPU 1\nInput: [B, 1, 16384]\nOutput: [B, 1, 16, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_2 [label="QKV Projection\nGPU 2\nInput: [B, 1, 16384]\nOutput: [B, 1, 16, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_3 [label="QKV Projection\nGPU 3\nInput: [B, 1, 16384]\nOutput: [B, 1, 16, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_4 [label="QKV Projection\nGPU 4\nInput: [B, 1, 16384]\nOutput: [B, 1, 16, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_5 [label="QKV Projection\nGPU 5\nInput: [B, 1, 16384]\nOutput: [B, 1, 16, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_6 [label="QKV Projection\nGPU 6\nInput: [B, 1, 16384]\nOutput: [B, 1, 16, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		qkv_7 [label="QKV Projection\nGPU 7\nInput: [B, 1, 16384]\nOutput: [B, 1, 16, 128]" fillcolor=lightyellow shape=rectangle style=filled]
		kv_cache_0 [label="KV Cache\nGPU 0\nFull cache: [B, S, 8, 128]\n(duplicated)" fillcolor=lightgray shape=rectangle style=filled]
		kv_cache_1 [label="KV Cache\nGPU 1\nFull cache: [B, S, 8, 128]\n(duplicated)" fillcolor=lightgray shape=rectangle style=filled]
		kv_cache_2 [label="KV Cache\nGPU 2\nFull cache: [B, S, 8, 128]\n(duplicated)" fillcolor=lightgray shape=rectangle style=filled]
		kv_cache_3 [label="KV Cache\nGPU 3\nFull cache: [B, S, 8, 128]\n(duplicated)" fillcolor=lightgray shape=rectangle style=filled]
		kv_cache_4 [label="KV Cache\nGPU 4\nFull cache: [B, S, 8, 128]\n(duplicated)" fillcolor=lightgray shape=rectangle style=filled]
		kv_cache_5 [label="KV Cache\nGPU 5\nFull cache: [B, S, 8, 128]\n(duplicated)" fillcolor=lightgray shape=rectangle style=filled]
		kv_cache_6 [label="KV Cache\nGPU 6\nFull cache: [B, S, 8, 128]\n(duplicated)" fillcolor=lightgray shape=rectangle style=filled]
		kv_cache_7 [label="KV Cache\nGPU 7\nFull cache: [B, S, 8, 128]\n(duplicated)" fillcolor=lightgray shape=rectangle style=filled]
		attn_0 [label="Attention\nGPU 0\nInput: [B, 1, 16, 128]\nOutput: [B, 1, 16, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_1 [label="Attention\nGPU 1\nInput: [B, 1, 16, 128]\nOutput: [B, 1, 16, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_2 [label="Attention\nGPU 2\nInput: [B, 1, 16, 128]\nOutput: [B, 1, 16, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_3 [label="Attention\nGPU 3\nInput: [B, 1, 16, 128]\nOutput: [B, 1, 16, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_4 [label="Attention\nGPU 4\nInput: [B, 1, 16, 128]\nOutput: [B, 1, 16, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_5 [label="Attention\nGPU 5\nInput: [B, 1, 16, 128]\nOutput: [B, 1, 16, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_6 [label="Attention\nGPU 6\nInput: [B, 1, 16, 128]\nOutput: [B, 1, 16, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_7 [label="Attention\nGPU 7\nInput: [B, 1, 16, 128]\nOutput: [B, 1, 16, 128]" fillcolor=lightgreen shape=rectangle style=filled]
		attn_allreduce [label="TP All-Reduce\nTP=8\nInput: [B, 1, 16, 128]×8\nOutput: [B, 1, 16384]" fillcolor=gold shape=parallelogram style=filled]
	}
	subgraph cluster_ffn {
		label="FFN Phase (TP=8×EP=1)" style=dashed
		gate_0 [label="Gate\nGPU 0\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_1 [label="Gate\nGPU 1\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_2 [label="Gate\nGPU 2\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_3 [label="Gate\nGPU 3\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_4 [label="Gate\nGPU 4\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_5 [label="Gate\nGPU 5\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_6 [label="Gate\nGPU 6\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		gate_7 [label="Gate\nGPU 7\nInput: [B, 1, 16384]\nOutput: [B, 1, 256]" fillcolor=lightpink shape=parallelogram style=filled]
		expert_0 [label="All Experts\nGPU 0\nExperts: [0:255]\nInput: [B, 8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_1 [label="All Experts\nGPU 1\nExperts: [0:255]\nInput: [B, 8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_2 [label="All Experts\nGPU 2\nExperts: [0:255]\nInput: [B, 8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_3 [label="All Experts\nGPU 3\nExperts: [0:255]\nInput: [B, 8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_4 [label="All Experts\nGPU 4\nExperts: [0:255]\nInput: [B, 8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_5 [label="All Experts\nGPU 5\nExperts: [0:255]\nInput: [B, 8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_6 [label="All Experts\nGPU 6\nExperts: [0:255]\nInput: [B, 8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		expert_7 [label="All Experts\nGPU 7\nExperts: [0:255]\nInput: [B, 8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightcyan shape=rectangle style=filled]
		ffn_0 [label="FFN\nGPU 0\nUP: [B, 8, 2048, 16384/8]\nGate: [B, 8, 2048, 16384/8]\nDown: [B, 8, 16384/8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_1 [label="FFN\nGPU 1\nUP: [B, 8, 2048, 16384/8]\nGate: [B, 8, 2048, 16384/8]\nDown: [B, 8, 16384/8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_2 [label="FFN\nGPU 2\nUP: [B, 8, 2048, 16384/8]\nGate: [B, 8, 2048, 16384/8]\nDown: [B, 8, 16384/8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_3 [label="FFN\nGPU 3\nUP: [B, 8, 2048, 16384/8]\nGate: [B, 8, 2048, 16384/8]\nDown: [B, 8, 16384/8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_4 [label="FFN\nGPU 4\nUP: [B, 8, 2048, 16384/8]\nGate: [B, 8, 2048, 16384/8]\nDown: [B, 8, 16384/8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_5 [label="FFN\nGPU 5\nUP: [B, 8, 2048, 16384/8]\nGate: [B, 8, 2048, 16384/8]\nDown: [B, 8, 16384/8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_6 [label="FFN\nGPU 6\nUP: [B, 8, 2048, 16384/8]\nGate: [B, 8, 2048, 16384/8]\nDown: [B, 8, 16384/8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_7 [label="FFN\nGPU 7\nUP: [B, 8, 2048, 16384/8]\nGate: [B, 8, 2048, 16384/8]\nDown: [B, 8, 16384/8, 2048]\nOutput: [B, 8, 2048]" fillcolor=lightsteelblue shape=rectangle style=filled]
		ffn_allreduce [label="TP All-Reduce\nTP=8\nInput: [B, 8, 2048]×8\nOutput: [B, 1, 16384]" fillcolor=gold shape=parallelogram style=filled]
	}
	output [label="Output\nInput: [B, 1, 16384]\nOutput: [B, 1, 16384]" fillcolor=lightgreen shape=ellipse style=filled]
	input -> qkv_0
	qkv_0 -> kv_cache_0
	kv_cache_0 -> attn_0
	attn_0 -> attn_allreduce
	attn_allreduce -> gate_0 [lhead=cluster_ffn]
	gate_0 -> expert_0
	expert_0 -> ffn_0
	ffn_0 -> ffn_allreduce
	input -> qkv_1
	qkv_1 -> kv_cache_1
	kv_cache_1 -> attn_1
	attn_1 -> attn_allreduce
	attn_allreduce -> gate_1 [lhead=cluster_ffn]
	gate_1 -> expert_1
	expert_1 -> ffn_1
	ffn_1 -> ffn_allreduce
	input -> qkv_2
	qkv_2 -> kv_cache_2
	kv_cache_2 -> attn_2
	attn_2 -> attn_allreduce
	attn_allreduce -> gate_2 [lhead=cluster_ffn]
	gate_2 -> expert_2
	expert_2 -> ffn_2
	ffn_2 -> ffn_allreduce
	input -> qkv_3
	qkv_3 -> kv_cache_3
	kv_cache_3 -> attn_3
	attn_3 -> attn_allreduce
	attn_allreduce -> gate_3 [lhead=cluster_ffn]
	gate_3 -> expert_3
	expert_3 -> ffn_3
	ffn_3 -> ffn_allreduce
	input -> qkv_4
	qkv_4 -> kv_cache_4
	kv_cache_4 -> attn_4
	attn_4 -> attn_allreduce
	attn_allreduce -> gate_4 [lhead=cluster_ffn]
	gate_4 -> expert_4
	expert_4 -> ffn_4
	ffn_4 -> ffn_allreduce
	input -> qkv_5
	qkv_5 -> kv_cache_5
	kv_cache_5 -> attn_5
	attn_5 -> attn_allreduce
	attn_allreduce -> gate_5 [lhead=cluster_ffn]
	gate_5 -> expert_5
	expert_5 -> ffn_5
	ffn_5 -> ffn_allreduce
	input -> qkv_6
	qkv_6 -> kv_cache_6
	kv_cache_6 -> attn_6
	attn_6 -> attn_allreduce
	attn_allreduce -> gate_6 [lhead=cluster_ffn]
	gate_6 -> expert_6
	expert_6 -> ffn_6
	ffn_6 -> ffn_allreduce
	input -> qkv_7
	qkv_7 -> kv_cache_7
	kv_cache_7 -> attn_7
	attn_7 -> attn_allreduce
	attn_allreduce -> gate_7 [lhead=cluster_ffn]
	gate_7 -> expert_7
	expert_7 -> ffn_7
	ffn_7 -> ffn_allreduce
	ffn_allreduce -> output
}
