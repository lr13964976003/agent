{
  "submission": {
    "models": {
      "helix_models": [
        {
          "name": "DeepSeek-R1-Helix",
          "description": "DeepSeek-R1 with Helix Parallelism (16 GPUs)",
          "parallel_strategy": "KV-Parallelism + TPÃ—EP",
          "configuration": {
            "attention_phase": {
              "type": "KV_Parallelism",
              "kvp_width": 16,
              "tpa_width": 1,
              "total_gpus": 16
            },
            "ffn_phase": {
              "type": "Expert_Parallelism",
              "tpf_width": 2,
              "ep_width": 8,
              "total_gpus": 16
            }
          },
          "files": {
            "dot": "../outputs/2025-10-29-11-58-32/deepseek_r1_helix.dot",
            "svg": "../outputs/2025-10-29-11-58-32/deepseek_r1_helix.svg"
          }
        },
        {
          "name": "Llama-405B-Helix",
          "description": "Llama-405B with Helix Parallelism (64 GPUs)",
          "parallel_strategy": "KV-Parallelism + Tensor Parallelism",
          "configuration": {
            "attention_phase": {
              "type": "KV_Parallelism",
              "kvp_width": 8,
              "tpa_width": 8,
              "total_gpus": 64
            },
            "ffn_phase": {
              "type": "Tensor_Parallelism",
              "tpf_width": 64,
              "total_gpus": 64
            }
          },
          "files": {
            "dot": "../outputs/2025-10-29-11-58-32/llama_405b_helix.dot",
            "svg": "../outputs/2025-10-29-11-58-32/llama_405b_helix.svg"
          }
        }
      ],
      "baseline_models": [
        {
          "name": "DeepSeek-R1-TP-Baseline",
          "description": "DeepSeek-R1 with traditional Tensor Parallelism (8 GPUs)",
          "parallel_strategy": "Pure Tensor Parallelism",
          "configuration": {
            "type": "Tensor_Parallelism",
            "tp_width": 8,
            "total_gpus": 8,
            "kv_duplicate": true
          },
          "files": {
            "dot": "../outputs/2025-10-29-11-58-32/deepseek_r1_baseline.dot",
            "svg": "../outputs/2025-10-29-11-58-32/deepseek_r1_baseline.svg"
          }
        },
        {
          "name": "Llama-405B-TP-Baseline",
          "description": "Llama-405B with traditional Tensor Parallelism (8 GPUs)",
          "parallel_strategy": "Pure Tensor Parallelism",
          "configuration": {
            "type": "Tensor_Parallelism",
            "tp_width": 8,
            "total_gpus": 8,
            "kv_duplicate": true
          },
          "files": {
            "dot": "../outputs/2025-10-29-11-58-32/llama_405b_baseline.dot",
            "svg": "../outputs/2025-10-29-11-58-32/llama_405b_baseline.svg"
          }
        }
      ]
    },
    "validation": {
      "all_dags_generated": true,
      "no_cycles_detected": true,
      "complete_input_output_flow": true,
      "all_dimensions_specified": true,
      "gpu_mappings_validated": true
    },
    "technical_summary": {
      "helix_innovation": "Temporal pipeline with KV-Parallelism + TP/EP reuse",
      "communication_patterns": {
        "helix": "HOP-B All-to-All with batch-wise overlap",
        "baseline": "Traditional TP All-Reduce"
      },
      "memory_optimization": {
        "helix": "KV cache sharding across sequence dimension",
        "baseline": "KV cache duplication across TP replicas"
      }
    }
  }
}