// 30B MoE Model 3D Parallel Deployment DAG
digraph {
	dpi=300 rankdir=TB size="30,40"
	node [fontname=Arial fontsize=10]
	node [fillcolor=lightblue shape=ellipse style=filled]
	node [fillcolor=lightgreen shape=rectangle style=filled]
	node [fillcolor=lightyellow shape=parallelogram style=filled]
	input_0 [label="Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=ellipse]
	input_0 -> layer_0_input
	layer_0_input -> layer_0_qkv_tp0
	layer_0_input -> layer_0_qkv_tp1
	layer_0_qkv_tp0 -> layer_0_attn_tp0
	layer_0_qkv_tp1 -> layer_0_attn_tp1
	layer_0_attn_tp0 -> layer_0_attn_comm
	layer_0_attn_tp1 -> layer_0_attn_comm
	layer_0_attn_comm -> layer_0_attn_out
	layer_0_attn_out -> layer_0_gate
	layer_0_gate -> layer_0_expert_select [style=dashed]
	layer_0_expert_select -> layer_0_experts_gpu0
	layer_0_expert_select -> layer_0_experts_gpu1
	layer_0_expert_select -> layer_0_experts_gpu2
	layer_0_expert_select -> layer_0_experts_gpu3
	layer_0_expert_select -> layer_0_experts_gpu4
	layer_0_expert_select -> layer_0_experts_gpu5
	layer_0_expert_select -> layer_0_experts_gpu6
	layer_0_expert_select -> layer_0_experts_gpu7
	layer_0_experts_gpu0 -> layer_0_expert_comm
	layer_0_experts_gpu1 -> layer_0_expert_comm
	layer_0_experts_gpu2 -> layer_0_expert_comm
	layer_0_experts_gpu3 -> layer_0_expert_comm
	layer_0_experts_gpu4 -> layer_0_expert_comm
	layer_0_experts_gpu5 -> layer_0_expert_comm
	layer_0_experts_gpu6 -> layer_0_expert_comm
	layer_0_experts_gpu7 -> layer_0_expert_comm
	layer_0_expert_comm -> layer_0_moe_out
	layer_0_moe_out -> layer_0_norm
	layer_0_norm -> layer_1_input
	layer_1_input -> layer_1_qkv_tp0
	layer_1_input -> layer_1_qkv_tp1
	layer_1_qkv_tp0 -> layer_1_attn_tp0
	layer_1_qkv_tp1 -> layer_1_attn_tp1
	layer_1_attn_tp0 -> layer_1_attn_comm
	layer_1_attn_tp1 -> layer_1_attn_comm
	layer_1_attn_comm -> layer_1_attn_out
	layer_1_attn_out -> layer_1_gate
	layer_1_gate -> layer_1_expert_select [style=dashed]
	layer_1_expert_select -> layer_1_experts_gpu0
	layer_1_expert_select -> layer_1_experts_gpu1
	layer_1_expert_select -> layer_1_experts_gpu2
	layer_1_expert_select -> layer_1_experts_gpu3
	layer_1_expert_select -> layer_1_experts_gpu4
	layer_1_expert_select -> layer_1_experts_gpu5
	layer_1_expert_select -> layer_1_experts_gpu6
	layer_1_expert_select -> layer_1_experts_gpu7
	layer_1_experts_gpu0 -> layer_1_expert_comm
	layer_1_experts_gpu1 -> layer_1_expert_comm
	layer_1_experts_gpu2 -> layer_1_expert_comm
	layer_1_experts_gpu3 -> layer_1_expert_comm
	layer_1_experts_gpu4 -> layer_1_expert_comm
	layer_1_experts_gpu5 -> layer_1_expert_comm
	layer_1_experts_gpu6 -> layer_1_expert_comm
	layer_1_experts_gpu7 -> layer_1_expert_comm
	layer_1_expert_comm -> layer_1_moe_out
	layer_1_moe_out -> layer_1_norm
	layer_1_norm -> layer_2_input
	layer_2_input -> layer_2_qkv_tp0
	layer_2_input -> layer_2_qkv_tp1
	layer_2_qkv_tp0 -> layer_2_attn_tp0
	layer_2_qkv_tp1 -> layer_2_attn_tp1
	layer_2_attn_tp0 -> layer_2_attn_comm
	layer_2_attn_tp1 -> layer_2_attn_comm
	layer_2_attn_comm -> layer_2_attn_out
	layer_2_attn_out -> layer_2_gate
	layer_2_gate -> layer_2_expert_select [style=dashed]
	layer_2_expert_select -> layer_2_experts_gpu0
	layer_2_expert_select -> layer_2_experts_gpu1
	layer_2_expert_select -> layer_2_experts_gpu2
	layer_2_expert_select -> layer_2_experts_gpu3
	layer_2_expert_select -> layer_2_experts_gpu4
	layer_2_expert_select -> layer_2_experts_gpu5
	layer_2_expert_select -> layer_2_experts_gpu6
	layer_2_expert_select -> layer_2_experts_gpu7
	layer_2_experts_gpu0 -> layer_2_expert_comm
	layer_2_experts_gpu1 -> layer_2_expert_comm
	layer_2_experts_gpu2 -> layer_2_expert_comm
	layer_2_experts_gpu3 -> layer_2_expert_comm
	layer_2_experts_gpu4 -> layer_2_expert_comm
	layer_2_experts_gpu5 -> layer_2_expert_comm
	layer_2_experts_gpu6 -> layer_2_expert_comm
	layer_2_experts_gpu7 -> layer_2_expert_comm
	layer_2_expert_comm -> layer_2_moe_out
	layer_2_moe_out -> layer_2_norm
	layer_2_norm -> layer_3_input
	layer_3_input -> layer_3_qkv_tp0
	layer_3_input -> layer_3_qkv_tp1
	layer_3_qkv_tp0 -> layer_3_attn_tp0
	layer_3_qkv_tp1 -> layer_3_attn_tp1
	layer_3_attn_tp0 -> layer_3_attn_comm
	layer_3_attn_tp1 -> layer_3_attn_comm
	layer_3_attn_comm -> layer_3_attn_out
	layer_3_attn_out -> layer_3_gate
	layer_3_gate -> layer_3_expert_select [style=dashed]
	layer_3_expert_select -> layer_3_experts_gpu0
	layer_3_expert_select -> layer_3_experts_gpu1
	layer_3_expert_select -> layer_3_experts_gpu2
	layer_3_expert_select -> layer_3_experts_gpu3
	layer_3_expert_select -> layer_3_experts_gpu4
	layer_3_expert_select -> layer_3_experts_gpu5
	layer_3_expert_select -> layer_3_experts_gpu6
	layer_3_expert_select -> layer_3_experts_gpu7
	layer_3_experts_gpu0 -> layer_3_expert_comm
	layer_3_experts_gpu1 -> layer_3_expert_comm
	layer_3_experts_gpu2 -> layer_3_expert_comm
	layer_3_experts_gpu3 -> layer_3_expert_comm
	layer_3_experts_gpu4 -> layer_3_expert_comm
	layer_3_experts_gpu5 -> layer_3_expert_comm
	layer_3_experts_gpu6 -> layer_3_expert_comm
	layer_3_experts_gpu7 -> layer_3_expert_comm
	layer_3_expert_comm -> layer_3_moe_out
	layer_3_moe_out -> layer_3_norm
	subgraph cluster_stage_0 {
		fillcolor=lightgray fontname="Arial Bold" label="Pipeline Stage 0 (GPUs 0-1)" style="rounded,filled"
		subgraph cluster_layer_0 {
			fillcolor=white fontname="Arial Bold" label="Layer 0" style="rounded,filled"
			layer_0_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_0_qkv_tp0 [label="QKV Projection TP0\nGPU 0\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_0_qkv_tp1 [label="QKV Projection TP1\nGPU 1\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_0_attn_tp0 [label="Attention Score TP0\nGPU 0\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_0_attn_tp1 [label="Attention Score TP1\nGPU 1\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_0_attn_comm [label="Attention Output All-Reduce\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_0_attn_out [label="Attention Output Projection\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_0_gate [label="MoE Gate Network\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_0_expert_select [label="Expert Selection\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_0_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_0_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_0_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_0_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_0_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_0_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_0_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_0_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_0_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_0_moe_out [label="MoE Output Aggregation\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_0_norm [label="Layer Normalization\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
		subgraph cluster_layer_1 {
			fillcolor=white fontname="Arial Bold" label="Layer 1" style="rounded,filled"
			layer_1_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_1_qkv_tp0 [label="QKV Projection TP0\nGPU 0\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_1_qkv_tp1 [label="QKV Projection TP1\nGPU 1\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_1_attn_tp0 [label="Attention Score TP0\nGPU 0\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_1_attn_tp1 [label="Attention Score TP1\nGPU 1\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_1_attn_comm [label="Attention Output All-Reduce\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_1_attn_out [label="Attention Output Projection\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_1_gate [label="MoE Gate Network\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_1_expert_select [label="Expert Selection\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_1_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_1_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_1_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_1_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_1_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_1_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_1_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_1_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_1_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_1_moe_out [label="MoE Output Aggregation\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_1_norm [label="Layer Normalization\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
		subgraph cluster_layer_2 {
			fillcolor=white fontname="Arial Bold" label="Layer 2" style="rounded,filled"
			layer_2_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_2_qkv_tp0 [label="QKV Projection TP0\nGPU 0\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_2_qkv_tp1 [label="QKV Projection TP1\nGPU 1\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_2_attn_tp0 [label="Attention Score TP0\nGPU 0\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_2_attn_tp1 [label="Attention Score TP1\nGPU 1\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_2_attn_comm [label="Attention Output All-Reduce\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_2_attn_out [label="Attention Output Projection\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_2_gate [label="MoE Gate Network\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_2_expert_select [label="Expert Selection\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_2_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_2_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_2_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_2_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_2_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_2_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_2_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_2_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_2_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_2_moe_out [label="MoE Output Aggregation\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_2_norm [label="Layer Normalization\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
		subgraph cluster_layer_3 {
			fillcolor=white fontname="Arial Bold" label="Layer 3" style="rounded,filled"
			layer_3_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_3_qkv_tp0 [label="QKV Projection TP0\nGPU 0\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_3_qkv_tp1 [label="QKV Projection TP1\nGPU 1\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_3_attn_tp0 [label="Attention Score TP0\nGPU 0\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_3_attn_tp1 [label="Attention Score TP1\nGPU 1\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_3_attn_comm [label="Attention Output All-Reduce\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_3_attn_out [label="Attention Output Projection\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_3_gate [label="MoE Gate Network\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_3_expert_select [label="Expert Selection\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_3_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_3_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_3_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_3_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_3_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_3_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_3_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_3_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_3_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_3_moe_out [label="MoE Output Aggregation\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_3_norm [label="Layer Normalization\nGPUs 0-1\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
	}
	layer_3_norm -> layer_4_input
	layer_4_input -> layer_4_qkv_tp0
	layer_4_input -> layer_4_qkv_tp1
	layer_4_qkv_tp0 -> layer_4_attn_tp0
	layer_4_qkv_tp1 -> layer_4_attn_tp1
	layer_4_attn_tp0 -> layer_4_attn_comm
	layer_4_attn_tp1 -> layer_4_attn_comm
	layer_4_attn_comm -> layer_4_attn_out
	layer_4_attn_out -> layer_4_gate
	layer_4_gate -> layer_4_expert_select [style=dashed]
	layer_4_expert_select -> layer_4_experts_gpu0
	layer_4_expert_select -> layer_4_experts_gpu1
	layer_4_expert_select -> layer_4_experts_gpu2
	layer_4_expert_select -> layer_4_experts_gpu3
	layer_4_expert_select -> layer_4_experts_gpu4
	layer_4_expert_select -> layer_4_experts_gpu5
	layer_4_expert_select -> layer_4_experts_gpu6
	layer_4_expert_select -> layer_4_experts_gpu7
	layer_4_experts_gpu0 -> layer_4_expert_comm
	layer_4_experts_gpu1 -> layer_4_expert_comm
	layer_4_experts_gpu2 -> layer_4_expert_comm
	layer_4_experts_gpu3 -> layer_4_expert_comm
	layer_4_experts_gpu4 -> layer_4_expert_comm
	layer_4_experts_gpu5 -> layer_4_expert_comm
	layer_4_experts_gpu6 -> layer_4_expert_comm
	layer_4_experts_gpu7 -> layer_4_expert_comm
	layer_4_expert_comm -> layer_4_moe_out
	layer_4_moe_out -> layer_4_norm
	layer_4_norm -> layer_5_input
	layer_5_input -> layer_5_qkv_tp0
	layer_5_input -> layer_5_qkv_tp1
	layer_5_qkv_tp0 -> layer_5_attn_tp0
	layer_5_qkv_tp1 -> layer_5_attn_tp1
	layer_5_attn_tp0 -> layer_5_attn_comm
	layer_5_attn_tp1 -> layer_5_attn_comm
	layer_5_attn_comm -> layer_5_attn_out
	layer_5_attn_out -> layer_5_gate
	layer_5_gate -> layer_5_expert_select [style=dashed]
	layer_5_expert_select -> layer_5_experts_gpu0
	layer_5_expert_select -> layer_5_experts_gpu1
	layer_5_expert_select -> layer_5_experts_gpu2
	layer_5_expert_select -> layer_5_experts_gpu3
	layer_5_expert_select -> layer_5_experts_gpu4
	layer_5_expert_select -> layer_5_experts_gpu5
	layer_5_expert_select -> layer_5_experts_gpu6
	layer_5_expert_select -> layer_5_experts_gpu7
	layer_5_experts_gpu0 -> layer_5_expert_comm
	layer_5_experts_gpu1 -> layer_5_expert_comm
	layer_5_experts_gpu2 -> layer_5_expert_comm
	layer_5_experts_gpu3 -> layer_5_expert_comm
	layer_5_experts_gpu4 -> layer_5_expert_comm
	layer_5_experts_gpu5 -> layer_5_expert_comm
	layer_5_experts_gpu6 -> layer_5_expert_comm
	layer_5_experts_gpu7 -> layer_5_expert_comm
	layer_5_expert_comm -> layer_5_moe_out
	layer_5_moe_out -> layer_5_norm
	layer_5_norm -> layer_6_input
	layer_6_input -> layer_6_qkv_tp0
	layer_6_input -> layer_6_qkv_tp1
	layer_6_qkv_tp0 -> layer_6_attn_tp0
	layer_6_qkv_tp1 -> layer_6_attn_tp1
	layer_6_attn_tp0 -> layer_6_attn_comm
	layer_6_attn_tp1 -> layer_6_attn_comm
	layer_6_attn_comm -> layer_6_attn_out
	layer_6_attn_out -> layer_6_gate
	layer_6_gate -> layer_6_expert_select [style=dashed]
	layer_6_expert_select -> layer_6_experts_gpu0
	layer_6_expert_select -> layer_6_experts_gpu1
	layer_6_expert_select -> layer_6_experts_gpu2
	layer_6_expert_select -> layer_6_experts_gpu3
	layer_6_expert_select -> layer_6_experts_gpu4
	layer_6_expert_select -> layer_6_experts_gpu5
	layer_6_expert_select -> layer_6_experts_gpu6
	layer_6_expert_select -> layer_6_experts_gpu7
	layer_6_experts_gpu0 -> layer_6_expert_comm
	layer_6_experts_gpu1 -> layer_6_expert_comm
	layer_6_experts_gpu2 -> layer_6_expert_comm
	layer_6_experts_gpu3 -> layer_6_expert_comm
	layer_6_experts_gpu4 -> layer_6_expert_comm
	layer_6_experts_gpu5 -> layer_6_expert_comm
	layer_6_experts_gpu6 -> layer_6_expert_comm
	layer_6_experts_gpu7 -> layer_6_expert_comm
	layer_6_expert_comm -> layer_6_moe_out
	layer_6_moe_out -> layer_6_norm
	layer_6_norm -> layer_7_input
	layer_7_input -> layer_7_qkv_tp0
	layer_7_input -> layer_7_qkv_tp1
	layer_7_qkv_tp0 -> layer_7_attn_tp0
	layer_7_qkv_tp1 -> layer_7_attn_tp1
	layer_7_attn_tp0 -> layer_7_attn_comm
	layer_7_attn_tp1 -> layer_7_attn_comm
	layer_7_attn_comm -> layer_7_attn_out
	layer_7_attn_out -> layer_7_gate
	layer_7_gate -> layer_7_expert_select [style=dashed]
	layer_7_expert_select -> layer_7_experts_gpu0
	layer_7_expert_select -> layer_7_experts_gpu1
	layer_7_expert_select -> layer_7_experts_gpu2
	layer_7_expert_select -> layer_7_experts_gpu3
	layer_7_expert_select -> layer_7_experts_gpu4
	layer_7_expert_select -> layer_7_experts_gpu5
	layer_7_expert_select -> layer_7_experts_gpu6
	layer_7_expert_select -> layer_7_experts_gpu7
	layer_7_experts_gpu0 -> layer_7_expert_comm
	layer_7_experts_gpu1 -> layer_7_expert_comm
	layer_7_experts_gpu2 -> layer_7_expert_comm
	layer_7_experts_gpu3 -> layer_7_expert_comm
	layer_7_experts_gpu4 -> layer_7_expert_comm
	layer_7_experts_gpu5 -> layer_7_expert_comm
	layer_7_experts_gpu6 -> layer_7_expert_comm
	layer_7_experts_gpu7 -> layer_7_expert_comm
	layer_7_expert_comm -> layer_7_moe_out
	layer_7_moe_out -> layer_7_norm
	subgraph cluster_stage_1 {
		fillcolor=lightgray fontname="Arial Bold" label="Pipeline Stage 1 (GPUs 2-3)" style="rounded,filled"
		subgraph cluster_layer_4 {
			fillcolor=white fontname="Arial Bold" label="Layer 4" style="rounded,filled"
			layer_4_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_4_qkv_tp0 [label="QKV Projection TP0\nGPU 2\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_4_qkv_tp1 [label="QKV Projection TP1\nGPU 3\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_4_attn_tp0 [label="Attention Score TP0\nGPU 2\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_4_attn_tp1 [label="Attention Score TP1\nGPU 3\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_4_attn_comm [label="Attention Output All-Reduce\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_4_attn_out [label="Attention Output Projection\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_4_gate [label="MoE Gate Network\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_4_expert_select [label="Expert Selection\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_4_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_4_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_4_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_4_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_4_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_4_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_4_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_4_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_4_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_4_moe_out [label="MoE Output Aggregation\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_4_norm [label="Layer Normalization\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
		subgraph cluster_layer_5 {
			fillcolor=white fontname="Arial Bold" label="Layer 5" style="rounded,filled"
			layer_5_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_5_qkv_tp0 [label="QKV Projection TP0\nGPU 2\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_5_qkv_tp1 [label="QKV Projection TP1\nGPU 3\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_5_attn_tp0 [label="Attention Score TP0\nGPU 2\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_5_attn_tp1 [label="Attention Score TP1\nGPU 3\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_5_attn_comm [label="Attention Output All-Reduce\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_5_attn_out [label="Attention Output Projection\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_5_gate [label="MoE Gate Network\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_5_expert_select [label="Expert Selection\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_5_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_5_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_5_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_5_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_5_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_5_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_5_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_5_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_5_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_5_moe_out [label="MoE Output Aggregation\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_5_norm [label="Layer Normalization\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
		subgraph cluster_layer_6 {
			fillcolor=white fontname="Arial Bold" label="Layer 6" style="rounded,filled"
			layer_6_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_6_qkv_tp0 [label="QKV Projection TP0\nGPU 2\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_6_qkv_tp1 [label="QKV Projection TP1\nGPU 3\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_6_attn_tp0 [label="Attention Score TP0\nGPU 2\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_6_attn_tp1 [label="Attention Score TP1\nGPU 3\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_6_attn_comm [label="Attention Output All-Reduce\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_6_attn_out [label="Attention Output Projection\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_6_gate [label="MoE Gate Network\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_6_expert_select [label="Expert Selection\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_6_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_6_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_6_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_6_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_6_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_6_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_6_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_6_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_6_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_6_moe_out [label="MoE Output Aggregation\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_6_norm [label="Layer Normalization\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
		subgraph cluster_layer_7 {
			fillcolor=white fontname="Arial Bold" label="Layer 7" style="rounded,filled"
			layer_7_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_7_qkv_tp0 [label="QKV Projection TP0\nGPU 2\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_7_qkv_tp1 [label="QKV Projection TP1\nGPU 3\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_7_attn_tp0 [label="Attention Score TP0\nGPU 2\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_7_attn_tp1 [label="Attention Score TP1\nGPU 3\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_7_attn_comm [label="Attention Output All-Reduce\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_7_attn_out [label="Attention Output Projection\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_7_gate [label="MoE Gate Network\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_7_expert_select [label="Expert Selection\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_7_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_7_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_7_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_7_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_7_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_7_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_7_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_7_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_7_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_7_moe_out [label="MoE Output Aggregation\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_7_norm [label="Layer Normalization\nGPUs 2-3\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
	}
	layer_7_norm -> layer_8_input
	layer_8_input -> layer_8_qkv_tp0
	layer_8_input -> layer_8_qkv_tp1
	layer_8_qkv_tp0 -> layer_8_attn_tp0
	layer_8_qkv_tp1 -> layer_8_attn_tp1
	layer_8_attn_tp0 -> layer_8_attn_comm
	layer_8_attn_tp1 -> layer_8_attn_comm
	layer_8_attn_comm -> layer_8_attn_out
	layer_8_attn_out -> layer_8_gate
	layer_8_gate -> layer_8_expert_select [style=dashed]
	layer_8_expert_select -> layer_8_experts_gpu0
	layer_8_expert_select -> layer_8_experts_gpu1
	layer_8_expert_select -> layer_8_experts_gpu2
	layer_8_expert_select -> layer_8_experts_gpu3
	layer_8_expert_select -> layer_8_experts_gpu4
	layer_8_expert_select -> layer_8_experts_gpu5
	layer_8_expert_select -> layer_8_experts_gpu6
	layer_8_expert_select -> layer_8_experts_gpu7
	layer_8_experts_gpu0 -> layer_8_expert_comm
	layer_8_experts_gpu1 -> layer_8_expert_comm
	layer_8_experts_gpu2 -> layer_8_expert_comm
	layer_8_experts_gpu3 -> layer_8_expert_comm
	layer_8_experts_gpu4 -> layer_8_expert_comm
	layer_8_experts_gpu5 -> layer_8_expert_comm
	layer_8_experts_gpu6 -> layer_8_expert_comm
	layer_8_experts_gpu7 -> layer_8_expert_comm
	layer_8_expert_comm -> layer_8_moe_out
	layer_8_moe_out -> layer_8_norm
	layer_8_norm -> layer_9_input
	layer_9_input -> layer_9_qkv_tp0
	layer_9_input -> layer_9_qkv_tp1
	layer_9_qkv_tp0 -> layer_9_attn_tp0
	layer_9_qkv_tp1 -> layer_9_attn_tp1
	layer_9_attn_tp0 -> layer_9_attn_comm
	layer_9_attn_tp1 -> layer_9_attn_comm
	layer_9_attn_comm -> layer_9_attn_out
	layer_9_attn_out -> layer_9_gate
	layer_9_gate -> layer_9_expert_select [style=dashed]
	layer_9_expert_select -> layer_9_experts_gpu0
	layer_9_expert_select -> layer_9_experts_gpu1
	layer_9_expert_select -> layer_9_experts_gpu2
	layer_9_expert_select -> layer_9_experts_gpu3
	layer_9_expert_select -> layer_9_experts_gpu4
	layer_9_expert_select -> layer_9_experts_gpu5
	layer_9_expert_select -> layer_9_experts_gpu6
	layer_9_expert_select -> layer_9_experts_gpu7
	layer_9_experts_gpu0 -> layer_9_expert_comm
	layer_9_experts_gpu1 -> layer_9_expert_comm
	layer_9_experts_gpu2 -> layer_9_expert_comm
	layer_9_experts_gpu3 -> layer_9_expert_comm
	layer_9_experts_gpu4 -> layer_9_expert_comm
	layer_9_experts_gpu5 -> layer_9_expert_comm
	layer_9_experts_gpu6 -> layer_9_expert_comm
	layer_9_experts_gpu7 -> layer_9_expert_comm
	layer_9_expert_comm -> layer_9_moe_out
	layer_9_moe_out -> layer_9_norm
	layer_9_norm -> layer_10_input
	layer_10_input -> layer_10_qkv_tp0
	layer_10_input -> layer_10_qkv_tp1
	layer_10_qkv_tp0 -> layer_10_attn_tp0
	layer_10_qkv_tp1 -> layer_10_attn_tp1
	layer_10_attn_tp0 -> layer_10_attn_comm
	layer_10_attn_tp1 -> layer_10_attn_comm
	layer_10_attn_comm -> layer_10_attn_out
	layer_10_attn_out -> layer_10_gate
	layer_10_gate -> layer_10_expert_select [style=dashed]
	layer_10_expert_select -> layer_10_experts_gpu0
	layer_10_expert_select -> layer_10_experts_gpu1
	layer_10_expert_select -> layer_10_experts_gpu2
	layer_10_expert_select -> layer_10_experts_gpu3
	layer_10_expert_select -> layer_10_experts_gpu4
	layer_10_expert_select -> layer_10_experts_gpu5
	layer_10_expert_select -> layer_10_experts_gpu6
	layer_10_expert_select -> layer_10_experts_gpu7
	layer_10_experts_gpu0 -> layer_10_expert_comm
	layer_10_experts_gpu1 -> layer_10_expert_comm
	layer_10_experts_gpu2 -> layer_10_expert_comm
	layer_10_experts_gpu3 -> layer_10_expert_comm
	layer_10_experts_gpu4 -> layer_10_expert_comm
	layer_10_experts_gpu5 -> layer_10_expert_comm
	layer_10_experts_gpu6 -> layer_10_expert_comm
	layer_10_experts_gpu7 -> layer_10_expert_comm
	layer_10_expert_comm -> layer_10_moe_out
	layer_10_moe_out -> layer_10_norm
	layer_10_norm -> layer_11_input
	layer_11_input -> layer_11_qkv_tp0
	layer_11_input -> layer_11_qkv_tp1
	layer_11_qkv_tp0 -> layer_11_attn_tp0
	layer_11_qkv_tp1 -> layer_11_attn_tp1
	layer_11_attn_tp0 -> layer_11_attn_comm
	layer_11_attn_tp1 -> layer_11_attn_comm
	layer_11_attn_comm -> layer_11_attn_out
	layer_11_attn_out -> layer_11_gate
	layer_11_gate -> layer_11_expert_select [style=dashed]
	layer_11_expert_select -> layer_11_experts_gpu0
	layer_11_expert_select -> layer_11_experts_gpu1
	layer_11_expert_select -> layer_11_experts_gpu2
	layer_11_expert_select -> layer_11_experts_gpu3
	layer_11_expert_select -> layer_11_experts_gpu4
	layer_11_expert_select -> layer_11_experts_gpu5
	layer_11_expert_select -> layer_11_experts_gpu6
	layer_11_expert_select -> layer_11_experts_gpu7
	layer_11_experts_gpu0 -> layer_11_expert_comm
	layer_11_experts_gpu1 -> layer_11_expert_comm
	layer_11_experts_gpu2 -> layer_11_expert_comm
	layer_11_experts_gpu3 -> layer_11_expert_comm
	layer_11_experts_gpu4 -> layer_11_expert_comm
	layer_11_experts_gpu5 -> layer_11_expert_comm
	layer_11_experts_gpu6 -> layer_11_expert_comm
	layer_11_experts_gpu7 -> layer_11_expert_comm
	layer_11_expert_comm -> layer_11_moe_out
	layer_11_moe_out -> layer_11_norm
	subgraph cluster_stage_2 {
		fillcolor=lightgray fontname="Arial Bold" label="Pipeline Stage 2 (GPUs 4-5)" style="rounded,filled"
		subgraph cluster_layer_8 {
			fillcolor=white fontname="Arial Bold" label="Layer 8" style="rounded,filled"
			layer_8_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_8_qkv_tp0 [label="QKV Projection TP0\nGPU 4\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_8_qkv_tp1 [label="QKV Projection TP1\nGPU 5\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_8_attn_tp0 [label="Attention Score TP0\nGPU 4\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_8_attn_tp1 [label="Attention Score TP1\nGPU 5\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_8_attn_comm [label="Attention Output All-Reduce\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_8_attn_out [label="Attention Output Projection\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_8_gate [label="MoE Gate Network\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_8_expert_select [label="Expert Selection\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_8_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_8_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_8_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_8_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_8_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_8_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_8_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_8_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_8_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_8_moe_out [label="MoE Output Aggregation\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_8_norm [label="Layer Normalization\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
		subgraph cluster_layer_9 {
			fillcolor=white fontname="Arial Bold" label="Layer 9" style="rounded,filled"
			layer_9_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_9_qkv_tp0 [label="QKV Projection TP0\nGPU 4\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_9_qkv_tp1 [label="QKV Projection TP1\nGPU 5\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_9_attn_tp0 [label="Attention Score TP0\nGPU 4\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_9_attn_tp1 [label="Attention Score TP1\nGPU 5\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_9_attn_comm [label="Attention Output All-Reduce\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_9_attn_out [label="Attention Output Projection\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_9_gate [label="MoE Gate Network\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_9_expert_select [label="Expert Selection\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_9_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_9_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_9_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_9_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_9_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_9_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_9_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_9_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_9_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_9_moe_out [label="MoE Output Aggregation\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_9_norm [label="Layer Normalization\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
		subgraph cluster_layer_10 {
			fillcolor=white fontname="Arial Bold" label="Layer 10" style="rounded,filled"
			layer_10_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_10_qkv_tp0 [label="QKV Projection TP0\nGPU 4\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_10_qkv_tp1 [label="QKV Projection TP1\nGPU 5\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_10_attn_tp0 [label="Attention Score TP0\nGPU 4\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_10_attn_tp1 [label="Attention Score TP1\nGPU 5\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_10_attn_comm [label="Attention Output All-Reduce\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_10_attn_out [label="Attention Output Projection\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_10_gate [label="MoE Gate Network\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_10_expert_select [label="Expert Selection\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_10_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_10_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_10_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_10_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_10_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_10_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_10_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_10_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_10_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_10_moe_out [label="MoE Output Aggregation\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_10_norm [label="Layer Normalization\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
		subgraph cluster_layer_11 {
			fillcolor=white fontname="Arial Bold" label="Layer 11" style="rounded,filled"
			layer_11_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_11_qkv_tp0 [label="QKV Projection TP0\nGPU 4\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_11_qkv_tp1 [label="QKV Projection TP1\nGPU 5\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_11_attn_tp0 [label="Attention Score TP0\nGPU 4\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_11_attn_tp1 [label="Attention Score TP1\nGPU 5\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_11_attn_comm [label="Attention Output All-Reduce\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_11_attn_out [label="Attention Output Projection\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_11_gate [label="MoE Gate Network\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_11_expert_select [label="Expert Selection\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_11_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_11_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_11_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_11_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_11_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_11_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_11_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_11_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_11_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_11_moe_out [label="MoE Output Aggregation\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_11_norm [label="Layer Normalization\nGPUs 4-5\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
	}
	layer_11_norm -> layer_12_input
	layer_12_input -> layer_12_qkv_tp0
	layer_12_input -> layer_12_qkv_tp1
	layer_12_qkv_tp0 -> layer_12_attn_tp0
	layer_12_qkv_tp1 -> layer_12_attn_tp1
	layer_12_attn_tp0 -> layer_12_attn_comm
	layer_12_attn_tp1 -> layer_12_attn_comm
	layer_12_attn_comm -> layer_12_attn_out
	layer_12_attn_out -> layer_12_gate
	layer_12_gate -> layer_12_expert_select [style=dashed]
	layer_12_expert_select -> layer_12_experts_gpu0
	layer_12_expert_select -> layer_12_experts_gpu1
	layer_12_expert_select -> layer_12_experts_gpu2
	layer_12_expert_select -> layer_12_experts_gpu3
	layer_12_expert_select -> layer_12_experts_gpu4
	layer_12_expert_select -> layer_12_experts_gpu5
	layer_12_expert_select -> layer_12_experts_gpu6
	layer_12_expert_select -> layer_12_experts_gpu7
	layer_12_experts_gpu0 -> layer_12_expert_comm
	layer_12_experts_gpu1 -> layer_12_expert_comm
	layer_12_experts_gpu2 -> layer_12_expert_comm
	layer_12_experts_gpu3 -> layer_12_expert_comm
	layer_12_experts_gpu4 -> layer_12_expert_comm
	layer_12_experts_gpu5 -> layer_12_expert_comm
	layer_12_experts_gpu6 -> layer_12_expert_comm
	layer_12_experts_gpu7 -> layer_12_expert_comm
	layer_12_expert_comm -> layer_12_moe_out
	layer_12_moe_out -> layer_12_norm
	layer_12_norm -> layer_13_input
	layer_13_input -> layer_13_qkv_tp0
	layer_13_input -> layer_13_qkv_tp1
	layer_13_qkv_tp0 -> layer_13_attn_tp0
	layer_13_qkv_tp1 -> layer_13_attn_tp1
	layer_13_attn_tp0 -> layer_13_attn_comm
	layer_13_attn_tp1 -> layer_13_attn_comm
	layer_13_attn_comm -> layer_13_attn_out
	layer_13_attn_out -> layer_13_gate
	layer_13_gate -> layer_13_expert_select [style=dashed]
	layer_13_expert_select -> layer_13_experts_gpu0
	layer_13_expert_select -> layer_13_experts_gpu1
	layer_13_expert_select -> layer_13_experts_gpu2
	layer_13_expert_select -> layer_13_experts_gpu3
	layer_13_expert_select -> layer_13_experts_gpu4
	layer_13_expert_select -> layer_13_experts_gpu5
	layer_13_expert_select -> layer_13_experts_gpu6
	layer_13_expert_select -> layer_13_experts_gpu7
	layer_13_experts_gpu0 -> layer_13_expert_comm
	layer_13_experts_gpu1 -> layer_13_expert_comm
	layer_13_experts_gpu2 -> layer_13_expert_comm
	layer_13_experts_gpu3 -> layer_13_expert_comm
	layer_13_experts_gpu4 -> layer_13_expert_comm
	layer_13_experts_gpu5 -> layer_13_expert_comm
	layer_13_experts_gpu6 -> layer_13_expert_comm
	layer_13_experts_gpu7 -> layer_13_expert_comm
	layer_13_expert_comm -> layer_13_moe_out
	layer_13_moe_out -> layer_13_norm
	layer_13_norm -> layer_14_input
	layer_14_input -> layer_14_qkv_tp0
	layer_14_input -> layer_14_qkv_tp1
	layer_14_qkv_tp0 -> layer_14_attn_tp0
	layer_14_qkv_tp1 -> layer_14_attn_tp1
	layer_14_attn_tp0 -> layer_14_attn_comm
	layer_14_attn_tp1 -> layer_14_attn_comm
	layer_14_attn_comm -> layer_14_attn_out
	layer_14_attn_out -> layer_14_gate
	layer_14_gate -> layer_14_expert_select [style=dashed]
	layer_14_expert_select -> layer_14_experts_gpu0
	layer_14_expert_select -> layer_14_experts_gpu1
	layer_14_expert_select -> layer_14_experts_gpu2
	layer_14_expert_select -> layer_14_experts_gpu3
	layer_14_expert_select -> layer_14_experts_gpu4
	layer_14_expert_select -> layer_14_experts_gpu5
	layer_14_expert_select -> layer_14_experts_gpu6
	layer_14_expert_select -> layer_14_experts_gpu7
	layer_14_experts_gpu0 -> layer_14_expert_comm
	layer_14_experts_gpu1 -> layer_14_expert_comm
	layer_14_experts_gpu2 -> layer_14_expert_comm
	layer_14_experts_gpu3 -> layer_14_expert_comm
	layer_14_experts_gpu4 -> layer_14_expert_comm
	layer_14_experts_gpu5 -> layer_14_expert_comm
	layer_14_experts_gpu6 -> layer_14_expert_comm
	layer_14_experts_gpu7 -> layer_14_expert_comm
	layer_14_expert_comm -> layer_14_moe_out
	layer_14_moe_out -> layer_14_norm
	layer_14_norm -> layer_15_input
	layer_15_input -> layer_15_qkv_tp0
	layer_15_input -> layer_15_qkv_tp1
	layer_15_qkv_tp0 -> layer_15_attn_tp0
	layer_15_qkv_tp1 -> layer_15_attn_tp1
	layer_15_attn_tp0 -> layer_15_attn_comm
	layer_15_attn_tp1 -> layer_15_attn_comm
	layer_15_attn_comm -> layer_15_attn_out
	layer_15_attn_out -> layer_15_gate
	layer_15_gate -> layer_15_expert_select [style=dashed]
	layer_15_expert_select -> layer_15_experts_gpu0
	layer_15_expert_select -> layer_15_experts_gpu1
	layer_15_expert_select -> layer_15_experts_gpu2
	layer_15_expert_select -> layer_15_experts_gpu3
	layer_15_expert_select -> layer_15_experts_gpu4
	layer_15_expert_select -> layer_15_experts_gpu5
	layer_15_expert_select -> layer_15_experts_gpu6
	layer_15_expert_select -> layer_15_experts_gpu7
	layer_15_experts_gpu0 -> layer_15_expert_comm
	layer_15_experts_gpu1 -> layer_15_expert_comm
	layer_15_experts_gpu2 -> layer_15_expert_comm
	layer_15_experts_gpu3 -> layer_15_expert_comm
	layer_15_experts_gpu4 -> layer_15_expert_comm
	layer_15_experts_gpu5 -> layer_15_expert_comm
	layer_15_experts_gpu6 -> layer_15_expert_comm
	layer_15_experts_gpu7 -> layer_15_expert_comm
	layer_15_expert_comm -> layer_15_moe_out
	layer_15_moe_out -> layer_15_norm
	subgraph cluster_stage_3 {
		fillcolor=lightgray fontname="Arial Bold" label="Pipeline Stage 3 (GPUs 6-7)" style="rounded,filled"
		subgraph cluster_layer_12 {
			fillcolor=white fontname="Arial Bold" label="Layer 12" style="rounded,filled"
			layer_12_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_12_qkv_tp0 [label="QKV Projection TP0\nGPU 6\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_12_qkv_tp1 [label="QKV Projection TP1\nGPU 7\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_12_attn_tp0 [label="Attention Score TP0\nGPU 6\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_12_attn_tp1 [label="Attention Score TP1\nGPU 7\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_12_attn_comm [label="Attention Output All-Reduce\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_12_attn_out [label="Attention Output Projection\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_12_gate [label="MoE Gate Network\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_12_expert_select [label="Expert Selection\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_12_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_12_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_12_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_12_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_12_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_12_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_12_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_12_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_12_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_12_moe_out [label="MoE Output Aggregation\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_12_norm [label="Layer Normalization\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
		subgraph cluster_layer_13 {
			fillcolor=white fontname="Arial Bold" label="Layer 13" style="rounded,filled"
			layer_13_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_13_qkv_tp0 [label="QKV Projection TP0\nGPU 6\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_13_qkv_tp1 [label="QKV Projection TP1\nGPU 7\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_13_attn_tp0 [label="Attention Score TP0\nGPU 6\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_13_attn_tp1 [label="Attention Score TP1\nGPU 7\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_13_attn_comm [label="Attention Output All-Reduce\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_13_attn_out [label="Attention Output Projection\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_13_gate [label="MoE Gate Network\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_13_expert_select [label="Expert Selection\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_13_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_13_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_13_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_13_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_13_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_13_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_13_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_13_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_13_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_13_moe_out [label="MoE Output Aggregation\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_13_norm [label="Layer Normalization\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
		subgraph cluster_layer_14 {
			fillcolor=white fontname="Arial Bold" label="Layer 14" style="rounded,filled"
			layer_14_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_14_qkv_tp0 [label="QKV Projection TP0\nGPU 6\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_14_qkv_tp1 [label="QKV Projection TP1\nGPU 7\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_14_attn_tp0 [label="Attention Score TP0\nGPU 6\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_14_attn_tp1 [label="Attention Score TP1\nGPU 7\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_14_attn_comm [label="Attention Output All-Reduce\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_14_attn_out [label="Attention Output Projection\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_14_gate [label="MoE Gate Network\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_14_expert_select [label="Expert Selection\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_14_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_14_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_14_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_14_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_14_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_14_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_14_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_14_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_14_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_14_moe_out [label="MoE Output Aggregation\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_14_norm [label="Layer Normalization\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
		subgraph cluster_layer_15 {
			fillcolor=white fontname="Arial Bold" label="Layer 15" style="rounded,filled"
			layer_15_input [label="Layer Input\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_15_qkv_tp0 [label="QKV Projection TP0\nGPU 6\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_15_qkv_tp1 [label="QKV Projection TP1\nGPU 7\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, 16 heads, 64 d_k]" fillcolor=lightgreen shape=rectangle]
			layer_15_attn_tp0 [label="Attention Score TP0\nGPU 6\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_15_attn_tp1 [label="Attention Score TP1\nGPU 7\nInput: [batch_size=128, seq_len=1024, 8 heads, 64 d_k]\nOutput: [batch_size=128, seq_len=1024, 8 heads, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_15_attn_comm [label="Attention Output All-Reduce\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightblue shape=ellipse]
			layer_15_attn_out [label="Attention Output Projection\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
			layer_15_gate [label="MoE Gate Network\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 64 experts]" fillcolor=lightyellow shape=parallelogram]
			layer_15_expert_select [label="Expert Selection\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 64 experts]\nOutput: [batch_size=128, seq_len=1024, top-2 experts]" fillcolor=lightyellow shape=parallelogram style=dashed]
			layer_15_experts_gpu0 [label="Experts 0-7\nGPU 0\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_15_experts_gpu1 [label="Experts 8-15\nGPU 1\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_15_experts_gpu2 [label="Experts 16-23\nGPU 2\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_15_experts_gpu3 [label="Experts 24-31\nGPU 3\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_15_experts_gpu4 [label="Experts 32-39\nGPU 4\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_15_experts_gpu5 [label="Experts 40-47\nGPU 5\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_15_experts_gpu6 [label="Experts 48-55\nGPU 6\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_15_experts_gpu7 [label="Experts 56-63\nGPU 7\nInput: [batch_size=16, seq_len=1024, 1024]\nOutput: [batch_size=16, seq_len=1024, 2048]" fillcolor=lightgreen shape=rectangle]
			layer_15_expert_comm [label="Expert All-to-All Communication\nAll GPUs\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 2048]" fillcolor=lightblue shape=ellipse]
			layer_15_moe_out [label="MoE Output Aggregation\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 2048]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightyellow shape=parallelogram]
			layer_15_norm [label="Layer Normalization\nGPUs 6-7\nInput: [batch_size=128, seq_len=1024, 1024]\nOutput: [batch_size=128, seq_len=1024, 1024]" fillcolor=lightgreen shape=rectangle]
		}
	}
	output_1 [label="Final Output\nInput: [batch_size=128, seq_len=1024, hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightblue shape=ellipse]
	layer_15_norm -> output_1
}
