{
  "dag_files": {
    "dot_file": "../outputs/2025-12-05-16-24-16/llm_30b_moe_deployment_dag.dot",
    "svg_file": "../outputs/2025-12-05-16-24-16/llm_30b_moe_deployment_dag.svg"
  },
  "verification": {
    "total_layers": 16,
    "total_gpus": 8,
    "experts_per_gpu": 8,
    "pipeline_stages": 4,
    "tensor_parallel_size": 2,
    "is_acyclic": true,
    "expert_parallelism": "64 experts distributed across 8 GPUs",
    "tensor_parallelism": "Attention and MoE layers split across 2 GPUs",
    "pipeline_parallelism": "16 layers distributed across 4 pipeline stages"
  }
}