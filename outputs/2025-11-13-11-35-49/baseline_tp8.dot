digraph Llama3_405B_Baseline_TP8 {
    rankdir=TB;
    node [shape=rectangle, style=filled, fillcolor=lightblue];
    
    // Input layer
    input [label="Model Input\nInput: [batch_size=1, seq_len=128000, d_model=16384]", 
           shape=parallelogram, fillcolor=lightgreen];
    
    // Embedding layer
    embedding [label="Token Embedding\nInput: [batch_size=1, seq_len=128000]\nOutput: [batch_size=1, seq_len=128000, d_model=16384]", 
               fillcolor=lightyellow];
    input -> embedding;
    
    // Layer normalization
    layer_norm_0 [label="LayerNorm\nInput: [batch_size=1, seq_len=128000, d_model=16384]\nOutput: [batch_size=1, seq_len=128000, d_model=16384]", 
                  fillcolor=pink];
    embedding -> layer_norm_0;
    
    // Attention Layer 1
    subgraph cluster_attention_layer_1 {
        label = "Attention Layer 1 (TP8 across GPUs 0-7)";
        style=dashed;
        
        // Query projection
        q_proj_1 [label="Query Projection\nInput: [batch_size=1, seq_len=128000, d_model=16384]\nOutput: [batch_size=1, seq_len=128000, heads=128, d_head=128]\nGPU: all GPUs 0-7", 
                  fillcolor=lightblue];
        
        // Key projection
        k_proj_1 [label="Key Projection\nInput: [batch_size=1, seq_len=128000, d_model=16384]\nOutput: [batch_size=1, seq_len=128000, kv_heads=8, d_head=128]\nGPU: all GPUs 0-7", 
                  fillcolor=lightblue];
        
        // Value projection
        v_proj_1 [label="Value Projection\nInput: [batch_size=1, seq_len=128000, d_model=16384]\nOutput: [batch_size=1, seq_len=128000, kv_heads=8, d_head=128]\nGPU: all GPUs 0-7", 
                  fillcolor=lightblue];
        
        // Multi-head attention
        attention_1 [label="Multi-Head Attention\nInput Q: [batch_size=1, seq_len=128000, heads=128, d_head=128]\nInput K/V: [batch_size=1, seq_len=128000, kv_heads=8, d_head=128]\nOutput: [batch_size=1, seq_len=128000, heads=128, d_head=128]\nGPU: all GPUs 0-7", 
                     fillcolor=orange];
        
        // Output projection
        o_proj_1 [label="Output Projection\nInput: [batch_size=1, seq_len=128000, heads=128, d_head=128]\nOutput: [batch_size=1, seq_len=128000, d_model=16384]\nGPU: all GPUs 0-7", 
                  fillcolor=lightblue];
        
        // Residual connections
        residual_add_1 [label="Residual Add\nInput: [batch_size=1, seq_len=128000, d_model=16384] x 2\nOutput: [batch_size=1, seq_len=128000, d_model=16384]", 
                       fillcolor=purple];
        
        // Layer norm after attention
        attn_layer_norm_1 [label="Post-Attention LayerNorm\nInput: [batch_size=1, seq_len=128000, d_model=16384]\nOutput: [batch_size=1, seq_len=128000, d_model=16384]", 
                           fillcolor=pink];
        
        // MLP Layer 1
        gate_proj_1 [label="Gate Projection\nInput: [batch_size=1, seq_len=128000, d_model=16384]\nOutput: [batch_size=1, seq_len=128000, ffn_dim=53248]\nGPU: all GPUs 0-7", 
                     fillcolor=lightblue];
        
        up_proj_1 [label="Up Projection\nInput: [batch_size=1, seq_len=128000, d_model=16384]\nOutput: [batch_size=1, seq_len=128000, ffn_dim=53248]\nGPU: all GPUs 0-7", 
                   fillcolor=lightblue];
        
        silu_1 [label="SiLU Activation\nInput: [batch_size=1, seq_len=128000, ffn_dim=53248]\nOutput: [batch_size=1, seq_len=128000, ffn_dim=53248]", 
                fillcolor=yellow];
        
        elem_mul_1 [label="Element-wise Multiply\nInput: [batch_size=1, seq_len=128000, ffn_dim=53248] x 2\nOutput: [batch_size=1, seq_len=128000, ffn_dim=53248]", 
                     fillcolor=yellow];
        
        down_proj_1 [label="Down Projection\nInput: [batch_size=1, seq_len=128000, ffn_dim=53248]\nOutput: [batch_size=1, seq_len=128000, d_model=16384]\nGPU: all GPUs 0-7", 
                     fillcolor=lightblue];
        
        mlp_residual_1 [label="MLP Residual Add\nInput: [batch_size=1, seq_len=128000, d_model=16384] x 2\nOutput: [batch_size=1, seq_len=128000, d_model=16384]", 
                        fillcolor=purple];
    }
    
    // Connect attention layer 1
    layer_norm_0 -> q_proj_1;
    layer_norm_0 -> k_proj_1;
    layer_norm_0 -> v_proj_1;
    
    q_proj_1 -> attention_1;
    k_proj_1 -> attention_1;
    v_proj_1 -> attention_1;
    attention_1 -> o_proj_1;
    o_proj_1 -> residual_add_1;
    layer_norm_0 -> residual_add_1;
    
    residual_add_1 -> attn_layer_norm_1;
    attn_layer_norm_1 -> gate_proj_1;
    attn_layer_norm_1 -> up_proj_1;
    
    gate_proj_1 -> silu_1;
    up_proj_1 -> elem_mul_1;
    silu_1 -> elem_mul_1;
    elem_mul_1 -> down_proj_1;
    down_proj_1 -> mlp_residual_1;
    residual_add_1 -> mlp_residual_1;
    
    // Layer 126 (using similar structure)
    layer_126_input [label="Layer 126 Input\nInput: [batch_size=1, seq_len=128000, d_model=16384]", style=invis];
    mlp_residual_1 -> layer_126_input [style=dashed];
    
    layer_126_output [label="Layer 126 Output\nOutput: [batch_size=1, seq_len=128000, d_model=16384]", style=invis];
    layer_126_input -> layer_126_output [style=dashed, label="... 124 layers skipped ..."];
    
    // Final layers
    final_layer_norm [label="Final LayerNorm\nInput: [batch_size=1, seq_len=128000, d_model=16384]\nOutput: [batch_size=1, seq_len=128000, d_model=16384]", 
                      fillcolor=pink];
    output_projection [label="Output Projection\nInput: [batch_size=1, seq_len=128000, d_model=16384]\nOutput: [batch_size=1, seq_len=128000, vocab_size=128256]", 
                       fillcolor=lightblue];
    
    layer_126_output -> final_layer_norm;
    final_layer_norm -> output_projection;
    
    // Model output
    output [label="Model Output\nInput: [batch_size=1, seq_len=128000, vocab_size=128256]", 
            shape=parallelogram, fillcolor=lightgreen];
    output_projection -> output;
}