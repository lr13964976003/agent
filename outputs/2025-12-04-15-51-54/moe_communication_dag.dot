// Communication Patterns in 128-GPU MoE Deployment
digraph MoE_128GPU_Communication {
	graph [bgcolor=white fontname=Arial fontsize=14 nodesep=1.0 rankdir=LR ranksep=2.0 splines=curved]
	node [fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	edge [fontname=Arial fontsize=12]
	input_data [label="Input Data\n[batch_size, seq_len, hidden_size]" fillcolor=lightgreen shape=ellipse style="filled,bold"]
	expert_parallel [label="Expert Parallelism\n64-way Partitioning" fillcolor=orange penwidth=3 shape=parallelogram style="filled,dashed"]
	tensor_parallel [label="Tensor Parallelism\n2-way Partitioning" fillcolor=purple penwidth=3 shape=parallelogram style="filled,dashed"]
	pipeline_parallel [label="Pipeline Parallelism\n4-stage Pipeline" fillcolor=green penwidth=3 shape=parallelogram style="filled,dashed"]
	allreduce_tensor [label="All-Reduce\nTensor Parallel" fillcolor=yellow penwidth=2 shape=ellipse style="filled,dashed"]
	allgather_expert [label="All-Gather\nExpert Parallel" fillcolor=yellow penwidth=2 shape=ellipse style="filled,dashed"]
	sendrecv_pipeline [label="Send/Recv\nPipeline Parallel" fillcolor=yellow penwidth=2 shape=ellipse style="filled,dashed"]
	gpu_group_0 [label="GPU Group 0\nGPUs 0-31\n(Pipeline Stage 0)" fillcolor=lightcyan penwidth=2]
	gpu_group_1 [label="GPU Group 1\nGPUs 32-63\n(Pipeline Stage 1)" fillcolor=lightcyan penwidth=2]
	gpu_group_2 [label="GPU Group 2\nGPUs 64-95\n(Pipeline Stage 2)" fillcolor=lightcyan penwidth=2]
	gpu_group_3 [label="GPU Group 3\nGPUs 96-127\n(Pipeline Stage 3)" fillcolor=lightcyan penwidth=2]
	input_data -> expert_parallel [penwidth=2 style=solid]
	expert_parallel -> tensor_parallel [color=red penwidth=2 style=dashed]
	tensor_parallel -> pipeline_parallel [color=blue penwidth=2 style=dashed]
	tensor_parallel -> allreduce_tensor [color=purple penwidth=2 style=dashed]
	expert_parallel -> allgather_expert [color=orange penwidth=2 style=dashed]
	pipeline_parallel -> sendrecv_pipeline [color=green penwidth=2 style=dashed]
	allreduce_tensor -> gpu_group_0 [penwidth=2 style=solid]
	allgather_expert -> gpu_group_0 [penwidth=2 style=solid]
	sendrecv_pipeline -> gpu_group_0 [penwidth=2 style=solid]
	allreduce_tensor -> gpu_group_1 [penwidth=2 style=solid]
	allgather_expert -> gpu_group_1 [penwidth=2 style=solid]
	sendrecv_pipeline -> gpu_group_1 [penwidth=2 style=solid]
	allreduce_tensor -> gpu_group_2 [penwidth=2 style=solid]
	allgather_expert -> gpu_group_2 [penwidth=2 style=solid]
	sendrecv_pipeline -> gpu_group_2 [penwidth=2 style=solid]
	allreduce_tensor -> gpu_group_3 [penwidth=2 style=solid]
	allgather_expert -> gpu_group_3 [penwidth=2 style=solid]
	sendrecv_pipeline -> gpu_group_3 [penwidth=2 style=solid]
	gpu_group_0 -> gpu_group_1 [label="Pipeline Send/Recv" color=green penwidth=2 style=dashed]
	gpu_group_1 -> gpu_group_2 [label="Pipeline Send/Recv" color=green penwidth=2 style=dashed]
	gpu_group_2 -> gpu_group_3 [label="Pipeline Send/Recv" color=green penwidth=2 style=dashed]
}
