{
  "generated_dag_files": {
    "dot_file": "../outputs/2025-12-22-11-52-26/llm_deployment_dag.dot",
    "svg_image": "../outputs/2025-12-22-11-52-26/llm_deployment_dag.svg",
    "python_generator": "../outputs/2025-12-22-11-52-26/generate_llm_deployment_dag_final.py"
  },
  "dag_validation": {
    "is_acyclic": true,
    "has_proper_structure": true,
    "total_nodes": 23,
    "total_edges": 20,
    "input_nodes": ["input"],
    "output_nodes": ["decode_end"]
  },
  "parallel_strategy_reflected": {
    "ep": 8,
    "tp": 4, 
    "pp": 2,
    "dp": 4,
    "total_gpus": 256,
    "configuration": "EP8-TP4-PP2-DP4"
  },
  "dag_features": {
    "node_types": {
      "computation": "Rectangles with lightgreen fill",
      "communication": "Ellipses with lightblue fill", 
      "routing": "Parallelograms with lightyellow fill"
    },
    "dimensions_included": true,
    "gpu_labels": true,
    "parallel_dimensions": true,
    "communication_patterns": {
      "all_reduce": "Embedding All-Reduce nodes",
      "all_to_all": "Expert Dispatch nodes (dashed)",
      "pipeline": "Pipeline Send nodes"
    },
    "moe_components": {
      "gate": "MoE Gate nodes (parallelograms)",
      "expert_dispatch": "Expert Dispatch (dashed)",
      "expert_compute": "Expert Compute nodes",
      "expert_combine": "Expert Combine nodes"
    },
    "attention_breakdown": true,
    "kv_cache_operations": true
  },
  "inference_phases": {
    "prefill": "Complete prefill phase with embedding, attention, and MoE layers",
    "decode": "Decode phase with KV cache operations"
  }
}