// 30B MoE Model Deployment DAG
digraph {
	rankdir=TB size="30,40"
	node [fontname=Arial fontsize=10]
	edge [fontname=Arial fontsize=8]
	node [fillcolor=lightblue shape=rectangle style=filled]
	node [fillcolor=lightgreen shape=ellipse style=filled]
	node [fillcolor=lightyellow shape=parallelogram style=filled]
	subgraph cluster_input {
		fillcolor=lightgray label="Input Layer" style="rounded,filled"
		input [label="Input Embedding\nGPU: 0-511\nInput: [batch_size=128, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]"]
	}
	subgraph cluster_pp_stage_0 {
		fillcolor=lightcyan label="Pipeline Stage 0 (GPUs 0-127)" style="rounded,filled"
		subgraph cluster_layer_0 {
			fillcolor=white label="Layer 0" style="rounded,filled"
			layer_0_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_0_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_0_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_0_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_0_qkv_comm [label="QKV AllGather\nGPUs: 0-127\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_0_attn_tp_0 [label="Attention Computation\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_0_attn_tp_1 [label="Attention Computation\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_0_attn_tp_2 [label="Attention Computation\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_0_attn_tp_3 [label="Attention Computation\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_0_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_0_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_0_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_0_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_0_attn_allreduce [label="Attention AllReduce\nGPUs: 0-127\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_0_gate_tp_0 [label="Gate Computation\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_0_gate_tp_1 [label="Gate Computation\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_0_gate_tp_2 [label="Gate Computation\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_0_gate_tp_3 [label="Gate Computation\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_0_route_ep_0 [label="Expert Routing\nGPU: 0-15\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_0_route_ep_1 [label="Expert Routing\nGPU: 16-31\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_0_route_ep_2 [label="Expert Routing\nGPU: 32-47\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_0_route_ep_3 [label="Expert Routing\nGPU: 48-63\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_0_route_ep_4 [label="Expert Routing\nGPU: 64-79\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_0_route_ep_5 [label="Expert Routing\nGPU: 80-95\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_0_route_ep_6 [label="Expert Routing\nGPU: 96-111\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_0_route_ep_7 [label="Expert Routing\nGPU: 112-127\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_0_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 0\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 1\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 2\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 3\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 4\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 5\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 6\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 7\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 16\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 17\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 18\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 19\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 20\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 21\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 22\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 23\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 32\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 33\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 34\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 35\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 36\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 37\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 38\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 39\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 48\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 49\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 50\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 51\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 52\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 53\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 54\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 55\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 64\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 65\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 66\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 67\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 68\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 69\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 70\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 71\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 80\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 81\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 82\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 83\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 84\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 85\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 86\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 87\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 96\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 97\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 98\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 99\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 100\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 101\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 102\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 103\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 112\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 113\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 114\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 115\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 116\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 117\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 118\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 119\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_0_agg_ep_0 [label="Expert Aggregation\nGPU: 0-15\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_0_agg_ep_1 [label="Expert Aggregation\nGPU: 16-31\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_0_agg_ep_2 [label="Expert Aggregation\nGPU: 32-47\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_0_agg_ep_3 [label="Expert Aggregation\nGPU: 48-63\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_0_agg_ep_4 [label="Expert Aggregation\nGPU: 64-79\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_0_agg_ep_5 [label="Expert Aggregation\nGPU: 80-95\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_0_agg_ep_6 [label="Expert Aggregation\nGPU: 96-111\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_0_agg_ep_7 [label="Expert Aggregation\nGPU: 112-127\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_0_moe_alltoall [label="MoE All-to-All\nGPUs: 0-127\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_0_ln_tp_0 [label="Layer Norm\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_0_ln_tp_1 [label="Layer Norm\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_0_ln_tp_2 [label="Layer Norm\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_0_ln_tp_3 [label="Layer Norm\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
		subgraph cluster_layer_1 {
			fillcolor=white label="Layer 1" style="rounded,filled"
			layer_1_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_1_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_1_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_1_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_1_qkv_comm [label="QKV AllGather\nGPUs: 0-127\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_1_attn_tp_0 [label="Attention Computation\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_1_attn_tp_1 [label="Attention Computation\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_1_attn_tp_2 [label="Attention Computation\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_1_attn_tp_3 [label="Attention Computation\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_1_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_1_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_1_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_1_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_1_attn_allreduce [label="Attention AllReduce\nGPUs: 0-127\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_1_gate_tp_0 [label="Gate Computation\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_1_gate_tp_1 [label="Gate Computation\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_1_gate_tp_2 [label="Gate Computation\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_1_gate_tp_3 [label="Gate Computation\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_1_route_ep_0 [label="Expert Routing\nGPU: 0-15\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_1_route_ep_1 [label="Expert Routing\nGPU: 16-31\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_1_route_ep_2 [label="Expert Routing\nGPU: 32-47\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_1_route_ep_3 [label="Expert Routing\nGPU: 48-63\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_1_route_ep_4 [label="Expert Routing\nGPU: 64-79\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_1_route_ep_5 [label="Expert Routing\nGPU: 80-95\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_1_route_ep_6 [label="Expert Routing\nGPU: 96-111\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_1_route_ep_7 [label="Expert Routing\nGPU: 112-127\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_1_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 0\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 1\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 2\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 3\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 4\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 5\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 6\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 7\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 16\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 17\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 18\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 19\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 20\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 21\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 22\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 23\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 32\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 33\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 34\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 35\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 36\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 37\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 38\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 39\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 48\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 49\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 50\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 51\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 52\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 53\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 54\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 55\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 64\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 65\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 66\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 67\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 68\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 69\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 70\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 71\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 80\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 81\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 82\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 83\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 84\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 85\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 86\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 87\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 96\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 97\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 98\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 99\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 100\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 101\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 102\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 103\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 112\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 113\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 114\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 115\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 116\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 117\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 118\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 119\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_1_agg_ep_0 [label="Expert Aggregation\nGPU: 0-15\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_1_agg_ep_1 [label="Expert Aggregation\nGPU: 16-31\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_1_agg_ep_2 [label="Expert Aggregation\nGPU: 32-47\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_1_agg_ep_3 [label="Expert Aggregation\nGPU: 48-63\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_1_agg_ep_4 [label="Expert Aggregation\nGPU: 64-79\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_1_agg_ep_5 [label="Expert Aggregation\nGPU: 80-95\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_1_agg_ep_6 [label="Expert Aggregation\nGPU: 96-111\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_1_agg_ep_7 [label="Expert Aggregation\nGPU: 112-127\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_1_moe_alltoall [label="MoE All-to-All\nGPUs: 0-127\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_1_ln_tp_0 [label="Layer Norm\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_1_ln_tp_1 [label="Layer Norm\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_1_ln_tp_2 [label="Layer Norm\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_1_ln_tp_3 [label="Layer Norm\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
		subgraph cluster_layer_2 {
			fillcolor=white label="Layer 2" style="rounded,filled"
			layer_2_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_2_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_2_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_2_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_2_qkv_comm [label="QKV AllGather\nGPUs: 0-127\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_2_attn_tp_0 [label="Attention Computation\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_2_attn_tp_1 [label="Attention Computation\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_2_attn_tp_2 [label="Attention Computation\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_2_attn_tp_3 [label="Attention Computation\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_2_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_2_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_2_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_2_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_2_attn_allreduce [label="Attention AllReduce\nGPUs: 0-127\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_2_gate_tp_0 [label="Gate Computation\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_2_gate_tp_1 [label="Gate Computation\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_2_gate_tp_2 [label="Gate Computation\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_2_gate_tp_3 [label="Gate Computation\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_2_route_ep_0 [label="Expert Routing\nGPU: 0-15\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_2_route_ep_1 [label="Expert Routing\nGPU: 16-31\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_2_route_ep_2 [label="Expert Routing\nGPU: 32-47\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_2_route_ep_3 [label="Expert Routing\nGPU: 48-63\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_2_route_ep_4 [label="Expert Routing\nGPU: 64-79\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_2_route_ep_5 [label="Expert Routing\nGPU: 80-95\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_2_route_ep_6 [label="Expert Routing\nGPU: 96-111\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_2_route_ep_7 [label="Expert Routing\nGPU: 112-127\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_2_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 0\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 1\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 2\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 3\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 4\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 5\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 6\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 7\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 16\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 17\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 18\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 19\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 20\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 21\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 22\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 23\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 32\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 33\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 34\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 35\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 36\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 37\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 38\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 39\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 48\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 49\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 50\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 51\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 52\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 53\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 54\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 55\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 64\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 65\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 66\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 67\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 68\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 69\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 70\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 71\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 80\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 81\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 82\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 83\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 84\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 85\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 86\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 87\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 96\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 97\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 98\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 99\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 100\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 101\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 102\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 103\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 112\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 113\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 114\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 115\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 116\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 117\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 118\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 119\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_2_agg_ep_0 [label="Expert Aggregation\nGPU: 0-15\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_2_agg_ep_1 [label="Expert Aggregation\nGPU: 16-31\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_2_agg_ep_2 [label="Expert Aggregation\nGPU: 32-47\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_2_agg_ep_3 [label="Expert Aggregation\nGPU: 48-63\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_2_agg_ep_4 [label="Expert Aggregation\nGPU: 64-79\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_2_agg_ep_5 [label="Expert Aggregation\nGPU: 80-95\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_2_agg_ep_6 [label="Expert Aggregation\nGPU: 96-111\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_2_agg_ep_7 [label="Expert Aggregation\nGPU: 112-127\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_2_moe_alltoall [label="MoE All-to-All\nGPUs: 0-127\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_2_ln_tp_0 [label="Layer Norm\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_2_ln_tp_1 [label="Layer Norm\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_2_ln_tp_2 [label="Layer Norm\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_2_ln_tp_3 [label="Layer Norm\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
		subgraph cluster_layer_3 {
			fillcolor=white label="Layer 3" style="rounded,filled"
			layer_3_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_3_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_3_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_3_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_3_qkv_comm [label="QKV AllGather\nGPUs: 0-127\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_3_attn_tp_0 [label="Attention Computation\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_3_attn_tp_1 [label="Attention Computation\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_3_attn_tp_2 [label="Attention Computation\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_3_attn_tp_3 [label="Attention Computation\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_3_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_3_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_3_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_3_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_3_attn_allreduce [label="Attention AllReduce\nGPUs: 0-127\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_3_gate_tp_0 [label="Gate Computation\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_3_gate_tp_1 [label="Gate Computation\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_3_gate_tp_2 [label="Gate Computation\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_3_gate_tp_3 [label="Gate Computation\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_3_route_ep_0 [label="Expert Routing\nGPU: 0-15\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_3_route_ep_1 [label="Expert Routing\nGPU: 16-31\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_3_route_ep_2 [label="Expert Routing\nGPU: 32-47\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_3_route_ep_3 [label="Expert Routing\nGPU: 48-63\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_3_route_ep_4 [label="Expert Routing\nGPU: 64-79\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_3_route_ep_5 [label="Expert Routing\nGPU: 80-95\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_3_route_ep_6 [label="Expert Routing\nGPU: 96-111\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_3_route_ep_7 [label="Expert Routing\nGPU: 112-127\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_3_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 0\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 1\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 2\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 3\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 4\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 5\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 6\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 7\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 16\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 17\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 18\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 19\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 20\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 21\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 22\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 23\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 32\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 33\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 34\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 35\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 36\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 37\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 38\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 39\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 48\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 49\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 50\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 51\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 52\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 53\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 54\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 55\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 64\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 65\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 66\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 67\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 68\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 69\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 70\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 71\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 80\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 81\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 82\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 83\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 84\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 85\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 86\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 87\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 96\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 97\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 98\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 99\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 100\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 101\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 102\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 103\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 112\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 113\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 114\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 115\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 116\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 117\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 118\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 119\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_3_agg_ep_0 [label="Expert Aggregation\nGPU: 0-15\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_3_agg_ep_1 [label="Expert Aggregation\nGPU: 16-31\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_3_agg_ep_2 [label="Expert Aggregation\nGPU: 32-47\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_3_agg_ep_3 [label="Expert Aggregation\nGPU: 48-63\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_3_agg_ep_4 [label="Expert Aggregation\nGPU: 64-79\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_3_agg_ep_5 [label="Expert Aggregation\nGPU: 80-95\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_3_agg_ep_6 [label="Expert Aggregation\nGPU: 96-111\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_3_agg_ep_7 [label="Expert Aggregation\nGPU: 112-127\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_3_moe_alltoall [label="MoE All-to-All\nGPUs: 0-127\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_3_ln_tp_0 [label="Layer Norm\nGPU: 0-31\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_3_ln_tp_1 [label="Layer Norm\nGPU: 32-63\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_3_ln_tp_2 [label="Layer Norm\nGPU: 64-95\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_3_ln_tp_3 [label="Layer Norm\nGPU: 96-127\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
	}
	subgraph cluster_pp_stage_1 {
		fillcolor=lightcyan label="Pipeline Stage 1 (GPUs 128-255)" style="rounded,filled"
		subgraph cluster_layer_4 {
			fillcolor=white label="Layer 4" style="rounded,filled"
			layer_4_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_4_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_4_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_4_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_4_qkv_comm [label="QKV AllGather\nGPUs: 128-255\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_4_attn_tp_0 [label="Attention Computation\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_4_attn_tp_1 [label="Attention Computation\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_4_attn_tp_2 [label="Attention Computation\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_4_attn_tp_3 [label="Attention Computation\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_4_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_4_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_4_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_4_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_4_attn_allreduce [label="Attention AllReduce\nGPUs: 128-255\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_4_gate_tp_0 [label="Gate Computation\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_4_gate_tp_1 [label="Gate Computation\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_4_gate_tp_2 [label="Gate Computation\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_4_gate_tp_3 [label="Gate Computation\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_4_route_ep_0 [label="Expert Routing\nGPU: 128-143\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_4_route_ep_1 [label="Expert Routing\nGPU: 144-159\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_4_route_ep_2 [label="Expert Routing\nGPU: 160-175\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_4_route_ep_3 [label="Expert Routing\nGPU: 176-191\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_4_route_ep_4 [label="Expert Routing\nGPU: 192-207\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_4_route_ep_5 [label="Expert Routing\nGPU: 208-223\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_4_route_ep_6 [label="Expert Routing\nGPU: 224-239\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_4_route_ep_7 [label="Expert Routing\nGPU: 240-255\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_4_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 128\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 129\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 130\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 131\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 132\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 133\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 134\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 135\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 144\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 145\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 146\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 147\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 148\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 149\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 150\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 151\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 160\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 161\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 162\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 163\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 164\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 165\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 166\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 167\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 176\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 177\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 178\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 179\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 180\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 181\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 182\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 183\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 192\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 193\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 194\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 195\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 196\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 197\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 198\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 199\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 208\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 209\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 210\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 211\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 212\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 213\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 214\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 215\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 224\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 225\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 226\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 227\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 228\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 229\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 230\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 231\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 240\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 241\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 242\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 243\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 244\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 245\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 246\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 247\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_4_agg_ep_0 [label="Expert Aggregation\nGPU: 128-143\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_4_agg_ep_1 [label="Expert Aggregation\nGPU: 144-159\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_4_agg_ep_2 [label="Expert Aggregation\nGPU: 160-175\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_4_agg_ep_3 [label="Expert Aggregation\nGPU: 176-191\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_4_agg_ep_4 [label="Expert Aggregation\nGPU: 192-207\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_4_agg_ep_5 [label="Expert Aggregation\nGPU: 208-223\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_4_agg_ep_6 [label="Expert Aggregation\nGPU: 224-239\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_4_agg_ep_7 [label="Expert Aggregation\nGPU: 240-255\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_4_moe_alltoall [label="MoE All-to-All\nGPUs: 128-255\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_4_ln_tp_0 [label="Layer Norm\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_4_ln_tp_1 [label="Layer Norm\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_4_ln_tp_2 [label="Layer Norm\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_4_ln_tp_3 [label="Layer Norm\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
		subgraph cluster_layer_5 {
			fillcolor=white label="Layer 5" style="rounded,filled"
			layer_5_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_5_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_5_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_5_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_5_qkv_comm [label="QKV AllGather\nGPUs: 128-255\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_5_attn_tp_0 [label="Attention Computation\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_5_attn_tp_1 [label="Attention Computation\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_5_attn_tp_2 [label="Attention Computation\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_5_attn_tp_3 [label="Attention Computation\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_5_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_5_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_5_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_5_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_5_attn_allreduce [label="Attention AllReduce\nGPUs: 128-255\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_5_gate_tp_0 [label="Gate Computation\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_5_gate_tp_1 [label="Gate Computation\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_5_gate_tp_2 [label="Gate Computation\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_5_gate_tp_3 [label="Gate Computation\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_5_route_ep_0 [label="Expert Routing\nGPU: 128-143\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_5_route_ep_1 [label="Expert Routing\nGPU: 144-159\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_5_route_ep_2 [label="Expert Routing\nGPU: 160-175\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_5_route_ep_3 [label="Expert Routing\nGPU: 176-191\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_5_route_ep_4 [label="Expert Routing\nGPU: 192-207\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_5_route_ep_5 [label="Expert Routing\nGPU: 208-223\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_5_route_ep_6 [label="Expert Routing\nGPU: 224-239\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_5_route_ep_7 [label="Expert Routing\nGPU: 240-255\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_5_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 128\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 129\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 130\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 131\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 132\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 133\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 134\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 135\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 144\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 145\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 146\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 147\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 148\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 149\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 150\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 151\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 160\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 161\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 162\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 163\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 164\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 165\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 166\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 167\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 176\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 177\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 178\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 179\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 180\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 181\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 182\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 183\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 192\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 193\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 194\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 195\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 196\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 197\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 198\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 199\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 208\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 209\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 210\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 211\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 212\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 213\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 214\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 215\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 224\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 225\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 226\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 227\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 228\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 229\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 230\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 231\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 240\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 241\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 242\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 243\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 244\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 245\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 246\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 247\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_5_agg_ep_0 [label="Expert Aggregation\nGPU: 128-143\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_5_agg_ep_1 [label="Expert Aggregation\nGPU: 144-159\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_5_agg_ep_2 [label="Expert Aggregation\nGPU: 160-175\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_5_agg_ep_3 [label="Expert Aggregation\nGPU: 176-191\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_5_agg_ep_4 [label="Expert Aggregation\nGPU: 192-207\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_5_agg_ep_5 [label="Expert Aggregation\nGPU: 208-223\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_5_agg_ep_6 [label="Expert Aggregation\nGPU: 224-239\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_5_agg_ep_7 [label="Expert Aggregation\nGPU: 240-255\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_5_moe_alltoall [label="MoE All-to-All\nGPUs: 128-255\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_5_ln_tp_0 [label="Layer Norm\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_5_ln_tp_1 [label="Layer Norm\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_5_ln_tp_2 [label="Layer Norm\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_5_ln_tp_3 [label="Layer Norm\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
		subgraph cluster_layer_6 {
			fillcolor=white label="Layer 6" style="rounded,filled"
			layer_6_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_6_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_6_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_6_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_6_qkv_comm [label="QKV AllGather\nGPUs: 128-255\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_6_attn_tp_0 [label="Attention Computation\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_6_attn_tp_1 [label="Attention Computation\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_6_attn_tp_2 [label="Attention Computation\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_6_attn_tp_3 [label="Attention Computation\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_6_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_6_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_6_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_6_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_6_attn_allreduce [label="Attention AllReduce\nGPUs: 128-255\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_6_gate_tp_0 [label="Gate Computation\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_6_gate_tp_1 [label="Gate Computation\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_6_gate_tp_2 [label="Gate Computation\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_6_gate_tp_3 [label="Gate Computation\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_6_route_ep_0 [label="Expert Routing\nGPU: 128-143\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_6_route_ep_1 [label="Expert Routing\nGPU: 144-159\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_6_route_ep_2 [label="Expert Routing\nGPU: 160-175\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_6_route_ep_3 [label="Expert Routing\nGPU: 176-191\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_6_route_ep_4 [label="Expert Routing\nGPU: 192-207\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_6_route_ep_5 [label="Expert Routing\nGPU: 208-223\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_6_route_ep_6 [label="Expert Routing\nGPU: 224-239\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_6_route_ep_7 [label="Expert Routing\nGPU: 240-255\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_6_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 128\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 129\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 130\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 131\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 132\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 133\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 134\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 135\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 144\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 145\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 146\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 147\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 148\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 149\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 150\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 151\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 160\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 161\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 162\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 163\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 164\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 165\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 166\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 167\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 176\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 177\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 178\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 179\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 180\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 181\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 182\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 183\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 192\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 193\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 194\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 195\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 196\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 197\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 198\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 199\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 208\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 209\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 210\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 211\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 212\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 213\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 214\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 215\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 224\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 225\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 226\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 227\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 228\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 229\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 230\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 231\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 240\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 241\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 242\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 243\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 244\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 245\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 246\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 247\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_6_agg_ep_0 [label="Expert Aggregation\nGPU: 128-143\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_6_agg_ep_1 [label="Expert Aggregation\nGPU: 144-159\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_6_agg_ep_2 [label="Expert Aggregation\nGPU: 160-175\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_6_agg_ep_3 [label="Expert Aggregation\nGPU: 176-191\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_6_agg_ep_4 [label="Expert Aggregation\nGPU: 192-207\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_6_agg_ep_5 [label="Expert Aggregation\nGPU: 208-223\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_6_agg_ep_6 [label="Expert Aggregation\nGPU: 224-239\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_6_agg_ep_7 [label="Expert Aggregation\nGPU: 240-255\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_6_moe_alltoall [label="MoE All-to-All\nGPUs: 128-255\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_6_ln_tp_0 [label="Layer Norm\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_6_ln_tp_1 [label="Layer Norm\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_6_ln_tp_2 [label="Layer Norm\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_6_ln_tp_3 [label="Layer Norm\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
		subgraph cluster_layer_7 {
			fillcolor=white label="Layer 7" style="rounded,filled"
			layer_7_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_7_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_7_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_7_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_7_qkv_comm [label="QKV AllGather\nGPUs: 128-255\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_7_attn_tp_0 [label="Attention Computation\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_7_attn_tp_1 [label="Attention Computation\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_7_attn_tp_2 [label="Attention Computation\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_7_attn_tp_3 [label="Attention Computation\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_7_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_7_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_7_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_7_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_7_attn_allreduce [label="Attention AllReduce\nGPUs: 128-255\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_7_gate_tp_0 [label="Gate Computation\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_7_gate_tp_1 [label="Gate Computation\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_7_gate_tp_2 [label="Gate Computation\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_7_gate_tp_3 [label="Gate Computation\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_7_route_ep_0 [label="Expert Routing\nGPU: 128-143\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_7_route_ep_1 [label="Expert Routing\nGPU: 144-159\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_7_route_ep_2 [label="Expert Routing\nGPU: 160-175\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_7_route_ep_3 [label="Expert Routing\nGPU: 176-191\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_7_route_ep_4 [label="Expert Routing\nGPU: 192-207\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_7_route_ep_5 [label="Expert Routing\nGPU: 208-223\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_7_route_ep_6 [label="Expert Routing\nGPU: 224-239\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_7_route_ep_7 [label="Expert Routing\nGPU: 240-255\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_7_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 128\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 129\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 130\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 131\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 132\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 133\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 134\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 135\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 144\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 145\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 146\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 147\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 148\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 149\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 150\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 151\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 160\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 161\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 162\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 163\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 164\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 165\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 166\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 167\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 176\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 177\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 178\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 179\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 180\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 181\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 182\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 183\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 192\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 193\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 194\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 195\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 196\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 197\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 198\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 199\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 208\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 209\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 210\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 211\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 212\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 213\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 214\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 215\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 224\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 225\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 226\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 227\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 228\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 229\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 230\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 231\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 240\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 241\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 242\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 243\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 244\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 245\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 246\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 247\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_7_agg_ep_0 [label="Expert Aggregation\nGPU: 128-143\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_7_agg_ep_1 [label="Expert Aggregation\nGPU: 144-159\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_7_agg_ep_2 [label="Expert Aggregation\nGPU: 160-175\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_7_agg_ep_3 [label="Expert Aggregation\nGPU: 176-191\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_7_agg_ep_4 [label="Expert Aggregation\nGPU: 192-207\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_7_agg_ep_5 [label="Expert Aggregation\nGPU: 208-223\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_7_agg_ep_6 [label="Expert Aggregation\nGPU: 224-239\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_7_agg_ep_7 [label="Expert Aggregation\nGPU: 240-255\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_7_moe_alltoall [label="MoE All-to-All\nGPUs: 128-255\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_7_ln_tp_0 [label="Layer Norm\nGPU: 128-159\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_7_ln_tp_1 [label="Layer Norm\nGPU: 160-191\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_7_ln_tp_2 [label="Layer Norm\nGPU: 192-223\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_7_ln_tp_3 [label="Layer Norm\nGPU: 224-255\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
	}
	subgraph cluster_pp_stage_2 {
		fillcolor=lightcyan label="Pipeline Stage 2 (GPUs 256-383)" style="rounded,filled"
		subgraph cluster_layer_8 {
			fillcolor=white label="Layer 8" style="rounded,filled"
			layer_8_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_8_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_8_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_8_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_8_qkv_comm [label="QKV AllGather\nGPUs: 256-383\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_8_attn_tp_0 [label="Attention Computation\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_8_attn_tp_1 [label="Attention Computation\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_8_attn_tp_2 [label="Attention Computation\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_8_attn_tp_3 [label="Attention Computation\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_8_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_8_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_8_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_8_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_8_attn_allreduce [label="Attention AllReduce\nGPUs: 256-383\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_8_gate_tp_0 [label="Gate Computation\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_8_gate_tp_1 [label="Gate Computation\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_8_gate_tp_2 [label="Gate Computation\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_8_gate_tp_3 [label="Gate Computation\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_8_route_ep_0 [label="Expert Routing\nGPU: 256-271\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_8_route_ep_1 [label="Expert Routing\nGPU: 272-287\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_8_route_ep_2 [label="Expert Routing\nGPU: 288-303\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_8_route_ep_3 [label="Expert Routing\nGPU: 304-319\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_8_route_ep_4 [label="Expert Routing\nGPU: 320-335\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_8_route_ep_5 [label="Expert Routing\nGPU: 336-351\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_8_route_ep_6 [label="Expert Routing\nGPU: 352-367\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_8_route_ep_7 [label="Expert Routing\nGPU: 368-383\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_8_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 256\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 257\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 258\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 259\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 260\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 261\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 262\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 263\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 272\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 273\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 274\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 275\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 276\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 277\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 278\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 279\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 288\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 289\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 290\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 291\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 292\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 293\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 294\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 295\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 304\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 305\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 306\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 307\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 308\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 309\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 310\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 311\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 320\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 321\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 322\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 323\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 324\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 325\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 326\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 327\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 336\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 337\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 338\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 339\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 340\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 341\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 342\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 343\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 352\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 353\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 354\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 355\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 356\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 357\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 358\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 359\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 368\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 369\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 370\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 371\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 372\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 373\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 374\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 375\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_8_agg_ep_0 [label="Expert Aggregation\nGPU: 256-271\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_8_agg_ep_1 [label="Expert Aggregation\nGPU: 272-287\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_8_agg_ep_2 [label="Expert Aggregation\nGPU: 288-303\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_8_agg_ep_3 [label="Expert Aggregation\nGPU: 304-319\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_8_agg_ep_4 [label="Expert Aggregation\nGPU: 320-335\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_8_agg_ep_5 [label="Expert Aggregation\nGPU: 336-351\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_8_agg_ep_6 [label="Expert Aggregation\nGPU: 352-367\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_8_agg_ep_7 [label="Expert Aggregation\nGPU: 368-383\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_8_moe_alltoall [label="MoE All-to-All\nGPUs: 256-383\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_8_ln_tp_0 [label="Layer Norm\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_8_ln_tp_1 [label="Layer Norm\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_8_ln_tp_2 [label="Layer Norm\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_8_ln_tp_3 [label="Layer Norm\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
		subgraph cluster_layer_9 {
			fillcolor=white label="Layer 9" style="rounded,filled"
			layer_9_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_9_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_9_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_9_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_9_qkv_comm [label="QKV AllGather\nGPUs: 256-383\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_9_attn_tp_0 [label="Attention Computation\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_9_attn_tp_1 [label="Attention Computation\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_9_attn_tp_2 [label="Attention Computation\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_9_attn_tp_3 [label="Attention Computation\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_9_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_9_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_9_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_9_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_9_attn_allreduce [label="Attention AllReduce\nGPUs: 256-383\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_9_gate_tp_0 [label="Gate Computation\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_9_gate_tp_1 [label="Gate Computation\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_9_gate_tp_2 [label="Gate Computation\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_9_gate_tp_3 [label="Gate Computation\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_9_route_ep_0 [label="Expert Routing\nGPU: 256-271\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_9_route_ep_1 [label="Expert Routing\nGPU: 272-287\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_9_route_ep_2 [label="Expert Routing\nGPU: 288-303\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_9_route_ep_3 [label="Expert Routing\nGPU: 304-319\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_9_route_ep_4 [label="Expert Routing\nGPU: 320-335\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_9_route_ep_5 [label="Expert Routing\nGPU: 336-351\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_9_route_ep_6 [label="Expert Routing\nGPU: 352-367\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_9_route_ep_7 [label="Expert Routing\nGPU: 368-383\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_9_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 256\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 257\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 258\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 259\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 260\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 261\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 262\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 263\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 272\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 273\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 274\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 275\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 276\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 277\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 278\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 279\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 288\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 289\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 290\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 291\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 292\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 293\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 294\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 295\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 304\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 305\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 306\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 307\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 308\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 309\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 310\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 311\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 320\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 321\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 322\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 323\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 324\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 325\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 326\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 327\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 336\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 337\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 338\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 339\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 340\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 341\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 342\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 343\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 352\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 353\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 354\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 355\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 356\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 357\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 358\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 359\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 368\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 369\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 370\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 371\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 372\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 373\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 374\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 375\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_9_agg_ep_0 [label="Expert Aggregation\nGPU: 256-271\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_9_agg_ep_1 [label="Expert Aggregation\nGPU: 272-287\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_9_agg_ep_2 [label="Expert Aggregation\nGPU: 288-303\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_9_agg_ep_3 [label="Expert Aggregation\nGPU: 304-319\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_9_agg_ep_4 [label="Expert Aggregation\nGPU: 320-335\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_9_agg_ep_5 [label="Expert Aggregation\nGPU: 336-351\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_9_agg_ep_6 [label="Expert Aggregation\nGPU: 352-367\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_9_agg_ep_7 [label="Expert Aggregation\nGPU: 368-383\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_9_moe_alltoall [label="MoE All-to-All\nGPUs: 256-383\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_9_ln_tp_0 [label="Layer Norm\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_9_ln_tp_1 [label="Layer Norm\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_9_ln_tp_2 [label="Layer Norm\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_9_ln_tp_3 [label="Layer Norm\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
		subgraph cluster_layer_10 {
			fillcolor=white label="Layer 10" style="rounded,filled"
			layer_10_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_10_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_10_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_10_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_10_qkv_comm [label="QKV AllGather\nGPUs: 256-383\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_10_attn_tp_0 [label="Attention Computation\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_10_attn_tp_1 [label="Attention Computation\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_10_attn_tp_2 [label="Attention Computation\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_10_attn_tp_3 [label="Attention Computation\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_10_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_10_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_10_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_10_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_10_attn_allreduce [label="Attention AllReduce\nGPUs: 256-383\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_10_gate_tp_0 [label="Gate Computation\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_10_gate_tp_1 [label="Gate Computation\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_10_gate_tp_2 [label="Gate Computation\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_10_gate_tp_3 [label="Gate Computation\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_10_route_ep_0 [label="Expert Routing\nGPU: 256-271\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_10_route_ep_1 [label="Expert Routing\nGPU: 272-287\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_10_route_ep_2 [label="Expert Routing\nGPU: 288-303\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_10_route_ep_3 [label="Expert Routing\nGPU: 304-319\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_10_route_ep_4 [label="Expert Routing\nGPU: 320-335\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_10_route_ep_5 [label="Expert Routing\nGPU: 336-351\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_10_route_ep_6 [label="Expert Routing\nGPU: 352-367\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_10_route_ep_7 [label="Expert Routing\nGPU: 368-383\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_10_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 256\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 257\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 258\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 259\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 260\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 261\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 262\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 263\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 272\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 273\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 274\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 275\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 276\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 277\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 278\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 279\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 288\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 289\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 290\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 291\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 292\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 293\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 294\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 295\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 304\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 305\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 306\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 307\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 308\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 309\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 310\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 311\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 320\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 321\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 322\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 323\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 324\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 325\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 326\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 327\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 336\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 337\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 338\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 339\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 340\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 341\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 342\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 343\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 352\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 353\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 354\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 355\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 356\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 357\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 358\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 359\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 368\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 369\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 370\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 371\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 372\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 373\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 374\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 375\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_10_agg_ep_0 [label="Expert Aggregation\nGPU: 256-271\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_10_agg_ep_1 [label="Expert Aggregation\nGPU: 272-287\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_10_agg_ep_2 [label="Expert Aggregation\nGPU: 288-303\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_10_agg_ep_3 [label="Expert Aggregation\nGPU: 304-319\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_10_agg_ep_4 [label="Expert Aggregation\nGPU: 320-335\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_10_agg_ep_5 [label="Expert Aggregation\nGPU: 336-351\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_10_agg_ep_6 [label="Expert Aggregation\nGPU: 352-367\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_10_agg_ep_7 [label="Expert Aggregation\nGPU: 368-383\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_10_moe_alltoall [label="MoE All-to-All\nGPUs: 256-383\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_10_ln_tp_0 [label="Layer Norm\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_10_ln_tp_1 [label="Layer Norm\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_10_ln_tp_2 [label="Layer Norm\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_10_ln_tp_3 [label="Layer Norm\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
		subgraph cluster_layer_11 {
			fillcolor=white label="Layer 11" style="rounded,filled"
			layer_11_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_11_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_11_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_11_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_11_qkv_comm [label="QKV AllGather\nGPUs: 256-383\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_11_attn_tp_0 [label="Attention Computation\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_11_attn_tp_1 [label="Attention Computation\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_11_attn_tp_2 [label="Attention Computation\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_11_attn_tp_3 [label="Attention Computation\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_11_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_11_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_11_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_11_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_11_attn_allreduce [label="Attention AllReduce\nGPUs: 256-383\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_11_gate_tp_0 [label="Gate Computation\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_11_gate_tp_1 [label="Gate Computation\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_11_gate_tp_2 [label="Gate Computation\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_11_gate_tp_3 [label="Gate Computation\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_11_route_ep_0 [label="Expert Routing\nGPU: 256-271\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_11_route_ep_1 [label="Expert Routing\nGPU: 272-287\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_11_route_ep_2 [label="Expert Routing\nGPU: 288-303\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_11_route_ep_3 [label="Expert Routing\nGPU: 304-319\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_11_route_ep_4 [label="Expert Routing\nGPU: 320-335\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_11_route_ep_5 [label="Expert Routing\nGPU: 336-351\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_11_route_ep_6 [label="Expert Routing\nGPU: 352-367\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_11_route_ep_7 [label="Expert Routing\nGPU: 368-383\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_11_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 256\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 257\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 258\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 259\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 260\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 261\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 262\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 263\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 272\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 273\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 274\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 275\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 276\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 277\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 278\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 279\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 288\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 289\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 290\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 291\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 292\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 293\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 294\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 295\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 304\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 305\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 306\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 307\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 308\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 309\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 310\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 311\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 320\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 321\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 322\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 323\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 324\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 325\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 326\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 327\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 336\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 337\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 338\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 339\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 340\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 341\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 342\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 343\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 352\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 353\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 354\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 355\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 356\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 357\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 358\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 359\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 368\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 369\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 370\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 371\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 372\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 373\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 374\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 375\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_11_agg_ep_0 [label="Expert Aggregation\nGPU: 256-271\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_11_agg_ep_1 [label="Expert Aggregation\nGPU: 272-287\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_11_agg_ep_2 [label="Expert Aggregation\nGPU: 288-303\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_11_agg_ep_3 [label="Expert Aggregation\nGPU: 304-319\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_11_agg_ep_4 [label="Expert Aggregation\nGPU: 320-335\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_11_agg_ep_5 [label="Expert Aggregation\nGPU: 336-351\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_11_agg_ep_6 [label="Expert Aggregation\nGPU: 352-367\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_11_agg_ep_7 [label="Expert Aggregation\nGPU: 368-383\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_11_moe_alltoall [label="MoE All-to-All\nGPUs: 256-383\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_11_ln_tp_0 [label="Layer Norm\nGPU: 256-287\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_11_ln_tp_1 [label="Layer Norm\nGPU: 288-319\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_11_ln_tp_2 [label="Layer Norm\nGPU: 320-351\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_11_ln_tp_3 [label="Layer Norm\nGPU: 352-383\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
	}
	subgraph cluster_pp_stage_3 {
		fillcolor=lightcyan label="Pipeline Stage 3 (GPUs 384-511)" style="rounded,filled"
		subgraph cluster_layer_12 {
			fillcolor=white label="Layer 12" style="rounded,filled"
			layer_12_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_12_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_12_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_12_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_12_qkv_comm [label="QKV AllGather\nGPUs: 384-511\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_12_attn_tp_0 [label="Attention Computation\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_12_attn_tp_1 [label="Attention Computation\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_12_attn_tp_2 [label="Attention Computation\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_12_attn_tp_3 [label="Attention Computation\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_12_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_12_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_12_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_12_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_12_attn_allreduce [label="Attention AllReduce\nGPUs: 384-511\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_12_gate_tp_0 [label="Gate Computation\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_12_gate_tp_1 [label="Gate Computation\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_12_gate_tp_2 [label="Gate Computation\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_12_gate_tp_3 [label="Gate Computation\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_12_route_ep_0 [label="Expert Routing\nGPU: 384-399\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_12_route_ep_1 [label="Expert Routing\nGPU: 400-415\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_12_route_ep_2 [label="Expert Routing\nGPU: 416-431\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_12_route_ep_3 [label="Expert Routing\nGPU: 432-447\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_12_route_ep_4 [label="Expert Routing\nGPU: 448-463\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_12_route_ep_5 [label="Expert Routing\nGPU: 464-479\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_12_route_ep_6 [label="Expert Routing\nGPU: 480-495\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_12_route_ep_7 [label="Expert Routing\nGPU: 496-511\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_12_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 384\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 385\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 386\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 387\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 388\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 389\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 390\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 391\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 400\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 401\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 402\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 403\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 404\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 405\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 406\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 407\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 416\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 417\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 418\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 419\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 420\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 421\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 422\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 423\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 432\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 433\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 434\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 435\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 436\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 437\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 438\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 439\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 448\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 449\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 450\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 451\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 452\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 453\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 454\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 455\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 464\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 465\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 466\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 467\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 468\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 469\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 470\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 471\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 480\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 481\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 482\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 483\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 484\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 485\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 486\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 487\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 496\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 497\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 498\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 499\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 500\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 501\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 502\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 503\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_12_agg_ep_0 [label="Expert Aggregation\nGPU: 384-399\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_12_agg_ep_1 [label="Expert Aggregation\nGPU: 400-415\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_12_agg_ep_2 [label="Expert Aggregation\nGPU: 416-431\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_12_agg_ep_3 [label="Expert Aggregation\nGPU: 432-447\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_12_agg_ep_4 [label="Expert Aggregation\nGPU: 448-463\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_12_agg_ep_5 [label="Expert Aggregation\nGPU: 464-479\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_12_agg_ep_6 [label="Expert Aggregation\nGPU: 480-495\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_12_agg_ep_7 [label="Expert Aggregation\nGPU: 496-511\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_12_moe_alltoall [label="MoE All-to-All\nGPUs: 384-511\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_12_ln_tp_0 [label="Layer Norm\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_12_ln_tp_1 [label="Layer Norm\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_12_ln_tp_2 [label="Layer Norm\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_12_ln_tp_3 [label="Layer Norm\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
		subgraph cluster_layer_13 {
			fillcolor=white label="Layer 13" style="rounded,filled"
			layer_13_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_13_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_13_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_13_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_13_qkv_comm [label="QKV AllGather\nGPUs: 384-511\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_13_attn_tp_0 [label="Attention Computation\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_13_attn_tp_1 [label="Attention Computation\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_13_attn_tp_2 [label="Attention Computation\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_13_attn_tp_3 [label="Attention Computation\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_13_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_13_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_13_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_13_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_13_attn_allreduce [label="Attention AllReduce\nGPUs: 384-511\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_13_gate_tp_0 [label="Gate Computation\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_13_gate_tp_1 [label="Gate Computation\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_13_gate_tp_2 [label="Gate Computation\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_13_gate_tp_3 [label="Gate Computation\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_13_route_ep_0 [label="Expert Routing\nGPU: 384-399\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_13_route_ep_1 [label="Expert Routing\nGPU: 400-415\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_13_route_ep_2 [label="Expert Routing\nGPU: 416-431\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_13_route_ep_3 [label="Expert Routing\nGPU: 432-447\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_13_route_ep_4 [label="Expert Routing\nGPU: 448-463\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_13_route_ep_5 [label="Expert Routing\nGPU: 464-479\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_13_route_ep_6 [label="Expert Routing\nGPU: 480-495\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_13_route_ep_7 [label="Expert Routing\nGPU: 496-511\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_13_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 384\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 385\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 386\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 387\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 388\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 389\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 390\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 391\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 400\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 401\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 402\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 403\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 404\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 405\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 406\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 407\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 416\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 417\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 418\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 419\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 420\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 421\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 422\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 423\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 432\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 433\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 434\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 435\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 436\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 437\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 438\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 439\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 448\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 449\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 450\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 451\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 452\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 453\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 454\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 455\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 464\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 465\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 466\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 467\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 468\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 469\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 470\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 471\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 480\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 481\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 482\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 483\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 484\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 485\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 486\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 487\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 496\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 497\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 498\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 499\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 500\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 501\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 502\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 503\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_13_agg_ep_0 [label="Expert Aggregation\nGPU: 384-399\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_13_agg_ep_1 [label="Expert Aggregation\nGPU: 400-415\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_13_agg_ep_2 [label="Expert Aggregation\nGPU: 416-431\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_13_agg_ep_3 [label="Expert Aggregation\nGPU: 432-447\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_13_agg_ep_4 [label="Expert Aggregation\nGPU: 448-463\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_13_agg_ep_5 [label="Expert Aggregation\nGPU: 464-479\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_13_agg_ep_6 [label="Expert Aggregation\nGPU: 480-495\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_13_agg_ep_7 [label="Expert Aggregation\nGPU: 496-511\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_13_moe_alltoall [label="MoE All-to-All\nGPUs: 384-511\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_13_ln_tp_0 [label="Layer Norm\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_13_ln_tp_1 [label="Layer Norm\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_13_ln_tp_2 [label="Layer Norm\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_13_ln_tp_3 [label="Layer Norm\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
		subgraph cluster_layer_14 {
			fillcolor=white label="Layer 14" style="rounded,filled"
			layer_14_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_14_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_14_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_14_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_14_qkv_comm [label="QKV AllGather\nGPUs: 384-511\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_14_attn_tp_0 [label="Attention Computation\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_14_attn_tp_1 [label="Attention Computation\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_14_attn_tp_2 [label="Attention Computation\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_14_attn_tp_3 [label="Attention Computation\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_14_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_14_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_14_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_14_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_14_attn_allreduce [label="Attention AllReduce\nGPUs: 384-511\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_14_gate_tp_0 [label="Gate Computation\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_14_gate_tp_1 [label="Gate Computation\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_14_gate_tp_2 [label="Gate Computation\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_14_gate_tp_3 [label="Gate Computation\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_14_route_ep_0 [label="Expert Routing\nGPU: 384-399\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_14_route_ep_1 [label="Expert Routing\nGPU: 400-415\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_14_route_ep_2 [label="Expert Routing\nGPU: 416-431\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_14_route_ep_3 [label="Expert Routing\nGPU: 432-447\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_14_route_ep_4 [label="Expert Routing\nGPU: 448-463\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_14_route_ep_5 [label="Expert Routing\nGPU: 464-479\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_14_route_ep_6 [label="Expert Routing\nGPU: 480-495\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_14_route_ep_7 [label="Expert Routing\nGPU: 496-511\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_14_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 384\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 385\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 386\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 387\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 388\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 389\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 390\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 391\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 400\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 401\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 402\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 403\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 404\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 405\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 406\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 407\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 416\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 417\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 418\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 419\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 420\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 421\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 422\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 423\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 432\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 433\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 434\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 435\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 436\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 437\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 438\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 439\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 448\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 449\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 450\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 451\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 452\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 453\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 454\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 455\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 464\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 465\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 466\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 467\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 468\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 469\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 470\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 471\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 480\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 481\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 482\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 483\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 484\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 485\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 486\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 487\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 496\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 497\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 498\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 499\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 500\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 501\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 502\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 503\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_14_agg_ep_0 [label="Expert Aggregation\nGPU: 384-399\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_14_agg_ep_1 [label="Expert Aggregation\nGPU: 400-415\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_14_agg_ep_2 [label="Expert Aggregation\nGPU: 416-431\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_14_agg_ep_3 [label="Expert Aggregation\nGPU: 432-447\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_14_agg_ep_4 [label="Expert Aggregation\nGPU: 448-463\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_14_agg_ep_5 [label="Expert Aggregation\nGPU: 464-479\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_14_agg_ep_6 [label="Expert Aggregation\nGPU: 480-495\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_14_agg_ep_7 [label="Expert Aggregation\nGPU: 496-511\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_14_moe_alltoall [label="MoE All-to-All\nGPUs: 384-511\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_14_ln_tp_0 [label="Layer Norm\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_14_ln_tp_1 [label="Layer Norm\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_14_ln_tp_2 [label="Layer Norm\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_14_ln_tp_3 [label="Layer Norm\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
		subgraph cluster_layer_15 {
			fillcolor=white label="Layer 15" style="rounded,filled"
			layer_15_qkv_tp_0 [label="QKV Projection (TP)\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_15_qkv_tp_1 [label="QKV Projection (TP)\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_15_qkv_tp_2 [label="QKV Projection (TP)\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_15_qkv_tp_3 [label="QKV Projection (TP)\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=768]"]
			layer_15_qkv_comm [label="QKV AllGather\nGPUs: 384-511\nInput: [batch_size=32, seq_len=1024, hidden_size=768]\nOutput: [batch_size=128, seq_len=1024, hidden_size=768]" fillcolor=lightgreen shape=ellipse]
			layer_15_attn_tp_0 [label="Attention Computation\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_15_attn_tp_1 [label="Attention Computation\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_15_attn_tp_2 [label="Attention Computation\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_15_attn_tp_3 [label="Attention Computation\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=192]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_15_attn_out_tp_0 [label="Attention Output (TP)\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_15_attn_out_tp_1 [label="Attention Output (TP)\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_15_attn_out_tp_2 [label="Attention Output (TP)\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_15_attn_out_tp_3 [label="Attention Output (TP)\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]"]
			layer_15_attn_allreduce [label="Attention AllReduce\nGPUs: 384-511\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_15_gate_tp_0 [label="Gate Computation\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_15_gate_tp_1 [label="Gate Computation\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_15_gate_tp_2 [label="Gate Computation\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_15_gate_tp_3 [label="Gate Computation\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, expert_scores=64]"]
			layer_15_route_ep_0 [label="Expert Routing\nGPU: 384-399\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_15_route_ep_1 [label="Expert Routing\nGPU: 400-415\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_15_route_ep_2 [label="Expert Routing\nGPU: 416-431\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_15_route_ep_3 [label="Expert Routing\nGPU: 432-447\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_15_route_ep_4 [label="Expert Routing\nGPU: 448-463\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_15_route_ep_5 [label="Expert Routing\nGPU: 464-479\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_15_route_ep_6 [label="Expert Routing\nGPU: 480-495\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_15_route_ep_7 [label="Expert Routing\nGPU: 496-511\nInput: [batch_size=16, seq_len=1024, expert_ids=2]\nOutput: [batch_size=16, seq_len=1024, routed_tokens=512]" fillcolor=lightyellow shape=parallelogram]
			layer_15_expert_0_ep_0 [label="Expert 0 (EP)\nGPU: 384\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_1_ep_0 [label="Expert 1 (EP)\nGPU: 385\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_2_ep_0 [label="Expert 2 (EP)\nGPU: 386\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_3_ep_0 [label="Expert 3 (EP)\nGPU: 387\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_4_ep_0 [label="Expert 4 (EP)\nGPU: 388\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_5_ep_0 [label="Expert 5 (EP)\nGPU: 389\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_6_ep_0 [label="Expert 6 (EP)\nGPU: 390\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_7_ep_0 [label="Expert 7 (EP)\nGPU: 391\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_0_ep_1 [label="Expert 0 (EP)\nGPU: 400\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_1_ep_1 [label="Expert 1 (EP)\nGPU: 401\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_2_ep_1 [label="Expert 2 (EP)\nGPU: 402\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_3_ep_1 [label="Expert 3 (EP)\nGPU: 403\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_4_ep_1 [label="Expert 4 (EP)\nGPU: 404\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_5_ep_1 [label="Expert 5 (EP)\nGPU: 405\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_6_ep_1 [label="Expert 6 (EP)\nGPU: 406\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_7_ep_1 [label="Expert 7 (EP)\nGPU: 407\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_0_ep_2 [label="Expert 0 (EP)\nGPU: 416\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_1_ep_2 [label="Expert 1 (EP)\nGPU: 417\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_2_ep_2 [label="Expert 2 (EP)\nGPU: 418\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_3_ep_2 [label="Expert 3 (EP)\nGPU: 419\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_4_ep_2 [label="Expert 4 (EP)\nGPU: 420\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_5_ep_2 [label="Expert 5 (EP)\nGPU: 421\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_6_ep_2 [label="Expert 6 (EP)\nGPU: 422\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_7_ep_2 [label="Expert 7 (EP)\nGPU: 423\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_0_ep_3 [label="Expert 0 (EP)\nGPU: 432\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_1_ep_3 [label="Expert 1 (EP)\nGPU: 433\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_2_ep_3 [label="Expert 2 (EP)\nGPU: 434\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_3_ep_3 [label="Expert 3 (EP)\nGPU: 435\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_4_ep_3 [label="Expert 4 (EP)\nGPU: 436\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_5_ep_3 [label="Expert 5 (EP)\nGPU: 437\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_6_ep_3 [label="Expert 6 (EP)\nGPU: 438\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_7_ep_3 [label="Expert 7 (EP)\nGPU: 439\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_0_ep_4 [label="Expert 0 (EP)\nGPU: 448\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_1_ep_4 [label="Expert 1 (EP)\nGPU: 449\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_2_ep_4 [label="Expert 2 (EP)\nGPU: 450\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_3_ep_4 [label="Expert 3 (EP)\nGPU: 451\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_4_ep_4 [label="Expert 4 (EP)\nGPU: 452\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_5_ep_4 [label="Expert 5 (EP)\nGPU: 453\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_6_ep_4 [label="Expert 6 (EP)\nGPU: 454\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_7_ep_4 [label="Expert 7 (EP)\nGPU: 455\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_0_ep_5 [label="Expert 0 (EP)\nGPU: 464\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_1_ep_5 [label="Expert 1 (EP)\nGPU: 465\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_2_ep_5 [label="Expert 2 (EP)\nGPU: 466\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_3_ep_5 [label="Expert 3 (EP)\nGPU: 467\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_4_ep_5 [label="Expert 4 (EP)\nGPU: 468\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_5_ep_5 [label="Expert 5 (EP)\nGPU: 469\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_6_ep_5 [label="Expert 6 (EP)\nGPU: 470\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_7_ep_5 [label="Expert 7 (EP)\nGPU: 471\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_0_ep_6 [label="Expert 0 (EP)\nGPU: 480\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_1_ep_6 [label="Expert 1 (EP)\nGPU: 481\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_2_ep_6 [label="Expert 2 (EP)\nGPU: 482\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_3_ep_6 [label="Expert 3 (EP)\nGPU: 483\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_4_ep_6 [label="Expert 4 (EP)\nGPU: 484\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_5_ep_6 [label="Expert 5 (EP)\nGPU: 485\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_6_ep_6 [label="Expert 6 (EP)\nGPU: 486\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_7_ep_6 [label="Expert 7 (EP)\nGPU: 487\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_0_ep_7 [label="Expert 0 (EP)\nGPU: 496\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_1_ep_7 [label="Expert 1 (EP)\nGPU: 497\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_2_ep_7 [label="Expert 2 (EP)\nGPU: 498\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_3_ep_7 [label="Expert 3 (EP)\nGPU: 499\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_4_ep_7 [label="Expert 4 (EP)\nGPU: 500\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_5_ep_7 [label="Expert 5 (EP)\nGPU: 501\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_6_ep_7 [label="Expert 6 (EP)\nGPU: 502\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_expert_7_ep_7 [label="Expert 7 (EP)\nGPU: 503\nInput: [batch_size=2, seq_len=512, hidden_size=1024]\nOutput: [batch_size=2, seq_len=512, hidden_size=1024]"]
			layer_15_agg_ep_0 [label="Expert Aggregation\nGPU: 384-399\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_15_agg_ep_1 [label="Expert Aggregation\nGPU: 400-415\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_15_agg_ep_2 [label="Expert Aggregation\nGPU: 416-431\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_15_agg_ep_3 [label="Expert Aggregation\nGPU: 432-447\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_15_agg_ep_4 [label="Expert Aggregation\nGPU: 448-463\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_15_agg_ep_5 [label="Expert Aggregation\nGPU: 464-479\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_15_agg_ep_6 [label="Expert Aggregation\nGPU: 480-495\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_15_agg_ep_7 [label="Expert Aggregation\nGPU: 496-511\nInput: [batch_size=16, seq_len=1024, expert_outputs=8]\nOutput: [batch_size=16, seq_len=1024, hidden_size=1024]" fillcolor=lightyellow shape=parallelogram]
			layer_15_moe_alltoall [label="MoE All-to-All\nGPUs: 384-511\nInput: [batch_size=16, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_size=1024]" fillcolor=lightgreen shape=ellipse]
			layer_15_ln_tp_0 [label="Layer Norm\nGPU: 384-415\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_15_ln_tp_1 [label="Layer Norm\nGPU: 416-447\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_15_ln_tp_2 [label="Layer Norm\nGPU: 448-479\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
			layer_15_ln_tp_3 [label="Layer Norm\nGPU: 480-511\nInput: [batch_size=32, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=32, seq_len=1024, hidden_size=1024]"]
		}
	}
	subgraph cluster_output {
		fillcolor=lightgray label="Output Layer" style="rounded,filled"
		output [label="Output Projection\nGPU: 0-511\nInput: [batch_size=128, seq_len=1024, hidden_size=1024]\nOutput: [batch_size=128, seq_len=1024, vocab_size=32000]"]
	}
	input -> layer_0_qkv_tp_0 [label="Data Parallel Split"]
	input -> layer_0_qkv_tp_1 [label="Data Parallel Split"]
	input -> layer_0_qkv_tp_2 [label="Data Parallel Split"]
	input -> layer_0_qkv_tp_3 [label="Data Parallel Split"]
	layer_0_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_0_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_0_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_0_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_0_attn_tp_0 -> layer_0_attn_out_tp_0
	layer_0_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_0_attn_tp_1 -> layer_0_attn_out_tp_1
	layer_0_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_0_attn_tp_2 -> layer_0_attn_out_tp_2
	layer_0_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_0_attn_tp_3 -> layer_0_attn_out_tp_3
	layer_0_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_0_gate_tp_0 -> layer_0_route_ep_0 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_0 -> layer_0_route_ep_1 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_0 -> layer_0_route_ep_2 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_0 -> layer_0_route_ep_3 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_0 -> layer_0_route_ep_4 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_0 -> layer_0_route_ep_5 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_0 -> layer_0_route_ep_6 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_0 -> layer_0_route_ep_7 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_1 -> layer_0_route_ep_0 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_1 -> layer_0_route_ep_1 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_1 -> layer_0_route_ep_2 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_1 -> layer_0_route_ep_3 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_1 -> layer_0_route_ep_4 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_1 -> layer_0_route_ep_5 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_1 -> layer_0_route_ep_6 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_1 -> layer_0_route_ep_7 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_2 -> layer_0_route_ep_0 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_2 -> layer_0_route_ep_1 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_2 -> layer_0_route_ep_2 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_2 -> layer_0_route_ep_3 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_2 -> layer_0_route_ep_4 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_2 -> layer_0_route_ep_5 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_2 -> layer_0_route_ep_6 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_2 -> layer_0_route_ep_7 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_3 -> layer_0_route_ep_0 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_3 -> layer_0_route_ep_1 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_3 -> layer_0_route_ep_2 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_3 -> layer_0_route_ep_3 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_3 -> layer_0_route_ep_4 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_3 -> layer_0_route_ep_5 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_3 -> layer_0_route_ep_6 [label="Gate Selection" style=dashed]
	layer_0_gate_tp_3 -> layer_0_route_ep_7 [label="Gate Selection" style=dashed]
	layer_0_route_ep_0 -> layer_0_expert_0_ep_0
	layer_0_route_ep_0 -> layer_0_expert_1_ep_0
	layer_0_route_ep_0 -> layer_0_expert_2_ep_0
	layer_0_route_ep_0 -> layer_0_expert_3_ep_0
	layer_0_route_ep_0 -> layer_0_expert_4_ep_0
	layer_0_route_ep_0 -> layer_0_expert_5_ep_0
	layer_0_route_ep_0 -> layer_0_expert_6_ep_0
	layer_0_route_ep_0 -> layer_0_expert_7_ep_0
	layer_0_route_ep_1 -> layer_0_expert_0_ep_1
	layer_0_route_ep_1 -> layer_0_expert_1_ep_1
	layer_0_route_ep_1 -> layer_0_expert_2_ep_1
	layer_0_route_ep_1 -> layer_0_expert_3_ep_1
	layer_0_route_ep_1 -> layer_0_expert_4_ep_1
	layer_0_route_ep_1 -> layer_0_expert_5_ep_1
	layer_0_route_ep_1 -> layer_0_expert_6_ep_1
	layer_0_route_ep_1 -> layer_0_expert_7_ep_1
	layer_0_route_ep_2 -> layer_0_expert_0_ep_2
	layer_0_route_ep_2 -> layer_0_expert_1_ep_2
	layer_0_route_ep_2 -> layer_0_expert_2_ep_2
	layer_0_route_ep_2 -> layer_0_expert_3_ep_2
	layer_0_route_ep_2 -> layer_0_expert_4_ep_2
	layer_0_route_ep_2 -> layer_0_expert_5_ep_2
	layer_0_route_ep_2 -> layer_0_expert_6_ep_2
	layer_0_route_ep_2 -> layer_0_expert_7_ep_2
	layer_0_route_ep_3 -> layer_0_expert_0_ep_3
	layer_0_route_ep_3 -> layer_0_expert_1_ep_3
	layer_0_route_ep_3 -> layer_0_expert_2_ep_3
	layer_0_route_ep_3 -> layer_0_expert_3_ep_3
	layer_0_route_ep_3 -> layer_0_expert_4_ep_3
	layer_0_route_ep_3 -> layer_0_expert_5_ep_3
	layer_0_route_ep_3 -> layer_0_expert_6_ep_3
	layer_0_route_ep_3 -> layer_0_expert_7_ep_3
	layer_0_route_ep_4 -> layer_0_expert_0_ep_4
	layer_0_route_ep_4 -> layer_0_expert_1_ep_4
	layer_0_route_ep_4 -> layer_0_expert_2_ep_4
	layer_0_route_ep_4 -> layer_0_expert_3_ep_4
	layer_0_route_ep_4 -> layer_0_expert_4_ep_4
	layer_0_route_ep_4 -> layer_0_expert_5_ep_4
	layer_0_route_ep_4 -> layer_0_expert_6_ep_4
	layer_0_route_ep_4 -> layer_0_expert_7_ep_4
	layer_0_route_ep_5 -> layer_0_expert_0_ep_5
	layer_0_route_ep_5 -> layer_0_expert_1_ep_5
	layer_0_route_ep_5 -> layer_0_expert_2_ep_5
	layer_0_route_ep_5 -> layer_0_expert_3_ep_5
	layer_0_route_ep_5 -> layer_0_expert_4_ep_5
	layer_0_route_ep_5 -> layer_0_expert_5_ep_5
	layer_0_route_ep_5 -> layer_0_expert_6_ep_5
	layer_0_route_ep_5 -> layer_0_expert_7_ep_5
	layer_0_route_ep_6 -> layer_0_expert_0_ep_6
	layer_0_route_ep_6 -> layer_0_expert_1_ep_6
	layer_0_route_ep_6 -> layer_0_expert_2_ep_6
	layer_0_route_ep_6 -> layer_0_expert_3_ep_6
	layer_0_route_ep_6 -> layer_0_expert_4_ep_6
	layer_0_route_ep_6 -> layer_0_expert_5_ep_6
	layer_0_route_ep_6 -> layer_0_expert_6_ep_6
	layer_0_route_ep_6 -> layer_0_expert_7_ep_6
	layer_0_route_ep_7 -> layer_0_expert_0_ep_7
	layer_0_route_ep_7 -> layer_0_expert_1_ep_7
	layer_0_route_ep_7 -> layer_0_expert_2_ep_7
	layer_0_route_ep_7 -> layer_0_expert_3_ep_7
	layer_0_route_ep_7 -> layer_0_expert_4_ep_7
	layer_0_route_ep_7 -> layer_0_expert_5_ep_7
	layer_0_route_ep_7 -> layer_0_expert_6_ep_7
	layer_0_route_ep_7 -> layer_0_expert_7_ep_7
	layer_0_expert_0_ep_0 -> layer_0_agg_ep_0
	layer_0_expert_1_ep_0 -> layer_0_agg_ep_0
	layer_0_expert_2_ep_0 -> layer_0_agg_ep_0
	layer_0_expert_3_ep_0 -> layer_0_agg_ep_0
	layer_0_expert_4_ep_0 -> layer_0_agg_ep_0
	layer_0_expert_5_ep_0 -> layer_0_agg_ep_0
	layer_0_expert_6_ep_0 -> layer_0_agg_ep_0
	layer_0_expert_7_ep_0 -> layer_0_agg_ep_0
	layer_0_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_0_expert_0_ep_1 -> layer_0_agg_ep_1
	layer_0_expert_1_ep_1 -> layer_0_agg_ep_1
	layer_0_expert_2_ep_1 -> layer_0_agg_ep_1
	layer_0_expert_3_ep_1 -> layer_0_agg_ep_1
	layer_0_expert_4_ep_1 -> layer_0_agg_ep_1
	layer_0_expert_5_ep_1 -> layer_0_agg_ep_1
	layer_0_expert_6_ep_1 -> layer_0_agg_ep_1
	layer_0_expert_7_ep_1 -> layer_0_agg_ep_1
	layer_0_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_0_expert_0_ep_2 -> layer_0_agg_ep_2
	layer_0_expert_1_ep_2 -> layer_0_agg_ep_2
	layer_0_expert_2_ep_2 -> layer_0_agg_ep_2
	layer_0_expert_3_ep_2 -> layer_0_agg_ep_2
	layer_0_expert_4_ep_2 -> layer_0_agg_ep_2
	layer_0_expert_5_ep_2 -> layer_0_agg_ep_2
	layer_0_expert_6_ep_2 -> layer_0_agg_ep_2
	layer_0_expert_7_ep_2 -> layer_0_agg_ep_2
	layer_0_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_0_expert_0_ep_3 -> layer_0_agg_ep_3
	layer_0_expert_1_ep_3 -> layer_0_agg_ep_3
	layer_0_expert_2_ep_3 -> layer_0_agg_ep_3
	layer_0_expert_3_ep_3 -> layer_0_agg_ep_3
	layer_0_expert_4_ep_3 -> layer_0_agg_ep_3
	layer_0_expert_5_ep_3 -> layer_0_agg_ep_3
	layer_0_expert_6_ep_3 -> layer_0_agg_ep_3
	layer_0_expert_7_ep_3 -> layer_0_agg_ep_3
	layer_0_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_0_expert_0_ep_4 -> layer_0_agg_ep_4
	layer_0_expert_1_ep_4 -> layer_0_agg_ep_4
	layer_0_expert_2_ep_4 -> layer_0_agg_ep_4
	layer_0_expert_3_ep_4 -> layer_0_agg_ep_4
	layer_0_expert_4_ep_4 -> layer_0_agg_ep_4
	layer_0_expert_5_ep_4 -> layer_0_agg_ep_4
	layer_0_expert_6_ep_4 -> layer_0_agg_ep_4
	layer_0_expert_7_ep_4 -> layer_0_agg_ep_4
	layer_0_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_0_expert_0_ep_5 -> layer_0_agg_ep_5
	layer_0_expert_1_ep_5 -> layer_0_agg_ep_5
	layer_0_expert_2_ep_5 -> layer_0_agg_ep_5
	layer_0_expert_3_ep_5 -> layer_0_agg_ep_5
	layer_0_expert_4_ep_5 -> layer_0_agg_ep_5
	layer_0_expert_5_ep_5 -> layer_0_agg_ep_5
	layer_0_expert_6_ep_5 -> layer_0_agg_ep_5
	layer_0_expert_7_ep_5 -> layer_0_agg_ep_5
	layer_0_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_0_expert_0_ep_6 -> layer_0_agg_ep_6
	layer_0_expert_1_ep_6 -> layer_0_agg_ep_6
	layer_0_expert_2_ep_6 -> layer_0_agg_ep_6
	layer_0_expert_3_ep_6 -> layer_0_agg_ep_6
	layer_0_expert_4_ep_6 -> layer_0_agg_ep_6
	layer_0_expert_5_ep_6 -> layer_0_agg_ep_6
	layer_0_expert_6_ep_6 -> layer_0_agg_ep_6
	layer_0_expert_7_ep_6 -> layer_0_agg_ep_6
	layer_0_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_0_expert_0_ep_7 -> layer_0_agg_ep_7
	layer_0_expert_1_ep_7 -> layer_0_agg_ep_7
	layer_0_expert_2_ep_7 -> layer_0_agg_ep_7
	layer_0_expert_3_ep_7 -> layer_0_agg_ep_7
	layer_0_expert_4_ep_7 -> layer_0_agg_ep_7
	layer_0_expert_5_ep_7 -> layer_0_agg_ep_7
	layer_0_expert_6_ep_7 -> layer_0_agg_ep_7
	layer_0_expert_7_ep_7 -> layer_0_agg_ep_7
	layer_0_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_0_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_0_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_0_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_0_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_0_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_0_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_0_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_0_ln_tp_3
	layer_1_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_1_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_1_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_1_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_1_attn_tp_0 -> layer_1_attn_out_tp_0
	layer_1_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_1_attn_tp_1 -> layer_1_attn_out_tp_1
	layer_1_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_1_attn_tp_2 -> layer_1_attn_out_tp_2
	layer_1_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_1_attn_tp_3 -> layer_1_attn_out_tp_3
	layer_1_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_1_gate_tp_0 -> layer_1_route_ep_0 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_0 -> layer_1_route_ep_1 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_0 -> layer_1_route_ep_2 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_0 -> layer_1_route_ep_3 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_0 -> layer_1_route_ep_4 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_0 -> layer_1_route_ep_5 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_0 -> layer_1_route_ep_6 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_0 -> layer_1_route_ep_7 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_1 -> layer_1_route_ep_0 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_1 -> layer_1_route_ep_1 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_1 -> layer_1_route_ep_2 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_1 -> layer_1_route_ep_3 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_1 -> layer_1_route_ep_4 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_1 -> layer_1_route_ep_5 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_1 -> layer_1_route_ep_6 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_1 -> layer_1_route_ep_7 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_2 -> layer_1_route_ep_0 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_2 -> layer_1_route_ep_1 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_2 -> layer_1_route_ep_2 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_2 -> layer_1_route_ep_3 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_2 -> layer_1_route_ep_4 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_2 -> layer_1_route_ep_5 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_2 -> layer_1_route_ep_6 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_2 -> layer_1_route_ep_7 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_3 -> layer_1_route_ep_0 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_3 -> layer_1_route_ep_1 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_3 -> layer_1_route_ep_2 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_3 -> layer_1_route_ep_3 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_3 -> layer_1_route_ep_4 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_3 -> layer_1_route_ep_5 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_3 -> layer_1_route_ep_6 [label="Gate Selection" style=dashed]
	layer_1_gate_tp_3 -> layer_1_route_ep_7 [label="Gate Selection" style=dashed]
	layer_1_route_ep_0 -> layer_1_expert_0_ep_0
	layer_1_route_ep_0 -> layer_1_expert_1_ep_0
	layer_1_route_ep_0 -> layer_1_expert_2_ep_0
	layer_1_route_ep_0 -> layer_1_expert_3_ep_0
	layer_1_route_ep_0 -> layer_1_expert_4_ep_0
	layer_1_route_ep_0 -> layer_1_expert_5_ep_0
	layer_1_route_ep_0 -> layer_1_expert_6_ep_0
	layer_1_route_ep_0 -> layer_1_expert_7_ep_0
	layer_1_route_ep_1 -> layer_1_expert_0_ep_1
	layer_1_route_ep_1 -> layer_1_expert_1_ep_1
	layer_1_route_ep_1 -> layer_1_expert_2_ep_1
	layer_1_route_ep_1 -> layer_1_expert_3_ep_1
	layer_1_route_ep_1 -> layer_1_expert_4_ep_1
	layer_1_route_ep_1 -> layer_1_expert_5_ep_1
	layer_1_route_ep_1 -> layer_1_expert_6_ep_1
	layer_1_route_ep_1 -> layer_1_expert_7_ep_1
	layer_1_route_ep_2 -> layer_1_expert_0_ep_2
	layer_1_route_ep_2 -> layer_1_expert_1_ep_2
	layer_1_route_ep_2 -> layer_1_expert_2_ep_2
	layer_1_route_ep_2 -> layer_1_expert_3_ep_2
	layer_1_route_ep_2 -> layer_1_expert_4_ep_2
	layer_1_route_ep_2 -> layer_1_expert_5_ep_2
	layer_1_route_ep_2 -> layer_1_expert_6_ep_2
	layer_1_route_ep_2 -> layer_1_expert_7_ep_2
	layer_1_route_ep_3 -> layer_1_expert_0_ep_3
	layer_1_route_ep_3 -> layer_1_expert_1_ep_3
	layer_1_route_ep_3 -> layer_1_expert_2_ep_3
	layer_1_route_ep_3 -> layer_1_expert_3_ep_3
	layer_1_route_ep_3 -> layer_1_expert_4_ep_3
	layer_1_route_ep_3 -> layer_1_expert_5_ep_3
	layer_1_route_ep_3 -> layer_1_expert_6_ep_3
	layer_1_route_ep_3 -> layer_1_expert_7_ep_3
	layer_1_route_ep_4 -> layer_1_expert_0_ep_4
	layer_1_route_ep_4 -> layer_1_expert_1_ep_4
	layer_1_route_ep_4 -> layer_1_expert_2_ep_4
	layer_1_route_ep_4 -> layer_1_expert_3_ep_4
	layer_1_route_ep_4 -> layer_1_expert_4_ep_4
	layer_1_route_ep_4 -> layer_1_expert_5_ep_4
	layer_1_route_ep_4 -> layer_1_expert_6_ep_4
	layer_1_route_ep_4 -> layer_1_expert_7_ep_4
	layer_1_route_ep_5 -> layer_1_expert_0_ep_5
	layer_1_route_ep_5 -> layer_1_expert_1_ep_5
	layer_1_route_ep_5 -> layer_1_expert_2_ep_5
	layer_1_route_ep_5 -> layer_1_expert_3_ep_5
	layer_1_route_ep_5 -> layer_1_expert_4_ep_5
	layer_1_route_ep_5 -> layer_1_expert_5_ep_5
	layer_1_route_ep_5 -> layer_1_expert_6_ep_5
	layer_1_route_ep_5 -> layer_1_expert_7_ep_5
	layer_1_route_ep_6 -> layer_1_expert_0_ep_6
	layer_1_route_ep_6 -> layer_1_expert_1_ep_6
	layer_1_route_ep_6 -> layer_1_expert_2_ep_6
	layer_1_route_ep_6 -> layer_1_expert_3_ep_6
	layer_1_route_ep_6 -> layer_1_expert_4_ep_6
	layer_1_route_ep_6 -> layer_1_expert_5_ep_6
	layer_1_route_ep_6 -> layer_1_expert_6_ep_6
	layer_1_route_ep_6 -> layer_1_expert_7_ep_6
	layer_1_route_ep_7 -> layer_1_expert_0_ep_7
	layer_1_route_ep_7 -> layer_1_expert_1_ep_7
	layer_1_route_ep_7 -> layer_1_expert_2_ep_7
	layer_1_route_ep_7 -> layer_1_expert_3_ep_7
	layer_1_route_ep_7 -> layer_1_expert_4_ep_7
	layer_1_route_ep_7 -> layer_1_expert_5_ep_7
	layer_1_route_ep_7 -> layer_1_expert_6_ep_7
	layer_1_route_ep_7 -> layer_1_expert_7_ep_7
	layer_1_expert_0_ep_0 -> layer_1_agg_ep_0
	layer_1_expert_1_ep_0 -> layer_1_agg_ep_0
	layer_1_expert_2_ep_0 -> layer_1_agg_ep_0
	layer_1_expert_3_ep_0 -> layer_1_agg_ep_0
	layer_1_expert_4_ep_0 -> layer_1_agg_ep_0
	layer_1_expert_5_ep_0 -> layer_1_agg_ep_0
	layer_1_expert_6_ep_0 -> layer_1_agg_ep_0
	layer_1_expert_7_ep_0 -> layer_1_agg_ep_0
	layer_1_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_1_expert_0_ep_1 -> layer_1_agg_ep_1
	layer_1_expert_1_ep_1 -> layer_1_agg_ep_1
	layer_1_expert_2_ep_1 -> layer_1_agg_ep_1
	layer_1_expert_3_ep_1 -> layer_1_agg_ep_1
	layer_1_expert_4_ep_1 -> layer_1_agg_ep_1
	layer_1_expert_5_ep_1 -> layer_1_agg_ep_1
	layer_1_expert_6_ep_1 -> layer_1_agg_ep_1
	layer_1_expert_7_ep_1 -> layer_1_agg_ep_1
	layer_1_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_1_expert_0_ep_2 -> layer_1_agg_ep_2
	layer_1_expert_1_ep_2 -> layer_1_agg_ep_2
	layer_1_expert_2_ep_2 -> layer_1_agg_ep_2
	layer_1_expert_3_ep_2 -> layer_1_agg_ep_2
	layer_1_expert_4_ep_2 -> layer_1_agg_ep_2
	layer_1_expert_5_ep_2 -> layer_1_agg_ep_2
	layer_1_expert_6_ep_2 -> layer_1_agg_ep_2
	layer_1_expert_7_ep_2 -> layer_1_agg_ep_2
	layer_1_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_1_expert_0_ep_3 -> layer_1_agg_ep_3
	layer_1_expert_1_ep_3 -> layer_1_agg_ep_3
	layer_1_expert_2_ep_3 -> layer_1_agg_ep_3
	layer_1_expert_3_ep_3 -> layer_1_agg_ep_3
	layer_1_expert_4_ep_3 -> layer_1_agg_ep_3
	layer_1_expert_5_ep_3 -> layer_1_agg_ep_3
	layer_1_expert_6_ep_3 -> layer_1_agg_ep_3
	layer_1_expert_7_ep_3 -> layer_1_agg_ep_3
	layer_1_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_1_expert_0_ep_4 -> layer_1_agg_ep_4
	layer_1_expert_1_ep_4 -> layer_1_agg_ep_4
	layer_1_expert_2_ep_4 -> layer_1_agg_ep_4
	layer_1_expert_3_ep_4 -> layer_1_agg_ep_4
	layer_1_expert_4_ep_4 -> layer_1_agg_ep_4
	layer_1_expert_5_ep_4 -> layer_1_agg_ep_4
	layer_1_expert_6_ep_4 -> layer_1_agg_ep_4
	layer_1_expert_7_ep_4 -> layer_1_agg_ep_4
	layer_1_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_1_expert_0_ep_5 -> layer_1_agg_ep_5
	layer_1_expert_1_ep_5 -> layer_1_agg_ep_5
	layer_1_expert_2_ep_5 -> layer_1_agg_ep_5
	layer_1_expert_3_ep_5 -> layer_1_agg_ep_5
	layer_1_expert_4_ep_5 -> layer_1_agg_ep_5
	layer_1_expert_5_ep_5 -> layer_1_agg_ep_5
	layer_1_expert_6_ep_5 -> layer_1_agg_ep_5
	layer_1_expert_7_ep_5 -> layer_1_agg_ep_5
	layer_1_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_1_expert_0_ep_6 -> layer_1_agg_ep_6
	layer_1_expert_1_ep_6 -> layer_1_agg_ep_6
	layer_1_expert_2_ep_6 -> layer_1_agg_ep_6
	layer_1_expert_3_ep_6 -> layer_1_agg_ep_6
	layer_1_expert_4_ep_6 -> layer_1_agg_ep_6
	layer_1_expert_5_ep_6 -> layer_1_agg_ep_6
	layer_1_expert_6_ep_6 -> layer_1_agg_ep_6
	layer_1_expert_7_ep_6 -> layer_1_agg_ep_6
	layer_1_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_1_expert_0_ep_7 -> layer_1_agg_ep_7
	layer_1_expert_1_ep_7 -> layer_1_agg_ep_7
	layer_1_expert_2_ep_7 -> layer_1_agg_ep_7
	layer_1_expert_3_ep_7 -> layer_1_agg_ep_7
	layer_1_expert_4_ep_7 -> layer_1_agg_ep_7
	layer_1_expert_5_ep_7 -> layer_1_agg_ep_7
	layer_1_expert_6_ep_7 -> layer_1_agg_ep_7
	layer_1_expert_7_ep_7 -> layer_1_agg_ep_7
	layer_1_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_1_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_1_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_1_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_1_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_1_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_1_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_1_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_1_ln_tp_3
	layer_2_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_2_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_2_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_2_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_2_attn_tp_0 -> layer_2_attn_out_tp_0
	layer_2_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_2_attn_tp_1 -> layer_2_attn_out_tp_1
	layer_2_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_2_attn_tp_2 -> layer_2_attn_out_tp_2
	layer_2_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_2_attn_tp_3 -> layer_2_attn_out_tp_3
	layer_2_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_2_gate_tp_0 -> layer_2_route_ep_0 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_0 -> layer_2_route_ep_1 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_0 -> layer_2_route_ep_2 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_0 -> layer_2_route_ep_3 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_0 -> layer_2_route_ep_4 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_0 -> layer_2_route_ep_5 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_0 -> layer_2_route_ep_6 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_0 -> layer_2_route_ep_7 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_1 -> layer_2_route_ep_0 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_1 -> layer_2_route_ep_1 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_1 -> layer_2_route_ep_2 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_1 -> layer_2_route_ep_3 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_1 -> layer_2_route_ep_4 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_1 -> layer_2_route_ep_5 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_1 -> layer_2_route_ep_6 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_1 -> layer_2_route_ep_7 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_2 -> layer_2_route_ep_0 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_2 -> layer_2_route_ep_1 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_2 -> layer_2_route_ep_2 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_2 -> layer_2_route_ep_3 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_2 -> layer_2_route_ep_4 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_2 -> layer_2_route_ep_5 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_2 -> layer_2_route_ep_6 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_2 -> layer_2_route_ep_7 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_3 -> layer_2_route_ep_0 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_3 -> layer_2_route_ep_1 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_3 -> layer_2_route_ep_2 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_3 -> layer_2_route_ep_3 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_3 -> layer_2_route_ep_4 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_3 -> layer_2_route_ep_5 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_3 -> layer_2_route_ep_6 [label="Gate Selection" style=dashed]
	layer_2_gate_tp_3 -> layer_2_route_ep_7 [label="Gate Selection" style=dashed]
	layer_2_route_ep_0 -> layer_2_expert_0_ep_0
	layer_2_route_ep_0 -> layer_2_expert_1_ep_0
	layer_2_route_ep_0 -> layer_2_expert_2_ep_0
	layer_2_route_ep_0 -> layer_2_expert_3_ep_0
	layer_2_route_ep_0 -> layer_2_expert_4_ep_0
	layer_2_route_ep_0 -> layer_2_expert_5_ep_0
	layer_2_route_ep_0 -> layer_2_expert_6_ep_0
	layer_2_route_ep_0 -> layer_2_expert_7_ep_0
	layer_2_route_ep_1 -> layer_2_expert_0_ep_1
	layer_2_route_ep_1 -> layer_2_expert_1_ep_1
	layer_2_route_ep_1 -> layer_2_expert_2_ep_1
	layer_2_route_ep_1 -> layer_2_expert_3_ep_1
	layer_2_route_ep_1 -> layer_2_expert_4_ep_1
	layer_2_route_ep_1 -> layer_2_expert_5_ep_1
	layer_2_route_ep_1 -> layer_2_expert_6_ep_1
	layer_2_route_ep_1 -> layer_2_expert_7_ep_1
	layer_2_route_ep_2 -> layer_2_expert_0_ep_2
	layer_2_route_ep_2 -> layer_2_expert_1_ep_2
	layer_2_route_ep_2 -> layer_2_expert_2_ep_2
	layer_2_route_ep_2 -> layer_2_expert_3_ep_2
	layer_2_route_ep_2 -> layer_2_expert_4_ep_2
	layer_2_route_ep_2 -> layer_2_expert_5_ep_2
	layer_2_route_ep_2 -> layer_2_expert_6_ep_2
	layer_2_route_ep_2 -> layer_2_expert_7_ep_2
	layer_2_route_ep_3 -> layer_2_expert_0_ep_3
	layer_2_route_ep_3 -> layer_2_expert_1_ep_3
	layer_2_route_ep_3 -> layer_2_expert_2_ep_3
	layer_2_route_ep_3 -> layer_2_expert_3_ep_3
	layer_2_route_ep_3 -> layer_2_expert_4_ep_3
	layer_2_route_ep_3 -> layer_2_expert_5_ep_3
	layer_2_route_ep_3 -> layer_2_expert_6_ep_3
	layer_2_route_ep_3 -> layer_2_expert_7_ep_3
	layer_2_route_ep_4 -> layer_2_expert_0_ep_4
	layer_2_route_ep_4 -> layer_2_expert_1_ep_4
	layer_2_route_ep_4 -> layer_2_expert_2_ep_4
	layer_2_route_ep_4 -> layer_2_expert_3_ep_4
	layer_2_route_ep_4 -> layer_2_expert_4_ep_4
	layer_2_route_ep_4 -> layer_2_expert_5_ep_4
	layer_2_route_ep_4 -> layer_2_expert_6_ep_4
	layer_2_route_ep_4 -> layer_2_expert_7_ep_4
	layer_2_route_ep_5 -> layer_2_expert_0_ep_5
	layer_2_route_ep_5 -> layer_2_expert_1_ep_5
	layer_2_route_ep_5 -> layer_2_expert_2_ep_5
	layer_2_route_ep_5 -> layer_2_expert_3_ep_5
	layer_2_route_ep_5 -> layer_2_expert_4_ep_5
	layer_2_route_ep_5 -> layer_2_expert_5_ep_5
	layer_2_route_ep_5 -> layer_2_expert_6_ep_5
	layer_2_route_ep_5 -> layer_2_expert_7_ep_5
	layer_2_route_ep_6 -> layer_2_expert_0_ep_6
	layer_2_route_ep_6 -> layer_2_expert_1_ep_6
	layer_2_route_ep_6 -> layer_2_expert_2_ep_6
	layer_2_route_ep_6 -> layer_2_expert_3_ep_6
	layer_2_route_ep_6 -> layer_2_expert_4_ep_6
	layer_2_route_ep_6 -> layer_2_expert_5_ep_6
	layer_2_route_ep_6 -> layer_2_expert_6_ep_6
	layer_2_route_ep_6 -> layer_2_expert_7_ep_6
	layer_2_route_ep_7 -> layer_2_expert_0_ep_7
	layer_2_route_ep_7 -> layer_2_expert_1_ep_7
	layer_2_route_ep_7 -> layer_2_expert_2_ep_7
	layer_2_route_ep_7 -> layer_2_expert_3_ep_7
	layer_2_route_ep_7 -> layer_2_expert_4_ep_7
	layer_2_route_ep_7 -> layer_2_expert_5_ep_7
	layer_2_route_ep_7 -> layer_2_expert_6_ep_7
	layer_2_route_ep_7 -> layer_2_expert_7_ep_7
	layer_2_expert_0_ep_0 -> layer_2_agg_ep_0
	layer_2_expert_1_ep_0 -> layer_2_agg_ep_0
	layer_2_expert_2_ep_0 -> layer_2_agg_ep_0
	layer_2_expert_3_ep_0 -> layer_2_agg_ep_0
	layer_2_expert_4_ep_0 -> layer_2_agg_ep_0
	layer_2_expert_5_ep_0 -> layer_2_agg_ep_0
	layer_2_expert_6_ep_0 -> layer_2_agg_ep_0
	layer_2_expert_7_ep_0 -> layer_2_agg_ep_0
	layer_2_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_2_expert_0_ep_1 -> layer_2_agg_ep_1
	layer_2_expert_1_ep_1 -> layer_2_agg_ep_1
	layer_2_expert_2_ep_1 -> layer_2_agg_ep_1
	layer_2_expert_3_ep_1 -> layer_2_agg_ep_1
	layer_2_expert_4_ep_1 -> layer_2_agg_ep_1
	layer_2_expert_5_ep_1 -> layer_2_agg_ep_1
	layer_2_expert_6_ep_1 -> layer_2_agg_ep_1
	layer_2_expert_7_ep_1 -> layer_2_agg_ep_1
	layer_2_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_2_expert_0_ep_2 -> layer_2_agg_ep_2
	layer_2_expert_1_ep_2 -> layer_2_agg_ep_2
	layer_2_expert_2_ep_2 -> layer_2_agg_ep_2
	layer_2_expert_3_ep_2 -> layer_2_agg_ep_2
	layer_2_expert_4_ep_2 -> layer_2_agg_ep_2
	layer_2_expert_5_ep_2 -> layer_2_agg_ep_2
	layer_2_expert_6_ep_2 -> layer_2_agg_ep_2
	layer_2_expert_7_ep_2 -> layer_2_agg_ep_2
	layer_2_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_2_expert_0_ep_3 -> layer_2_agg_ep_3
	layer_2_expert_1_ep_3 -> layer_2_agg_ep_3
	layer_2_expert_2_ep_3 -> layer_2_agg_ep_3
	layer_2_expert_3_ep_3 -> layer_2_agg_ep_3
	layer_2_expert_4_ep_3 -> layer_2_agg_ep_3
	layer_2_expert_5_ep_3 -> layer_2_agg_ep_3
	layer_2_expert_6_ep_3 -> layer_2_agg_ep_3
	layer_2_expert_7_ep_3 -> layer_2_agg_ep_3
	layer_2_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_2_expert_0_ep_4 -> layer_2_agg_ep_4
	layer_2_expert_1_ep_4 -> layer_2_agg_ep_4
	layer_2_expert_2_ep_4 -> layer_2_agg_ep_4
	layer_2_expert_3_ep_4 -> layer_2_agg_ep_4
	layer_2_expert_4_ep_4 -> layer_2_agg_ep_4
	layer_2_expert_5_ep_4 -> layer_2_agg_ep_4
	layer_2_expert_6_ep_4 -> layer_2_agg_ep_4
	layer_2_expert_7_ep_4 -> layer_2_agg_ep_4
	layer_2_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_2_expert_0_ep_5 -> layer_2_agg_ep_5
	layer_2_expert_1_ep_5 -> layer_2_agg_ep_5
	layer_2_expert_2_ep_5 -> layer_2_agg_ep_5
	layer_2_expert_3_ep_5 -> layer_2_agg_ep_5
	layer_2_expert_4_ep_5 -> layer_2_agg_ep_5
	layer_2_expert_5_ep_5 -> layer_2_agg_ep_5
	layer_2_expert_6_ep_5 -> layer_2_agg_ep_5
	layer_2_expert_7_ep_5 -> layer_2_agg_ep_5
	layer_2_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_2_expert_0_ep_6 -> layer_2_agg_ep_6
	layer_2_expert_1_ep_6 -> layer_2_agg_ep_6
	layer_2_expert_2_ep_6 -> layer_2_agg_ep_6
	layer_2_expert_3_ep_6 -> layer_2_agg_ep_6
	layer_2_expert_4_ep_6 -> layer_2_agg_ep_6
	layer_2_expert_5_ep_6 -> layer_2_agg_ep_6
	layer_2_expert_6_ep_6 -> layer_2_agg_ep_6
	layer_2_expert_7_ep_6 -> layer_2_agg_ep_6
	layer_2_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_2_expert_0_ep_7 -> layer_2_agg_ep_7
	layer_2_expert_1_ep_7 -> layer_2_agg_ep_7
	layer_2_expert_2_ep_7 -> layer_2_agg_ep_7
	layer_2_expert_3_ep_7 -> layer_2_agg_ep_7
	layer_2_expert_4_ep_7 -> layer_2_agg_ep_7
	layer_2_expert_5_ep_7 -> layer_2_agg_ep_7
	layer_2_expert_6_ep_7 -> layer_2_agg_ep_7
	layer_2_expert_7_ep_7 -> layer_2_agg_ep_7
	layer_2_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_2_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_2_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_2_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_2_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_2_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_2_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_2_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_2_ln_tp_3
	layer_3_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_3_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_3_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_3_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_3_attn_tp_0 -> layer_3_attn_out_tp_0
	layer_3_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_3_attn_tp_1 -> layer_3_attn_out_tp_1
	layer_3_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_3_attn_tp_2 -> layer_3_attn_out_tp_2
	layer_3_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_3_attn_tp_3 -> layer_3_attn_out_tp_3
	layer_3_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_3_gate_tp_0 -> layer_3_route_ep_0 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_0 -> layer_3_route_ep_1 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_0 -> layer_3_route_ep_2 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_0 -> layer_3_route_ep_3 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_0 -> layer_3_route_ep_4 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_0 -> layer_3_route_ep_5 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_0 -> layer_3_route_ep_6 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_0 -> layer_3_route_ep_7 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_1 -> layer_3_route_ep_0 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_1 -> layer_3_route_ep_1 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_1 -> layer_3_route_ep_2 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_1 -> layer_3_route_ep_3 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_1 -> layer_3_route_ep_4 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_1 -> layer_3_route_ep_5 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_1 -> layer_3_route_ep_6 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_1 -> layer_3_route_ep_7 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_2 -> layer_3_route_ep_0 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_2 -> layer_3_route_ep_1 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_2 -> layer_3_route_ep_2 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_2 -> layer_3_route_ep_3 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_2 -> layer_3_route_ep_4 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_2 -> layer_3_route_ep_5 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_2 -> layer_3_route_ep_6 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_2 -> layer_3_route_ep_7 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_3 -> layer_3_route_ep_0 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_3 -> layer_3_route_ep_1 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_3 -> layer_3_route_ep_2 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_3 -> layer_3_route_ep_3 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_3 -> layer_3_route_ep_4 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_3 -> layer_3_route_ep_5 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_3 -> layer_3_route_ep_6 [label="Gate Selection" style=dashed]
	layer_3_gate_tp_3 -> layer_3_route_ep_7 [label="Gate Selection" style=dashed]
	layer_3_route_ep_0 -> layer_3_expert_0_ep_0
	layer_3_route_ep_0 -> layer_3_expert_1_ep_0
	layer_3_route_ep_0 -> layer_3_expert_2_ep_0
	layer_3_route_ep_0 -> layer_3_expert_3_ep_0
	layer_3_route_ep_0 -> layer_3_expert_4_ep_0
	layer_3_route_ep_0 -> layer_3_expert_5_ep_0
	layer_3_route_ep_0 -> layer_3_expert_6_ep_0
	layer_3_route_ep_0 -> layer_3_expert_7_ep_0
	layer_3_route_ep_1 -> layer_3_expert_0_ep_1
	layer_3_route_ep_1 -> layer_3_expert_1_ep_1
	layer_3_route_ep_1 -> layer_3_expert_2_ep_1
	layer_3_route_ep_1 -> layer_3_expert_3_ep_1
	layer_3_route_ep_1 -> layer_3_expert_4_ep_1
	layer_3_route_ep_1 -> layer_3_expert_5_ep_1
	layer_3_route_ep_1 -> layer_3_expert_6_ep_1
	layer_3_route_ep_1 -> layer_3_expert_7_ep_1
	layer_3_route_ep_2 -> layer_3_expert_0_ep_2
	layer_3_route_ep_2 -> layer_3_expert_1_ep_2
	layer_3_route_ep_2 -> layer_3_expert_2_ep_2
	layer_3_route_ep_2 -> layer_3_expert_3_ep_2
	layer_3_route_ep_2 -> layer_3_expert_4_ep_2
	layer_3_route_ep_2 -> layer_3_expert_5_ep_2
	layer_3_route_ep_2 -> layer_3_expert_6_ep_2
	layer_3_route_ep_2 -> layer_3_expert_7_ep_2
	layer_3_route_ep_3 -> layer_3_expert_0_ep_3
	layer_3_route_ep_3 -> layer_3_expert_1_ep_3
	layer_3_route_ep_3 -> layer_3_expert_2_ep_3
	layer_3_route_ep_3 -> layer_3_expert_3_ep_3
	layer_3_route_ep_3 -> layer_3_expert_4_ep_3
	layer_3_route_ep_3 -> layer_3_expert_5_ep_3
	layer_3_route_ep_3 -> layer_3_expert_6_ep_3
	layer_3_route_ep_3 -> layer_3_expert_7_ep_3
	layer_3_route_ep_4 -> layer_3_expert_0_ep_4
	layer_3_route_ep_4 -> layer_3_expert_1_ep_4
	layer_3_route_ep_4 -> layer_3_expert_2_ep_4
	layer_3_route_ep_4 -> layer_3_expert_3_ep_4
	layer_3_route_ep_4 -> layer_3_expert_4_ep_4
	layer_3_route_ep_4 -> layer_3_expert_5_ep_4
	layer_3_route_ep_4 -> layer_3_expert_6_ep_4
	layer_3_route_ep_4 -> layer_3_expert_7_ep_4
	layer_3_route_ep_5 -> layer_3_expert_0_ep_5
	layer_3_route_ep_5 -> layer_3_expert_1_ep_5
	layer_3_route_ep_5 -> layer_3_expert_2_ep_5
	layer_3_route_ep_5 -> layer_3_expert_3_ep_5
	layer_3_route_ep_5 -> layer_3_expert_4_ep_5
	layer_3_route_ep_5 -> layer_3_expert_5_ep_5
	layer_3_route_ep_5 -> layer_3_expert_6_ep_5
	layer_3_route_ep_5 -> layer_3_expert_7_ep_5
	layer_3_route_ep_6 -> layer_3_expert_0_ep_6
	layer_3_route_ep_6 -> layer_3_expert_1_ep_6
	layer_3_route_ep_6 -> layer_3_expert_2_ep_6
	layer_3_route_ep_6 -> layer_3_expert_3_ep_6
	layer_3_route_ep_6 -> layer_3_expert_4_ep_6
	layer_3_route_ep_6 -> layer_3_expert_5_ep_6
	layer_3_route_ep_6 -> layer_3_expert_6_ep_6
	layer_3_route_ep_6 -> layer_3_expert_7_ep_6
	layer_3_route_ep_7 -> layer_3_expert_0_ep_7
	layer_3_route_ep_7 -> layer_3_expert_1_ep_7
	layer_3_route_ep_7 -> layer_3_expert_2_ep_7
	layer_3_route_ep_7 -> layer_3_expert_3_ep_7
	layer_3_route_ep_7 -> layer_3_expert_4_ep_7
	layer_3_route_ep_7 -> layer_3_expert_5_ep_7
	layer_3_route_ep_7 -> layer_3_expert_6_ep_7
	layer_3_route_ep_7 -> layer_3_expert_7_ep_7
	layer_3_expert_0_ep_0 -> layer_3_agg_ep_0
	layer_3_expert_1_ep_0 -> layer_3_agg_ep_0
	layer_3_expert_2_ep_0 -> layer_3_agg_ep_0
	layer_3_expert_3_ep_0 -> layer_3_agg_ep_0
	layer_3_expert_4_ep_0 -> layer_3_agg_ep_0
	layer_3_expert_5_ep_0 -> layer_3_agg_ep_0
	layer_3_expert_6_ep_0 -> layer_3_agg_ep_0
	layer_3_expert_7_ep_0 -> layer_3_agg_ep_0
	layer_3_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_3_expert_0_ep_1 -> layer_3_agg_ep_1
	layer_3_expert_1_ep_1 -> layer_3_agg_ep_1
	layer_3_expert_2_ep_1 -> layer_3_agg_ep_1
	layer_3_expert_3_ep_1 -> layer_3_agg_ep_1
	layer_3_expert_4_ep_1 -> layer_3_agg_ep_1
	layer_3_expert_5_ep_1 -> layer_3_agg_ep_1
	layer_3_expert_6_ep_1 -> layer_3_agg_ep_1
	layer_3_expert_7_ep_1 -> layer_3_agg_ep_1
	layer_3_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_3_expert_0_ep_2 -> layer_3_agg_ep_2
	layer_3_expert_1_ep_2 -> layer_3_agg_ep_2
	layer_3_expert_2_ep_2 -> layer_3_agg_ep_2
	layer_3_expert_3_ep_2 -> layer_3_agg_ep_2
	layer_3_expert_4_ep_2 -> layer_3_agg_ep_2
	layer_3_expert_5_ep_2 -> layer_3_agg_ep_2
	layer_3_expert_6_ep_2 -> layer_3_agg_ep_2
	layer_3_expert_7_ep_2 -> layer_3_agg_ep_2
	layer_3_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_3_expert_0_ep_3 -> layer_3_agg_ep_3
	layer_3_expert_1_ep_3 -> layer_3_agg_ep_3
	layer_3_expert_2_ep_3 -> layer_3_agg_ep_3
	layer_3_expert_3_ep_3 -> layer_3_agg_ep_3
	layer_3_expert_4_ep_3 -> layer_3_agg_ep_3
	layer_3_expert_5_ep_3 -> layer_3_agg_ep_3
	layer_3_expert_6_ep_3 -> layer_3_agg_ep_3
	layer_3_expert_7_ep_3 -> layer_3_agg_ep_3
	layer_3_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_3_expert_0_ep_4 -> layer_3_agg_ep_4
	layer_3_expert_1_ep_4 -> layer_3_agg_ep_4
	layer_3_expert_2_ep_4 -> layer_3_agg_ep_4
	layer_3_expert_3_ep_4 -> layer_3_agg_ep_4
	layer_3_expert_4_ep_4 -> layer_3_agg_ep_4
	layer_3_expert_5_ep_4 -> layer_3_agg_ep_4
	layer_3_expert_6_ep_4 -> layer_3_agg_ep_4
	layer_3_expert_7_ep_4 -> layer_3_agg_ep_4
	layer_3_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_3_expert_0_ep_5 -> layer_3_agg_ep_5
	layer_3_expert_1_ep_5 -> layer_3_agg_ep_5
	layer_3_expert_2_ep_5 -> layer_3_agg_ep_5
	layer_3_expert_3_ep_5 -> layer_3_agg_ep_5
	layer_3_expert_4_ep_5 -> layer_3_agg_ep_5
	layer_3_expert_5_ep_5 -> layer_3_agg_ep_5
	layer_3_expert_6_ep_5 -> layer_3_agg_ep_5
	layer_3_expert_7_ep_5 -> layer_3_agg_ep_5
	layer_3_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_3_expert_0_ep_6 -> layer_3_agg_ep_6
	layer_3_expert_1_ep_6 -> layer_3_agg_ep_6
	layer_3_expert_2_ep_6 -> layer_3_agg_ep_6
	layer_3_expert_3_ep_6 -> layer_3_agg_ep_6
	layer_3_expert_4_ep_6 -> layer_3_agg_ep_6
	layer_3_expert_5_ep_6 -> layer_3_agg_ep_6
	layer_3_expert_6_ep_6 -> layer_3_agg_ep_6
	layer_3_expert_7_ep_6 -> layer_3_agg_ep_6
	layer_3_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_3_expert_0_ep_7 -> layer_3_agg_ep_7
	layer_3_expert_1_ep_7 -> layer_3_agg_ep_7
	layer_3_expert_2_ep_7 -> layer_3_agg_ep_7
	layer_3_expert_3_ep_7 -> layer_3_agg_ep_7
	layer_3_expert_4_ep_7 -> layer_3_agg_ep_7
	layer_3_expert_5_ep_7 -> layer_3_agg_ep_7
	layer_3_expert_6_ep_7 -> layer_3_agg_ep_7
	layer_3_expert_7_ep_7 -> layer_3_agg_ep_7
	layer_3_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_3_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_3_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_3_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_3_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_3_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_3_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_3_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_3_ln_tp_3
	layer_4_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_4_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_4_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_4_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_4_attn_tp_0 -> layer_4_attn_out_tp_0
	layer_4_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_4_attn_tp_1 -> layer_4_attn_out_tp_1
	layer_4_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_4_attn_tp_2 -> layer_4_attn_out_tp_2
	layer_4_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_4_attn_tp_3 -> layer_4_attn_out_tp_3
	layer_4_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_4_gate_tp_0 -> layer_4_route_ep_0 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_0 -> layer_4_route_ep_1 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_0 -> layer_4_route_ep_2 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_0 -> layer_4_route_ep_3 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_0 -> layer_4_route_ep_4 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_0 -> layer_4_route_ep_5 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_0 -> layer_4_route_ep_6 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_0 -> layer_4_route_ep_7 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_1 -> layer_4_route_ep_0 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_1 -> layer_4_route_ep_1 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_1 -> layer_4_route_ep_2 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_1 -> layer_4_route_ep_3 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_1 -> layer_4_route_ep_4 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_1 -> layer_4_route_ep_5 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_1 -> layer_4_route_ep_6 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_1 -> layer_4_route_ep_7 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_2 -> layer_4_route_ep_0 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_2 -> layer_4_route_ep_1 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_2 -> layer_4_route_ep_2 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_2 -> layer_4_route_ep_3 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_2 -> layer_4_route_ep_4 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_2 -> layer_4_route_ep_5 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_2 -> layer_4_route_ep_6 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_2 -> layer_4_route_ep_7 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_3 -> layer_4_route_ep_0 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_3 -> layer_4_route_ep_1 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_3 -> layer_4_route_ep_2 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_3 -> layer_4_route_ep_3 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_3 -> layer_4_route_ep_4 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_3 -> layer_4_route_ep_5 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_3 -> layer_4_route_ep_6 [label="Gate Selection" style=dashed]
	layer_4_gate_tp_3 -> layer_4_route_ep_7 [label="Gate Selection" style=dashed]
	layer_4_route_ep_0 -> layer_4_expert_0_ep_0
	layer_4_route_ep_0 -> layer_4_expert_1_ep_0
	layer_4_route_ep_0 -> layer_4_expert_2_ep_0
	layer_4_route_ep_0 -> layer_4_expert_3_ep_0
	layer_4_route_ep_0 -> layer_4_expert_4_ep_0
	layer_4_route_ep_0 -> layer_4_expert_5_ep_0
	layer_4_route_ep_0 -> layer_4_expert_6_ep_0
	layer_4_route_ep_0 -> layer_4_expert_7_ep_0
	layer_4_route_ep_1 -> layer_4_expert_0_ep_1
	layer_4_route_ep_1 -> layer_4_expert_1_ep_1
	layer_4_route_ep_1 -> layer_4_expert_2_ep_1
	layer_4_route_ep_1 -> layer_4_expert_3_ep_1
	layer_4_route_ep_1 -> layer_4_expert_4_ep_1
	layer_4_route_ep_1 -> layer_4_expert_5_ep_1
	layer_4_route_ep_1 -> layer_4_expert_6_ep_1
	layer_4_route_ep_1 -> layer_4_expert_7_ep_1
	layer_4_route_ep_2 -> layer_4_expert_0_ep_2
	layer_4_route_ep_2 -> layer_4_expert_1_ep_2
	layer_4_route_ep_2 -> layer_4_expert_2_ep_2
	layer_4_route_ep_2 -> layer_4_expert_3_ep_2
	layer_4_route_ep_2 -> layer_4_expert_4_ep_2
	layer_4_route_ep_2 -> layer_4_expert_5_ep_2
	layer_4_route_ep_2 -> layer_4_expert_6_ep_2
	layer_4_route_ep_2 -> layer_4_expert_7_ep_2
	layer_4_route_ep_3 -> layer_4_expert_0_ep_3
	layer_4_route_ep_3 -> layer_4_expert_1_ep_3
	layer_4_route_ep_3 -> layer_4_expert_2_ep_3
	layer_4_route_ep_3 -> layer_4_expert_3_ep_3
	layer_4_route_ep_3 -> layer_4_expert_4_ep_3
	layer_4_route_ep_3 -> layer_4_expert_5_ep_3
	layer_4_route_ep_3 -> layer_4_expert_6_ep_3
	layer_4_route_ep_3 -> layer_4_expert_7_ep_3
	layer_4_route_ep_4 -> layer_4_expert_0_ep_4
	layer_4_route_ep_4 -> layer_4_expert_1_ep_4
	layer_4_route_ep_4 -> layer_4_expert_2_ep_4
	layer_4_route_ep_4 -> layer_4_expert_3_ep_4
	layer_4_route_ep_4 -> layer_4_expert_4_ep_4
	layer_4_route_ep_4 -> layer_4_expert_5_ep_4
	layer_4_route_ep_4 -> layer_4_expert_6_ep_4
	layer_4_route_ep_4 -> layer_4_expert_7_ep_4
	layer_4_route_ep_5 -> layer_4_expert_0_ep_5
	layer_4_route_ep_5 -> layer_4_expert_1_ep_5
	layer_4_route_ep_5 -> layer_4_expert_2_ep_5
	layer_4_route_ep_5 -> layer_4_expert_3_ep_5
	layer_4_route_ep_5 -> layer_4_expert_4_ep_5
	layer_4_route_ep_5 -> layer_4_expert_5_ep_5
	layer_4_route_ep_5 -> layer_4_expert_6_ep_5
	layer_4_route_ep_5 -> layer_4_expert_7_ep_5
	layer_4_route_ep_6 -> layer_4_expert_0_ep_6
	layer_4_route_ep_6 -> layer_4_expert_1_ep_6
	layer_4_route_ep_6 -> layer_4_expert_2_ep_6
	layer_4_route_ep_6 -> layer_4_expert_3_ep_6
	layer_4_route_ep_6 -> layer_4_expert_4_ep_6
	layer_4_route_ep_6 -> layer_4_expert_5_ep_6
	layer_4_route_ep_6 -> layer_4_expert_6_ep_6
	layer_4_route_ep_6 -> layer_4_expert_7_ep_6
	layer_4_route_ep_7 -> layer_4_expert_0_ep_7
	layer_4_route_ep_7 -> layer_4_expert_1_ep_7
	layer_4_route_ep_7 -> layer_4_expert_2_ep_7
	layer_4_route_ep_7 -> layer_4_expert_3_ep_7
	layer_4_route_ep_7 -> layer_4_expert_4_ep_7
	layer_4_route_ep_7 -> layer_4_expert_5_ep_7
	layer_4_route_ep_7 -> layer_4_expert_6_ep_7
	layer_4_route_ep_7 -> layer_4_expert_7_ep_7
	layer_4_expert_0_ep_0 -> layer_4_agg_ep_0
	layer_4_expert_1_ep_0 -> layer_4_agg_ep_0
	layer_4_expert_2_ep_0 -> layer_4_agg_ep_0
	layer_4_expert_3_ep_0 -> layer_4_agg_ep_0
	layer_4_expert_4_ep_0 -> layer_4_agg_ep_0
	layer_4_expert_5_ep_0 -> layer_4_agg_ep_0
	layer_4_expert_6_ep_0 -> layer_4_agg_ep_0
	layer_4_expert_7_ep_0 -> layer_4_agg_ep_0
	layer_4_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_4_expert_0_ep_1 -> layer_4_agg_ep_1
	layer_4_expert_1_ep_1 -> layer_4_agg_ep_1
	layer_4_expert_2_ep_1 -> layer_4_agg_ep_1
	layer_4_expert_3_ep_1 -> layer_4_agg_ep_1
	layer_4_expert_4_ep_1 -> layer_4_agg_ep_1
	layer_4_expert_5_ep_1 -> layer_4_agg_ep_1
	layer_4_expert_6_ep_1 -> layer_4_agg_ep_1
	layer_4_expert_7_ep_1 -> layer_4_agg_ep_1
	layer_4_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_4_expert_0_ep_2 -> layer_4_agg_ep_2
	layer_4_expert_1_ep_2 -> layer_4_agg_ep_2
	layer_4_expert_2_ep_2 -> layer_4_agg_ep_2
	layer_4_expert_3_ep_2 -> layer_4_agg_ep_2
	layer_4_expert_4_ep_2 -> layer_4_agg_ep_2
	layer_4_expert_5_ep_2 -> layer_4_agg_ep_2
	layer_4_expert_6_ep_2 -> layer_4_agg_ep_2
	layer_4_expert_7_ep_2 -> layer_4_agg_ep_2
	layer_4_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_4_expert_0_ep_3 -> layer_4_agg_ep_3
	layer_4_expert_1_ep_3 -> layer_4_agg_ep_3
	layer_4_expert_2_ep_3 -> layer_4_agg_ep_3
	layer_4_expert_3_ep_3 -> layer_4_agg_ep_3
	layer_4_expert_4_ep_3 -> layer_4_agg_ep_3
	layer_4_expert_5_ep_3 -> layer_4_agg_ep_3
	layer_4_expert_6_ep_3 -> layer_4_agg_ep_3
	layer_4_expert_7_ep_3 -> layer_4_agg_ep_3
	layer_4_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_4_expert_0_ep_4 -> layer_4_agg_ep_4
	layer_4_expert_1_ep_4 -> layer_4_agg_ep_4
	layer_4_expert_2_ep_4 -> layer_4_agg_ep_4
	layer_4_expert_3_ep_4 -> layer_4_agg_ep_4
	layer_4_expert_4_ep_4 -> layer_4_agg_ep_4
	layer_4_expert_5_ep_4 -> layer_4_agg_ep_4
	layer_4_expert_6_ep_4 -> layer_4_agg_ep_4
	layer_4_expert_7_ep_4 -> layer_4_agg_ep_4
	layer_4_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_4_expert_0_ep_5 -> layer_4_agg_ep_5
	layer_4_expert_1_ep_5 -> layer_4_agg_ep_5
	layer_4_expert_2_ep_5 -> layer_4_agg_ep_5
	layer_4_expert_3_ep_5 -> layer_4_agg_ep_5
	layer_4_expert_4_ep_5 -> layer_4_agg_ep_5
	layer_4_expert_5_ep_5 -> layer_4_agg_ep_5
	layer_4_expert_6_ep_5 -> layer_4_agg_ep_5
	layer_4_expert_7_ep_5 -> layer_4_agg_ep_5
	layer_4_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_4_expert_0_ep_6 -> layer_4_agg_ep_6
	layer_4_expert_1_ep_6 -> layer_4_agg_ep_6
	layer_4_expert_2_ep_6 -> layer_4_agg_ep_6
	layer_4_expert_3_ep_6 -> layer_4_agg_ep_6
	layer_4_expert_4_ep_6 -> layer_4_agg_ep_6
	layer_4_expert_5_ep_6 -> layer_4_agg_ep_6
	layer_4_expert_6_ep_6 -> layer_4_agg_ep_6
	layer_4_expert_7_ep_6 -> layer_4_agg_ep_6
	layer_4_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_4_expert_0_ep_7 -> layer_4_agg_ep_7
	layer_4_expert_1_ep_7 -> layer_4_agg_ep_7
	layer_4_expert_2_ep_7 -> layer_4_agg_ep_7
	layer_4_expert_3_ep_7 -> layer_4_agg_ep_7
	layer_4_expert_4_ep_7 -> layer_4_agg_ep_7
	layer_4_expert_5_ep_7 -> layer_4_agg_ep_7
	layer_4_expert_6_ep_7 -> layer_4_agg_ep_7
	layer_4_expert_7_ep_7 -> layer_4_agg_ep_7
	layer_4_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_4_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_4_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_4_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_4_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_4_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_4_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_4_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_4_ln_tp_3
	layer_5_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_5_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_5_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_5_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_5_attn_tp_0 -> layer_5_attn_out_tp_0
	layer_5_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_5_attn_tp_1 -> layer_5_attn_out_tp_1
	layer_5_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_5_attn_tp_2 -> layer_5_attn_out_tp_2
	layer_5_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_5_attn_tp_3 -> layer_5_attn_out_tp_3
	layer_5_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_5_gate_tp_0 -> layer_5_route_ep_0 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_0 -> layer_5_route_ep_1 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_0 -> layer_5_route_ep_2 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_0 -> layer_5_route_ep_3 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_0 -> layer_5_route_ep_4 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_0 -> layer_5_route_ep_5 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_0 -> layer_5_route_ep_6 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_0 -> layer_5_route_ep_7 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_1 -> layer_5_route_ep_0 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_1 -> layer_5_route_ep_1 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_1 -> layer_5_route_ep_2 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_1 -> layer_5_route_ep_3 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_1 -> layer_5_route_ep_4 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_1 -> layer_5_route_ep_5 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_1 -> layer_5_route_ep_6 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_1 -> layer_5_route_ep_7 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_2 -> layer_5_route_ep_0 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_2 -> layer_5_route_ep_1 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_2 -> layer_5_route_ep_2 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_2 -> layer_5_route_ep_3 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_2 -> layer_5_route_ep_4 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_2 -> layer_5_route_ep_5 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_2 -> layer_5_route_ep_6 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_2 -> layer_5_route_ep_7 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_3 -> layer_5_route_ep_0 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_3 -> layer_5_route_ep_1 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_3 -> layer_5_route_ep_2 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_3 -> layer_5_route_ep_3 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_3 -> layer_5_route_ep_4 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_3 -> layer_5_route_ep_5 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_3 -> layer_5_route_ep_6 [label="Gate Selection" style=dashed]
	layer_5_gate_tp_3 -> layer_5_route_ep_7 [label="Gate Selection" style=dashed]
	layer_5_route_ep_0 -> layer_5_expert_0_ep_0
	layer_5_route_ep_0 -> layer_5_expert_1_ep_0
	layer_5_route_ep_0 -> layer_5_expert_2_ep_0
	layer_5_route_ep_0 -> layer_5_expert_3_ep_0
	layer_5_route_ep_0 -> layer_5_expert_4_ep_0
	layer_5_route_ep_0 -> layer_5_expert_5_ep_0
	layer_5_route_ep_0 -> layer_5_expert_6_ep_0
	layer_5_route_ep_0 -> layer_5_expert_7_ep_0
	layer_5_route_ep_1 -> layer_5_expert_0_ep_1
	layer_5_route_ep_1 -> layer_5_expert_1_ep_1
	layer_5_route_ep_1 -> layer_5_expert_2_ep_1
	layer_5_route_ep_1 -> layer_5_expert_3_ep_1
	layer_5_route_ep_1 -> layer_5_expert_4_ep_1
	layer_5_route_ep_1 -> layer_5_expert_5_ep_1
	layer_5_route_ep_1 -> layer_5_expert_6_ep_1
	layer_5_route_ep_1 -> layer_5_expert_7_ep_1
	layer_5_route_ep_2 -> layer_5_expert_0_ep_2
	layer_5_route_ep_2 -> layer_5_expert_1_ep_2
	layer_5_route_ep_2 -> layer_5_expert_2_ep_2
	layer_5_route_ep_2 -> layer_5_expert_3_ep_2
	layer_5_route_ep_2 -> layer_5_expert_4_ep_2
	layer_5_route_ep_2 -> layer_5_expert_5_ep_2
	layer_5_route_ep_2 -> layer_5_expert_6_ep_2
	layer_5_route_ep_2 -> layer_5_expert_7_ep_2
	layer_5_route_ep_3 -> layer_5_expert_0_ep_3
	layer_5_route_ep_3 -> layer_5_expert_1_ep_3
	layer_5_route_ep_3 -> layer_5_expert_2_ep_3
	layer_5_route_ep_3 -> layer_5_expert_3_ep_3
	layer_5_route_ep_3 -> layer_5_expert_4_ep_3
	layer_5_route_ep_3 -> layer_5_expert_5_ep_3
	layer_5_route_ep_3 -> layer_5_expert_6_ep_3
	layer_5_route_ep_3 -> layer_5_expert_7_ep_3
	layer_5_route_ep_4 -> layer_5_expert_0_ep_4
	layer_5_route_ep_4 -> layer_5_expert_1_ep_4
	layer_5_route_ep_4 -> layer_5_expert_2_ep_4
	layer_5_route_ep_4 -> layer_5_expert_3_ep_4
	layer_5_route_ep_4 -> layer_5_expert_4_ep_4
	layer_5_route_ep_4 -> layer_5_expert_5_ep_4
	layer_5_route_ep_4 -> layer_5_expert_6_ep_4
	layer_5_route_ep_4 -> layer_5_expert_7_ep_4
	layer_5_route_ep_5 -> layer_5_expert_0_ep_5
	layer_5_route_ep_5 -> layer_5_expert_1_ep_5
	layer_5_route_ep_5 -> layer_5_expert_2_ep_5
	layer_5_route_ep_5 -> layer_5_expert_3_ep_5
	layer_5_route_ep_5 -> layer_5_expert_4_ep_5
	layer_5_route_ep_5 -> layer_5_expert_5_ep_5
	layer_5_route_ep_5 -> layer_5_expert_6_ep_5
	layer_5_route_ep_5 -> layer_5_expert_7_ep_5
	layer_5_route_ep_6 -> layer_5_expert_0_ep_6
	layer_5_route_ep_6 -> layer_5_expert_1_ep_6
	layer_5_route_ep_6 -> layer_5_expert_2_ep_6
	layer_5_route_ep_6 -> layer_5_expert_3_ep_6
	layer_5_route_ep_6 -> layer_5_expert_4_ep_6
	layer_5_route_ep_6 -> layer_5_expert_5_ep_6
	layer_5_route_ep_6 -> layer_5_expert_6_ep_6
	layer_5_route_ep_6 -> layer_5_expert_7_ep_6
	layer_5_route_ep_7 -> layer_5_expert_0_ep_7
	layer_5_route_ep_7 -> layer_5_expert_1_ep_7
	layer_5_route_ep_7 -> layer_5_expert_2_ep_7
	layer_5_route_ep_7 -> layer_5_expert_3_ep_7
	layer_5_route_ep_7 -> layer_5_expert_4_ep_7
	layer_5_route_ep_7 -> layer_5_expert_5_ep_7
	layer_5_route_ep_7 -> layer_5_expert_6_ep_7
	layer_5_route_ep_7 -> layer_5_expert_7_ep_7
	layer_5_expert_0_ep_0 -> layer_5_agg_ep_0
	layer_5_expert_1_ep_0 -> layer_5_agg_ep_0
	layer_5_expert_2_ep_0 -> layer_5_agg_ep_0
	layer_5_expert_3_ep_0 -> layer_5_agg_ep_0
	layer_5_expert_4_ep_0 -> layer_5_agg_ep_0
	layer_5_expert_5_ep_0 -> layer_5_agg_ep_0
	layer_5_expert_6_ep_0 -> layer_5_agg_ep_0
	layer_5_expert_7_ep_0 -> layer_5_agg_ep_0
	layer_5_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_5_expert_0_ep_1 -> layer_5_agg_ep_1
	layer_5_expert_1_ep_1 -> layer_5_agg_ep_1
	layer_5_expert_2_ep_1 -> layer_5_agg_ep_1
	layer_5_expert_3_ep_1 -> layer_5_agg_ep_1
	layer_5_expert_4_ep_1 -> layer_5_agg_ep_1
	layer_5_expert_5_ep_1 -> layer_5_agg_ep_1
	layer_5_expert_6_ep_1 -> layer_5_agg_ep_1
	layer_5_expert_7_ep_1 -> layer_5_agg_ep_1
	layer_5_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_5_expert_0_ep_2 -> layer_5_agg_ep_2
	layer_5_expert_1_ep_2 -> layer_5_agg_ep_2
	layer_5_expert_2_ep_2 -> layer_5_agg_ep_2
	layer_5_expert_3_ep_2 -> layer_5_agg_ep_2
	layer_5_expert_4_ep_2 -> layer_5_agg_ep_2
	layer_5_expert_5_ep_2 -> layer_5_agg_ep_2
	layer_5_expert_6_ep_2 -> layer_5_agg_ep_2
	layer_5_expert_7_ep_2 -> layer_5_agg_ep_2
	layer_5_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_5_expert_0_ep_3 -> layer_5_agg_ep_3
	layer_5_expert_1_ep_3 -> layer_5_agg_ep_3
	layer_5_expert_2_ep_3 -> layer_5_agg_ep_3
	layer_5_expert_3_ep_3 -> layer_5_agg_ep_3
	layer_5_expert_4_ep_3 -> layer_5_agg_ep_3
	layer_5_expert_5_ep_3 -> layer_5_agg_ep_3
	layer_5_expert_6_ep_3 -> layer_5_agg_ep_3
	layer_5_expert_7_ep_3 -> layer_5_agg_ep_3
	layer_5_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_5_expert_0_ep_4 -> layer_5_agg_ep_4
	layer_5_expert_1_ep_4 -> layer_5_agg_ep_4
	layer_5_expert_2_ep_4 -> layer_5_agg_ep_4
	layer_5_expert_3_ep_4 -> layer_5_agg_ep_4
	layer_5_expert_4_ep_4 -> layer_5_agg_ep_4
	layer_5_expert_5_ep_4 -> layer_5_agg_ep_4
	layer_5_expert_6_ep_4 -> layer_5_agg_ep_4
	layer_5_expert_7_ep_4 -> layer_5_agg_ep_4
	layer_5_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_5_expert_0_ep_5 -> layer_5_agg_ep_5
	layer_5_expert_1_ep_5 -> layer_5_agg_ep_5
	layer_5_expert_2_ep_5 -> layer_5_agg_ep_5
	layer_5_expert_3_ep_5 -> layer_5_agg_ep_5
	layer_5_expert_4_ep_5 -> layer_5_agg_ep_5
	layer_5_expert_5_ep_5 -> layer_5_agg_ep_5
	layer_5_expert_6_ep_5 -> layer_5_agg_ep_5
	layer_5_expert_7_ep_5 -> layer_5_agg_ep_5
	layer_5_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_5_expert_0_ep_6 -> layer_5_agg_ep_6
	layer_5_expert_1_ep_6 -> layer_5_agg_ep_6
	layer_5_expert_2_ep_6 -> layer_5_agg_ep_6
	layer_5_expert_3_ep_6 -> layer_5_agg_ep_6
	layer_5_expert_4_ep_6 -> layer_5_agg_ep_6
	layer_5_expert_5_ep_6 -> layer_5_agg_ep_6
	layer_5_expert_6_ep_6 -> layer_5_agg_ep_6
	layer_5_expert_7_ep_6 -> layer_5_agg_ep_6
	layer_5_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_5_expert_0_ep_7 -> layer_5_agg_ep_7
	layer_5_expert_1_ep_7 -> layer_5_agg_ep_7
	layer_5_expert_2_ep_7 -> layer_5_agg_ep_7
	layer_5_expert_3_ep_7 -> layer_5_agg_ep_7
	layer_5_expert_4_ep_7 -> layer_5_agg_ep_7
	layer_5_expert_5_ep_7 -> layer_5_agg_ep_7
	layer_5_expert_6_ep_7 -> layer_5_agg_ep_7
	layer_5_expert_7_ep_7 -> layer_5_agg_ep_7
	layer_5_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_5_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_5_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_5_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_5_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_5_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_5_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_5_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_5_ln_tp_3
	layer_6_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_6_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_6_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_6_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_6_attn_tp_0 -> layer_6_attn_out_tp_0
	layer_6_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_6_attn_tp_1 -> layer_6_attn_out_tp_1
	layer_6_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_6_attn_tp_2 -> layer_6_attn_out_tp_2
	layer_6_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_6_attn_tp_3 -> layer_6_attn_out_tp_3
	layer_6_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_6_gate_tp_0 -> layer_6_route_ep_0 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_0 -> layer_6_route_ep_1 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_0 -> layer_6_route_ep_2 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_0 -> layer_6_route_ep_3 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_0 -> layer_6_route_ep_4 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_0 -> layer_6_route_ep_5 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_0 -> layer_6_route_ep_6 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_0 -> layer_6_route_ep_7 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_1 -> layer_6_route_ep_0 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_1 -> layer_6_route_ep_1 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_1 -> layer_6_route_ep_2 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_1 -> layer_6_route_ep_3 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_1 -> layer_6_route_ep_4 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_1 -> layer_6_route_ep_5 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_1 -> layer_6_route_ep_6 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_1 -> layer_6_route_ep_7 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_2 -> layer_6_route_ep_0 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_2 -> layer_6_route_ep_1 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_2 -> layer_6_route_ep_2 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_2 -> layer_6_route_ep_3 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_2 -> layer_6_route_ep_4 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_2 -> layer_6_route_ep_5 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_2 -> layer_6_route_ep_6 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_2 -> layer_6_route_ep_7 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_3 -> layer_6_route_ep_0 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_3 -> layer_6_route_ep_1 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_3 -> layer_6_route_ep_2 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_3 -> layer_6_route_ep_3 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_3 -> layer_6_route_ep_4 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_3 -> layer_6_route_ep_5 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_3 -> layer_6_route_ep_6 [label="Gate Selection" style=dashed]
	layer_6_gate_tp_3 -> layer_6_route_ep_7 [label="Gate Selection" style=dashed]
	layer_6_route_ep_0 -> layer_6_expert_0_ep_0
	layer_6_route_ep_0 -> layer_6_expert_1_ep_0
	layer_6_route_ep_0 -> layer_6_expert_2_ep_0
	layer_6_route_ep_0 -> layer_6_expert_3_ep_0
	layer_6_route_ep_0 -> layer_6_expert_4_ep_0
	layer_6_route_ep_0 -> layer_6_expert_5_ep_0
	layer_6_route_ep_0 -> layer_6_expert_6_ep_0
	layer_6_route_ep_0 -> layer_6_expert_7_ep_0
	layer_6_route_ep_1 -> layer_6_expert_0_ep_1
	layer_6_route_ep_1 -> layer_6_expert_1_ep_1
	layer_6_route_ep_1 -> layer_6_expert_2_ep_1
	layer_6_route_ep_1 -> layer_6_expert_3_ep_1
	layer_6_route_ep_1 -> layer_6_expert_4_ep_1
	layer_6_route_ep_1 -> layer_6_expert_5_ep_1
	layer_6_route_ep_1 -> layer_6_expert_6_ep_1
	layer_6_route_ep_1 -> layer_6_expert_7_ep_1
	layer_6_route_ep_2 -> layer_6_expert_0_ep_2
	layer_6_route_ep_2 -> layer_6_expert_1_ep_2
	layer_6_route_ep_2 -> layer_6_expert_2_ep_2
	layer_6_route_ep_2 -> layer_6_expert_3_ep_2
	layer_6_route_ep_2 -> layer_6_expert_4_ep_2
	layer_6_route_ep_2 -> layer_6_expert_5_ep_2
	layer_6_route_ep_2 -> layer_6_expert_6_ep_2
	layer_6_route_ep_2 -> layer_6_expert_7_ep_2
	layer_6_route_ep_3 -> layer_6_expert_0_ep_3
	layer_6_route_ep_3 -> layer_6_expert_1_ep_3
	layer_6_route_ep_3 -> layer_6_expert_2_ep_3
	layer_6_route_ep_3 -> layer_6_expert_3_ep_3
	layer_6_route_ep_3 -> layer_6_expert_4_ep_3
	layer_6_route_ep_3 -> layer_6_expert_5_ep_3
	layer_6_route_ep_3 -> layer_6_expert_6_ep_3
	layer_6_route_ep_3 -> layer_6_expert_7_ep_3
	layer_6_route_ep_4 -> layer_6_expert_0_ep_4
	layer_6_route_ep_4 -> layer_6_expert_1_ep_4
	layer_6_route_ep_4 -> layer_6_expert_2_ep_4
	layer_6_route_ep_4 -> layer_6_expert_3_ep_4
	layer_6_route_ep_4 -> layer_6_expert_4_ep_4
	layer_6_route_ep_4 -> layer_6_expert_5_ep_4
	layer_6_route_ep_4 -> layer_6_expert_6_ep_4
	layer_6_route_ep_4 -> layer_6_expert_7_ep_4
	layer_6_route_ep_5 -> layer_6_expert_0_ep_5
	layer_6_route_ep_5 -> layer_6_expert_1_ep_5
	layer_6_route_ep_5 -> layer_6_expert_2_ep_5
	layer_6_route_ep_5 -> layer_6_expert_3_ep_5
	layer_6_route_ep_5 -> layer_6_expert_4_ep_5
	layer_6_route_ep_5 -> layer_6_expert_5_ep_5
	layer_6_route_ep_5 -> layer_6_expert_6_ep_5
	layer_6_route_ep_5 -> layer_6_expert_7_ep_5
	layer_6_route_ep_6 -> layer_6_expert_0_ep_6
	layer_6_route_ep_6 -> layer_6_expert_1_ep_6
	layer_6_route_ep_6 -> layer_6_expert_2_ep_6
	layer_6_route_ep_6 -> layer_6_expert_3_ep_6
	layer_6_route_ep_6 -> layer_6_expert_4_ep_6
	layer_6_route_ep_6 -> layer_6_expert_5_ep_6
	layer_6_route_ep_6 -> layer_6_expert_6_ep_6
	layer_6_route_ep_6 -> layer_6_expert_7_ep_6
	layer_6_route_ep_7 -> layer_6_expert_0_ep_7
	layer_6_route_ep_7 -> layer_6_expert_1_ep_7
	layer_6_route_ep_7 -> layer_6_expert_2_ep_7
	layer_6_route_ep_7 -> layer_6_expert_3_ep_7
	layer_6_route_ep_7 -> layer_6_expert_4_ep_7
	layer_6_route_ep_7 -> layer_6_expert_5_ep_7
	layer_6_route_ep_7 -> layer_6_expert_6_ep_7
	layer_6_route_ep_7 -> layer_6_expert_7_ep_7
	layer_6_expert_0_ep_0 -> layer_6_agg_ep_0
	layer_6_expert_1_ep_0 -> layer_6_agg_ep_0
	layer_6_expert_2_ep_0 -> layer_6_agg_ep_0
	layer_6_expert_3_ep_0 -> layer_6_agg_ep_0
	layer_6_expert_4_ep_0 -> layer_6_agg_ep_0
	layer_6_expert_5_ep_0 -> layer_6_agg_ep_0
	layer_6_expert_6_ep_0 -> layer_6_agg_ep_0
	layer_6_expert_7_ep_0 -> layer_6_agg_ep_0
	layer_6_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_6_expert_0_ep_1 -> layer_6_agg_ep_1
	layer_6_expert_1_ep_1 -> layer_6_agg_ep_1
	layer_6_expert_2_ep_1 -> layer_6_agg_ep_1
	layer_6_expert_3_ep_1 -> layer_6_agg_ep_1
	layer_6_expert_4_ep_1 -> layer_6_agg_ep_1
	layer_6_expert_5_ep_1 -> layer_6_agg_ep_1
	layer_6_expert_6_ep_1 -> layer_6_agg_ep_1
	layer_6_expert_7_ep_1 -> layer_6_agg_ep_1
	layer_6_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_6_expert_0_ep_2 -> layer_6_agg_ep_2
	layer_6_expert_1_ep_2 -> layer_6_agg_ep_2
	layer_6_expert_2_ep_2 -> layer_6_agg_ep_2
	layer_6_expert_3_ep_2 -> layer_6_agg_ep_2
	layer_6_expert_4_ep_2 -> layer_6_agg_ep_2
	layer_6_expert_5_ep_2 -> layer_6_agg_ep_2
	layer_6_expert_6_ep_2 -> layer_6_agg_ep_2
	layer_6_expert_7_ep_2 -> layer_6_agg_ep_2
	layer_6_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_6_expert_0_ep_3 -> layer_6_agg_ep_3
	layer_6_expert_1_ep_3 -> layer_6_agg_ep_3
	layer_6_expert_2_ep_3 -> layer_6_agg_ep_3
	layer_6_expert_3_ep_3 -> layer_6_agg_ep_3
	layer_6_expert_4_ep_3 -> layer_6_agg_ep_3
	layer_6_expert_5_ep_3 -> layer_6_agg_ep_3
	layer_6_expert_6_ep_3 -> layer_6_agg_ep_3
	layer_6_expert_7_ep_3 -> layer_6_agg_ep_3
	layer_6_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_6_expert_0_ep_4 -> layer_6_agg_ep_4
	layer_6_expert_1_ep_4 -> layer_6_agg_ep_4
	layer_6_expert_2_ep_4 -> layer_6_agg_ep_4
	layer_6_expert_3_ep_4 -> layer_6_agg_ep_4
	layer_6_expert_4_ep_4 -> layer_6_agg_ep_4
	layer_6_expert_5_ep_4 -> layer_6_agg_ep_4
	layer_6_expert_6_ep_4 -> layer_6_agg_ep_4
	layer_6_expert_7_ep_4 -> layer_6_agg_ep_4
	layer_6_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_6_expert_0_ep_5 -> layer_6_agg_ep_5
	layer_6_expert_1_ep_5 -> layer_6_agg_ep_5
	layer_6_expert_2_ep_5 -> layer_6_agg_ep_5
	layer_6_expert_3_ep_5 -> layer_6_agg_ep_5
	layer_6_expert_4_ep_5 -> layer_6_agg_ep_5
	layer_6_expert_5_ep_5 -> layer_6_agg_ep_5
	layer_6_expert_6_ep_5 -> layer_6_agg_ep_5
	layer_6_expert_7_ep_5 -> layer_6_agg_ep_5
	layer_6_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_6_expert_0_ep_6 -> layer_6_agg_ep_6
	layer_6_expert_1_ep_6 -> layer_6_agg_ep_6
	layer_6_expert_2_ep_6 -> layer_6_agg_ep_6
	layer_6_expert_3_ep_6 -> layer_6_agg_ep_6
	layer_6_expert_4_ep_6 -> layer_6_agg_ep_6
	layer_6_expert_5_ep_6 -> layer_6_agg_ep_6
	layer_6_expert_6_ep_6 -> layer_6_agg_ep_6
	layer_6_expert_7_ep_6 -> layer_6_agg_ep_6
	layer_6_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_6_expert_0_ep_7 -> layer_6_agg_ep_7
	layer_6_expert_1_ep_7 -> layer_6_agg_ep_7
	layer_6_expert_2_ep_7 -> layer_6_agg_ep_7
	layer_6_expert_3_ep_7 -> layer_6_agg_ep_7
	layer_6_expert_4_ep_7 -> layer_6_agg_ep_7
	layer_6_expert_5_ep_7 -> layer_6_agg_ep_7
	layer_6_expert_6_ep_7 -> layer_6_agg_ep_7
	layer_6_expert_7_ep_7 -> layer_6_agg_ep_7
	layer_6_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_6_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_6_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_6_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_6_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_6_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_6_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_6_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_6_ln_tp_3
	layer_7_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_7_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_7_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_7_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_7_attn_tp_0 -> layer_7_attn_out_tp_0
	layer_7_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_7_attn_tp_1 -> layer_7_attn_out_tp_1
	layer_7_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_7_attn_tp_2 -> layer_7_attn_out_tp_2
	layer_7_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_7_attn_tp_3 -> layer_7_attn_out_tp_3
	layer_7_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_7_gate_tp_0 -> layer_7_route_ep_0 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_0 -> layer_7_route_ep_1 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_0 -> layer_7_route_ep_2 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_0 -> layer_7_route_ep_3 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_0 -> layer_7_route_ep_4 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_0 -> layer_7_route_ep_5 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_0 -> layer_7_route_ep_6 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_0 -> layer_7_route_ep_7 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_1 -> layer_7_route_ep_0 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_1 -> layer_7_route_ep_1 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_1 -> layer_7_route_ep_2 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_1 -> layer_7_route_ep_3 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_1 -> layer_7_route_ep_4 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_1 -> layer_7_route_ep_5 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_1 -> layer_7_route_ep_6 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_1 -> layer_7_route_ep_7 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_2 -> layer_7_route_ep_0 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_2 -> layer_7_route_ep_1 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_2 -> layer_7_route_ep_2 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_2 -> layer_7_route_ep_3 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_2 -> layer_7_route_ep_4 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_2 -> layer_7_route_ep_5 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_2 -> layer_7_route_ep_6 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_2 -> layer_7_route_ep_7 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_3 -> layer_7_route_ep_0 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_3 -> layer_7_route_ep_1 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_3 -> layer_7_route_ep_2 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_3 -> layer_7_route_ep_3 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_3 -> layer_7_route_ep_4 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_3 -> layer_7_route_ep_5 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_3 -> layer_7_route_ep_6 [label="Gate Selection" style=dashed]
	layer_7_gate_tp_3 -> layer_7_route_ep_7 [label="Gate Selection" style=dashed]
	layer_7_route_ep_0 -> layer_7_expert_0_ep_0
	layer_7_route_ep_0 -> layer_7_expert_1_ep_0
	layer_7_route_ep_0 -> layer_7_expert_2_ep_0
	layer_7_route_ep_0 -> layer_7_expert_3_ep_0
	layer_7_route_ep_0 -> layer_7_expert_4_ep_0
	layer_7_route_ep_0 -> layer_7_expert_5_ep_0
	layer_7_route_ep_0 -> layer_7_expert_6_ep_0
	layer_7_route_ep_0 -> layer_7_expert_7_ep_0
	layer_7_route_ep_1 -> layer_7_expert_0_ep_1
	layer_7_route_ep_1 -> layer_7_expert_1_ep_1
	layer_7_route_ep_1 -> layer_7_expert_2_ep_1
	layer_7_route_ep_1 -> layer_7_expert_3_ep_1
	layer_7_route_ep_1 -> layer_7_expert_4_ep_1
	layer_7_route_ep_1 -> layer_7_expert_5_ep_1
	layer_7_route_ep_1 -> layer_7_expert_6_ep_1
	layer_7_route_ep_1 -> layer_7_expert_7_ep_1
	layer_7_route_ep_2 -> layer_7_expert_0_ep_2
	layer_7_route_ep_2 -> layer_7_expert_1_ep_2
	layer_7_route_ep_2 -> layer_7_expert_2_ep_2
	layer_7_route_ep_2 -> layer_7_expert_3_ep_2
	layer_7_route_ep_2 -> layer_7_expert_4_ep_2
	layer_7_route_ep_2 -> layer_7_expert_5_ep_2
	layer_7_route_ep_2 -> layer_7_expert_6_ep_2
	layer_7_route_ep_2 -> layer_7_expert_7_ep_2
	layer_7_route_ep_3 -> layer_7_expert_0_ep_3
	layer_7_route_ep_3 -> layer_7_expert_1_ep_3
	layer_7_route_ep_3 -> layer_7_expert_2_ep_3
	layer_7_route_ep_3 -> layer_7_expert_3_ep_3
	layer_7_route_ep_3 -> layer_7_expert_4_ep_3
	layer_7_route_ep_3 -> layer_7_expert_5_ep_3
	layer_7_route_ep_3 -> layer_7_expert_6_ep_3
	layer_7_route_ep_3 -> layer_7_expert_7_ep_3
	layer_7_route_ep_4 -> layer_7_expert_0_ep_4
	layer_7_route_ep_4 -> layer_7_expert_1_ep_4
	layer_7_route_ep_4 -> layer_7_expert_2_ep_4
	layer_7_route_ep_4 -> layer_7_expert_3_ep_4
	layer_7_route_ep_4 -> layer_7_expert_4_ep_4
	layer_7_route_ep_4 -> layer_7_expert_5_ep_4
	layer_7_route_ep_4 -> layer_7_expert_6_ep_4
	layer_7_route_ep_4 -> layer_7_expert_7_ep_4
	layer_7_route_ep_5 -> layer_7_expert_0_ep_5
	layer_7_route_ep_5 -> layer_7_expert_1_ep_5
	layer_7_route_ep_5 -> layer_7_expert_2_ep_5
	layer_7_route_ep_5 -> layer_7_expert_3_ep_5
	layer_7_route_ep_5 -> layer_7_expert_4_ep_5
	layer_7_route_ep_5 -> layer_7_expert_5_ep_5
	layer_7_route_ep_5 -> layer_7_expert_6_ep_5
	layer_7_route_ep_5 -> layer_7_expert_7_ep_5
	layer_7_route_ep_6 -> layer_7_expert_0_ep_6
	layer_7_route_ep_6 -> layer_7_expert_1_ep_6
	layer_7_route_ep_6 -> layer_7_expert_2_ep_6
	layer_7_route_ep_6 -> layer_7_expert_3_ep_6
	layer_7_route_ep_6 -> layer_7_expert_4_ep_6
	layer_7_route_ep_6 -> layer_7_expert_5_ep_6
	layer_7_route_ep_6 -> layer_7_expert_6_ep_6
	layer_7_route_ep_6 -> layer_7_expert_7_ep_6
	layer_7_route_ep_7 -> layer_7_expert_0_ep_7
	layer_7_route_ep_7 -> layer_7_expert_1_ep_7
	layer_7_route_ep_7 -> layer_7_expert_2_ep_7
	layer_7_route_ep_7 -> layer_7_expert_3_ep_7
	layer_7_route_ep_7 -> layer_7_expert_4_ep_7
	layer_7_route_ep_7 -> layer_7_expert_5_ep_7
	layer_7_route_ep_7 -> layer_7_expert_6_ep_7
	layer_7_route_ep_7 -> layer_7_expert_7_ep_7
	layer_7_expert_0_ep_0 -> layer_7_agg_ep_0
	layer_7_expert_1_ep_0 -> layer_7_agg_ep_0
	layer_7_expert_2_ep_0 -> layer_7_agg_ep_0
	layer_7_expert_3_ep_0 -> layer_7_agg_ep_0
	layer_7_expert_4_ep_0 -> layer_7_agg_ep_0
	layer_7_expert_5_ep_0 -> layer_7_agg_ep_0
	layer_7_expert_6_ep_0 -> layer_7_agg_ep_0
	layer_7_expert_7_ep_0 -> layer_7_agg_ep_0
	layer_7_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_7_expert_0_ep_1 -> layer_7_agg_ep_1
	layer_7_expert_1_ep_1 -> layer_7_agg_ep_1
	layer_7_expert_2_ep_1 -> layer_7_agg_ep_1
	layer_7_expert_3_ep_1 -> layer_7_agg_ep_1
	layer_7_expert_4_ep_1 -> layer_7_agg_ep_1
	layer_7_expert_5_ep_1 -> layer_7_agg_ep_1
	layer_7_expert_6_ep_1 -> layer_7_agg_ep_1
	layer_7_expert_7_ep_1 -> layer_7_agg_ep_1
	layer_7_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_7_expert_0_ep_2 -> layer_7_agg_ep_2
	layer_7_expert_1_ep_2 -> layer_7_agg_ep_2
	layer_7_expert_2_ep_2 -> layer_7_agg_ep_2
	layer_7_expert_3_ep_2 -> layer_7_agg_ep_2
	layer_7_expert_4_ep_2 -> layer_7_agg_ep_2
	layer_7_expert_5_ep_2 -> layer_7_agg_ep_2
	layer_7_expert_6_ep_2 -> layer_7_agg_ep_2
	layer_7_expert_7_ep_2 -> layer_7_agg_ep_2
	layer_7_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_7_expert_0_ep_3 -> layer_7_agg_ep_3
	layer_7_expert_1_ep_3 -> layer_7_agg_ep_3
	layer_7_expert_2_ep_3 -> layer_7_agg_ep_3
	layer_7_expert_3_ep_3 -> layer_7_agg_ep_3
	layer_7_expert_4_ep_3 -> layer_7_agg_ep_3
	layer_7_expert_5_ep_3 -> layer_7_agg_ep_3
	layer_7_expert_6_ep_3 -> layer_7_agg_ep_3
	layer_7_expert_7_ep_3 -> layer_7_agg_ep_3
	layer_7_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_7_expert_0_ep_4 -> layer_7_agg_ep_4
	layer_7_expert_1_ep_4 -> layer_7_agg_ep_4
	layer_7_expert_2_ep_4 -> layer_7_agg_ep_4
	layer_7_expert_3_ep_4 -> layer_7_agg_ep_4
	layer_7_expert_4_ep_4 -> layer_7_agg_ep_4
	layer_7_expert_5_ep_4 -> layer_7_agg_ep_4
	layer_7_expert_6_ep_4 -> layer_7_agg_ep_4
	layer_7_expert_7_ep_4 -> layer_7_agg_ep_4
	layer_7_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_7_expert_0_ep_5 -> layer_7_agg_ep_5
	layer_7_expert_1_ep_5 -> layer_7_agg_ep_5
	layer_7_expert_2_ep_5 -> layer_7_agg_ep_5
	layer_7_expert_3_ep_5 -> layer_7_agg_ep_5
	layer_7_expert_4_ep_5 -> layer_7_agg_ep_5
	layer_7_expert_5_ep_5 -> layer_7_agg_ep_5
	layer_7_expert_6_ep_5 -> layer_7_agg_ep_5
	layer_7_expert_7_ep_5 -> layer_7_agg_ep_5
	layer_7_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_7_expert_0_ep_6 -> layer_7_agg_ep_6
	layer_7_expert_1_ep_6 -> layer_7_agg_ep_6
	layer_7_expert_2_ep_6 -> layer_7_agg_ep_6
	layer_7_expert_3_ep_6 -> layer_7_agg_ep_6
	layer_7_expert_4_ep_6 -> layer_7_agg_ep_6
	layer_7_expert_5_ep_6 -> layer_7_agg_ep_6
	layer_7_expert_6_ep_6 -> layer_7_agg_ep_6
	layer_7_expert_7_ep_6 -> layer_7_agg_ep_6
	layer_7_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_7_expert_0_ep_7 -> layer_7_agg_ep_7
	layer_7_expert_1_ep_7 -> layer_7_agg_ep_7
	layer_7_expert_2_ep_7 -> layer_7_agg_ep_7
	layer_7_expert_3_ep_7 -> layer_7_agg_ep_7
	layer_7_expert_4_ep_7 -> layer_7_agg_ep_7
	layer_7_expert_5_ep_7 -> layer_7_agg_ep_7
	layer_7_expert_6_ep_7 -> layer_7_agg_ep_7
	layer_7_expert_7_ep_7 -> layer_7_agg_ep_7
	layer_7_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_7_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_7_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_7_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_7_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_7_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_7_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_7_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_7_ln_tp_3
	layer_8_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_8_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_8_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_8_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_8_attn_tp_0 -> layer_8_attn_out_tp_0
	layer_8_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_8_attn_tp_1 -> layer_8_attn_out_tp_1
	layer_8_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_8_attn_tp_2 -> layer_8_attn_out_tp_2
	layer_8_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_8_attn_tp_3 -> layer_8_attn_out_tp_3
	layer_8_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_8_gate_tp_0 -> layer_8_route_ep_0 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_0 -> layer_8_route_ep_1 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_0 -> layer_8_route_ep_2 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_0 -> layer_8_route_ep_3 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_0 -> layer_8_route_ep_4 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_0 -> layer_8_route_ep_5 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_0 -> layer_8_route_ep_6 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_0 -> layer_8_route_ep_7 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_1 -> layer_8_route_ep_0 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_1 -> layer_8_route_ep_1 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_1 -> layer_8_route_ep_2 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_1 -> layer_8_route_ep_3 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_1 -> layer_8_route_ep_4 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_1 -> layer_8_route_ep_5 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_1 -> layer_8_route_ep_6 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_1 -> layer_8_route_ep_7 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_2 -> layer_8_route_ep_0 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_2 -> layer_8_route_ep_1 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_2 -> layer_8_route_ep_2 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_2 -> layer_8_route_ep_3 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_2 -> layer_8_route_ep_4 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_2 -> layer_8_route_ep_5 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_2 -> layer_8_route_ep_6 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_2 -> layer_8_route_ep_7 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_3 -> layer_8_route_ep_0 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_3 -> layer_8_route_ep_1 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_3 -> layer_8_route_ep_2 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_3 -> layer_8_route_ep_3 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_3 -> layer_8_route_ep_4 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_3 -> layer_8_route_ep_5 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_3 -> layer_8_route_ep_6 [label="Gate Selection" style=dashed]
	layer_8_gate_tp_3 -> layer_8_route_ep_7 [label="Gate Selection" style=dashed]
	layer_8_route_ep_0 -> layer_8_expert_0_ep_0
	layer_8_route_ep_0 -> layer_8_expert_1_ep_0
	layer_8_route_ep_0 -> layer_8_expert_2_ep_0
	layer_8_route_ep_0 -> layer_8_expert_3_ep_0
	layer_8_route_ep_0 -> layer_8_expert_4_ep_0
	layer_8_route_ep_0 -> layer_8_expert_5_ep_0
	layer_8_route_ep_0 -> layer_8_expert_6_ep_0
	layer_8_route_ep_0 -> layer_8_expert_7_ep_0
	layer_8_route_ep_1 -> layer_8_expert_0_ep_1
	layer_8_route_ep_1 -> layer_8_expert_1_ep_1
	layer_8_route_ep_1 -> layer_8_expert_2_ep_1
	layer_8_route_ep_1 -> layer_8_expert_3_ep_1
	layer_8_route_ep_1 -> layer_8_expert_4_ep_1
	layer_8_route_ep_1 -> layer_8_expert_5_ep_1
	layer_8_route_ep_1 -> layer_8_expert_6_ep_1
	layer_8_route_ep_1 -> layer_8_expert_7_ep_1
	layer_8_route_ep_2 -> layer_8_expert_0_ep_2
	layer_8_route_ep_2 -> layer_8_expert_1_ep_2
	layer_8_route_ep_2 -> layer_8_expert_2_ep_2
	layer_8_route_ep_2 -> layer_8_expert_3_ep_2
	layer_8_route_ep_2 -> layer_8_expert_4_ep_2
	layer_8_route_ep_2 -> layer_8_expert_5_ep_2
	layer_8_route_ep_2 -> layer_8_expert_6_ep_2
	layer_8_route_ep_2 -> layer_8_expert_7_ep_2
	layer_8_route_ep_3 -> layer_8_expert_0_ep_3
	layer_8_route_ep_3 -> layer_8_expert_1_ep_3
	layer_8_route_ep_3 -> layer_8_expert_2_ep_3
	layer_8_route_ep_3 -> layer_8_expert_3_ep_3
	layer_8_route_ep_3 -> layer_8_expert_4_ep_3
	layer_8_route_ep_3 -> layer_8_expert_5_ep_3
	layer_8_route_ep_3 -> layer_8_expert_6_ep_3
	layer_8_route_ep_3 -> layer_8_expert_7_ep_3
	layer_8_route_ep_4 -> layer_8_expert_0_ep_4
	layer_8_route_ep_4 -> layer_8_expert_1_ep_4
	layer_8_route_ep_4 -> layer_8_expert_2_ep_4
	layer_8_route_ep_4 -> layer_8_expert_3_ep_4
	layer_8_route_ep_4 -> layer_8_expert_4_ep_4
	layer_8_route_ep_4 -> layer_8_expert_5_ep_4
	layer_8_route_ep_4 -> layer_8_expert_6_ep_4
	layer_8_route_ep_4 -> layer_8_expert_7_ep_4
	layer_8_route_ep_5 -> layer_8_expert_0_ep_5
	layer_8_route_ep_5 -> layer_8_expert_1_ep_5
	layer_8_route_ep_5 -> layer_8_expert_2_ep_5
	layer_8_route_ep_5 -> layer_8_expert_3_ep_5
	layer_8_route_ep_5 -> layer_8_expert_4_ep_5
	layer_8_route_ep_5 -> layer_8_expert_5_ep_5
	layer_8_route_ep_5 -> layer_8_expert_6_ep_5
	layer_8_route_ep_5 -> layer_8_expert_7_ep_5
	layer_8_route_ep_6 -> layer_8_expert_0_ep_6
	layer_8_route_ep_6 -> layer_8_expert_1_ep_6
	layer_8_route_ep_6 -> layer_8_expert_2_ep_6
	layer_8_route_ep_6 -> layer_8_expert_3_ep_6
	layer_8_route_ep_6 -> layer_8_expert_4_ep_6
	layer_8_route_ep_6 -> layer_8_expert_5_ep_6
	layer_8_route_ep_6 -> layer_8_expert_6_ep_6
	layer_8_route_ep_6 -> layer_8_expert_7_ep_6
	layer_8_route_ep_7 -> layer_8_expert_0_ep_7
	layer_8_route_ep_7 -> layer_8_expert_1_ep_7
	layer_8_route_ep_7 -> layer_8_expert_2_ep_7
	layer_8_route_ep_7 -> layer_8_expert_3_ep_7
	layer_8_route_ep_7 -> layer_8_expert_4_ep_7
	layer_8_route_ep_7 -> layer_8_expert_5_ep_7
	layer_8_route_ep_7 -> layer_8_expert_6_ep_7
	layer_8_route_ep_7 -> layer_8_expert_7_ep_7
	layer_8_expert_0_ep_0 -> layer_8_agg_ep_0
	layer_8_expert_1_ep_0 -> layer_8_agg_ep_0
	layer_8_expert_2_ep_0 -> layer_8_agg_ep_0
	layer_8_expert_3_ep_0 -> layer_8_agg_ep_0
	layer_8_expert_4_ep_0 -> layer_8_agg_ep_0
	layer_8_expert_5_ep_0 -> layer_8_agg_ep_0
	layer_8_expert_6_ep_0 -> layer_8_agg_ep_0
	layer_8_expert_7_ep_0 -> layer_8_agg_ep_0
	layer_8_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_8_expert_0_ep_1 -> layer_8_agg_ep_1
	layer_8_expert_1_ep_1 -> layer_8_agg_ep_1
	layer_8_expert_2_ep_1 -> layer_8_agg_ep_1
	layer_8_expert_3_ep_1 -> layer_8_agg_ep_1
	layer_8_expert_4_ep_1 -> layer_8_agg_ep_1
	layer_8_expert_5_ep_1 -> layer_8_agg_ep_1
	layer_8_expert_6_ep_1 -> layer_8_agg_ep_1
	layer_8_expert_7_ep_1 -> layer_8_agg_ep_1
	layer_8_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_8_expert_0_ep_2 -> layer_8_agg_ep_2
	layer_8_expert_1_ep_2 -> layer_8_agg_ep_2
	layer_8_expert_2_ep_2 -> layer_8_agg_ep_2
	layer_8_expert_3_ep_2 -> layer_8_agg_ep_2
	layer_8_expert_4_ep_2 -> layer_8_agg_ep_2
	layer_8_expert_5_ep_2 -> layer_8_agg_ep_2
	layer_8_expert_6_ep_2 -> layer_8_agg_ep_2
	layer_8_expert_7_ep_2 -> layer_8_agg_ep_2
	layer_8_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_8_expert_0_ep_3 -> layer_8_agg_ep_3
	layer_8_expert_1_ep_3 -> layer_8_agg_ep_3
	layer_8_expert_2_ep_3 -> layer_8_agg_ep_3
	layer_8_expert_3_ep_3 -> layer_8_agg_ep_3
	layer_8_expert_4_ep_3 -> layer_8_agg_ep_3
	layer_8_expert_5_ep_3 -> layer_8_agg_ep_3
	layer_8_expert_6_ep_3 -> layer_8_agg_ep_3
	layer_8_expert_7_ep_3 -> layer_8_agg_ep_3
	layer_8_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_8_expert_0_ep_4 -> layer_8_agg_ep_4
	layer_8_expert_1_ep_4 -> layer_8_agg_ep_4
	layer_8_expert_2_ep_4 -> layer_8_agg_ep_4
	layer_8_expert_3_ep_4 -> layer_8_agg_ep_4
	layer_8_expert_4_ep_4 -> layer_8_agg_ep_4
	layer_8_expert_5_ep_4 -> layer_8_agg_ep_4
	layer_8_expert_6_ep_4 -> layer_8_agg_ep_4
	layer_8_expert_7_ep_4 -> layer_8_agg_ep_4
	layer_8_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_8_expert_0_ep_5 -> layer_8_agg_ep_5
	layer_8_expert_1_ep_5 -> layer_8_agg_ep_5
	layer_8_expert_2_ep_5 -> layer_8_agg_ep_5
	layer_8_expert_3_ep_5 -> layer_8_agg_ep_5
	layer_8_expert_4_ep_5 -> layer_8_agg_ep_5
	layer_8_expert_5_ep_5 -> layer_8_agg_ep_5
	layer_8_expert_6_ep_5 -> layer_8_agg_ep_5
	layer_8_expert_7_ep_5 -> layer_8_agg_ep_5
	layer_8_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_8_expert_0_ep_6 -> layer_8_agg_ep_6
	layer_8_expert_1_ep_6 -> layer_8_agg_ep_6
	layer_8_expert_2_ep_6 -> layer_8_agg_ep_6
	layer_8_expert_3_ep_6 -> layer_8_agg_ep_6
	layer_8_expert_4_ep_6 -> layer_8_agg_ep_6
	layer_8_expert_5_ep_6 -> layer_8_agg_ep_6
	layer_8_expert_6_ep_6 -> layer_8_agg_ep_6
	layer_8_expert_7_ep_6 -> layer_8_agg_ep_6
	layer_8_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_8_expert_0_ep_7 -> layer_8_agg_ep_7
	layer_8_expert_1_ep_7 -> layer_8_agg_ep_7
	layer_8_expert_2_ep_7 -> layer_8_agg_ep_7
	layer_8_expert_3_ep_7 -> layer_8_agg_ep_7
	layer_8_expert_4_ep_7 -> layer_8_agg_ep_7
	layer_8_expert_5_ep_7 -> layer_8_agg_ep_7
	layer_8_expert_6_ep_7 -> layer_8_agg_ep_7
	layer_8_expert_7_ep_7 -> layer_8_agg_ep_7
	layer_8_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_8_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_8_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_8_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_8_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_8_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_8_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_8_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_8_ln_tp_3
	layer_9_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_9_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_9_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_9_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_9_attn_tp_0 -> layer_9_attn_out_tp_0
	layer_9_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_9_attn_tp_1 -> layer_9_attn_out_tp_1
	layer_9_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_9_attn_tp_2 -> layer_9_attn_out_tp_2
	layer_9_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_9_attn_tp_3 -> layer_9_attn_out_tp_3
	layer_9_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_9_gate_tp_0 -> layer_9_route_ep_0 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_0 -> layer_9_route_ep_1 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_0 -> layer_9_route_ep_2 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_0 -> layer_9_route_ep_3 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_0 -> layer_9_route_ep_4 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_0 -> layer_9_route_ep_5 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_0 -> layer_9_route_ep_6 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_0 -> layer_9_route_ep_7 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_1 -> layer_9_route_ep_0 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_1 -> layer_9_route_ep_1 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_1 -> layer_9_route_ep_2 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_1 -> layer_9_route_ep_3 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_1 -> layer_9_route_ep_4 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_1 -> layer_9_route_ep_5 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_1 -> layer_9_route_ep_6 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_1 -> layer_9_route_ep_7 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_2 -> layer_9_route_ep_0 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_2 -> layer_9_route_ep_1 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_2 -> layer_9_route_ep_2 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_2 -> layer_9_route_ep_3 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_2 -> layer_9_route_ep_4 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_2 -> layer_9_route_ep_5 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_2 -> layer_9_route_ep_6 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_2 -> layer_9_route_ep_7 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_3 -> layer_9_route_ep_0 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_3 -> layer_9_route_ep_1 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_3 -> layer_9_route_ep_2 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_3 -> layer_9_route_ep_3 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_3 -> layer_9_route_ep_4 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_3 -> layer_9_route_ep_5 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_3 -> layer_9_route_ep_6 [label="Gate Selection" style=dashed]
	layer_9_gate_tp_3 -> layer_9_route_ep_7 [label="Gate Selection" style=dashed]
	layer_9_route_ep_0 -> layer_9_expert_0_ep_0
	layer_9_route_ep_0 -> layer_9_expert_1_ep_0
	layer_9_route_ep_0 -> layer_9_expert_2_ep_0
	layer_9_route_ep_0 -> layer_9_expert_3_ep_0
	layer_9_route_ep_0 -> layer_9_expert_4_ep_0
	layer_9_route_ep_0 -> layer_9_expert_5_ep_0
	layer_9_route_ep_0 -> layer_9_expert_6_ep_0
	layer_9_route_ep_0 -> layer_9_expert_7_ep_0
	layer_9_route_ep_1 -> layer_9_expert_0_ep_1
	layer_9_route_ep_1 -> layer_9_expert_1_ep_1
	layer_9_route_ep_1 -> layer_9_expert_2_ep_1
	layer_9_route_ep_1 -> layer_9_expert_3_ep_1
	layer_9_route_ep_1 -> layer_9_expert_4_ep_1
	layer_9_route_ep_1 -> layer_9_expert_5_ep_1
	layer_9_route_ep_1 -> layer_9_expert_6_ep_1
	layer_9_route_ep_1 -> layer_9_expert_7_ep_1
	layer_9_route_ep_2 -> layer_9_expert_0_ep_2
	layer_9_route_ep_2 -> layer_9_expert_1_ep_2
	layer_9_route_ep_2 -> layer_9_expert_2_ep_2
	layer_9_route_ep_2 -> layer_9_expert_3_ep_2
	layer_9_route_ep_2 -> layer_9_expert_4_ep_2
	layer_9_route_ep_2 -> layer_9_expert_5_ep_2
	layer_9_route_ep_2 -> layer_9_expert_6_ep_2
	layer_9_route_ep_2 -> layer_9_expert_7_ep_2
	layer_9_route_ep_3 -> layer_9_expert_0_ep_3
	layer_9_route_ep_3 -> layer_9_expert_1_ep_3
	layer_9_route_ep_3 -> layer_9_expert_2_ep_3
	layer_9_route_ep_3 -> layer_9_expert_3_ep_3
	layer_9_route_ep_3 -> layer_9_expert_4_ep_3
	layer_9_route_ep_3 -> layer_9_expert_5_ep_3
	layer_9_route_ep_3 -> layer_9_expert_6_ep_3
	layer_9_route_ep_3 -> layer_9_expert_7_ep_3
	layer_9_route_ep_4 -> layer_9_expert_0_ep_4
	layer_9_route_ep_4 -> layer_9_expert_1_ep_4
	layer_9_route_ep_4 -> layer_9_expert_2_ep_4
	layer_9_route_ep_4 -> layer_9_expert_3_ep_4
	layer_9_route_ep_4 -> layer_9_expert_4_ep_4
	layer_9_route_ep_4 -> layer_9_expert_5_ep_4
	layer_9_route_ep_4 -> layer_9_expert_6_ep_4
	layer_9_route_ep_4 -> layer_9_expert_7_ep_4
	layer_9_route_ep_5 -> layer_9_expert_0_ep_5
	layer_9_route_ep_5 -> layer_9_expert_1_ep_5
	layer_9_route_ep_5 -> layer_9_expert_2_ep_5
	layer_9_route_ep_5 -> layer_9_expert_3_ep_5
	layer_9_route_ep_5 -> layer_9_expert_4_ep_5
	layer_9_route_ep_5 -> layer_9_expert_5_ep_5
	layer_9_route_ep_5 -> layer_9_expert_6_ep_5
	layer_9_route_ep_5 -> layer_9_expert_7_ep_5
	layer_9_route_ep_6 -> layer_9_expert_0_ep_6
	layer_9_route_ep_6 -> layer_9_expert_1_ep_6
	layer_9_route_ep_6 -> layer_9_expert_2_ep_6
	layer_9_route_ep_6 -> layer_9_expert_3_ep_6
	layer_9_route_ep_6 -> layer_9_expert_4_ep_6
	layer_9_route_ep_6 -> layer_9_expert_5_ep_6
	layer_9_route_ep_6 -> layer_9_expert_6_ep_6
	layer_9_route_ep_6 -> layer_9_expert_7_ep_6
	layer_9_route_ep_7 -> layer_9_expert_0_ep_7
	layer_9_route_ep_7 -> layer_9_expert_1_ep_7
	layer_9_route_ep_7 -> layer_9_expert_2_ep_7
	layer_9_route_ep_7 -> layer_9_expert_3_ep_7
	layer_9_route_ep_7 -> layer_9_expert_4_ep_7
	layer_9_route_ep_7 -> layer_9_expert_5_ep_7
	layer_9_route_ep_7 -> layer_9_expert_6_ep_7
	layer_9_route_ep_7 -> layer_9_expert_7_ep_7
	layer_9_expert_0_ep_0 -> layer_9_agg_ep_0
	layer_9_expert_1_ep_0 -> layer_9_agg_ep_0
	layer_9_expert_2_ep_0 -> layer_9_agg_ep_0
	layer_9_expert_3_ep_0 -> layer_9_agg_ep_0
	layer_9_expert_4_ep_0 -> layer_9_agg_ep_0
	layer_9_expert_5_ep_0 -> layer_9_agg_ep_0
	layer_9_expert_6_ep_0 -> layer_9_agg_ep_0
	layer_9_expert_7_ep_0 -> layer_9_agg_ep_0
	layer_9_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_9_expert_0_ep_1 -> layer_9_agg_ep_1
	layer_9_expert_1_ep_1 -> layer_9_agg_ep_1
	layer_9_expert_2_ep_1 -> layer_9_agg_ep_1
	layer_9_expert_3_ep_1 -> layer_9_agg_ep_1
	layer_9_expert_4_ep_1 -> layer_9_agg_ep_1
	layer_9_expert_5_ep_1 -> layer_9_agg_ep_1
	layer_9_expert_6_ep_1 -> layer_9_agg_ep_1
	layer_9_expert_7_ep_1 -> layer_9_agg_ep_1
	layer_9_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_9_expert_0_ep_2 -> layer_9_agg_ep_2
	layer_9_expert_1_ep_2 -> layer_9_agg_ep_2
	layer_9_expert_2_ep_2 -> layer_9_agg_ep_2
	layer_9_expert_3_ep_2 -> layer_9_agg_ep_2
	layer_9_expert_4_ep_2 -> layer_9_agg_ep_2
	layer_9_expert_5_ep_2 -> layer_9_agg_ep_2
	layer_9_expert_6_ep_2 -> layer_9_agg_ep_2
	layer_9_expert_7_ep_2 -> layer_9_agg_ep_2
	layer_9_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_9_expert_0_ep_3 -> layer_9_agg_ep_3
	layer_9_expert_1_ep_3 -> layer_9_agg_ep_3
	layer_9_expert_2_ep_3 -> layer_9_agg_ep_3
	layer_9_expert_3_ep_3 -> layer_9_agg_ep_3
	layer_9_expert_4_ep_3 -> layer_9_agg_ep_3
	layer_9_expert_5_ep_3 -> layer_9_agg_ep_3
	layer_9_expert_6_ep_3 -> layer_9_agg_ep_3
	layer_9_expert_7_ep_3 -> layer_9_agg_ep_3
	layer_9_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_9_expert_0_ep_4 -> layer_9_agg_ep_4
	layer_9_expert_1_ep_4 -> layer_9_agg_ep_4
	layer_9_expert_2_ep_4 -> layer_9_agg_ep_4
	layer_9_expert_3_ep_4 -> layer_9_agg_ep_4
	layer_9_expert_4_ep_4 -> layer_9_agg_ep_4
	layer_9_expert_5_ep_4 -> layer_9_agg_ep_4
	layer_9_expert_6_ep_4 -> layer_9_agg_ep_4
	layer_9_expert_7_ep_4 -> layer_9_agg_ep_4
	layer_9_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_9_expert_0_ep_5 -> layer_9_agg_ep_5
	layer_9_expert_1_ep_5 -> layer_9_agg_ep_5
	layer_9_expert_2_ep_5 -> layer_9_agg_ep_5
	layer_9_expert_3_ep_5 -> layer_9_agg_ep_5
	layer_9_expert_4_ep_5 -> layer_9_agg_ep_5
	layer_9_expert_5_ep_5 -> layer_9_agg_ep_5
	layer_9_expert_6_ep_5 -> layer_9_agg_ep_5
	layer_9_expert_7_ep_5 -> layer_9_agg_ep_5
	layer_9_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_9_expert_0_ep_6 -> layer_9_agg_ep_6
	layer_9_expert_1_ep_6 -> layer_9_agg_ep_6
	layer_9_expert_2_ep_6 -> layer_9_agg_ep_6
	layer_9_expert_3_ep_6 -> layer_9_agg_ep_6
	layer_9_expert_4_ep_6 -> layer_9_agg_ep_6
	layer_9_expert_5_ep_6 -> layer_9_agg_ep_6
	layer_9_expert_6_ep_6 -> layer_9_agg_ep_6
	layer_9_expert_7_ep_6 -> layer_9_agg_ep_6
	layer_9_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_9_expert_0_ep_7 -> layer_9_agg_ep_7
	layer_9_expert_1_ep_7 -> layer_9_agg_ep_7
	layer_9_expert_2_ep_7 -> layer_9_agg_ep_7
	layer_9_expert_3_ep_7 -> layer_9_agg_ep_7
	layer_9_expert_4_ep_7 -> layer_9_agg_ep_7
	layer_9_expert_5_ep_7 -> layer_9_agg_ep_7
	layer_9_expert_6_ep_7 -> layer_9_agg_ep_7
	layer_9_expert_7_ep_7 -> layer_9_agg_ep_7
	layer_9_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_9_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_9_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_9_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_9_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_9_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_9_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_9_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_9_ln_tp_3
	layer_10_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_10_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_10_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_10_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_10_attn_tp_0 -> layer_10_attn_out_tp_0
	layer_10_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_10_attn_tp_1 -> layer_10_attn_out_tp_1
	layer_10_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_10_attn_tp_2 -> layer_10_attn_out_tp_2
	layer_10_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_10_attn_tp_3 -> layer_10_attn_out_tp_3
	layer_10_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_10_gate_tp_0 -> layer_10_route_ep_0 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_0 -> layer_10_route_ep_1 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_0 -> layer_10_route_ep_2 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_0 -> layer_10_route_ep_3 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_0 -> layer_10_route_ep_4 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_0 -> layer_10_route_ep_5 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_0 -> layer_10_route_ep_6 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_0 -> layer_10_route_ep_7 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_1 -> layer_10_route_ep_0 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_1 -> layer_10_route_ep_1 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_1 -> layer_10_route_ep_2 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_1 -> layer_10_route_ep_3 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_1 -> layer_10_route_ep_4 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_1 -> layer_10_route_ep_5 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_1 -> layer_10_route_ep_6 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_1 -> layer_10_route_ep_7 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_2 -> layer_10_route_ep_0 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_2 -> layer_10_route_ep_1 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_2 -> layer_10_route_ep_2 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_2 -> layer_10_route_ep_3 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_2 -> layer_10_route_ep_4 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_2 -> layer_10_route_ep_5 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_2 -> layer_10_route_ep_6 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_2 -> layer_10_route_ep_7 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_3 -> layer_10_route_ep_0 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_3 -> layer_10_route_ep_1 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_3 -> layer_10_route_ep_2 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_3 -> layer_10_route_ep_3 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_3 -> layer_10_route_ep_4 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_3 -> layer_10_route_ep_5 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_3 -> layer_10_route_ep_6 [label="Gate Selection" style=dashed]
	layer_10_gate_tp_3 -> layer_10_route_ep_7 [label="Gate Selection" style=dashed]
	layer_10_route_ep_0 -> layer_10_expert_0_ep_0
	layer_10_route_ep_0 -> layer_10_expert_1_ep_0
	layer_10_route_ep_0 -> layer_10_expert_2_ep_0
	layer_10_route_ep_0 -> layer_10_expert_3_ep_0
	layer_10_route_ep_0 -> layer_10_expert_4_ep_0
	layer_10_route_ep_0 -> layer_10_expert_5_ep_0
	layer_10_route_ep_0 -> layer_10_expert_6_ep_0
	layer_10_route_ep_0 -> layer_10_expert_7_ep_0
	layer_10_route_ep_1 -> layer_10_expert_0_ep_1
	layer_10_route_ep_1 -> layer_10_expert_1_ep_1
	layer_10_route_ep_1 -> layer_10_expert_2_ep_1
	layer_10_route_ep_1 -> layer_10_expert_3_ep_1
	layer_10_route_ep_1 -> layer_10_expert_4_ep_1
	layer_10_route_ep_1 -> layer_10_expert_5_ep_1
	layer_10_route_ep_1 -> layer_10_expert_6_ep_1
	layer_10_route_ep_1 -> layer_10_expert_7_ep_1
	layer_10_route_ep_2 -> layer_10_expert_0_ep_2
	layer_10_route_ep_2 -> layer_10_expert_1_ep_2
	layer_10_route_ep_2 -> layer_10_expert_2_ep_2
	layer_10_route_ep_2 -> layer_10_expert_3_ep_2
	layer_10_route_ep_2 -> layer_10_expert_4_ep_2
	layer_10_route_ep_2 -> layer_10_expert_5_ep_2
	layer_10_route_ep_2 -> layer_10_expert_6_ep_2
	layer_10_route_ep_2 -> layer_10_expert_7_ep_2
	layer_10_route_ep_3 -> layer_10_expert_0_ep_3
	layer_10_route_ep_3 -> layer_10_expert_1_ep_3
	layer_10_route_ep_3 -> layer_10_expert_2_ep_3
	layer_10_route_ep_3 -> layer_10_expert_3_ep_3
	layer_10_route_ep_3 -> layer_10_expert_4_ep_3
	layer_10_route_ep_3 -> layer_10_expert_5_ep_3
	layer_10_route_ep_3 -> layer_10_expert_6_ep_3
	layer_10_route_ep_3 -> layer_10_expert_7_ep_3
	layer_10_route_ep_4 -> layer_10_expert_0_ep_4
	layer_10_route_ep_4 -> layer_10_expert_1_ep_4
	layer_10_route_ep_4 -> layer_10_expert_2_ep_4
	layer_10_route_ep_4 -> layer_10_expert_3_ep_4
	layer_10_route_ep_4 -> layer_10_expert_4_ep_4
	layer_10_route_ep_4 -> layer_10_expert_5_ep_4
	layer_10_route_ep_4 -> layer_10_expert_6_ep_4
	layer_10_route_ep_4 -> layer_10_expert_7_ep_4
	layer_10_route_ep_5 -> layer_10_expert_0_ep_5
	layer_10_route_ep_5 -> layer_10_expert_1_ep_5
	layer_10_route_ep_5 -> layer_10_expert_2_ep_5
	layer_10_route_ep_5 -> layer_10_expert_3_ep_5
	layer_10_route_ep_5 -> layer_10_expert_4_ep_5
	layer_10_route_ep_5 -> layer_10_expert_5_ep_5
	layer_10_route_ep_5 -> layer_10_expert_6_ep_5
	layer_10_route_ep_5 -> layer_10_expert_7_ep_5
	layer_10_route_ep_6 -> layer_10_expert_0_ep_6
	layer_10_route_ep_6 -> layer_10_expert_1_ep_6
	layer_10_route_ep_6 -> layer_10_expert_2_ep_6
	layer_10_route_ep_6 -> layer_10_expert_3_ep_6
	layer_10_route_ep_6 -> layer_10_expert_4_ep_6
	layer_10_route_ep_6 -> layer_10_expert_5_ep_6
	layer_10_route_ep_6 -> layer_10_expert_6_ep_6
	layer_10_route_ep_6 -> layer_10_expert_7_ep_6
	layer_10_route_ep_7 -> layer_10_expert_0_ep_7
	layer_10_route_ep_7 -> layer_10_expert_1_ep_7
	layer_10_route_ep_7 -> layer_10_expert_2_ep_7
	layer_10_route_ep_7 -> layer_10_expert_3_ep_7
	layer_10_route_ep_7 -> layer_10_expert_4_ep_7
	layer_10_route_ep_7 -> layer_10_expert_5_ep_7
	layer_10_route_ep_7 -> layer_10_expert_6_ep_7
	layer_10_route_ep_7 -> layer_10_expert_7_ep_7
	layer_10_expert_0_ep_0 -> layer_10_agg_ep_0
	layer_10_expert_1_ep_0 -> layer_10_agg_ep_0
	layer_10_expert_2_ep_0 -> layer_10_agg_ep_0
	layer_10_expert_3_ep_0 -> layer_10_agg_ep_0
	layer_10_expert_4_ep_0 -> layer_10_agg_ep_0
	layer_10_expert_5_ep_0 -> layer_10_agg_ep_0
	layer_10_expert_6_ep_0 -> layer_10_agg_ep_0
	layer_10_expert_7_ep_0 -> layer_10_agg_ep_0
	layer_10_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_10_expert_0_ep_1 -> layer_10_agg_ep_1
	layer_10_expert_1_ep_1 -> layer_10_agg_ep_1
	layer_10_expert_2_ep_1 -> layer_10_agg_ep_1
	layer_10_expert_3_ep_1 -> layer_10_agg_ep_1
	layer_10_expert_4_ep_1 -> layer_10_agg_ep_1
	layer_10_expert_5_ep_1 -> layer_10_agg_ep_1
	layer_10_expert_6_ep_1 -> layer_10_agg_ep_1
	layer_10_expert_7_ep_1 -> layer_10_agg_ep_1
	layer_10_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_10_expert_0_ep_2 -> layer_10_agg_ep_2
	layer_10_expert_1_ep_2 -> layer_10_agg_ep_2
	layer_10_expert_2_ep_2 -> layer_10_agg_ep_2
	layer_10_expert_3_ep_2 -> layer_10_agg_ep_2
	layer_10_expert_4_ep_2 -> layer_10_agg_ep_2
	layer_10_expert_5_ep_2 -> layer_10_agg_ep_2
	layer_10_expert_6_ep_2 -> layer_10_agg_ep_2
	layer_10_expert_7_ep_2 -> layer_10_agg_ep_2
	layer_10_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_10_expert_0_ep_3 -> layer_10_agg_ep_3
	layer_10_expert_1_ep_3 -> layer_10_agg_ep_3
	layer_10_expert_2_ep_3 -> layer_10_agg_ep_3
	layer_10_expert_3_ep_3 -> layer_10_agg_ep_3
	layer_10_expert_4_ep_3 -> layer_10_agg_ep_3
	layer_10_expert_5_ep_3 -> layer_10_agg_ep_3
	layer_10_expert_6_ep_3 -> layer_10_agg_ep_3
	layer_10_expert_7_ep_3 -> layer_10_agg_ep_3
	layer_10_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_10_expert_0_ep_4 -> layer_10_agg_ep_4
	layer_10_expert_1_ep_4 -> layer_10_agg_ep_4
	layer_10_expert_2_ep_4 -> layer_10_agg_ep_4
	layer_10_expert_3_ep_4 -> layer_10_agg_ep_4
	layer_10_expert_4_ep_4 -> layer_10_agg_ep_4
	layer_10_expert_5_ep_4 -> layer_10_agg_ep_4
	layer_10_expert_6_ep_4 -> layer_10_agg_ep_4
	layer_10_expert_7_ep_4 -> layer_10_agg_ep_4
	layer_10_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_10_expert_0_ep_5 -> layer_10_agg_ep_5
	layer_10_expert_1_ep_5 -> layer_10_agg_ep_5
	layer_10_expert_2_ep_5 -> layer_10_agg_ep_5
	layer_10_expert_3_ep_5 -> layer_10_agg_ep_5
	layer_10_expert_4_ep_5 -> layer_10_agg_ep_5
	layer_10_expert_5_ep_5 -> layer_10_agg_ep_5
	layer_10_expert_6_ep_5 -> layer_10_agg_ep_5
	layer_10_expert_7_ep_5 -> layer_10_agg_ep_5
	layer_10_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_10_expert_0_ep_6 -> layer_10_agg_ep_6
	layer_10_expert_1_ep_6 -> layer_10_agg_ep_6
	layer_10_expert_2_ep_6 -> layer_10_agg_ep_6
	layer_10_expert_3_ep_6 -> layer_10_agg_ep_6
	layer_10_expert_4_ep_6 -> layer_10_agg_ep_6
	layer_10_expert_5_ep_6 -> layer_10_agg_ep_6
	layer_10_expert_6_ep_6 -> layer_10_agg_ep_6
	layer_10_expert_7_ep_6 -> layer_10_agg_ep_6
	layer_10_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_10_expert_0_ep_7 -> layer_10_agg_ep_7
	layer_10_expert_1_ep_7 -> layer_10_agg_ep_7
	layer_10_expert_2_ep_7 -> layer_10_agg_ep_7
	layer_10_expert_3_ep_7 -> layer_10_agg_ep_7
	layer_10_expert_4_ep_7 -> layer_10_agg_ep_7
	layer_10_expert_5_ep_7 -> layer_10_agg_ep_7
	layer_10_expert_6_ep_7 -> layer_10_agg_ep_7
	layer_10_expert_7_ep_7 -> layer_10_agg_ep_7
	layer_10_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_10_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_10_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_10_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_10_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_10_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_10_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_10_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_10_ln_tp_3
	layer_11_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_11_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_11_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_11_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_11_attn_tp_0 -> layer_11_attn_out_tp_0
	layer_11_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_11_attn_tp_1 -> layer_11_attn_out_tp_1
	layer_11_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_11_attn_tp_2 -> layer_11_attn_out_tp_2
	layer_11_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_11_attn_tp_3 -> layer_11_attn_out_tp_3
	layer_11_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_11_gate_tp_0 -> layer_11_route_ep_0 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_0 -> layer_11_route_ep_1 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_0 -> layer_11_route_ep_2 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_0 -> layer_11_route_ep_3 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_0 -> layer_11_route_ep_4 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_0 -> layer_11_route_ep_5 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_0 -> layer_11_route_ep_6 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_0 -> layer_11_route_ep_7 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_1 -> layer_11_route_ep_0 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_1 -> layer_11_route_ep_1 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_1 -> layer_11_route_ep_2 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_1 -> layer_11_route_ep_3 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_1 -> layer_11_route_ep_4 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_1 -> layer_11_route_ep_5 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_1 -> layer_11_route_ep_6 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_1 -> layer_11_route_ep_7 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_2 -> layer_11_route_ep_0 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_2 -> layer_11_route_ep_1 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_2 -> layer_11_route_ep_2 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_2 -> layer_11_route_ep_3 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_2 -> layer_11_route_ep_4 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_2 -> layer_11_route_ep_5 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_2 -> layer_11_route_ep_6 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_2 -> layer_11_route_ep_7 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_3 -> layer_11_route_ep_0 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_3 -> layer_11_route_ep_1 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_3 -> layer_11_route_ep_2 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_3 -> layer_11_route_ep_3 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_3 -> layer_11_route_ep_4 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_3 -> layer_11_route_ep_5 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_3 -> layer_11_route_ep_6 [label="Gate Selection" style=dashed]
	layer_11_gate_tp_3 -> layer_11_route_ep_7 [label="Gate Selection" style=dashed]
	layer_11_route_ep_0 -> layer_11_expert_0_ep_0
	layer_11_route_ep_0 -> layer_11_expert_1_ep_0
	layer_11_route_ep_0 -> layer_11_expert_2_ep_0
	layer_11_route_ep_0 -> layer_11_expert_3_ep_0
	layer_11_route_ep_0 -> layer_11_expert_4_ep_0
	layer_11_route_ep_0 -> layer_11_expert_5_ep_0
	layer_11_route_ep_0 -> layer_11_expert_6_ep_0
	layer_11_route_ep_0 -> layer_11_expert_7_ep_0
	layer_11_route_ep_1 -> layer_11_expert_0_ep_1
	layer_11_route_ep_1 -> layer_11_expert_1_ep_1
	layer_11_route_ep_1 -> layer_11_expert_2_ep_1
	layer_11_route_ep_1 -> layer_11_expert_3_ep_1
	layer_11_route_ep_1 -> layer_11_expert_4_ep_1
	layer_11_route_ep_1 -> layer_11_expert_5_ep_1
	layer_11_route_ep_1 -> layer_11_expert_6_ep_1
	layer_11_route_ep_1 -> layer_11_expert_7_ep_1
	layer_11_route_ep_2 -> layer_11_expert_0_ep_2
	layer_11_route_ep_2 -> layer_11_expert_1_ep_2
	layer_11_route_ep_2 -> layer_11_expert_2_ep_2
	layer_11_route_ep_2 -> layer_11_expert_3_ep_2
	layer_11_route_ep_2 -> layer_11_expert_4_ep_2
	layer_11_route_ep_2 -> layer_11_expert_5_ep_2
	layer_11_route_ep_2 -> layer_11_expert_6_ep_2
	layer_11_route_ep_2 -> layer_11_expert_7_ep_2
	layer_11_route_ep_3 -> layer_11_expert_0_ep_3
	layer_11_route_ep_3 -> layer_11_expert_1_ep_3
	layer_11_route_ep_3 -> layer_11_expert_2_ep_3
	layer_11_route_ep_3 -> layer_11_expert_3_ep_3
	layer_11_route_ep_3 -> layer_11_expert_4_ep_3
	layer_11_route_ep_3 -> layer_11_expert_5_ep_3
	layer_11_route_ep_3 -> layer_11_expert_6_ep_3
	layer_11_route_ep_3 -> layer_11_expert_7_ep_3
	layer_11_route_ep_4 -> layer_11_expert_0_ep_4
	layer_11_route_ep_4 -> layer_11_expert_1_ep_4
	layer_11_route_ep_4 -> layer_11_expert_2_ep_4
	layer_11_route_ep_4 -> layer_11_expert_3_ep_4
	layer_11_route_ep_4 -> layer_11_expert_4_ep_4
	layer_11_route_ep_4 -> layer_11_expert_5_ep_4
	layer_11_route_ep_4 -> layer_11_expert_6_ep_4
	layer_11_route_ep_4 -> layer_11_expert_7_ep_4
	layer_11_route_ep_5 -> layer_11_expert_0_ep_5
	layer_11_route_ep_5 -> layer_11_expert_1_ep_5
	layer_11_route_ep_5 -> layer_11_expert_2_ep_5
	layer_11_route_ep_5 -> layer_11_expert_3_ep_5
	layer_11_route_ep_5 -> layer_11_expert_4_ep_5
	layer_11_route_ep_5 -> layer_11_expert_5_ep_5
	layer_11_route_ep_5 -> layer_11_expert_6_ep_5
	layer_11_route_ep_5 -> layer_11_expert_7_ep_5
	layer_11_route_ep_6 -> layer_11_expert_0_ep_6
	layer_11_route_ep_6 -> layer_11_expert_1_ep_6
	layer_11_route_ep_6 -> layer_11_expert_2_ep_6
	layer_11_route_ep_6 -> layer_11_expert_3_ep_6
	layer_11_route_ep_6 -> layer_11_expert_4_ep_6
	layer_11_route_ep_6 -> layer_11_expert_5_ep_6
	layer_11_route_ep_6 -> layer_11_expert_6_ep_6
	layer_11_route_ep_6 -> layer_11_expert_7_ep_6
	layer_11_route_ep_7 -> layer_11_expert_0_ep_7
	layer_11_route_ep_7 -> layer_11_expert_1_ep_7
	layer_11_route_ep_7 -> layer_11_expert_2_ep_7
	layer_11_route_ep_7 -> layer_11_expert_3_ep_7
	layer_11_route_ep_7 -> layer_11_expert_4_ep_7
	layer_11_route_ep_7 -> layer_11_expert_5_ep_7
	layer_11_route_ep_7 -> layer_11_expert_6_ep_7
	layer_11_route_ep_7 -> layer_11_expert_7_ep_7
	layer_11_expert_0_ep_0 -> layer_11_agg_ep_0
	layer_11_expert_1_ep_0 -> layer_11_agg_ep_0
	layer_11_expert_2_ep_0 -> layer_11_agg_ep_0
	layer_11_expert_3_ep_0 -> layer_11_agg_ep_0
	layer_11_expert_4_ep_0 -> layer_11_agg_ep_0
	layer_11_expert_5_ep_0 -> layer_11_agg_ep_0
	layer_11_expert_6_ep_0 -> layer_11_agg_ep_0
	layer_11_expert_7_ep_0 -> layer_11_agg_ep_0
	layer_11_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_11_expert_0_ep_1 -> layer_11_agg_ep_1
	layer_11_expert_1_ep_1 -> layer_11_agg_ep_1
	layer_11_expert_2_ep_1 -> layer_11_agg_ep_1
	layer_11_expert_3_ep_1 -> layer_11_agg_ep_1
	layer_11_expert_4_ep_1 -> layer_11_agg_ep_1
	layer_11_expert_5_ep_1 -> layer_11_agg_ep_1
	layer_11_expert_6_ep_1 -> layer_11_agg_ep_1
	layer_11_expert_7_ep_1 -> layer_11_agg_ep_1
	layer_11_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_11_expert_0_ep_2 -> layer_11_agg_ep_2
	layer_11_expert_1_ep_2 -> layer_11_agg_ep_2
	layer_11_expert_2_ep_2 -> layer_11_agg_ep_2
	layer_11_expert_3_ep_2 -> layer_11_agg_ep_2
	layer_11_expert_4_ep_2 -> layer_11_agg_ep_2
	layer_11_expert_5_ep_2 -> layer_11_agg_ep_2
	layer_11_expert_6_ep_2 -> layer_11_agg_ep_2
	layer_11_expert_7_ep_2 -> layer_11_agg_ep_2
	layer_11_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_11_expert_0_ep_3 -> layer_11_agg_ep_3
	layer_11_expert_1_ep_3 -> layer_11_agg_ep_3
	layer_11_expert_2_ep_3 -> layer_11_agg_ep_3
	layer_11_expert_3_ep_3 -> layer_11_agg_ep_3
	layer_11_expert_4_ep_3 -> layer_11_agg_ep_3
	layer_11_expert_5_ep_3 -> layer_11_agg_ep_3
	layer_11_expert_6_ep_3 -> layer_11_agg_ep_3
	layer_11_expert_7_ep_3 -> layer_11_agg_ep_3
	layer_11_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_11_expert_0_ep_4 -> layer_11_agg_ep_4
	layer_11_expert_1_ep_4 -> layer_11_agg_ep_4
	layer_11_expert_2_ep_4 -> layer_11_agg_ep_4
	layer_11_expert_3_ep_4 -> layer_11_agg_ep_4
	layer_11_expert_4_ep_4 -> layer_11_agg_ep_4
	layer_11_expert_5_ep_4 -> layer_11_agg_ep_4
	layer_11_expert_6_ep_4 -> layer_11_agg_ep_4
	layer_11_expert_7_ep_4 -> layer_11_agg_ep_4
	layer_11_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_11_expert_0_ep_5 -> layer_11_agg_ep_5
	layer_11_expert_1_ep_5 -> layer_11_agg_ep_5
	layer_11_expert_2_ep_5 -> layer_11_agg_ep_5
	layer_11_expert_3_ep_5 -> layer_11_agg_ep_5
	layer_11_expert_4_ep_5 -> layer_11_agg_ep_5
	layer_11_expert_5_ep_5 -> layer_11_agg_ep_5
	layer_11_expert_6_ep_5 -> layer_11_agg_ep_5
	layer_11_expert_7_ep_5 -> layer_11_agg_ep_5
	layer_11_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_11_expert_0_ep_6 -> layer_11_agg_ep_6
	layer_11_expert_1_ep_6 -> layer_11_agg_ep_6
	layer_11_expert_2_ep_6 -> layer_11_agg_ep_6
	layer_11_expert_3_ep_6 -> layer_11_agg_ep_6
	layer_11_expert_4_ep_6 -> layer_11_agg_ep_6
	layer_11_expert_5_ep_6 -> layer_11_agg_ep_6
	layer_11_expert_6_ep_6 -> layer_11_agg_ep_6
	layer_11_expert_7_ep_6 -> layer_11_agg_ep_6
	layer_11_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_11_expert_0_ep_7 -> layer_11_agg_ep_7
	layer_11_expert_1_ep_7 -> layer_11_agg_ep_7
	layer_11_expert_2_ep_7 -> layer_11_agg_ep_7
	layer_11_expert_3_ep_7 -> layer_11_agg_ep_7
	layer_11_expert_4_ep_7 -> layer_11_agg_ep_7
	layer_11_expert_5_ep_7 -> layer_11_agg_ep_7
	layer_11_expert_6_ep_7 -> layer_11_agg_ep_7
	layer_11_expert_7_ep_7 -> layer_11_agg_ep_7
	layer_11_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_11_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_11_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_11_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_11_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_11_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_11_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_11_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_11_ln_tp_3
	layer_12_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_12_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_12_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_12_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_12_attn_tp_0 -> layer_12_attn_out_tp_0
	layer_12_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_12_attn_tp_1 -> layer_12_attn_out_tp_1
	layer_12_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_12_attn_tp_2 -> layer_12_attn_out_tp_2
	layer_12_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_12_attn_tp_3 -> layer_12_attn_out_tp_3
	layer_12_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_12_gate_tp_0 -> layer_12_route_ep_0 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_0 -> layer_12_route_ep_1 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_0 -> layer_12_route_ep_2 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_0 -> layer_12_route_ep_3 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_0 -> layer_12_route_ep_4 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_0 -> layer_12_route_ep_5 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_0 -> layer_12_route_ep_6 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_0 -> layer_12_route_ep_7 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_1 -> layer_12_route_ep_0 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_1 -> layer_12_route_ep_1 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_1 -> layer_12_route_ep_2 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_1 -> layer_12_route_ep_3 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_1 -> layer_12_route_ep_4 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_1 -> layer_12_route_ep_5 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_1 -> layer_12_route_ep_6 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_1 -> layer_12_route_ep_7 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_2 -> layer_12_route_ep_0 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_2 -> layer_12_route_ep_1 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_2 -> layer_12_route_ep_2 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_2 -> layer_12_route_ep_3 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_2 -> layer_12_route_ep_4 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_2 -> layer_12_route_ep_5 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_2 -> layer_12_route_ep_6 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_2 -> layer_12_route_ep_7 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_3 -> layer_12_route_ep_0 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_3 -> layer_12_route_ep_1 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_3 -> layer_12_route_ep_2 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_3 -> layer_12_route_ep_3 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_3 -> layer_12_route_ep_4 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_3 -> layer_12_route_ep_5 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_3 -> layer_12_route_ep_6 [label="Gate Selection" style=dashed]
	layer_12_gate_tp_3 -> layer_12_route_ep_7 [label="Gate Selection" style=dashed]
	layer_12_route_ep_0 -> layer_12_expert_0_ep_0
	layer_12_route_ep_0 -> layer_12_expert_1_ep_0
	layer_12_route_ep_0 -> layer_12_expert_2_ep_0
	layer_12_route_ep_0 -> layer_12_expert_3_ep_0
	layer_12_route_ep_0 -> layer_12_expert_4_ep_0
	layer_12_route_ep_0 -> layer_12_expert_5_ep_0
	layer_12_route_ep_0 -> layer_12_expert_6_ep_0
	layer_12_route_ep_0 -> layer_12_expert_7_ep_0
	layer_12_route_ep_1 -> layer_12_expert_0_ep_1
	layer_12_route_ep_1 -> layer_12_expert_1_ep_1
	layer_12_route_ep_1 -> layer_12_expert_2_ep_1
	layer_12_route_ep_1 -> layer_12_expert_3_ep_1
	layer_12_route_ep_1 -> layer_12_expert_4_ep_1
	layer_12_route_ep_1 -> layer_12_expert_5_ep_1
	layer_12_route_ep_1 -> layer_12_expert_6_ep_1
	layer_12_route_ep_1 -> layer_12_expert_7_ep_1
	layer_12_route_ep_2 -> layer_12_expert_0_ep_2
	layer_12_route_ep_2 -> layer_12_expert_1_ep_2
	layer_12_route_ep_2 -> layer_12_expert_2_ep_2
	layer_12_route_ep_2 -> layer_12_expert_3_ep_2
	layer_12_route_ep_2 -> layer_12_expert_4_ep_2
	layer_12_route_ep_2 -> layer_12_expert_5_ep_2
	layer_12_route_ep_2 -> layer_12_expert_6_ep_2
	layer_12_route_ep_2 -> layer_12_expert_7_ep_2
	layer_12_route_ep_3 -> layer_12_expert_0_ep_3
	layer_12_route_ep_3 -> layer_12_expert_1_ep_3
	layer_12_route_ep_3 -> layer_12_expert_2_ep_3
	layer_12_route_ep_3 -> layer_12_expert_3_ep_3
	layer_12_route_ep_3 -> layer_12_expert_4_ep_3
	layer_12_route_ep_3 -> layer_12_expert_5_ep_3
	layer_12_route_ep_3 -> layer_12_expert_6_ep_3
	layer_12_route_ep_3 -> layer_12_expert_7_ep_3
	layer_12_route_ep_4 -> layer_12_expert_0_ep_4
	layer_12_route_ep_4 -> layer_12_expert_1_ep_4
	layer_12_route_ep_4 -> layer_12_expert_2_ep_4
	layer_12_route_ep_4 -> layer_12_expert_3_ep_4
	layer_12_route_ep_4 -> layer_12_expert_4_ep_4
	layer_12_route_ep_4 -> layer_12_expert_5_ep_4
	layer_12_route_ep_4 -> layer_12_expert_6_ep_4
	layer_12_route_ep_4 -> layer_12_expert_7_ep_4
	layer_12_route_ep_5 -> layer_12_expert_0_ep_5
	layer_12_route_ep_5 -> layer_12_expert_1_ep_5
	layer_12_route_ep_5 -> layer_12_expert_2_ep_5
	layer_12_route_ep_5 -> layer_12_expert_3_ep_5
	layer_12_route_ep_5 -> layer_12_expert_4_ep_5
	layer_12_route_ep_5 -> layer_12_expert_5_ep_5
	layer_12_route_ep_5 -> layer_12_expert_6_ep_5
	layer_12_route_ep_5 -> layer_12_expert_7_ep_5
	layer_12_route_ep_6 -> layer_12_expert_0_ep_6
	layer_12_route_ep_6 -> layer_12_expert_1_ep_6
	layer_12_route_ep_6 -> layer_12_expert_2_ep_6
	layer_12_route_ep_6 -> layer_12_expert_3_ep_6
	layer_12_route_ep_6 -> layer_12_expert_4_ep_6
	layer_12_route_ep_6 -> layer_12_expert_5_ep_6
	layer_12_route_ep_6 -> layer_12_expert_6_ep_6
	layer_12_route_ep_6 -> layer_12_expert_7_ep_6
	layer_12_route_ep_7 -> layer_12_expert_0_ep_7
	layer_12_route_ep_7 -> layer_12_expert_1_ep_7
	layer_12_route_ep_7 -> layer_12_expert_2_ep_7
	layer_12_route_ep_7 -> layer_12_expert_3_ep_7
	layer_12_route_ep_7 -> layer_12_expert_4_ep_7
	layer_12_route_ep_7 -> layer_12_expert_5_ep_7
	layer_12_route_ep_7 -> layer_12_expert_6_ep_7
	layer_12_route_ep_7 -> layer_12_expert_7_ep_7
	layer_12_expert_0_ep_0 -> layer_12_agg_ep_0
	layer_12_expert_1_ep_0 -> layer_12_agg_ep_0
	layer_12_expert_2_ep_0 -> layer_12_agg_ep_0
	layer_12_expert_3_ep_0 -> layer_12_agg_ep_0
	layer_12_expert_4_ep_0 -> layer_12_agg_ep_0
	layer_12_expert_5_ep_0 -> layer_12_agg_ep_0
	layer_12_expert_6_ep_0 -> layer_12_agg_ep_0
	layer_12_expert_7_ep_0 -> layer_12_agg_ep_0
	layer_12_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_12_expert_0_ep_1 -> layer_12_agg_ep_1
	layer_12_expert_1_ep_1 -> layer_12_agg_ep_1
	layer_12_expert_2_ep_1 -> layer_12_agg_ep_1
	layer_12_expert_3_ep_1 -> layer_12_agg_ep_1
	layer_12_expert_4_ep_1 -> layer_12_agg_ep_1
	layer_12_expert_5_ep_1 -> layer_12_agg_ep_1
	layer_12_expert_6_ep_1 -> layer_12_agg_ep_1
	layer_12_expert_7_ep_1 -> layer_12_agg_ep_1
	layer_12_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_12_expert_0_ep_2 -> layer_12_agg_ep_2
	layer_12_expert_1_ep_2 -> layer_12_agg_ep_2
	layer_12_expert_2_ep_2 -> layer_12_agg_ep_2
	layer_12_expert_3_ep_2 -> layer_12_agg_ep_2
	layer_12_expert_4_ep_2 -> layer_12_agg_ep_2
	layer_12_expert_5_ep_2 -> layer_12_agg_ep_2
	layer_12_expert_6_ep_2 -> layer_12_agg_ep_2
	layer_12_expert_7_ep_2 -> layer_12_agg_ep_2
	layer_12_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_12_expert_0_ep_3 -> layer_12_agg_ep_3
	layer_12_expert_1_ep_3 -> layer_12_agg_ep_3
	layer_12_expert_2_ep_3 -> layer_12_agg_ep_3
	layer_12_expert_3_ep_3 -> layer_12_agg_ep_3
	layer_12_expert_4_ep_3 -> layer_12_agg_ep_3
	layer_12_expert_5_ep_3 -> layer_12_agg_ep_3
	layer_12_expert_6_ep_3 -> layer_12_agg_ep_3
	layer_12_expert_7_ep_3 -> layer_12_agg_ep_3
	layer_12_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_12_expert_0_ep_4 -> layer_12_agg_ep_4
	layer_12_expert_1_ep_4 -> layer_12_agg_ep_4
	layer_12_expert_2_ep_4 -> layer_12_agg_ep_4
	layer_12_expert_3_ep_4 -> layer_12_agg_ep_4
	layer_12_expert_4_ep_4 -> layer_12_agg_ep_4
	layer_12_expert_5_ep_4 -> layer_12_agg_ep_4
	layer_12_expert_6_ep_4 -> layer_12_agg_ep_4
	layer_12_expert_7_ep_4 -> layer_12_agg_ep_4
	layer_12_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_12_expert_0_ep_5 -> layer_12_agg_ep_5
	layer_12_expert_1_ep_5 -> layer_12_agg_ep_5
	layer_12_expert_2_ep_5 -> layer_12_agg_ep_5
	layer_12_expert_3_ep_5 -> layer_12_agg_ep_5
	layer_12_expert_4_ep_5 -> layer_12_agg_ep_5
	layer_12_expert_5_ep_5 -> layer_12_agg_ep_5
	layer_12_expert_6_ep_5 -> layer_12_agg_ep_5
	layer_12_expert_7_ep_5 -> layer_12_agg_ep_5
	layer_12_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_12_expert_0_ep_6 -> layer_12_agg_ep_6
	layer_12_expert_1_ep_6 -> layer_12_agg_ep_6
	layer_12_expert_2_ep_6 -> layer_12_agg_ep_6
	layer_12_expert_3_ep_6 -> layer_12_agg_ep_6
	layer_12_expert_4_ep_6 -> layer_12_agg_ep_6
	layer_12_expert_5_ep_6 -> layer_12_agg_ep_6
	layer_12_expert_6_ep_6 -> layer_12_agg_ep_6
	layer_12_expert_7_ep_6 -> layer_12_agg_ep_6
	layer_12_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_12_expert_0_ep_7 -> layer_12_agg_ep_7
	layer_12_expert_1_ep_7 -> layer_12_agg_ep_7
	layer_12_expert_2_ep_7 -> layer_12_agg_ep_7
	layer_12_expert_3_ep_7 -> layer_12_agg_ep_7
	layer_12_expert_4_ep_7 -> layer_12_agg_ep_7
	layer_12_expert_5_ep_7 -> layer_12_agg_ep_7
	layer_12_expert_6_ep_7 -> layer_12_agg_ep_7
	layer_12_expert_7_ep_7 -> layer_12_agg_ep_7
	layer_12_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_12_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_12_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_12_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_12_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_12_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_12_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_12_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_12_ln_tp_3
	layer_13_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_13_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_13_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_13_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_13_attn_tp_0 -> layer_13_attn_out_tp_0
	layer_13_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_13_attn_tp_1 -> layer_13_attn_out_tp_1
	layer_13_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_13_attn_tp_2 -> layer_13_attn_out_tp_2
	layer_13_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_13_attn_tp_3 -> layer_13_attn_out_tp_3
	layer_13_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_13_gate_tp_0 -> layer_13_route_ep_0 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_0 -> layer_13_route_ep_1 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_0 -> layer_13_route_ep_2 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_0 -> layer_13_route_ep_3 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_0 -> layer_13_route_ep_4 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_0 -> layer_13_route_ep_5 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_0 -> layer_13_route_ep_6 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_0 -> layer_13_route_ep_7 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_1 -> layer_13_route_ep_0 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_1 -> layer_13_route_ep_1 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_1 -> layer_13_route_ep_2 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_1 -> layer_13_route_ep_3 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_1 -> layer_13_route_ep_4 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_1 -> layer_13_route_ep_5 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_1 -> layer_13_route_ep_6 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_1 -> layer_13_route_ep_7 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_2 -> layer_13_route_ep_0 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_2 -> layer_13_route_ep_1 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_2 -> layer_13_route_ep_2 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_2 -> layer_13_route_ep_3 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_2 -> layer_13_route_ep_4 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_2 -> layer_13_route_ep_5 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_2 -> layer_13_route_ep_6 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_2 -> layer_13_route_ep_7 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_3 -> layer_13_route_ep_0 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_3 -> layer_13_route_ep_1 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_3 -> layer_13_route_ep_2 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_3 -> layer_13_route_ep_3 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_3 -> layer_13_route_ep_4 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_3 -> layer_13_route_ep_5 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_3 -> layer_13_route_ep_6 [label="Gate Selection" style=dashed]
	layer_13_gate_tp_3 -> layer_13_route_ep_7 [label="Gate Selection" style=dashed]
	layer_13_route_ep_0 -> layer_13_expert_0_ep_0
	layer_13_route_ep_0 -> layer_13_expert_1_ep_0
	layer_13_route_ep_0 -> layer_13_expert_2_ep_0
	layer_13_route_ep_0 -> layer_13_expert_3_ep_0
	layer_13_route_ep_0 -> layer_13_expert_4_ep_0
	layer_13_route_ep_0 -> layer_13_expert_5_ep_0
	layer_13_route_ep_0 -> layer_13_expert_6_ep_0
	layer_13_route_ep_0 -> layer_13_expert_7_ep_0
	layer_13_route_ep_1 -> layer_13_expert_0_ep_1
	layer_13_route_ep_1 -> layer_13_expert_1_ep_1
	layer_13_route_ep_1 -> layer_13_expert_2_ep_1
	layer_13_route_ep_1 -> layer_13_expert_3_ep_1
	layer_13_route_ep_1 -> layer_13_expert_4_ep_1
	layer_13_route_ep_1 -> layer_13_expert_5_ep_1
	layer_13_route_ep_1 -> layer_13_expert_6_ep_1
	layer_13_route_ep_1 -> layer_13_expert_7_ep_1
	layer_13_route_ep_2 -> layer_13_expert_0_ep_2
	layer_13_route_ep_2 -> layer_13_expert_1_ep_2
	layer_13_route_ep_2 -> layer_13_expert_2_ep_2
	layer_13_route_ep_2 -> layer_13_expert_3_ep_2
	layer_13_route_ep_2 -> layer_13_expert_4_ep_2
	layer_13_route_ep_2 -> layer_13_expert_5_ep_2
	layer_13_route_ep_2 -> layer_13_expert_6_ep_2
	layer_13_route_ep_2 -> layer_13_expert_7_ep_2
	layer_13_route_ep_3 -> layer_13_expert_0_ep_3
	layer_13_route_ep_3 -> layer_13_expert_1_ep_3
	layer_13_route_ep_3 -> layer_13_expert_2_ep_3
	layer_13_route_ep_3 -> layer_13_expert_3_ep_3
	layer_13_route_ep_3 -> layer_13_expert_4_ep_3
	layer_13_route_ep_3 -> layer_13_expert_5_ep_3
	layer_13_route_ep_3 -> layer_13_expert_6_ep_3
	layer_13_route_ep_3 -> layer_13_expert_7_ep_3
	layer_13_route_ep_4 -> layer_13_expert_0_ep_4
	layer_13_route_ep_4 -> layer_13_expert_1_ep_4
	layer_13_route_ep_4 -> layer_13_expert_2_ep_4
	layer_13_route_ep_4 -> layer_13_expert_3_ep_4
	layer_13_route_ep_4 -> layer_13_expert_4_ep_4
	layer_13_route_ep_4 -> layer_13_expert_5_ep_4
	layer_13_route_ep_4 -> layer_13_expert_6_ep_4
	layer_13_route_ep_4 -> layer_13_expert_7_ep_4
	layer_13_route_ep_5 -> layer_13_expert_0_ep_5
	layer_13_route_ep_5 -> layer_13_expert_1_ep_5
	layer_13_route_ep_5 -> layer_13_expert_2_ep_5
	layer_13_route_ep_5 -> layer_13_expert_3_ep_5
	layer_13_route_ep_5 -> layer_13_expert_4_ep_5
	layer_13_route_ep_5 -> layer_13_expert_5_ep_5
	layer_13_route_ep_5 -> layer_13_expert_6_ep_5
	layer_13_route_ep_5 -> layer_13_expert_7_ep_5
	layer_13_route_ep_6 -> layer_13_expert_0_ep_6
	layer_13_route_ep_6 -> layer_13_expert_1_ep_6
	layer_13_route_ep_6 -> layer_13_expert_2_ep_6
	layer_13_route_ep_6 -> layer_13_expert_3_ep_6
	layer_13_route_ep_6 -> layer_13_expert_4_ep_6
	layer_13_route_ep_6 -> layer_13_expert_5_ep_6
	layer_13_route_ep_6 -> layer_13_expert_6_ep_6
	layer_13_route_ep_6 -> layer_13_expert_7_ep_6
	layer_13_route_ep_7 -> layer_13_expert_0_ep_7
	layer_13_route_ep_7 -> layer_13_expert_1_ep_7
	layer_13_route_ep_7 -> layer_13_expert_2_ep_7
	layer_13_route_ep_7 -> layer_13_expert_3_ep_7
	layer_13_route_ep_7 -> layer_13_expert_4_ep_7
	layer_13_route_ep_7 -> layer_13_expert_5_ep_7
	layer_13_route_ep_7 -> layer_13_expert_6_ep_7
	layer_13_route_ep_7 -> layer_13_expert_7_ep_7
	layer_13_expert_0_ep_0 -> layer_13_agg_ep_0
	layer_13_expert_1_ep_0 -> layer_13_agg_ep_0
	layer_13_expert_2_ep_0 -> layer_13_agg_ep_0
	layer_13_expert_3_ep_0 -> layer_13_agg_ep_0
	layer_13_expert_4_ep_0 -> layer_13_agg_ep_0
	layer_13_expert_5_ep_0 -> layer_13_agg_ep_0
	layer_13_expert_6_ep_0 -> layer_13_agg_ep_0
	layer_13_expert_7_ep_0 -> layer_13_agg_ep_0
	layer_13_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_13_expert_0_ep_1 -> layer_13_agg_ep_1
	layer_13_expert_1_ep_1 -> layer_13_agg_ep_1
	layer_13_expert_2_ep_1 -> layer_13_agg_ep_1
	layer_13_expert_3_ep_1 -> layer_13_agg_ep_1
	layer_13_expert_4_ep_1 -> layer_13_agg_ep_1
	layer_13_expert_5_ep_1 -> layer_13_agg_ep_1
	layer_13_expert_6_ep_1 -> layer_13_agg_ep_1
	layer_13_expert_7_ep_1 -> layer_13_agg_ep_1
	layer_13_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_13_expert_0_ep_2 -> layer_13_agg_ep_2
	layer_13_expert_1_ep_2 -> layer_13_agg_ep_2
	layer_13_expert_2_ep_2 -> layer_13_agg_ep_2
	layer_13_expert_3_ep_2 -> layer_13_agg_ep_2
	layer_13_expert_4_ep_2 -> layer_13_agg_ep_2
	layer_13_expert_5_ep_2 -> layer_13_agg_ep_2
	layer_13_expert_6_ep_2 -> layer_13_agg_ep_2
	layer_13_expert_7_ep_2 -> layer_13_agg_ep_2
	layer_13_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_13_expert_0_ep_3 -> layer_13_agg_ep_3
	layer_13_expert_1_ep_3 -> layer_13_agg_ep_3
	layer_13_expert_2_ep_3 -> layer_13_agg_ep_3
	layer_13_expert_3_ep_3 -> layer_13_agg_ep_3
	layer_13_expert_4_ep_3 -> layer_13_agg_ep_3
	layer_13_expert_5_ep_3 -> layer_13_agg_ep_3
	layer_13_expert_6_ep_3 -> layer_13_agg_ep_3
	layer_13_expert_7_ep_3 -> layer_13_agg_ep_3
	layer_13_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_13_expert_0_ep_4 -> layer_13_agg_ep_4
	layer_13_expert_1_ep_4 -> layer_13_agg_ep_4
	layer_13_expert_2_ep_4 -> layer_13_agg_ep_4
	layer_13_expert_3_ep_4 -> layer_13_agg_ep_4
	layer_13_expert_4_ep_4 -> layer_13_agg_ep_4
	layer_13_expert_5_ep_4 -> layer_13_agg_ep_4
	layer_13_expert_6_ep_4 -> layer_13_agg_ep_4
	layer_13_expert_7_ep_4 -> layer_13_agg_ep_4
	layer_13_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_13_expert_0_ep_5 -> layer_13_agg_ep_5
	layer_13_expert_1_ep_5 -> layer_13_agg_ep_5
	layer_13_expert_2_ep_5 -> layer_13_agg_ep_5
	layer_13_expert_3_ep_5 -> layer_13_agg_ep_5
	layer_13_expert_4_ep_5 -> layer_13_agg_ep_5
	layer_13_expert_5_ep_5 -> layer_13_agg_ep_5
	layer_13_expert_6_ep_5 -> layer_13_agg_ep_5
	layer_13_expert_7_ep_5 -> layer_13_agg_ep_5
	layer_13_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_13_expert_0_ep_6 -> layer_13_agg_ep_6
	layer_13_expert_1_ep_6 -> layer_13_agg_ep_6
	layer_13_expert_2_ep_6 -> layer_13_agg_ep_6
	layer_13_expert_3_ep_6 -> layer_13_agg_ep_6
	layer_13_expert_4_ep_6 -> layer_13_agg_ep_6
	layer_13_expert_5_ep_6 -> layer_13_agg_ep_6
	layer_13_expert_6_ep_6 -> layer_13_agg_ep_6
	layer_13_expert_7_ep_6 -> layer_13_agg_ep_6
	layer_13_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_13_expert_0_ep_7 -> layer_13_agg_ep_7
	layer_13_expert_1_ep_7 -> layer_13_agg_ep_7
	layer_13_expert_2_ep_7 -> layer_13_agg_ep_7
	layer_13_expert_3_ep_7 -> layer_13_agg_ep_7
	layer_13_expert_4_ep_7 -> layer_13_agg_ep_7
	layer_13_expert_5_ep_7 -> layer_13_agg_ep_7
	layer_13_expert_6_ep_7 -> layer_13_agg_ep_7
	layer_13_expert_7_ep_7 -> layer_13_agg_ep_7
	layer_13_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_13_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_13_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_13_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_13_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_13_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_13_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_13_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_13_ln_tp_3
	layer_14_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_14_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_14_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_14_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_14_attn_tp_0 -> layer_14_attn_out_tp_0
	layer_14_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_14_attn_tp_1 -> layer_14_attn_out_tp_1
	layer_14_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_14_attn_tp_2 -> layer_14_attn_out_tp_2
	layer_14_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_14_attn_tp_3 -> layer_14_attn_out_tp_3
	layer_14_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_14_gate_tp_0 -> layer_14_route_ep_0 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_0 -> layer_14_route_ep_1 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_0 -> layer_14_route_ep_2 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_0 -> layer_14_route_ep_3 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_0 -> layer_14_route_ep_4 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_0 -> layer_14_route_ep_5 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_0 -> layer_14_route_ep_6 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_0 -> layer_14_route_ep_7 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_1 -> layer_14_route_ep_0 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_1 -> layer_14_route_ep_1 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_1 -> layer_14_route_ep_2 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_1 -> layer_14_route_ep_3 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_1 -> layer_14_route_ep_4 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_1 -> layer_14_route_ep_5 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_1 -> layer_14_route_ep_6 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_1 -> layer_14_route_ep_7 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_2 -> layer_14_route_ep_0 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_2 -> layer_14_route_ep_1 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_2 -> layer_14_route_ep_2 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_2 -> layer_14_route_ep_3 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_2 -> layer_14_route_ep_4 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_2 -> layer_14_route_ep_5 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_2 -> layer_14_route_ep_6 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_2 -> layer_14_route_ep_7 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_3 -> layer_14_route_ep_0 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_3 -> layer_14_route_ep_1 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_3 -> layer_14_route_ep_2 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_3 -> layer_14_route_ep_3 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_3 -> layer_14_route_ep_4 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_3 -> layer_14_route_ep_5 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_3 -> layer_14_route_ep_6 [label="Gate Selection" style=dashed]
	layer_14_gate_tp_3 -> layer_14_route_ep_7 [label="Gate Selection" style=dashed]
	layer_14_route_ep_0 -> layer_14_expert_0_ep_0
	layer_14_route_ep_0 -> layer_14_expert_1_ep_0
	layer_14_route_ep_0 -> layer_14_expert_2_ep_0
	layer_14_route_ep_0 -> layer_14_expert_3_ep_0
	layer_14_route_ep_0 -> layer_14_expert_4_ep_0
	layer_14_route_ep_0 -> layer_14_expert_5_ep_0
	layer_14_route_ep_0 -> layer_14_expert_6_ep_0
	layer_14_route_ep_0 -> layer_14_expert_7_ep_0
	layer_14_route_ep_1 -> layer_14_expert_0_ep_1
	layer_14_route_ep_1 -> layer_14_expert_1_ep_1
	layer_14_route_ep_1 -> layer_14_expert_2_ep_1
	layer_14_route_ep_1 -> layer_14_expert_3_ep_1
	layer_14_route_ep_1 -> layer_14_expert_4_ep_1
	layer_14_route_ep_1 -> layer_14_expert_5_ep_1
	layer_14_route_ep_1 -> layer_14_expert_6_ep_1
	layer_14_route_ep_1 -> layer_14_expert_7_ep_1
	layer_14_route_ep_2 -> layer_14_expert_0_ep_2
	layer_14_route_ep_2 -> layer_14_expert_1_ep_2
	layer_14_route_ep_2 -> layer_14_expert_2_ep_2
	layer_14_route_ep_2 -> layer_14_expert_3_ep_2
	layer_14_route_ep_2 -> layer_14_expert_4_ep_2
	layer_14_route_ep_2 -> layer_14_expert_5_ep_2
	layer_14_route_ep_2 -> layer_14_expert_6_ep_2
	layer_14_route_ep_2 -> layer_14_expert_7_ep_2
	layer_14_route_ep_3 -> layer_14_expert_0_ep_3
	layer_14_route_ep_3 -> layer_14_expert_1_ep_3
	layer_14_route_ep_3 -> layer_14_expert_2_ep_3
	layer_14_route_ep_3 -> layer_14_expert_3_ep_3
	layer_14_route_ep_3 -> layer_14_expert_4_ep_3
	layer_14_route_ep_3 -> layer_14_expert_5_ep_3
	layer_14_route_ep_3 -> layer_14_expert_6_ep_3
	layer_14_route_ep_3 -> layer_14_expert_7_ep_3
	layer_14_route_ep_4 -> layer_14_expert_0_ep_4
	layer_14_route_ep_4 -> layer_14_expert_1_ep_4
	layer_14_route_ep_4 -> layer_14_expert_2_ep_4
	layer_14_route_ep_4 -> layer_14_expert_3_ep_4
	layer_14_route_ep_4 -> layer_14_expert_4_ep_4
	layer_14_route_ep_4 -> layer_14_expert_5_ep_4
	layer_14_route_ep_4 -> layer_14_expert_6_ep_4
	layer_14_route_ep_4 -> layer_14_expert_7_ep_4
	layer_14_route_ep_5 -> layer_14_expert_0_ep_5
	layer_14_route_ep_5 -> layer_14_expert_1_ep_5
	layer_14_route_ep_5 -> layer_14_expert_2_ep_5
	layer_14_route_ep_5 -> layer_14_expert_3_ep_5
	layer_14_route_ep_5 -> layer_14_expert_4_ep_5
	layer_14_route_ep_5 -> layer_14_expert_5_ep_5
	layer_14_route_ep_5 -> layer_14_expert_6_ep_5
	layer_14_route_ep_5 -> layer_14_expert_7_ep_5
	layer_14_route_ep_6 -> layer_14_expert_0_ep_6
	layer_14_route_ep_6 -> layer_14_expert_1_ep_6
	layer_14_route_ep_6 -> layer_14_expert_2_ep_6
	layer_14_route_ep_6 -> layer_14_expert_3_ep_6
	layer_14_route_ep_6 -> layer_14_expert_4_ep_6
	layer_14_route_ep_6 -> layer_14_expert_5_ep_6
	layer_14_route_ep_6 -> layer_14_expert_6_ep_6
	layer_14_route_ep_6 -> layer_14_expert_7_ep_6
	layer_14_route_ep_7 -> layer_14_expert_0_ep_7
	layer_14_route_ep_7 -> layer_14_expert_1_ep_7
	layer_14_route_ep_7 -> layer_14_expert_2_ep_7
	layer_14_route_ep_7 -> layer_14_expert_3_ep_7
	layer_14_route_ep_7 -> layer_14_expert_4_ep_7
	layer_14_route_ep_7 -> layer_14_expert_5_ep_7
	layer_14_route_ep_7 -> layer_14_expert_6_ep_7
	layer_14_route_ep_7 -> layer_14_expert_7_ep_7
	layer_14_expert_0_ep_0 -> layer_14_agg_ep_0
	layer_14_expert_1_ep_0 -> layer_14_agg_ep_0
	layer_14_expert_2_ep_0 -> layer_14_agg_ep_0
	layer_14_expert_3_ep_0 -> layer_14_agg_ep_0
	layer_14_expert_4_ep_0 -> layer_14_agg_ep_0
	layer_14_expert_5_ep_0 -> layer_14_agg_ep_0
	layer_14_expert_6_ep_0 -> layer_14_agg_ep_0
	layer_14_expert_7_ep_0 -> layer_14_agg_ep_0
	layer_14_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_14_expert_0_ep_1 -> layer_14_agg_ep_1
	layer_14_expert_1_ep_1 -> layer_14_agg_ep_1
	layer_14_expert_2_ep_1 -> layer_14_agg_ep_1
	layer_14_expert_3_ep_1 -> layer_14_agg_ep_1
	layer_14_expert_4_ep_1 -> layer_14_agg_ep_1
	layer_14_expert_5_ep_1 -> layer_14_agg_ep_1
	layer_14_expert_6_ep_1 -> layer_14_agg_ep_1
	layer_14_expert_7_ep_1 -> layer_14_agg_ep_1
	layer_14_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_14_expert_0_ep_2 -> layer_14_agg_ep_2
	layer_14_expert_1_ep_2 -> layer_14_agg_ep_2
	layer_14_expert_2_ep_2 -> layer_14_agg_ep_2
	layer_14_expert_3_ep_2 -> layer_14_agg_ep_2
	layer_14_expert_4_ep_2 -> layer_14_agg_ep_2
	layer_14_expert_5_ep_2 -> layer_14_agg_ep_2
	layer_14_expert_6_ep_2 -> layer_14_agg_ep_2
	layer_14_expert_7_ep_2 -> layer_14_agg_ep_2
	layer_14_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_14_expert_0_ep_3 -> layer_14_agg_ep_3
	layer_14_expert_1_ep_3 -> layer_14_agg_ep_3
	layer_14_expert_2_ep_3 -> layer_14_agg_ep_3
	layer_14_expert_3_ep_3 -> layer_14_agg_ep_3
	layer_14_expert_4_ep_3 -> layer_14_agg_ep_3
	layer_14_expert_5_ep_3 -> layer_14_agg_ep_3
	layer_14_expert_6_ep_3 -> layer_14_agg_ep_3
	layer_14_expert_7_ep_3 -> layer_14_agg_ep_3
	layer_14_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_14_expert_0_ep_4 -> layer_14_agg_ep_4
	layer_14_expert_1_ep_4 -> layer_14_agg_ep_4
	layer_14_expert_2_ep_4 -> layer_14_agg_ep_4
	layer_14_expert_3_ep_4 -> layer_14_agg_ep_4
	layer_14_expert_4_ep_4 -> layer_14_agg_ep_4
	layer_14_expert_5_ep_4 -> layer_14_agg_ep_4
	layer_14_expert_6_ep_4 -> layer_14_agg_ep_4
	layer_14_expert_7_ep_4 -> layer_14_agg_ep_4
	layer_14_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_14_expert_0_ep_5 -> layer_14_agg_ep_5
	layer_14_expert_1_ep_5 -> layer_14_agg_ep_5
	layer_14_expert_2_ep_5 -> layer_14_agg_ep_5
	layer_14_expert_3_ep_5 -> layer_14_agg_ep_5
	layer_14_expert_4_ep_5 -> layer_14_agg_ep_5
	layer_14_expert_5_ep_5 -> layer_14_agg_ep_5
	layer_14_expert_6_ep_5 -> layer_14_agg_ep_5
	layer_14_expert_7_ep_5 -> layer_14_agg_ep_5
	layer_14_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_14_expert_0_ep_6 -> layer_14_agg_ep_6
	layer_14_expert_1_ep_6 -> layer_14_agg_ep_6
	layer_14_expert_2_ep_6 -> layer_14_agg_ep_6
	layer_14_expert_3_ep_6 -> layer_14_agg_ep_6
	layer_14_expert_4_ep_6 -> layer_14_agg_ep_6
	layer_14_expert_5_ep_6 -> layer_14_agg_ep_6
	layer_14_expert_6_ep_6 -> layer_14_agg_ep_6
	layer_14_expert_7_ep_6 -> layer_14_agg_ep_6
	layer_14_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_14_expert_0_ep_7 -> layer_14_agg_ep_7
	layer_14_expert_1_ep_7 -> layer_14_agg_ep_7
	layer_14_expert_2_ep_7 -> layer_14_agg_ep_7
	layer_14_expert_3_ep_7 -> layer_14_agg_ep_7
	layer_14_expert_4_ep_7 -> layer_14_agg_ep_7
	layer_14_expert_5_ep_7 -> layer_14_agg_ep_7
	layer_14_expert_6_ep_7 -> layer_14_agg_ep_7
	layer_14_expert_7_ep_7 -> layer_14_agg_ep_7
	layer_14_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_14_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_14_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_14_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_14_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_14_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_14_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_14_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_14_ln_tp_3
	layer_15_qkv_tp_0 -> "layer_{layer_id}_qkv_comm"
	layer_15_qkv_tp_1 -> "layer_{layer_id}_qkv_comm"
	layer_15_qkv_tp_2 -> "layer_{layer_id}_qkv_comm"
	layer_15_qkv_tp_3 -> "layer_{layer_id}_qkv_comm"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_0"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_1"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_2"
	"layer_{layer_id}_qkv_comm" -> "layer_{layer_id}_attn_tp_3"
	layer_15_attn_tp_0 -> layer_15_attn_out_tp_0
	layer_15_attn_out_tp_0 -> "layer_{layer_id}_attn_allreduce"
	layer_15_attn_tp_1 -> layer_15_attn_out_tp_1
	layer_15_attn_out_tp_1 -> "layer_{layer_id}_attn_allreduce"
	layer_15_attn_tp_2 -> layer_15_attn_out_tp_2
	layer_15_attn_out_tp_2 -> "layer_{layer_id}_attn_allreduce"
	layer_15_attn_tp_3 -> layer_15_attn_out_tp_3
	layer_15_attn_out_tp_3 -> "layer_{layer_id}_attn_allreduce"
	layer_15_gate_tp_0 -> layer_15_route_ep_0 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_0 -> layer_15_route_ep_1 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_0 -> layer_15_route_ep_2 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_0 -> layer_15_route_ep_3 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_0 -> layer_15_route_ep_4 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_0 -> layer_15_route_ep_5 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_0 -> layer_15_route_ep_6 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_0 -> layer_15_route_ep_7 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_1 -> layer_15_route_ep_0 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_1 -> layer_15_route_ep_1 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_1 -> layer_15_route_ep_2 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_1 -> layer_15_route_ep_3 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_1 -> layer_15_route_ep_4 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_1 -> layer_15_route_ep_5 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_1 -> layer_15_route_ep_6 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_1 -> layer_15_route_ep_7 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_2 -> layer_15_route_ep_0 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_2 -> layer_15_route_ep_1 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_2 -> layer_15_route_ep_2 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_2 -> layer_15_route_ep_3 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_2 -> layer_15_route_ep_4 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_2 -> layer_15_route_ep_5 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_2 -> layer_15_route_ep_6 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_2 -> layer_15_route_ep_7 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_3 -> layer_15_route_ep_0 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_3 -> layer_15_route_ep_1 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_3 -> layer_15_route_ep_2 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_3 -> layer_15_route_ep_3 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_3 -> layer_15_route_ep_4 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_3 -> layer_15_route_ep_5 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_3 -> layer_15_route_ep_6 [label="Gate Selection" style=dashed]
	layer_15_gate_tp_3 -> layer_15_route_ep_7 [label="Gate Selection" style=dashed]
	layer_15_route_ep_0 -> layer_15_expert_0_ep_0
	layer_15_route_ep_0 -> layer_15_expert_1_ep_0
	layer_15_route_ep_0 -> layer_15_expert_2_ep_0
	layer_15_route_ep_0 -> layer_15_expert_3_ep_0
	layer_15_route_ep_0 -> layer_15_expert_4_ep_0
	layer_15_route_ep_0 -> layer_15_expert_5_ep_0
	layer_15_route_ep_0 -> layer_15_expert_6_ep_0
	layer_15_route_ep_0 -> layer_15_expert_7_ep_0
	layer_15_route_ep_1 -> layer_15_expert_0_ep_1
	layer_15_route_ep_1 -> layer_15_expert_1_ep_1
	layer_15_route_ep_1 -> layer_15_expert_2_ep_1
	layer_15_route_ep_1 -> layer_15_expert_3_ep_1
	layer_15_route_ep_1 -> layer_15_expert_4_ep_1
	layer_15_route_ep_1 -> layer_15_expert_5_ep_1
	layer_15_route_ep_1 -> layer_15_expert_6_ep_1
	layer_15_route_ep_1 -> layer_15_expert_7_ep_1
	layer_15_route_ep_2 -> layer_15_expert_0_ep_2
	layer_15_route_ep_2 -> layer_15_expert_1_ep_2
	layer_15_route_ep_2 -> layer_15_expert_2_ep_2
	layer_15_route_ep_2 -> layer_15_expert_3_ep_2
	layer_15_route_ep_2 -> layer_15_expert_4_ep_2
	layer_15_route_ep_2 -> layer_15_expert_5_ep_2
	layer_15_route_ep_2 -> layer_15_expert_6_ep_2
	layer_15_route_ep_2 -> layer_15_expert_7_ep_2
	layer_15_route_ep_3 -> layer_15_expert_0_ep_3
	layer_15_route_ep_3 -> layer_15_expert_1_ep_3
	layer_15_route_ep_3 -> layer_15_expert_2_ep_3
	layer_15_route_ep_3 -> layer_15_expert_3_ep_3
	layer_15_route_ep_3 -> layer_15_expert_4_ep_3
	layer_15_route_ep_3 -> layer_15_expert_5_ep_3
	layer_15_route_ep_3 -> layer_15_expert_6_ep_3
	layer_15_route_ep_3 -> layer_15_expert_7_ep_3
	layer_15_route_ep_4 -> layer_15_expert_0_ep_4
	layer_15_route_ep_4 -> layer_15_expert_1_ep_4
	layer_15_route_ep_4 -> layer_15_expert_2_ep_4
	layer_15_route_ep_4 -> layer_15_expert_3_ep_4
	layer_15_route_ep_4 -> layer_15_expert_4_ep_4
	layer_15_route_ep_4 -> layer_15_expert_5_ep_4
	layer_15_route_ep_4 -> layer_15_expert_6_ep_4
	layer_15_route_ep_4 -> layer_15_expert_7_ep_4
	layer_15_route_ep_5 -> layer_15_expert_0_ep_5
	layer_15_route_ep_5 -> layer_15_expert_1_ep_5
	layer_15_route_ep_5 -> layer_15_expert_2_ep_5
	layer_15_route_ep_5 -> layer_15_expert_3_ep_5
	layer_15_route_ep_5 -> layer_15_expert_4_ep_5
	layer_15_route_ep_5 -> layer_15_expert_5_ep_5
	layer_15_route_ep_5 -> layer_15_expert_6_ep_5
	layer_15_route_ep_5 -> layer_15_expert_7_ep_5
	layer_15_route_ep_6 -> layer_15_expert_0_ep_6
	layer_15_route_ep_6 -> layer_15_expert_1_ep_6
	layer_15_route_ep_6 -> layer_15_expert_2_ep_6
	layer_15_route_ep_6 -> layer_15_expert_3_ep_6
	layer_15_route_ep_6 -> layer_15_expert_4_ep_6
	layer_15_route_ep_6 -> layer_15_expert_5_ep_6
	layer_15_route_ep_6 -> layer_15_expert_6_ep_6
	layer_15_route_ep_6 -> layer_15_expert_7_ep_6
	layer_15_route_ep_7 -> layer_15_expert_0_ep_7
	layer_15_route_ep_7 -> layer_15_expert_1_ep_7
	layer_15_route_ep_7 -> layer_15_expert_2_ep_7
	layer_15_route_ep_7 -> layer_15_expert_3_ep_7
	layer_15_route_ep_7 -> layer_15_expert_4_ep_7
	layer_15_route_ep_7 -> layer_15_expert_5_ep_7
	layer_15_route_ep_7 -> layer_15_expert_6_ep_7
	layer_15_route_ep_7 -> layer_15_expert_7_ep_7
	layer_15_expert_0_ep_0 -> layer_15_agg_ep_0
	layer_15_expert_1_ep_0 -> layer_15_agg_ep_0
	layer_15_expert_2_ep_0 -> layer_15_agg_ep_0
	layer_15_expert_3_ep_0 -> layer_15_agg_ep_0
	layer_15_expert_4_ep_0 -> layer_15_agg_ep_0
	layer_15_expert_5_ep_0 -> layer_15_agg_ep_0
	layer_15_expert_6_ep_0 -> layer_15_agg_ep_0
	layer_15_expert_7_ep_0 -> layer_15_agg_ep_0
	layer_15_agg_ep_0 -> "layer_{layer_id}_moe_alltoall"
	layer_15_expert_0_ep_1 -> layer_15_agg_ep_1
	layer_15_expert_1_ep_1 -> layer_15_agg_ep_1
	layer_15_expert_2_ep_1 -> layer_15_agg_ep_1
	layer_15_expert_3_ep_1 -> layer_15_agg_ep_1
	layer_15_expert_4_ep_1 -> layer_15_agg_ep_1
	layer_15_expert_5_ep_1 -> layer_15_agg_ep_1
	layer_15_expert_6_ep_1 -> layer_15_agg_ep_1
	layer_15_expert_7_ep_1 -> layer_15_agg_ep_1
	layer_15_agg_ep_1 -> "layer_{layer_id}_moe_alltoall"
	layer_15_expert_0_ep_2 -> layer_15_agg_ep_2
	layer_15_expert_1_ep_2 -> layer_15_agg_ep_2
	layer_15_expert_2_ep_2 -> layer_15_agg_ep_2
	layer_15_expert_3_ep_2 -> layer_15_agg_ep_2
	layer_15_expert_4_ep_2 -> layer_15_agg_ep_2
	layer_15_expert_5_ep_2 -> layer_15_agg_ep_2
	layer_15_expert_6_ep_2 -> layer_15_agg_ep_2
	layer_15_expert_7_ep_2 -> layer_15_agg_ep_2
	layer_15_agg_ep_2 -> "layer_{layer_id}_moe_alltoall"
	layer_15_expert_0_ep_3 -> layer_15_agg_ep_3
	layer_15_expert_1_ep_3 -> layer_15_agg_ep_3
	layer_15_expert_2_ep_3 -> layer_15_agg_ep_3
	layer_15_expert_3_ep_3 -> layer_15_agg_ep_3
	layer_15_expert_4_ep_3 -> layer_15_agg_ep_3
	layer_15_expert_5_ep_3 -> layer_15_agg_ep_3
	layer_15_expert_6_ep_3 -> layer_15_agg_ep_3
	layer_15_expert_7_ep_3 -> layer_15_agg_ep_3
	layer_15_agg_ep_3 -> "layer_{layer_id}_moe_alltoall"
	layer_15_expert_0_ep_4 -> layer_15_agg_ep_4
	layer_15_expert_1_ep_4 -> layer_15_agg_ep_4
	layer_15_expert_2_ep_4 -> layer_15_agg_ep_4
	layer_15_expert_3_ep_4 -> layer_15_agg_ep_4
	layer_15_expert_4_ep_4 -> layer_15_agg_ep_4
	layer_15_expert_5_ep_4 -> layer_15_agg_ep_4
	layer_15_expert_6_ep_4 -> layer_15_agg_ep_4
	layer_15_expert_7_ep_4 -> layer_15_agg_ep_4
	layer_15_agg_ep_4 -> "layer_{layer_id}_moe_alltoall"
	layer_15_expert_0_ep_5 -> layer_15_agg_ep_5
	layer_15_expert_1_ep_5 -> layer_15_agg_ep_5
	layer_15_expert_2_ep_5 -> layer_15_agg_ep_5
	layer_15_expert_3_ep_5 -> layer_15_agg_ep_5
	layer_15_expert_4_ep_5 -> layer_15_agg_ep_5
	layer_15_expert_5_ep_5 -> layer_15_agg_ep_5
	layer_15_expert_6_ep_5 -> layer_15_agg_ep_5
	layer_15_expert_7_ep_5 -> layer_15_agg_ep_5
	layer_15_agg_ep_5 -> "layer_{layer_id}_moe_alltoall"
	layer_15_expert_0_ep_6 -> layer_15_agg_ep_6
	layer_15_expert_1_ep_6 -> layer_15_agg_ep_6
	layer_15_expert_2_ep_6 -> layer_15_agg_ep_6
	layer_15_expert_3_ep_6 -> layer_15_agg_ep_6
	layer_15_expert_4_ep_6 -> layer_15_agg_ep_6
	layer_15_expert_5_ep_6 -> layer_15_agg_ep_6
	layer_15_expert_6_ep_6 -> layer_15_agg_ep_6
	layer_15_expert_7_ep_6 -> layer_15_agg_ep_6
	layer_15_agg_ep_6 -> "layer_{layer_id}_moe_alltoall"
	layer_15_expert_0_ep_7 -> layer_15_agg_ep_7
	layer_15_expert_1_ep_7 -> layer_15_agg_ep_7
	layer_15_expert_2_ep_7 -> layer_15_agg_ep_7
	layer_15_expert_3_ep_7 -> layer_15_agg_ep_7
	layer_15_expert_4_ep_7 -> layer_15_agg_ep_7
	layer_15_expert_5_ep_7 -> layer_15_agg_ep_7
	layer_15_expert_6_ep_7 -> layer_15_agg_ep_7
	layer_15_expert_7_ep_7 -> layer_15_agg_ep_7
	layer_15_agg_ep_7 -> "layer_{layer_id}_moe_alltoall"
	"layer_{layer_id}_attn_allreduce" -> layer_15_gate_tp_0
	"layer_{layer_id}_moe_alltoall" -> layer_15_ln_tp_0
	"layer_{layer_id}_attn_allreduce" -> layer_15_gate_tp_1
	"layer_{layer_id}_moe_alltoall" -> layer_15_ln_tp_1
	"layer_{layer_id}_attn_allreduce" -> layer_15_gate_tp_2
	"layer_{layer_id}_moe_alltoall" -> layer_15_ln_tp_2
	"layer_{layer_id}_attn_allreduce" -> layer_15_gate_tp_3
	"layer_{layer_id}_moe_alltoall" -> layer_15_ln_tp_3
	layer_0_ln_tp_0 -> layer_1_qkv_tp_0
	layer_0_ln_tp_1 -> layer_1_qkv_tp_1
	layer_0_ln_tp_2 -> layer_1_qkv_tp_2
	layer_0_ln_tp_3 -> layer_1_qkv_tp_3
	layer_1_ln_tp_0 -> layer_2_qkv_tp_0
	layer_1_ln_tp_1 -> layer_2_qkv_tp_1
	layer_1_ln_tp_2 -> layer_2_qkv_tp_2
	layer_1_ln_tp_3 -> layer_2_qkv_tp_3
	layer_2_ln_tp_0 -> layer_3_qkv_tp_0
	layer_2_ln_tp_1 -> layer_3_qkv_tp_1
	layer_2_ln_tp_2 -> layer_3_qkv_tp_2
	layer_2_ln_tp_3 -> layer_3_qkv_tp_3
	layer_3_ln_tp_0 -> layer_4_qkv_tp_0
	layer_3_ln_tp_1 -> layer_4_qkv_tp_1
	layer_3_ln_tp_2 -> layer_4_qkv_tp_2
	layer_3_ln_tp_3 -> layer_4_qkv_tp_3
	layer_4_ln_tp_0 -> layer_5_qkv_tp_0
	layer_4_ln_tp_1 -> layer_5_qkv_tp_1
	layer_4_ln_tp_2 -> layer_5_qkv_tp_2
	layer_4_ln_tp_3 -> layer_5_qkv_tp_3
	layer_5_ln_tp_0 -> layer_6_qkv_tp_0
	layer_5_ln_tp_1 -> layer_6_qkv_tp_1
	layer_5_ln_tp_2 -> layer_6_qkv_tp_2
	layer_5_ln_tp_3 -> layer_6_qkv_tp_3
	layer_6_ln_tp_0 -> layer_7_qkv_tp_0
	layer_6_ln_tp_1 -> layer_7_qkv_tp_1
	layer_6_ln_tp_2 -> layer_7_qkv_tp_2
	layer_6_ln_tp_3 -> layer_7_qkv_tp_3
	layer_7_ln_tp_0 -> layer_8_qkv_tp_0
	layer_7_ln_tp_1 -> layer_8_qkv_tp_1
	layer_7_ln_tp_2 -> layer_8_qkv_tp_2
	layer_7_ln_tp_3 -> layer_8_qkv_tp_3
	layer_8_ln_tp_0 -> layer_9_qkv_tp_0
	layer_8_ln_tp_1 -> layer_9_qkv_tp_1
	layer_8_ln_tp_2 -> layer_9_qkv_tp_2
	layer_8_ln_tp_3 -> layer_9_qkv_tp_3
	layer_9_ln_tp_0 -> layer_10_qkv_tp_0
	layer_9_ln_tp_1 -> layer_10_qkv_tp_1
	layer_9_ln_tp_2 -> layer_10_qkv_tp_2
	layer_9_ln_tp_3 -> layer_10_qkv_tp_3
	layer_10_ln_tp_0 -> layer_11_qkv_tp_0
	layer_10_ln_tp_1 -> layer_11_qkv_tp_1
	layer_10_ln_tp_2 -> layer_11_qkv_tp_2
	layer_10_ln_tp_3 -> layer_11_qkv_tp_3
	layer_11_ln_tp_0 -> layer_12_qkv_tp_0
	layer_11_ln_tp_1 -> layer_12_qkv_tp_1
	layer_11_ln_tp_2 -> layer_12_qkv_tp_2
	layer_11_ln_tp_3 -> layer_12_qkv_tp_3
	layer_12_ln_tp_0 -> layer_13_qkv_tp_0
	layer_12_ln_tp_1 -> layer_13_qkv_tp_1
	layer_12_ln_tp_2 -> layer_13_qkv_tp_2
	layer_12_ln_tp_3 -> layer_13_qkv_tp_3
	layer_13_ln_tp_0 -> layer_14_qkv_tp_0
	layer_13_ln_tp_1 -> layer_14_qkv_tp_1
	layer_13_ln_tp_2 -> layer_14_qkv_tp_2
	layer_13_ln_tp_3 -> layer_14_qkv_tp_3
	layer_14_ln_tp_0 -> layer_15_qkv_tp_0
	layer_14_ln_tp_1 -> layer_15_qkv_tp_1
	layer_14_ln_tp_2 -> layer_15_qkv_tp_2
	layer_14_ln_tp_3 -> layer_15_qkv_tp_3
	"layer_15_ln_tp_{tp_rank}" -> output
	"layer_15_ln_tp_{tp_rank}" -> output
	"layer_15_ln_tp_{tp_rank}" -> output
	"layer_15_ln_tp_{tp_rank}" -> output
}
