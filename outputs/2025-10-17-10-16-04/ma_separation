// MA Separation MoE Transformer DAG
digraph MA_Separation_MoE {
	graph [rankdir=TB splines=ortho]
	node [fillcolor=lightgray shape=rectangle style=filled]
	input [label="Input\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightblue shape=ellipse]
	subgraph cluster_layer_0 {
		fillcolor=lightyellow label="Layer 0" style=rounded
		layer0_ln1 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightgreen]
		layer0_qkv_proj_0 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 0" fillcolor=lightcoral]
		layer0_qkv_proj_1 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 1" fillcolor=lightcoral]
		layer0_qkv_proj_2 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 2" fillcolor=lightcoral]
		layer0_qkv_proj_3 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 4" fillcolor=lightcoral]
		layer0_qkv_proj_4 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 5" fillcolor=lightcoral]
		layer0_qkv_proj_5 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 6" fillcolor=lightcoral]
		layer0_qkv_proj_6 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 8" fillcolor=lightcoral]
		layer0_qkv_proj_7 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 9" fillcolor=lightcoral]
		layer0_qkv_proj_8 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 10" fillcolor=lightcoral]
		layer0_qkv_proj_9 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 12" fillcolor=lightcoral]
		layer0_qkv_proj_10 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 13" fillcolor=lightcoral]
		layer0_qkv_proj_11 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 14" fillcolor=lightcoral]
		layer0_attention_0 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 0" fillcolor=lightcoral]
		layer0_attention_1 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 1" fillcolor=lightcoral]
		layer0_attention_2 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 2" fillcolor=lightcoral]
		layer0_attention_3 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 4" fillcolor=lightcoral]
		layer0_attention_4 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 5" fillcolor=lightcoral]
		layer0_attention_5 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 6" fillcolor=lightcoral]
		layer0_attention_6 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 8" fillcolor=lightcoral]
		layer0_attention_7 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 9" fillcolor=lightcoral]
		layer0_attention_8 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 10" fillcolor=lightcoral]
		layer0_attention_9 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 12" fillcolor=lightcoral]
		layer0_attention_10 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 13" fillcolor=lightcoral]
		layer0_attention_11 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 14" fillcolor=lightcoral]
		layer0_attn_out_proj_0 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 0" fillcolor=lightcoral]
		layer0_attn_out_proj_1 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 1" fillcolor=lightcoral]
		layer0_attn_out_proj_2 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 2" fillcolor=lightcoral]
		layer0_attn_out_proj_3 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 4" fillcolor=lightcoral]
		layer0_attn_out_proj_4 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 5" fillcolor=lightcoral]
		layer0_attn_out_proj_5 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 6" fillcolor=lightcoral]
		layer0_attn_out_proj_6 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 8" fillcolor=lightcoral]
		layer0_attn_out_proj_7 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 9" fillcolor=lightcoral]
		layer0_attn_out_proj_8 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 10" fillcolor=lightcoral]
		layer0_attn_out_proj_9 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 12" fillcolor=lightcoral]
		layer0_attn_out_proj_10 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 13" fillcolor=lightcoral]
		layer0_attn_out_proj_11 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 14" fillcolor=lightcoral]
		layer0_attn_allreduce [label="Attention All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightyellow shape=parallelogram]
		layer0_residual1 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightpink shape=diamond]
		layer0_ln2 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightgreen]
		layer0_gate [label="Gate Computation\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPUs: all GPUs" fillcolor=lightblue shape=parallelogram]
		layer0_route_0 [label="Expert Routing\nExperts=[0-3]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer0_expert_0 [label="Expert 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer0_expert_1 [label="Expert 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer0_expert_2 [label="Expert 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer0_expert_3 [label="Expert 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer0_expert_agg_0 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 3" fillcolor=lightyellow shape=parallelogram]
		layer0_route_1 [label="Expert Routing\nExperts=[4-7]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer0_expert_4 [label="Expert 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer0_expert_5 [label="Expert 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer0_expert_6 [label="Expert 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer0_expert_7 [label="Expert 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer0_expert_agg_1 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 7" fillcolor=lightyellow shape=parallelogram]
		layer0_route_2 [label="Expert Routing\nExperts=[8-11]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer0_expert_8 [label="Expert 8\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer0_expert_9 [label="Expert 9\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer0_expert_10 [label="Expert 10\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer0_expert_11 [label="Expert 11\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer0_expert_agg_2 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 11" fillcolor=lightyellow shape=parallelogram]
		layer0_route_3 [label="Expert Routing\nExperts=[12-15]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer0_expert_12 [label="Expert 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer0_expert_13 [label="Expert 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer0_expert_14 [label="Expert 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer0_expert_15 [label="Expert 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer0_expert_agg_3 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15" fillcolor=lightyellow shape=parallelogram]
		layer0_expert_allreduce [label="MoE All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightyellow shape=parallelogram]
		layer0_residual2 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightpink shape=diamond]
	}
	subgraph cluster_layer_1 {
		fillcolor=lightyellow label="Layer 1" style=rounded
		layer1_ln1 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightgreen]
		layer1_qkv_proj_0 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 0" fillcolor=lightcoral]
		layer1_qkv_proj_1 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 1" fillcolor=lightcoral]
		layer1_qkv_proj_2 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 2" fillcolor=lightcoral]
		layer1_qkv_proj_3 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 4" fillcolor=lightcoral]
		layer1_qkv_proj_4 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 5" fillcolor=lightcoral]
		layer1_qkv_proj_5 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 6" fillcolor=lightcoral]
		layer1_qkv_proj_6 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 8" fillcolor=lightcoral]
		layer1_qkv_proj_7 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 9" fillcolor=lightcoral]
		layer1_qkv_proj_8 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 10" fillcolor=lightcoral]
		layer1_qkv_proj_9 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 12" fillcolor=lightcoral]
		layer1_qkv_proj_10 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 13" fillcolor=lightcoral]
		layer1_qkv_proj_11 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 14" fillcolor=lightcoral]
		layer1_attention_0 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 0" fillcolor=lightcoral]
		layer1_attention_1 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 1" fillcolor=lightcoral]
		layer1_attention_2 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 2" fillcolor=lightcoral]
		layer1_attention_3 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 4" fillcolor=lightcoral]
		layer1_attention_4 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 5" fillcolor=lightcoral]
		layer1_attention_5 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 6" fillcolor=lightcoral]
		layer1_attention_6 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 8" fillcolor=lightcoral]
		layer1_attention_7 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 9" fillcolor=lightcoral]
		layer1_attention_8 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 10" fillcolor=lightcoral]
		layer1_attention_9 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 12" fillcolor=lightcoral]
		layer1_attention_10 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 13" fillcolor=lightcoral]
		layer1_attention_11 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 14" fillcolor=lightcoral]
		layer1_attn_out_proj_0 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 0" fillcolor=lightcoral]
		layer1_attn_out_proj_1 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 1" fillcolor=lightcoral]
		layer1_attn_out_proj_2 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 2" fillcolor=lightcoral]
		layer1_attn_out_proj_3 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 4" fillcolor=lightcoral]
		layer1_attn_out_proj_4 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 5" fillcolor=lightcoral]
		layer1_attn_out_proj_5 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 6" fillcolor=lightcoral]
		layer1_attn_out_proj_6 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 8" fillcolor=lightcoral]
		layer1_attn_out_proj_7 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 9" fillcolor=lightcoral]
		layer1_attn_out_proj_8 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 10" fillcolor=lightcoral]
		layer1_attn_out_proj_9 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 12" fillcolor=lightcoral]
		layer1_attn_out_proj_10 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 13" fillcolor=lightcoral]
		layer1_attn_out_proj_11 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 14" fillcolor=lightcoral]
		layer1_attn_allreduce [label="Attention All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightyellow shape=parallelogram]
		layer1_residual1 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightpink shape=diamond]
		layer1_ln2 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightgreen]
		layer1_gate [label="Gate Computation\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPUs: all GPUs" fillcolor=lightblue shape=parallelogram]
		layer1_route_0 [label="Expert Routing\nExperts=[0-3]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer1_expert_0 [label="Expert 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer1_expert_1 [label="Expert 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer1_expert_2 [label="Expert 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer1_expert_3 [label="Expert 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer1_expert_agg_0 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 3" fillcolor=lightyellow shape=parallelogram]
		layer1_route_1 [label="Expert Routing\nExperts=[4-7]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer1_expert_4 [label="Expert 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer1_expert_5 [label="Expert 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer1_expert_6 [label="Expert 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer1_expert_7 [label="Expert 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer1_expert_agg_1 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 7" fillcolor=lightyellow shape=parallelogram]
		layer1_route_2 [label="Expert Routing\nExperts=[8-11]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer1_expert_8 [label="Expert 8\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer1_expert_9 [label="Expert 9\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer1_expert_10 [label="Expert 10\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer1_expert_11 [label="Expert 11\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer1_expert_agg_2 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 11" fillcolor=lightyellow shape=parallelogram]
		layer1_route_3 [label="Expert Routing\nExperts=[12-15]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer1_expert_12 [label="Expert 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer1_expert_13 [label="Expert 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer1_expert_14 [label="Expert 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer1_expert_15 [label="Expert 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer1_expert_agg_3 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15" fillcolor=lightyellow shape=parallelogram]
		layer1_expert_allreduce [label="MoE All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightyellow shape=parallelogram]
		layer1_residual2 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightpink shape=diamond]
	}
	subgraph cluster_layer_2 {
		fillcolor=lightyellow label="Layer 2" style=rounded
		layer2_ln1 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightgreen]
		layer2_qkv_proj_0 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 0" fillcolor=lightcoral]
		layer2_qkv_proj_1 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 1" fillcolor=lightcoral]
		layer2_qkv_proj_2 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 2" fillcolor=lightcoral]
		layer2_qkv_proj_3 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 4" fillcolor=lightcoral]
		layer2_qkv_proj_4 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 5" fillcolor=lightcoral]
		layer2_qkv_proj_5 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 6" fillcolor=lightcoral]
		layer2_qkv_proj_6 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 8" fillcolor=lightcoral]
		layer2_qkv_proj_7 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 9" fillcolor=lightcoral]
		layer2_qkv_proj_8 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 10" fillcolor=lightcoral]
		layer2_qkv_proj_9 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 12" fillcolor=lightcoral]
		layer2_qkv_proj_10 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 13" fillcolor=lightcoral]
		layer2_qkv_proj_11 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 14" fillcolor=lightcoral]
		layer2_attention_0 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 0" fillcolor=lightcoral]
		layer2_attention_1 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 1" fillcolor=lightcoral]
		layer2_attention_2 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 2" fillcolor=lightcoral]
		layer2_attention_3 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 4" fillcolor=lightcoral]
		layer2_attention_4 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 5" fillcolor=lightcoral]
		layer2_attention_5 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 6" fillcolor=lightcoral]
		layer2_attention_6 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 8" fillcolor=lightcoral]
		layer2_attention_7 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 9" fillcolor=lightcoral]
		layer2_attention_8 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 10" fillcolor=lightcoral]
		layer2_attention_9 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 12" fillcolor=lightcoral]
		layer2_attention_10 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 13" fillcolor=lightcoral]
		layer2_attention_11 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 14" fillcolor=lightcoral]
		layer2_attn_out_proj_0 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 0" fillcolor=lightcoral]
		layer2_attn_out_proj_1 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 1" fillcolor=lightcoral]
		layer2_attn_out_proj_2 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 2" fillcolor=lightcoral]
		layer2_attn_out_proj_3 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 4" fillcolor=lightcoral]
		layer2_attn_out_proj_4 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 5" fillcolor=lightcoral]
		layer2_attn_out_proj_5 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 6" fillcolor=lightcoral]
		layer2_attn_out_proj_6 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 8" fillcolor=lightcoral]
		layer2_attn_out_proj_7 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 9" fillcolor=lightcoral]
		layer2_attn_out_proj_8 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 10" fillcolor=lightcoral]
		layer2_attn_out_proj_9 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 12" fillcolor=lightcoral]
		layer2_attn_out_proj_10 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 13" fillcolor=lightcoral]
		layer2_attn_out_proj_11 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 14" fillcolor=lightcoral]
		layer2_attn_allreduce [label="Attention All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightyellow shape=parallelogram]
		layer2_residual1 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightpink shape=diamond]
		layer2_ln2 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightgreen]
		layer2_gate [label="Gate Computation\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPUs: all GPUs" fillcolor=lightblue shape=parallelogram]
		layer2_route_0 [label="Expert Routing\nExperts=[0-3]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer2_expert_0 [label="Expert 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer2_expert_1 [label="Expert 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer2_expert_2 [label="Expert 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer2_expert_3 [label="Expert 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer2_expert_agg_0 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 3" fillcolor=lightyellow shape=parallelogram]
		layer2_route_1 [label="Expert Routing\nExperts=[4-7]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer2_expert_4 [label="Expert 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer2_expert_5 [label="Expert 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer2_expert_6 [label="Expert 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer2_expert_7 [label="Expert 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer2_expert_agg_1 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 7" fillcolor=lightyellow shape=parallelogram]
		layer2_route_2 [label="Expert Routing\nExperts=[8-11]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer2_expert_8 [label="Expert 8\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer2_expert_9 [label="Expert 9\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer2_expert_10 [label="Expert 10\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer2_expert_11 [label="Expert 11\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer2_expert_agg_2 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 11" fillcolor=lightyellow shape=parallelogram]
		layer2_route_3 [label="Expert Routing\nExperts=[12-15]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer2_expert_12 [label="Expert 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer2_expert_13 [label="Expert 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer2_expert_14 [label="Expert 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer2_expert_15 [label="Expert 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer2_expert_agg_3 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15" fillcolor=lightyellow shape=parallelogram]
		layer2_expert_allreduce [label="MoE All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightyellow shape=parallelogram]
		layer2_residual2 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightpink shape=diamond]
	}
	subgraph cluster_layer_3 {
		fillcolor=lightyellow label="Layer 3" style=rounded
		layer3_ln1 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightgreen]
		layer3_qkv_proj_0 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 0" fillcolor=lightcoral]
		layer3_qkv_proj_1 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 1" fillcolor=lightcoral]
		layer3_qkv_proj_2 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 2" fillcolor=lightcoral]
		layer3_qkv_proj_3 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 4" fillcolor=lightcoral]
		layer3_qkv_proj_4 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 5" fillcolor=lightcoral]
		layer3_qkv_proj_5 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 6" fillcolor=lightcoral]
		layer3_qkv_proj_6 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 8" fillcolor=lightcoral]
		layer3_qkv_proj_7 [label="QKV Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 9" fillcolor=lightcoral]
		layer3_qkv_proj_8 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 10" fillcolor=lightcoral]
		layer3_qkv_proj_9 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 12" fillcolor=lightcoral]
		layer3_qkv_proj_10 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 13" fillcolor=lightcoral]
		layer3_qkv_proj_11 [label="QKV Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 14" fillcolor=lightcoral]
		layer3_attention_0 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 0" fillcolor=lightcoral]
		layer3_attention_1 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 1" fillcolor=lightcoral]
		layer3_attention_2 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 2" fillcolor=lightcoral]
		layer3_attention_3 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 4" fillcolor=lightcoral]
		layer3_attention_4 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 5" fillcolor=lightcoral]
		layer3_attention_5 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 6" fillcolor=lightcoral]
		layer3_attention_6 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 8" fillcolor=lightcoral]
		layer3_attention_7 [label="Attention Computation\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 9" fillcolor=lightcoral]
		layer3_attention_8 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 10" fillcolor=lightcoral]
		layer3_attention_9 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 12" fillcolor=lightcoral]
		layer3_attention_10 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 13" fillcolor=lightcoral]
		layer3_attention_11 [label="Attention Computation\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 14" fillcolor=lightcoral]
		layer3_attn_out_proj_0 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 0" fillcolor=lightcoral]
		layer3_attn_out_proj_1 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 1" fillcolor=lightcoral]
		layer3_attn_out_proj_2 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 2" fillcolor=lightcoral]
		layer3_attn_out_proj_3 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 4" fillcolor=lightcoral]
		layer3_attn_out_proj_4 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 5" fillcolor=lightcoral]
		layer3_attn_out_proj_5 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 6" fillcolor=lightcoral]
		layer3_attn_out_proj_6 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 8" fillcolor=lightcoral]
		layer3_attn_out_proj_7 [label="Attention Output Projection\nHeads=3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=384]\nGPU: 9" fillcolor=lightcoral]
		layer3_attn_out_proj_8 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 10" fillcolor=lightcoral]
		layer3_attn_out_proj_9 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 12" fillcolor=lightcoral]
		layer3_attn_out_proj_10 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 13" fillcolor=lightcoral]
		layer3_attn_out_proj_11 [label="Attention Output Projection\nHeads=2\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=256]\nGPU: 14" fillcolor=lightcoral]
		layer3_attn_allreduce [label="Attention All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightyellow shape=parallelogram]
		layer3_residual1 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightpink shape=diamond]
		layer3_ln2 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightgreen]
		layer3_gate [label="Gate Computation\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPUs: all GPUs" fillcolor=lightblue shape=parallelogram]
		layer3_route_0 [label="Expert Routing\nExperts=[0-3]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer3_expert_0 [label="Expert 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer3_expert_1 [label="Expert 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer3_expert_2 [label="Expert 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer3_expert_3 [label="Expert 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 3" fillcolor=lightsteelblue]
		layer3_expert_agg_0 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 3" fillcolor=lightyellow shape=parallelogram]
		layer3_route_1 [label="Expert Routing\nExperts=[4-7]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer3_expert_4 [label="Expert 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer3_expert_5 [label="Expert 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer3_expert_6 [label="Expert 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer3_expert_7 [label="Expert 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 7" fillcolor=lightsteelblue]
		layer3_expert_agg_1 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 7" fillcolor=lightyellow shape=parallelogram]
		layer3_route_2 [label="Expert Routing\nExperts=[8-11]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer3_expert_8 [label="Expert 8\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer3_expert_9 [label="Expert 9\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer3_expert_10 [label="Expert 10\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer3_expert_11 [label="Expert 11\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 11" fillcolor=lightsteelblue]
		layer3_expert_agg_2 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 11" fillcolor=lightyellow shape=parallelogram]
		layer3_route_3 [label="Expert Routing\nExperts=[12-15]\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightyellow shape=parallelogram style=dashed]
		layer3_expert_12 [label="Expert 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer3_expert_13 [label="Expert 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer3_expert_14 [label="Expert 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer3_expert_15 [label="Expert 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nGPU: 15" fillcolor=lightsteelblue]
		layer3_expert_agg_3 [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15" fillcolor=lightyellow shape=parallelogram]
		layer3_expert_allreduce [label="MoE All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightyellow shape=parallelogram]
		layer3_residual2 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightpink shape=diamond]
	}
	output [label="Output\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: all GPUs" fillcolor=lightgreen shape=ellipse]
	input -> layer0_ln1
	layer0_ln1 -> layer0_qkv_proj_0
	layer0_qkv_proj_0 -> layer0_attention_0
	layer0_attention_0 -> layer0_attn_out_proj_0
	layer0_attn_out_proj_0 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_proj_1
	layer0_qkv_proj_1 -> layer0_attention_1
	layer0_attention_1 -> layer0_attn_out_proj_1
	layer0_attn_out_proj_1 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_proj_2
	layer0_qkv_proj_2 -> layer0_attention_2
	layer0_attention_2 -> layer0_attn_out_proj_2
	layer0_attn_out_proj_2 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_proj_3
	layer0_qkv_proj_3 -> layer0_attention_3
	layer0_attention_3 -> layer0_attn_out_proj_3
	layer0_attn_out_proj_3 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_proj_4
	layer0_qkv_proj_4 -> layer0_attention_4
	layer0_attention_4 -> layer0_attn_out_proj_4
	layer0_attn_out_proj_4 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_proj_5
	layer0_qkv_proj_5 -> layer0_attention_5
	layer0_attention_5 -> layer0_attn_out_proj_5
	layer0_attn_out_proj_5 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_proj_6
	layer0_qkv_proj_6 -> layer0_attention_6
	layer0_attention_6 -> layer0_attn_out_proj_6
	layer0_attn_out_proj_6 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_proj_7
	layer0_qkv_proj_7 -> layer0_attention_7
	layer0_attention_7 -> layer0_attn_out_proj_7
	layer0_attn_out_proj_7 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_proj_8
	layer0_qkv_proj_8 -> layer0_attention_8
	layer0_attention_8 -> layer0_attn_out_proj_8
	layer0_attn_out_proj_8 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_proj_9
	layer0_qkv_proj_9 -> layer0_attention_9
	layer0_attention_9 -> layer0_attn_out_proj_9
	layer0_attn_out_proj_9 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_proj_10
	layer0_qkv_proj_10 -> layer0_attention_10
	layer0_attention_10 -> layer0_attn_out_proj_10
	layer0_attn_out_proj_10 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_proj_11
	layer0_qkv_proj_11 -> layer0_attention_11
	layer0_attention_11 -> layer0_attn_out_proj_11
	layer0_attn_out_proj_11 -> layer0_attn_allreduce
	layer0_attn_allreduce -> layer0_residual1
	input -> layer0_residual1
	layer0_residual1 -> layer0_ln2
	layer0_ln2 -> layer0_gate
	layer0_gate -> layer0_route_0 [style=dashed]
	layer0_route_0 -> layer0_expert_0
	layer0_expert_0 -> layer0_expert_agg_0
	layer0_route_0 -> layer0_expert_1
	layer0_expert_1 -> layer0_expert_agg_0
	layer0_route_0 -> layer0_expert_2
	layer0_expert_2 -> layer0_expert_agg_0
	layer0_route_0 -> layer0_expert_3
	layer0_expert_3 -> layer0_expert_agg_0
	layer0_gate -> layer0_route_1 [style=dashed]
	layer0_route_1 -> layer0_expert_4
	layer0_expert_4 -> layer0_expert_agg_1
	layer0_route_1 -> layer0_expert_5
	layer0_expert_5 -> layer0_expert_agg_1
	layer0_route_1 -> layer0_expert_6
	layer0_expert_6 -> layer0_expert_agg_1
	layer0_route_1 -> layer0_expert_7
	layer0_expert_7 -> layer0_expert_agg_1
	layer0_gate -> layer0_route_2 [style=dashed]
	layer0_route_2 -> layer0_expert_8
	layer0_expert_8 -> layer0_expert_agg_2
	layer0_route_2 -> layer0_expert_9
	layer0_expert_9 -> layer0_expert_agg_2
	layer0_route_2 -> layer0_expert_10
	layer0_expert_10 -> layer0_expert_agg_2
	layer0_route_2 -> layer0_expert_11
	layer0_expert_11 -> layer0_expert_agg_2
	layer0_gate -> layer0_route_3 [style=dashed]
	layer0_route_3 -> layer0_expert_12
	layer0_expert_12 -> layer0_expert_agg_3
	layer0_route_3 -> layer0_expert_13
	layer0_expert_13 -> layer0_expert_agg_3
	layer0_route_3 -> layer0_expert_14
	layer0_expert_14 -> layer0_expert_agg_3
	layer0_route_3 -> layer0_expert_15
	layer0_expert_15 -> layer0_expert_agg_3
	layer0_expert_agg_0 -> layer0_expert_allreduce
	layer0_expert_agg_1 -> layer0_expert_allreduce
	layer0_expert_agg_2 -> layer0_expert_allreduce
	layer0_expert_agg_3 -> layer0_expert_allreduce
	layer0_expert_allreduce -> layer0_residual2
	layer0_residual1 -> layer0_residual2
	layer0_residual2 -> layer1_ln1
	layer1_ln1 -> layer1_qkv_proj_0
	layer1_qkv_proj_0 -> layer1_attention_0
	layer1_attention_0 -> layer1_attn_out_proj_0
	layer1_attn_out_proj_0 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_proj_1
	layer1_qkv_proj_1 -> layer1_attention_1
	layer1_attention_1 -> layer1_attn_out_proj_1
	layer1_attn_out_proj_1 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_proj_2
	layer1_qkv_proj_2 -> layer1_attention_2
	layer1_attention_2 -> layer1_attn_out_proj_2
	layer1_attn_out_proj_2 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_proj_3
	layer1_qkv_proj_3 -> layer1_attention_3
	layer1_attention_3 -> layer1_attn_out_proj_3
	layer1_attn_out_proj_3 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_proj_4
	layer1_qkv_proj_4 -> layer1_attention_4
	layer1_attention_4 -> layer1_attn_out_proj_4
	layer1_attn_out_proj_4 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_proj_5
	layer1_qkv_proj_5 -> layer1_attention_5
	layer1_attention_5 -> layer1_attn_out_proj_5
	layer1_attn_out_proj_5 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_proj_6
	layer1_qkv_proj_6 -> layer1_attention_6
	layer1_attention_6 -> layer1_attn_out_proj_6
	layer1_attn_out_proj_6 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_proj_7
	layer1_qkv_proj_7 -> layer1_attention_7
	layer1_attention_7 -> layer1_attn_out_proj_7
	layer1_attn_out_proj_7 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_proj_8
	layer1_qkv_proj_8 -> layer1_attention_8
	layer1_attention_8 -> layer1_attn_out_proj_8
	layer1_attn_out_proj_8 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_proj_9
	layer1_qkv_proj_9 -> layer1_attention_9
	layer1_attention_9 -> layer1_attn_out_proj_9
	layer1_attn_out_proj_9 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_proj_10
	layer1_qkv_proj_10 -> layer1_attention_10
	layer1_attention_10 -> layer1_attn_out_proj_10
	layer1_attn_out_proj_10 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_proj_11
	layer1_qkv_proj_11 -> layer1_attention_11
	layer1_attention_11 -> layer1_attn_out_proj_11
	layer1_attn_out_proj_11 -> layer1_attn_allreduce
	layer1_attn_allreduce -> layer1_residual1
	layer0_residual2 -> layer1_residual1
	layer1_residual1 -> layer1_ln2
	layer1_ln2 -> layer1_gate
	layer1_gate -> layer1_route_0 [style=dashed]
	layer1_route_0 -> layer1_expert_0
	layer1_expert_0 -> layer1_expert_agg_0
	layer1_route_0 -> layer1_expert_1
	layer1_expert_1 -> layer1_expert_agg_0
	layer1_route_0 -> layer1_expert_2
	layer1_expert_2 -> layer1_expert_agg_0
	layer1_route_0 -> layer1_expert_3
	layer1_expert_3 -> layer1_expert_agg_0
	layer1_gate -> layer1_route_1 [style=dashed]
	layer1_route_1 -> layer1_expert_4
	layer1_expert_4 -> layer1_expert_agg_1
	layer1_route_1 -> layer1_expert_5
	layer1_expert_5 -> layer1_expert_agg_1
	layer1_route_1 -> layer1_expert_6
	layer1_expert_6 -> layer1_expert_agg_1
	layer1_route_1 -> layer1_expert_7
	layer1_expert_7 -> layer1_expert_agg_1
	layer1_gate -> layer1_route_2 [style=dashed]
	layer1_route_2 -> layer1_expert_8
	layer1_expert_8 -> layer1_expert_agg_2
	layer1_route_2 -> layer1_expert_9
	layer1_expert_9 -> layer1_expert_agg_2
	layer1_route_2 -> layer1_expert_10
	layer1_expert_10 -> layer1_expert_agg_2
	layer1_route_2 -> layer1_expert_11
	layer1_expert_11 -> layer1_expert_agg_2
	layer1_gate -> layer1_route_3 [style=dashed]
	layer1_route_3 -> layer1_expert_12
	layer1_expert_12 -> layer1_expert_agg_3
	layer1_route_3 -> layer1_expert_13
	layer1_expert_13 -> layer1_expert_agg_3
	layer1_route_3 -> layer1_expert_14
	layer1_expert_14 -> layer1_expert_agg_3
	layer1_route_3 -> layer1_expert_15
	layer1_expert_15 -> layer1_expert_agg_3
	layer1_expert_agg_0 -> layer1_expert_allreduce
	layer1_expert_agg_1 -> layer1_expert_allreduce
	layer1_expert_agg_2 -> layer1_expert_allreduce
	layer1_expert_agg_3 -> layer1_expert_allreduce
	layer1_expert_allreduce -> layer1_residual2
	layer1_residual1 -> layer1_residual2
	layer1_residual2 -> layer2_ln1
	layer2_ln1 -> layer2_qkv_proj_0
	layer2_qkv_proj_0 -> layer2_attention_0
	layer2_attention_0 -> layer2_attn_out_proj_0
	layer2_attn_out_proj_0 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_proj_1
	layer2_qkv_proj_1 -> layer2_attention_1
	layer2_attention_1 -> layer2_attn_out_proj_1
	layer2_attn_out_proj_1 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_proj_2
	layer2_qkv_proj_2 -> layer2_attention_2
	layer2_attention_2 -> layer2_attn_out_proj_2
	layer2_attn_out_proj_2 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_proj_3
	layer2_qkv_proj_3 -> layer2_attention_3
	layer2_attention_3 -> layer2_attn_out_proj_3
	layer2_attn_out_proj_3 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_proj_4
	layer2_qkv_proj_4 -> layer2_attention_4
	layer2_attention_4 -> layer2_attn_out_proj_4
	layer2_attn_out_proj_4 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_proj_5
	layer2_qkv_proj_5 -> layer2_attention_5
	layer2_attention_5 -> layer2_attn_out_proj_5
	layer2_attn_out_proj_5 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_proj_6
	layer2_qkv_proj_6 -> layer2_attention_6
	layer2_attention_6 -> layer2_attn_out_proj_6
	layer2_attn_out_proj_6 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_proj_7
	layer2_qkv_proj_7 -> layer2_attention_7
	layer2_attention_7 -> layer2_attn_out_proj_7
	layer2_attn_out_proj_7 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_proj_8
	layer2_qkv_proj_8 -> layer2_attention_8
	layer2_attention_8 -> layer2_attn_out_proj_8
	layer2_attn_out_proj_8 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_proj_9
	layer2_qkv_proj_9 -> layer2_attention_9
	layer2_attention_9 -> layer2_attn_out_proj_9
	layer2_attn_out_proj_9 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_proj_10
	layer2_qkv_proj_10 -> layer2_attention_10
	layer2_attention_10 -> layer2_attn_out_proj_10
	layer2_attn_out_proj_10 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_proj_11
	layer2_qkv_proj_11 -> layer2_attention_11
	layer2_attention_11 -> layer2_attn_out_proj_11
	layer2_attn_out_proj_11 -> layer2_attn_allreduce
	layer2_attn_allreduce -> layer2_residual1
	layer1_residual2 -> layer2_residual1
	layer2_residual1 -> layer2_ln2
	layer2_ln2 -> layer2_gate
	layer2_gate -> layer2_route_0 [style=dashed]
	layer2_route_0 -> layer2_expert_0
	layer2_expert_0 -> layer2_expert_agg_0
	layer2_route_0 -> layer2_expert_1
	layer2_expert_1 -> layer2_expert_agg_0
	layer2_route_0 -> layer2_expert_2
	layer2_expert_2 -> layer2_expert_agg_0
	layer2_route_0 -> layer2_expert_3
	layer2_expert_3 -> layer2_expert_agg_0
	layer2_gate -> layer2_route_1 [style=dashed]
	layer2_route_1 -> layer2_expert_4
	layer2_expert_4 -> layer2_expert_agg_1
	layer2_route_1 -> layer2_expert_5
	layer2_expert_5 -> layer2_expert_agg_1
	layer2_route_1 -> layer2_expert_6
	layer2_expert_6 -> layer2_expert_agg_1
	layer2_route_1 -> layer2_expert_7
	layer2_expert_7 -> layer2_expert_agg_1
	layer2_gate -> layer2_route_2 [style=dashed]
	layer2_route_2 -> layer2_expert_8
	layer2_expert_8 -> layer2_expert_agg_2
	layer2_route_2 -> layer2_expert_9
	layer2_expert_9 -> layer2_expert_agg_2
	layer2_route_2 -> layer2_expert_10
	layer2_expert_10 -> layer2_expert_agg_2
	layer2_route_2 -> layer2_expert_11
	layer2_expert_11 -> layer2_expert_agg_2
	layer2_gate -> layer2_route_3 [style=dashed]
	layer2_route_3 -> layer2_expert_12
	layer2_expert_12 -> layer2_expert_agg_3
	layer2_route_3 -> layer2_expert_13
	layer2_expert_13 -> layer2_expert_agg_3
	layer2_route_3 -> layer2_expert_14
	layer2_expert_14 -> layer2_expert_agg_3
	layer2_route_3 -> layer2_expert_15
	layer2_expert_15 -> layer2_expert_agg_3
	layer2_expert_agg_0 -> layer2_expert_allreduce
	layer2_expert_agg_1 -> layer2_expert_allreduce
	layer2_expert_agg_2 -> layer2_expert_allreduce
	layer2_expert_agg_3 -> layer2_expert_allreduce
	layer2_expert_allreduce -> layer2_residual2
	layer2_residual1 -> layer2_residual2
	layer2_residual2 -> layer3_ln1
	layer3_ln1 -> layer3_qkv_proj_0
	layer3_qkv_proj_0 -> layer3_attention_0
	layer3_attention_0 -> layer3_attn_out_proj_0
	layer3_attn_out_proj_0 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_proj_1
	layer3_qkv_proj_1 -> layer3_attention_1
	layer3_attention_1 -> layer3_attn_out_proj_1
	layer3_attn_out_proj_1 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_proj_2
	layer3_qkv_proj_2 -> layer3_attention_2
	layer3_attention_2 -> layer3_attn_out_proj_2
	layer3_attn_out_proj_2 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_proj_3
	layer3_qkv_proj_3 -> layer3_attention_3
	layer3_attention_3 -> layer3_attn_out_proj_3
	layer3_attn_out_proj_3 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_proj_4
	layer3_qkv_proj_4 -> layer3_attention_4
	layer3_attention_4 -> layer3_attn_out_proj_4
	layer3_attn_out_proj_4 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_proj_5
	layer3_qkv_proj_5 -> layer3_attention_5
	layer3_attention_5 -> layer3_attn_out_proj_5
	layer3_attn_out_proj_5 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_proj_6
	layer3_qkv_proj_6 -> layer3_attention_6
	layer3_attention_6 -> layer3_attn_out_proj_6
	layer3_attn_out_proj_6 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_proj_7
	layer3_qkv_proj_7 -> layer3_attention_7
	layer3_attention_7 -> layer3_attn_out_proj_7
	layer3_attn_out_proj_7 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_proj_8
	layer3_qkv_proj_8 -> layer3_attention_8
	layer3_attention_8 -> layer3_attn_out_proj_8
	layer3_attn_out_proj_8 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_proj_9
	layer3_qkv_proj_9 -> layer3_attention_9
	layer3_attention_9 -> layer3_attn_out_proj_9
	layer3_attn_out_proj_9 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_proj_10
	layer3_qkv_proj_10 -> layer3_attention_10
	layer3_attention_10 -> layer3_attn_out_proj_10
	layer3_attn_out_proj_10 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_proj_11
	layer3_qkv_proj_11 -> layer3_attention_11
	layer3_attention_11 -> layer3_attn_out_proj_11
	layer3_attn_out_proj_11 -> layer3_attn_allreduce
	layer3_attn_allreduce -> layer3_residual1
	layer2_residual2 -> layer3_residual1
	layer3_residual1 -> layer3_ln2
	layer3_ln2 -> layer3_gate
	layer3_gate -> layer3_route_0 [style=dashed]
	layer3_route_0 -> layer3_expert_0
	layer3_expert_0 -> layer3_expert_agg_0
	layer3_route_0 -> layer3_expert_1
	layer3_expert_1 -> layer3_expert_agg_0
	layer3_route_0 -> layer3_expert_2
	layer3_expert_2 -> layer3_expert_agg_0
	layer3_route_0 -> layer3_expert_3
	layer3_expert_3 -> layer3_expert_agg_0
	layer3_gate -> layer3_route_1 [style=dashed]
	layer3_route_1 -> layer3_expert_4
	layer3_expert_4 -> layer3_expert_agg_1
	layer3_route_1 -> layer3_expert_5
	layer3_expert_5 -> layer3_expert_agg_1
	layer3_route_1 -> layer3_expert_6
	layer3_expert_6 -> layer3_expert_agg_1
	layer3_route_1 -> layer3_expert_7
	layer3_expert_7 -> layer3_expert_agg_1
	layer3_gate -> layer3_route_2 [style=dashed]
	layer3_route_2 -> layer3_expert_8
	layer3_expert_8 -> layer3_expert_agg_2
	layer3_route_2 -> layer3_expert_9
	layer3_expert_9 -> layer3_expert_agg_2
	layer3_route_2 -> layer3_expert_10
	layer3_expert_10 -> layer3_expert_agg_2
	layer3_route_2 -> layer3_expert_11
	layer3_expert_11 -> layer3_expert_agg_2
	layer3_gate -> layer3_route_3 [style=dashed]
	layer3_route_3 -> layer3_expert_12
	layer3_expert_12 -> layer3_expert_agg_3
	layer3_route_3 -> layer3_expert_13
	layer3_expert_13 -> layer3_expert_agg_3
	layer3_route_3 -> layer3_expert_14
	layer3_expert_14 -> layer3_expert_agg_3
	layer3_route_3 -> layer3_expert_15
	layer3_expert_15 -> layer3_expert_agg_3
	layer3_expert_agg_0 -> layer3_expert_allreduce
	layer3_expert_agg_1 -> layer3_expert_allreduce
	layer3_expert_agg_2 -> layer3_expert_allreduce
	layer3_expert_agg_3 -> layer3_expert_allreduce
	layer3_expert_allreduce -> layer3_residual2
	layer3_residual1 -> layer3_residual2
	layer3_residual2 -> output
}
