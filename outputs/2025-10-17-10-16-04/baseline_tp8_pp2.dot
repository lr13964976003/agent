// Baseline TP=8, PP=2 MoE Transformer DAG
digraph Baseline_TP8_PP2 {
	graph [rankdir=TB splines=ortho]
	node [fillcolor=lightgray shape=rectangle style=filled]
	input [label="Input\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: stage0" fillcolor=lightblue shape=ellipse]
	subgraph cluster_stage0 {
		fillcolor=lightcyan label="Pipeline Stage 0 (GPUs 0-7)" style=rounded
		subgraph cluster_stage0_layer0 {
			fillcolor=lightyellow label="Stage 0 Layer 0" style=rounded
			stage0_layer0_ln1 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightgreen]
			stage0_layer0_qkv_tp_0 [label="QKV Projection\nTP Rank 0\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 0" fillcolor=lightcoral]
			stage0_layer0_qkv_tp_1 [label="QKV Projection\nTP Rank 1\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 1" fillcolor=lightcoral]
			stage0_layer0_qkv_tp_2 [label="QKV Projection\nTP Rank 2\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 2" fillcolor=lightcoral]
			stage0_layer0_qkv_tp_3 [label="QKV Projection\nTP Rank 3\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 3" fillcolor=lightcoral]
			stage0_layer0_qkv_tp_4 [label="QKV Projection\nTP Rank 4\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 4" fillcolor=lightcoral]
			stage0_layer0_qkv_tp_5 [label="QKV Projection\nTP Rank 5\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 5" fillcolor=lightcoral]
			stage0_layer0_qkv_tp_6 [label="QKV Projection\nTP Rank 6\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 6" fillcolor=lightcoral]
			stage0_layer0_qkv_tp_7 [label="QKV Projection\nTP Rank 7\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 7" fillcolor=lightcoral]
			stage0_layer0_attn_tp_0 [label="Attention Computation\nTP Rank 0\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 0" fillcolor=lightcoral]
			stage0_layer0_attn_tp_1 [label="Attention Computation\nTP Rank 1\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 1" fillcolor=lightcoral]
			stage0_layer0_attn_tp_2 [label="Attention Computation\nTP Rank 2\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 2" fillcolor=lightcoral]
			stage0_layer0_attn_tp_3 [label="Attention Computation\nTP Rank 3\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 3" fillcolor=lightcoral]
			stage0_layer0_attn_tp_4 [label="Attention Computation\nTP Rank 4\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 4" fillcolor=lightcoral]
			stage0_layer0_attn_tp_5 [label="Attention Computation\nTP Rank 5\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 5" fillcolor=lightcoral]
			stage0_layer0_attn_tp_6 [label="Attention Computation\nTP Rank 6\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 6" fillcolor=lightcoral]
			stage0_layer0_attn_tp_7 [label="Attention Computation\nTP Rank 7\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 7" fillcolor=lightcoral]
			stage0_layer0_attn_out_tp_0 [label="Attention Output\nTP Rank 0\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 0" fillcolor=lightcoral]
			stage0_layer0_attn_out_tp_1 [label="Attention Output\nTP Rank 1\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 1" fillcolor=lightcoral]
			stage0_layer0_attn_out_tp_2 [label="Attention Output\nTP Rank 2\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 2" fillcolor=lightcoral]
			stage0_layer0_attn_out_tp_3 [label="Attention Output\nTP Rank 3\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 3" fillcolor=lightcoral]
			stage0_layer0_attn_out_tp_4 [label="Attention Output\nTP Rank 4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 4" fillcolor=lightcoral]
			stage0_layer0_attn_out_tp_5 [label="Attention Output\nTP Rank 5\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 5" fillcolor=lightcoral]
			stage0_layer0_attn_out_tp_6 [label="Attention Output\nTP Rank 6\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 6" fillcolor=lightcoral]
			stage0_layer0_attn_out_tp_7 [label="Attention Output\nTP Rank 7\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 7" fillcolor=lightcoral]
			stage0_layer0_attn_allreduce [label="Attention All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightyellow shape=parallelogram]
			stage0_layer0_residual1 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightpink shape=diamond]
			stage0_layer0_ln2 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightgreen]
			stage0_layer0_gate [label="Gate Computation\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPUs: 0-7" fillcolor=lightblue shape=parallelogram]
			stage0_layer0_expert_0_tp_0 [label="Expert 0\nTP Rank 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 0" fillcolor=lightsteelblue]
			stage0_layer0_expert_1_tp_0 [label="Expert 1\nTP Rank 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 0" fillcolor=lightsteelblue]
			stage0_layer0_expert_2_tp_1 [label="Expert 2\nTP Rank 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 1" fillcolor=lightsteelblue]
			stage0_layer0_expert_3_tp_1 [label="Expert 3\nTP Rank 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 1" fillcolor=lightsteelblue]
			stage0_layer0_expert_4_tp_2 [label="Expert 4\nTP Rank 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 2" fillcolor=lightsteelblue]
			stage0_layer0_expert_5_tp_2 [label="Expert 5\nTP Rank 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 2" fillcolor=lightsteelblue]
			stage0_layer0_expert_6_tp_3 [label="Expert 6\nTP Rank 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 3" fillcolor=lightsteelblue]
			stage0_layer0_expert_7_tp_3 [label="Expert 7\nTP Rank 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 3" fillcolor=lightsteelblue]
			stage0_layer0_expert_8_tp_4 [label="Expert 8\nTP Rank 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 4" fillcolor=lightsteelblue]
			stage0_layer0_expert_9_tp_4 [label="Expert 9\nTP Rank 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 4" fillcolor=lightsteelblue]
			stage0_layer0_expert_10_tp_5 [label="Expert 10\nTP Rank 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 5" fillcolor=lightsteelblue]
			stage0_layer0_expert_11_tp_5 [label="Expert 11\nTP Rank 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 5" fillcolor=lightsteelblue]
			stage0_layer0_expert_12_tp_6 [label="Expert 12\nTP Rank 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 6" fillcolor=lightsteelblue]
			stage0_layer0_expert_13_tp_6 [label="Expert 13\nTP Rank 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 6" fillcolor=lightsteelblue]
			stage0_layer0_expert_14_tp_7 [label="Expert 14\nTP Rank 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 7" fillcolor=lightsteelblue]
			stage0_layer0_expert_15_tp_7 [label="Expert 15\nTP Rank 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 7" fillcolor=lightsteelblue]
			stage0_layer0_expert_agg [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightyellow shape=parallelogram]
			stage0_layer0_expert_allreduce [label="MoE All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightyellow shape=parallelogram]
			stage0_layer0_residual2 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightpink shape=diamond]
		}
		subgraph cluster_stage0_layer1 {
			fillcolor=lightyellow label="Stage 0 Layer 1" style=rounded
			stage0_layer1_ln1 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightgreen]
			stage0_layer1_qkv_tp_0 [label="QKV Projection\nTP Rank 0\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 0" fillcolor=lightcoral]
			stage0_layer1_qkv_tp_1 [label="QKV Projection\nTP Rank 1\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 1" fillcolor=lightcoral]
			stage0_layer1_qkv_tp_2 [label="QKV Projection\nTP Rank 2\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 2" fillcolor=lightcoral]
			stage0_layer1_qkv_tp_3 [label="QKV Projection\nTP Rank 3\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 3" fillcolor=lightcoral]
			stage0_layer1_qkv_tp_4 [label="QKV Projection\nTP Rank 4\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 4" fillcolor=lightcoral]
			stage0_layer1_qkv_tp_5 [label="QKV Projection\nTP Rank 5\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 5" fillcolor=lightcoral]
			stage0_layer1_qkv_tp_6 [label="QKV Projection\nTP Rank 6\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 6" fillcolor=lightcoral]
			stage0_layer1_qkv_tp_7 [label="QKV Projection\nTP Rank 7\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 7" fillcolor=lightcoral]
			stage0_layer1_attn_tp_0 [label="Attention Computation\nTP Rank 0\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 0" fillcolor=lightcoral]
			stage0_layer1_attn_tp_1 [label="Attention Computation\nTP Rank 1\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 1" fillcolor=lightcoral]
			stage0_layer1_attn_tp_2 [label="Attention Computation\nTP Rank 2\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 2" fillcolor=lightcoral]
			stage0_layer1_attn_tp_3 [label="Attention Computation\nTP Rank 3\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 3" fillcolor=lightcoral]
			stage0_layer1_attn_tp_4 [label="Attention Computation\nTP Rank 4\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 4" fillcolor=lightcoral]
			stage0_layer1_attn_tp_5 [label="Attention Computation\nTP Rank 5\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 5" fillcolor=lightcoral]
			stage0_layer1_attn_tp_6 [label="Attention Computation\nTP Rank 6\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 6" fillcolor=lightcoral]
			stage0_layer1_attn_tp_7 [label="Attention Computation\nTP Rank 7\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 7" fillcolor=lightcoral]
			stage0_layer1_attn_out_tp_0 [label="Attention Output\nTP Rank 0\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 0" fillcolor=lightcoral]
			stage0_layer1_attn_out_tp_1 [label="Attention Output\nTP Rank 1\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 1" fillcolor=lightcoral]
			stage0_layer1_attn_out_tp_2 [label="Attention Output\nTP Rank 2\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 2" fillcolor=lightcoral]
			stage0_layer1_attn_out_tp_3 [label="Attention Output\nTP Rank 3\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 3" fillcolor=lightcoral]
			stage0_layer1_attn_out_tp_4 [label="Attention Output\nTP Rank 4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 4" fillcolor=lightcoral]
			stage0_layer1_attn_out_tp_5 [label="Attention Output\nTP Rank 5\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 5" fillcolor=lightcoral]
			stage0_layer1_attn_out_tp_6 [label="Attention Output\nTP Rank 6\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 6" fillcolor=lightcoral]
			stage0_layer1_attn_out_tp_7 [label="Attention Output\nTP Rank 7\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 7" fillcolor=lightcoral]
			stage0_layer1_attn_allreduce [label="Attention All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightyellow shape=parallelogram]
			stage0_layer1_residual1 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightpink shape=diamond]
			stage0_layer1_ln2 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightgreen]
			stage0_layer1_gate [label="Gate Computation\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPUs: 0-7" fillcolor=lightblue shape=parallelogram]
			stage0_layer1_expert_0_tp_0 [label="Expert 0\nTP Rank 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 0" fillcolor=lightsteelblue]
			stage0_layer1_expert_1_tp_0 [label="Expert 1\nTP Rank 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 0" fillcolor=lightsteelblue]
			stage0_layer1_expert_2_tp_1 [label="Expert 2\nTP Rank 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 1" fillcolor=lightsteelblue]
			stage0_layer1_expert_3_tp_1 [label="Expert 3\nTP Rank 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 1" fillcolor=lightsteelblue]
			stage0_layer1_expert_4_tp_2 [label="Expert 4\nTP Rank 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 2" fillcolor=lightsteelblue]
			stage0_layer1_expert_5_tp_2 [label="Expert 5\nTP Rank 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 2" fillcolor=lightsteelblue]
			stage0_layer1_expert_6_tp_3 [label="Expert 6\nTP Rank 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 3" fillcolor=lightsteelblue]
			stage0_layer1_expert_7_tp_3 [label="Expert 7\nTP Rank 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 3" fillcolor=lightsteelblue]
			stage0_layer1_expert_8_tp_4 [label="Expert 8\nTP Rank 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 4" fillcolor=lightsteelblue]
			stage0_layer1_expert_9_tp_4 [label="Expert 9\nTP Rank 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 4" fillcolor=lightsteelblue]
			stage0_layer1_expert_10_tp_5 [label="Expert 10\nTP Rank 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 5" fillcolor=lightsteelblue]
			stage0_layer1_expert_11_tp_5 [label="Expert 11\nTP Rank 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 5" fillcolor=lightsteelblue]
			stage0_layer1_expert_12_tp_6 [label="Expert 12\nTP Rank 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 6" fillcolor=lightsteelblue]
			stage0_layer1_expert_13_tp_6 [label="Expert 13\nTP Rank 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 6" fillcolor=lightsteelblue]
			stage0_layer1_expert_14_tp_7 [label="Expert 14\nTP Rank 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 7" fillcolor=lightsteelblue]
			stage0_layer1_expert_15_tp_7 [label="Expert 15\nTP Rank 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 7" fillcolor=lightsteelblue]
			stage0_layer1_expert_agg [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightyellow shape=parallelogram]
			stage0_layer1_expert_allreduce [label="MoE All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightyellow shape=parallelogram]
			stage0_layer1_residual2 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 0-7" fillcolor=lightpink shape=diamond]
		}
	}
	pipeline_comm_stage0_to_stage1 [label="Pipeline Communication\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 7→8" fillcolor=orange shape=parallelogram]
	subgraph cluster_stage1 {
		fillcolor=lightcyan label="Pipeline Stage 1 (GPUs 8-15)" style=rounded
		subgraph cluster_stage1_layer2 {
			fillcolor=lightyellow label="Stage 1 Layer 2" style=rounded
			stage1_layer2_ln1 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightgreen]
			stage1_layer2_qkv_tp_0 [label="QKV Projection\nTP Rank 0\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 8" fillcolor=lightcoral]
			stage1_layer2_qkv_tp_1 [label="QKV Projection\nTP Rank 1\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 9" fillcolor=lightcoral]
			stage1_layer2_qkv_tp_2 [label="QKV Projection\nTP Rank 2\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 10" fillcolor=lightcoral]
			stage1_layer2_qkv_tp_3 [label="QKV Projection\nTP Rank 3\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 11" fillcolor=lightcoral]
			stage1_layer2_qkv_tp_4 [label="QKV Projection\nTP Rank 4\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 12" fillcolor=lightcoral]
			stage1_layer2_qkv_tp_5 [label="QKV Projection\nTP Rank 5\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 13" fillcolor=lightcoral]
			stage1_layer2_qkv_tp_6 [label="QKV Projection\nTP Rank 6\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 14" fillcolor=lightcoral]
			stage1_layer2_qkv_tp_7 [label="QKV Projection\nTP Rank 7\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 15" fillcolor=lightcoral]
			stage1_layer2_attn_tp_0 [label="Attention Computation\nTP Rank 0\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 8" fillcolor=lightcoral]
			stage1_layer2_attn_tp_1 [label="Attention Computation\nTP Rank 1\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 9" fillcolor=lightcoral]
			stage1_layer2_attn_tp_2 [label="Attention Computation\nTP Rank 2\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 10" fillcolor=lightcoral]
			stage1_layer2_attn_tp_3 [label="Attention Computation\nTP Rank 3\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 11" fillcolor=lightcoral]
			stage1_layer2_attn_tp_4 [label="Attention Computation\nTP Rank 4\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 12" fillcolor=lightcoral]
			stage1_layer2_attn_tp_5 [label="Attention Computation\nTP Rank 5\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 13" fillcolor=lightcoral]
			stage1_layer2_attn_tp_6 [label="Attention Computation\nTP Rank 6\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 14" fillcolor=lightcoral]
			stage1_layer2_attn_tp_7 [label="Attention Computation\nTP Rank 7\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 15" fillcolor=lightcoral]
			stage1_layer2_attn_out_tp_0 [label="Attention Output\nTP Rank 0\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 8" fillcolor=lightcoral]
			stage1_layer2_attn_out_tp_1 [label="Attention Output\nTP Rank 1\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 9" fillcolor=lightcoral]
			stage1_layer2_attn_out_tp_2 [label="Attention Output\nTP Rank 2\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 10" fillcolor=lightcoral]
			stage1_layer2_attn_out_tp_3 [label="Attention Output\nTP Rank 3\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 11" fillcolor=lightcoral]
			stage1_layer2_attn_out_tp_4 [label="Attention Output\nTP Rank 4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 12" fillcolor=lightcoral]
			stage1_layer2_attn_out_tp_5 [label="Attention Output\nTP Rank 5\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 13" fillcolor=lightcoral]
			stage1_layer2_attn_out_tp_6 [label="Attention Output\nTP Rank 6\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 14" fillcolor=lightcoral]
			stage1_layer2_attn_out_tp_7 [label="Attention Output\nTP Rank 7\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 15" fillcolor=lightcoral]
			stage1_layer2_attn_allreduce [label="Attention All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightyellow shape=parallelogram]
			stage1_layer2_residual1 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightpink shape=diamond]
			stage1_layer2_ln2 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightgreen]
			stage1_layer2_gate [label="Gate Computation\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPUs: 8-15" fillcolor=lightblue shape=parallelogram]
			stage1_layer2_expert_0_tp_0 [label="Expert 0\nTP Rank 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 8" fillcolor=lightsteelblue]
			stage1_layer2_expert_1_tp_0 [label="Expert 1\nTP Rank 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 8" fillcolor=lightsteelblue]
			stage1_layer2_expert_2_tp_1 [label="Expert 2\nTP Rank 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 9" fillcolor=lightsteelblue]
			stage1_layer2_expert_3_tp_1 [label="Expert 3\nTP Rank 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 9" fillcolor=lightsteelblue]
			stage1_layer2_expert_4_tp_2 [label="Expert 4\nTP Rank 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 10" fillcolor=lightsteelblue]
			stage1_layer2_expert_5_tp_2 [label="Expert 5\nTP Rank 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 10" fillcolor=lightsteelblue]
			stage1_layer2_expert_6_tp_3 [label="Expert 6\nTP Rank 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 11" fillcolor=lightsteelblue]
			stage1_layer2_expert_7_tp_3 [label="Expert 7\nTP Rank 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 11" fillcolor=lightsteelblue]
			stage1_layer2_expert_8_tp_4 [label="Expert 8\nTP Rank 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 12" fillcolor=lightsteelblue]
			stage1_layer2_expert_9_tp_4 [label="Expert 9\nTP Rank 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 12" fillcolor=lightsteelblue]
			stage1_layer2_expert_10_tp_5 [label="Expert 10\nTP Rank 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 13" fillcolor=lightsteelblue]
			stage1_layer2_expert_11_tp_5 [label="Expert 11\nTP Rank 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 13" fillcolor=lightsteelblue]
			stage1_layer2_expert_12_tp_6 [label="Expert 12\nTP Rank 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 14" fillcolor=lightsteelblue]
			stage1_layer2_expert_13_tp_6 [label="Expert 13\nTP Rank 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 14" fillcolor=lightsteelblue]
			stage1_layer2_expert_14_tp_7 [label="Expert 14\nTP Rank 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 15" fillcolor=lightsteelblue]
			stage1_layer2_expert_15_tp_7 [label="Expert 15\nTP Rank 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 15" fillcolor=lightsteelblue]
			stage1_layer2_expert_agg [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightyellow shape=parallelogram]
			stage1_layer2_expert_allreduce [label="MoE All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightyellow shape=parallelogram]
			stage1_layer2_residual2 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightpink shape=diamond]
		}
		subgraph cluster_stage1_layer3 {
			fillcolor=lightyellow label="Stage 1 Layer 3" style=rounded
			stage1_layer3_ln1 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightgreen]
			stage1_layer3_qkv_tp_0 [label="QKV Projection\nTP Rank 0\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 8" fillcolor=lightcoral]
			stage1_layer3_qkv_tp_1 [label="QKV Projection\nTP Rank 1\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 9" fillcolor=lightcoral]
			stage1_layer3_qkv_tp_2 [label="QKV Projection\nTP Rank 2\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 10" fillcolor=lightcoral]
			stage1_layer3_qkv_tp_3 [label="QKV Projection\nTP Rank 3\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 11" fillcolor=lightcoral]
			stage1_layer3_qkv_tp_4 [label="QKV Projection\nTP Rank 4\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 12" fillcolor=lightcoral]
			stage1_layer3_qkv_tp_5 [label="QKV Projection\nTP Rank 5\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 13" fillcolor=lightcoral]
			stage1_layer3_qkv_tp_6 [label="QKV Projection\nTP Rank 6\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 14" fillcolor=lightcoral]
			stage1_layer3_qkv_tp_7 [label="QKV Projection\nTP Rank 7\nHeads=4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 15" fillcolor=lightcoral]
			stage1_layer3_attn_tp_0 [label="Attention Computation\nTP Rank 0\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 8" fillcolor=lightcoral]
			stage1_layer3_attn_tp_1 [label="Attention Computation\nTP Rank 1\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 9" fillcolor=lightcoral]
			stage1_layer3_attn_tp_2 [label="Attention Computation\nTP Rank 2\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 10" fillcolor=lightcoral]
			stage1_layer3_attn_tp_3 [label="Attention Computation\nTP Rank 3\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 11" fillcolor=lightcoral]
			stage1_layer3_attn_tp_4 [label="Attention Computation\nTP Rank 4\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 12" fillcolor=lightcoral]
			stage1_layer3_attn_tp_5 [label="Attention Computation\nTP Rank 5\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 13" fillcolor=lightcoral]
			stage1_layer3_attn_tp_6 [label="Attention Computation\nTP Rank 6\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 14" fillcolor=lightcoral]
			stage1_layer3_attn_tp_7 [label="Attention Computation\nTP Rank 7\nHeads=4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nGPU: 15" fillcolor=lightcoral]
			stage1_layer3_attn_out_tp_0 [label="Attention Output\nTP Rank 0\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 8" fillcolor=lightcoral]
			stage1_layer3_attn_out_tp_1 [label="Attention Output\nTP Rank 1\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 9" fillcolor=lightcoral]
			stage1_layer3_attn_out_tp_2 [label="Attention Output\nTP Rank 2\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 10" fillcolor=lightcoral]
			stage1_layer3_attn_out_tp_3 [label="Attention Output\nTP Rank 3\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 11" fillcolor=lightcoral]
			stage1_layer3_attn_out_tp_4 [label="Attention Output\nTP Rank 4\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 12" fillcolor=lightcoral]
			stage1_layer3_attn_out_tp_5 [label="Attention Output\nTP Rank 5\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 13" fillcolor=lightcoral]
			stage1_layer3_attn_out_tp_6 [label="Attention Output\nTP Rank 6\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 14" fillcolor=lightcoral]
			stage1_layer3_attn_out_tp_7 [label="Attention Output\nTP Rank 7\nInput: [batch_size=1024, seq_len=2048, heads=4, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nGPU: 15" fillcolor=lightcoral]
			stage1_layer3_attn_allreduce [label="Attention All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightyellow shape=parallelogram]
			stage1_layer3_residual1 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightpink shape=diamond]
			stage1_layer3_ln2 [label="LayerNorm\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightgreen]
			stage1_layer3_gate [label="Gate Computation\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPUs: 8-15" fillcolor=lightblue shape=parallelogram]
			stage1_layer3_expert_0_tp_0 [label="Expert 0\nTP Rank 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 8" fillcolor=lightsteelblue]
			stage1_layer3_expert_1_tp_0 [label="Expert 1\nTP Rank 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 8" fillcolor=lightsteelblue]
			stage1_layer3_expert_2_tp_1 [label="Expert 2\nTP Rank 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 9" fillcolor=lightsteelblue]
			stage1_layer3_expert_3_tp_1 [label="Expert 3\nTP Rank 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 9" fillcolor=lightsteelblue]
			stage1_layer3_expert_4_tp_2 [label="Expert 4\nTP Rank 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 10" fillcolor=lightsteelblue]
			stage1_layer3_expert_5_tp_2 [label="Expert 5\nTP Rank 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 10" fillcolor=lightsteelblue]
			stage1_layer3_expert_6_tp_3 [label="Expert 6\nTP Rank 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 11" fillcolor=lightsteelblue]
			stage1_layer3_expert_7_tp_3 [label="Expert 7\nTP Rank 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 11" fillcolor=lightsteelblue]
			stage1_layer3_expert_8_tp_4 [label="Expert 8\nTP Rank 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 12" fillcolor=lightsteelblue]
			stage1_layer3_expert_9_tp_4 [label="Expert 9\nTP Rank 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 12" fillcolor=lightsteelblue]
			stage1_layer3_expert_10_tp_5 [label="Expert 10\nTP Rank 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 13" fillcolor=lightsteelblue]
			stage1_layer3_expert_11_tp_5 [label="Expert 11\nTP Rank 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 13" fillcolor=lightsteelblue]
			stage1_layer3_expert_12_tp_6 [label="Expert 12\nTP Rank 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 14" fillcolor=lightsteelblue]
			stage1_layer3_expert_13_tp_6 [label="Expert 13\nTP Rank 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 14" fillcolor=lightsteelblue]
			stage1_layer3_expert_14_tp_7 [label="Expert 14\nTP Rank 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 15" fillcolor=lightsteelblue]
			stage1_layer3_expert_15_tp_7 [label="Expert 15\nTP Rank 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=512]\nOutput: [batch_size=1024, seq_len=2048, expert_dim=2048]\nGPU: 15" fillcolor=lightsteelblue]
			stage1_layer3_expert_agg [label="Expert Aggregation\nInput: [batch_size=1024, seq_len=2048, expert_dim=16384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightyellow shape=parallelogram]
			stage1_layer3_expert_allreduce [label="MoE All-Reduce\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightyellow shape=parallelogram]
			stage1_layer3_residual2 [label="Residual Add\nInput1: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: 8-15" fillcolor=lightpink shape=diamond]
		}
	}
	output [label="Output\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPUs: stage1" fillcolor=lightgreen shape=ellipse]
	input -> stage0_layer0_ln1
	stage0_layer0_ln1 -> stage0_layer0_qkv_tp_0
	stage0_layer0_qkv_tp_0 -> stage0_layer0_attn_tp_0
	stage0_layer0_attn_tp_0 -> stage0_layer0_attn_out_tp_0
	stage0_layer0_attn_out_tp_0 -> stage0_layer0_attn_allreduce
	stage0_layer0_ln1 -> stage0_layer0_qkv_tp_1
	stage0_layer0_qkv_tp_1 -> stage0_layer0_attn_tp_1
	stage0_layer0_attn_tp_1 -> stage0_layer0_attn_out_tp_1
	stage0_layer0_attn_out_tp_1 -> stage0_layer0_attn_allreduce
	stage0_layer0_ln1 -> stage0_layer0_qkv_tp_2
	stage0_layer0_qkv_tp_2 -> stage0_layer0_attn_tp_2
	stage0_layer0_attn_tp_2 -> stage0_layer0_attn_out_tp_2
	stage0_layer0_attn_out_tp_2 -> stage0_layer0_attn_allreduce
	stage0_layer0_ln1 -> stage0_layer0_qkv_tp_3
	stage0_layer0_qkv_tp_3 -> stage0_layer0_attn_tp_3
	stage0_layer0_attn_tp_3 -> stage0_layer0_attn_out_tp_3
	stage0_layer0_attn_out_tp_3 -> stage0_layer0_attn_allreduce
	stage0_layer0_ln1 -> stage0_layer0_qkv_tp_4
	stage0_layer0_qkv_tp_4 -> stage0_layer0_attn_tp_4
	stage0_layer0_attn_tp_4 -> stage0_layer0_attn_out_tp_4
	stage0_layer0_attn_out_tp_4 -> stage0_layer0_attn_allreduce
	stage0_layer0_ln1 -> stage0_layer0_qkv_tp_5
	stage0_layer0_qkv_tp_5 -> stage0_layer0_attn_tp_5
	stage0_layer0_attn_tp_5 -> stage0_layer0_attn_out_tp_5
	stage0_layer0_attn_out_tp_5 -> stage0_layer0_attn_allreduce
	stage0_layer0_ln1 -> stage0_layer0_qkv_tp_6
	stage0_layer0_qkv_tp_6 -> stage0_layer0_attn_tp_6
	stage0_layer0_attn_tp_6 -> stage0_layer0_attn_out_tp_6
	stage0_layer0_attn_out_tp_6 -> stage0_layer0_attn_allreduce
	stage0_layer0_ln1 -> stage0_layer0_qkv_tp_7
	stage0_layer0_qkv_tp_7 -> stage0_layer0_attn_tp_7
	stage0_layer0_attn_tp_7 -> stage0_layer0_attn_out_tp_7
	stage0_layer0_attn_out_tp_7 -> stage0_layer0_attn_allreduce
	stage0_layer0_attn_allreduce -> stage0_layer0_residual1
	input -> stage0_layer0_residual1
	stage0_layer0_residual1 -> stage0_layer0_ln2
	stage0_layer0_ln2 -> stage0_layer0_gate
	stage0_layer0_gate -> stage0_layer0_expert_0_tp_0 [style=dashed]
	stage0_layer0_gate -> stage0_layer0_expert_1_tp_0 [style=dashed]
	stage0_layer0_expert_0_tp_0 -> stage0_layer0_expert_agg
	stage0_layer0_expert_1_tp_0 -> stage0_layer0_expert_agg
	stage0_layer0_gate -> stage0_layer0_expert_2_tp_1 [style=dashed]
	stage0_layer0_gate -> stage0_layer0_expert_3_tp_1 [style=dashed]
	stage0_layer0_expert_2_tp_1 -> stage0_layer0_expert_agg
	stage0_layer0_expert_3_tp_1 -> stage0_layer0_expert_agg
	stage0_layer0_gate -> stage0_layer0_expert_4_tp_2 [style=dashed]
	stage0_layer0_gate -> stage0_layer0_expert_5_tp_2 [style=dashed]
	stage0_layer0_expert_4_tp_2 -> stage0_layer0_expert_agg
	stage0_layer0_expert_5_tp_2 -> stage0_layer0_expert_agg
	stage0_layer0_gate -> stage0_layer0_expert_6_tp_3 [style=dashed]
	stage0_layer0_gate -> stage0_layer0_expert_7_tp_3 [style=dashed]
	stage0_layer0_expert_6_tp_3 -> stage0_layer0_expert_agg
	stage0_layer0_expert_7_tp_3 -> stage0_layer0_expert_agg
	stage0_layer0_gate -> stage0_layer0_expert_8_tp_4 [style=dashed]
	stage0_layer0_gate -> stage0_layer0_expert_9_tp_4 [style=dashed]
	stage0_layer0_expert_8_tp_4 -> stage0_layer0_expert_agg
	stage0_layer0_expert_9_tp_4 -> stage0_layer0_expert_agg
	stage0_layer0_gate -> stage0_layer0_expert_10_tp_5 [style=dashed]
	stage0_layer0_gate -> stage0_layer0_expert_11_tp_5 [style=dashed]
	stage0_layer0_expert_10_tp_5 -> stage0_layer0_expert_agg
	stage0_layer0_expert_11_tp_5 -> stage0_layer0_expert_agg
	stage0_layer0_gate -> stage0_layer0_expert_12_tp_6 [style=dashed]
	stage0_layer0_gate -> stage0_layer0_expert_13_tp_6 [style=dashed]
	stage0_layer0_expert_12_tp_6 -> stage0_layer0_expert_agg
	stage0_layer0_expert_13_tp_6 -> stage0_layer0_expert_agg
	stage0_layer0_gate -> stage0_layer0_expert_14_tp_7 [style=dashed]
	stage0_layer0_gate -> stage0_layer0_expert_15_tp_7 [style=dashed]
	stage0_layer0_expert_14_tp_7 -> stage0_layer0_expert_agg
	stage0_layer0_expert_15_tp_7 -> stage0_layer0_expert_agg
	stage0_layer0_expert_agg -> stage0_layer0_expert_allreduce
	stage0_layer0_expert_allreduce -> stage0_layer0_residual2
	stage0_layer0_residual1 -> stage0_layer0_residual2
	stage0_layer0_residual2 -> stage0_layer1_ln1
	stage0_layer1_ln1 -> stage0_layer1_qkv_tp_0
	stage0_layer1_qkv_tp_0 -> stage0_layer1_attn_tp_0
	stage0_layer1_attn_tp_0 -> stage0_layer1_attn_out_tp_0
	stage0_layer1_attn_out_tp_0 -> stage0_layer1_attn_allreduce
	stage0_layer1_ln1 -> stage0_layer1_qkv_tp_1
	stage0_layer1_qkv_tp_1 -> stage0_layer1_attn_tp_1
	stage0_layer1_attn_tp_1 -> stage0_layer1_attn_out_tp_1
	stage0_layer1_attn_out_tp_1 -> stage0_layer1_attn_allreduce
	stage0_layer1_ln1 -> stage0_layer1_qkv_tp_2
	stage0_layer1_qkv_tp_2 -> stage0_layer1_attn_tp_2
	stage0_layer1_attn_tp_2 -> stage0_layer1_attn_out_tp_2
	stage0_layer1_attn_out_tp_2 -> stage0_layer1_attn_allreduce
	stage0_layer1_ln1 -> stage0_layer1_qkv_tp_3
	stage0_layer1_qkv_tp_3 -> stage0_layer1_attn_tp_3
	stage0_layer1_attn_tp_3 -> stage0_layer1_attn_out_tp_3
	stage0_layer1_attn_out_tp_3 -> stage0_layer1_attn_allreduce
	stage0_layer1_ln1 -> stage0_layer1_qkv_tp_4
	stage0_layer1_qkv_tp_4 -> stage0_layer1_attn_tp_4
	stage0_layer1_attn_tp_4 -> stage0_layer1_attn_out_tp_4
	stage0_layer1_attn_out_tp_4 -> stage0_layer1_attn_allreduce
	stage0_layer1_ln1 -> stage0_layer1_qkv_tp_5
	stage0_layer1_qkv_tp_5 -> stage0_layer1_attn_tp_5
	stage0_layer1_attn_tp_5 -> stage0_layer1_attn_out_tp_5
	stage0_layer1_attn_out_tp_5 -> stage0_layer1_attn_allreduce
	stage0_layer1_ln1 -> stage0_layer1_qkv_tp_6
	stage0_layer1_qkv_tp_6 -> stage0_layer1_attn_tp_6
	stage0_layer1_attn_tp_6 -> stage0_layer1_attn_out_tp_6
	stage0_layer1_attn_out_tp_6 -> stage0_layer1_attn_allreduce
	stage0_layer1_ln1 -> stage0_layer1_qkv_tp_7
	stage0_layer1_qkv_tp_7 -> stage0_layer1_attn_tp_7
	stage0_layer1_attn_tp_7 -> stage0_layer1_attn_out_tp_7
	stage0_layer1_attn_out_tp_7 -> stage0_layer1_attn_allreduce
	stage0_layer1_attn_allreduce -> stage0_layer1_residual1
	stage0_layer0_residual2 -> stage0_layer1_residual1
	stage0_layer1_residual1 -> stage0_layer1_ln2
	stage0_layer1_ln2 -> stage0_layer1_gate
	stage0_layer1_gate -> stage0_layer1_expert_0_tp_0 [style=dashed]
	stage0_layer1_gate -> stage0_layer1_expert_1_tp_0 [style=dashed]
	stage0_layer1_expert_0_tp_0 -> stage0_layer1_expert_agg
	stage0_layer1_expert_1_tp_0 -> stage0_layer1_expert_agg
	stage0_layer1_gate -> stage0_layer1_expert_2_tp_1 [style=dashed]
	stage0_layer1_gate -> stage0_layer1_expert_3_tp_1 [style=dashed]
	stage0_layer1_expert_2_tp_1 -> stage0_layer1_expert_agg
	stage0_layer1_expert_3_tp_1 -> stage0_layer1_expert_agg
	stage0_layer1_gate -> stage0_layer1_expert_4_tp_2 [style=dashed]
	stage0_layer1_gate -> stage0_layer1_expert_5_tp_2 [style=dashed]
	stage0_layer1_expert_4_tp_2 -> stage0_layer1_expert_agg
	stage0_layer1_expert_5_tp_2 -> stage0_layer1_expert_agg
	stage0_layer1_gate -> stage0_layer1_expert_6_tp_3 [style=dashed]
	stage0_layer1_gate -> stage0_layer1_expert_7_tp_3 [style=dashed]
	stage0_layer1_expert_6_tp_3 -> stage0_layer1_expert_agg
	stage0_layer1_expert_7_tp_3 -> stage0_layer1_expert_agg
	stage0_layer1_gate -> stage0_layer1_expert_8_tp_4 [style=dashed]
	stage0_layer1_gate -> stage0_layer1_expert_9_tp_4 [style=dashed]
	stage0_layer1_expert_8_tp_4 -> stage0_layer1_expert_agg
	stage0_layer1_expert_9_tp_4 -> stage0_layer1_expert_agg
	stage0_layer1_gate -> stage0_layer1_expert_10_tp_5 [style=dashed]
	stage0_layer1_gate -> stage0_layer1_expert_11_tp_5 [style=dashed]
	stage0_layer1_expert_10_tp_5 -> stage0_layer1_expert_agg
	stage0_layer1_expert_11_tp_5 -> stage0_layer1_expert_agg
	stage0_layer1_gate -> stage0_layer1_expert_12_tp_6 [style=dashed]
	stage0_layer1_gate -> stage0_layer1_expert_13_tp_6 [style=dashed]
	stage0_layer1_expert_12_tp_6 -> stage0_layer1_expert_agg
	stage0_layer1_expert_13_tp_6 -> stage0_layer1_expert_agg
	stage0_layer1_gate -> stage0_layer1_expert_14_tp_7 [style=dashed]
	stage0_layer1_gate -> stage0_layer1_expert_15_tp_7 [style=dashed]
	stage0_layer1_expert_14_tp_7 -> stage0_layer1_expert_agg
	stage0_layer1_expert_15_tp_7 -> stage0_layer1_expert_agg
	stage0_layer1_expert_agg -> stage0_layer1_expert_allreduce
	stage0_layer1_expert_allreduce -> stage0_layer1_residual2
	stage0_layer1_residual1 -> stage0_layer1_residual2
	stage0_layer1_residual2 -> pipeline_comm_stage0_to_stage1
	pipeline_comm_stage0_to_stage1 -> stage1_layer2_ln1
	stage1_layer2_ln1 -> stage1_layer2_qkv_tp_0
	stage1_layer2_qkv_tp_0 -> stage1_layer2_attn_tp_0
	stage1_layer2_attn_tp_0 -> stage1_layer2_attn_out_tp_0
	stage1_layer2_attn_out_tp_0 -> stage1_layer2_attn_allreduce
	stage1_layer2_ln1 -> stage1_layer2_qkv_tp_1
	stage1_layer2_qkv_tp_1 -> stage1_layer2_attn_tp_1
	stage1_layer2_attn_tp_1 -> stage1_layer2_attn_out_tp_1
	stage1_layer2_attn_out_tp_1 -> stage1_layer2_attn_allreduce
	stage1_layer2_ln1 -> stage1_layer2_qkv_tp_2
	stage1_layer2_qkv_tp_2 -> stage1_layer2_attn_tp_2
	stage1_layer2_attn_tp_2 -> stage1_layer2_attn_out_tp_2
	stage1_layer2_attn_out_tp_2 -> stage1_layer2_attn_allreduce
	stage1_layer2_ln1 -> stage1_layer2_qkv_tp_3
	stage1_layer2_qkv_tp_3 -> stage1_layer2_attn_tp_3
	stage1_layer2_attn_tp_3 -> stage1_layer2_attn_out_tp_3
	stage1_layer2_attn_out_tp_3 -> stage1_layer2_attn_allreduce
	stage1_layer2_ln1 -> stage1_layer2_qkv_tp_4
	stage1_layer2_qkv_tp_4 -> stage1_layer2_attn_tp_4
	stage1_layer2_attn_tp_4 -> stage1_layer2_attn_out_tp_4
	stage1_layer2_attn_out_tp_4 -> stage1_layer2_attn_allreduce
	stage1_layer2_ln1 -> stage1_layer2_qkv_tp_5
	stage1_layer2_qkv_tp_5 -> stage1_layer2_attn_tp_5
	stage1_layer2_attn_tp_5 -> stage1_layer2_attn_out_tp_5
	stage1_layer2_attn_out_tp_5 -> stage1_layer2_attn_allreduce
	stage1_layer2_ln1 -> stage1_layer2_qkv_tp_6
	stage1_layer2_qkv_tp_6 -> stage1_layer2_attn_tp_6
	stage1_layer2_attn_tp_6 -> stage1_layer2_attn_out_tp_6
	stage1_layer2_attn_out_tp_6 -> stage1_layer2_attn_allreduce
	stage1_layer2_ln1 -> stage1_layer2_qkv_tp_7
	stage1_layer2_qkv_tp_7 -> stage1_layer2_attn_tp_7
	stage1_layer2_attn_tp_7 -> stage1_layer2_attn_out_tp_7
	stage1_layer2_attn_out_tp_7 -> stage1_layer2_attn_allreduce
	stage1_layer2_attn_allreduce -> stage1_layer2_residual1
	pipeline_comm_stage0_to_stage1 -> stage1_layer2_residual1
	stage1_layer2_residual1 -> stage1_layer2_ln2
	stage1_layer2_ln2 -> stage1_layer2_gate
	stage1_layer2_gate -> stage1_layer2_expert_0_tp_0 [style=dashed]
	stage1_layer2_gate -> stage1_layer2_expert_1_tp_0 [style=dashed]
	stage1_layer2_expert_0_tp_0 -> stage1_layer2_expert_agg
	stage1_layer2_expert_1_tp_0 -> stage1_layer2_expert_agg
	stage1_layer2_gate -> stage1_layer2_expert_2_tp_1 [style=dashed]
	stage1_layer2_gate -> stage1_layer2_expert_3_tp_1 [style=dashed]
	stage1_layer2_expert_2_tp_1 -> stage1_layer2_expert_agg
	stage1_layer2_expert_3_tp_1 -> stage1_layer2_expert_agg
	stage1_layer2_gate -> stage1_layer2_expert_4_tp_2 [style=dashed]
	stage1_layer2_gate -> stage1_layer2_expert_5_tp_2 [style=dashed]
	stage1_layer2_expert_4_tp_2 -> stage1_layer2_expert_agg
	stage1_layer2_expert_5_tp_2 -> stage1_layer2_expert_agg
	stage1_layer2_gate -> stage1_layer2_expert_6_tp_3 [style=dashed]
	stage1_layer2_gate -> stage1_layer2_expert_7_tp_3 [style=dashed]
	stage1_layer2_expert_6_tp_3 -> stage1_layer2_expert_agg
	stage1_layer2_expert_7_tp_3 -> stage1_layer2_expert_agg
	stage1_layer2_gate -> stage1_layer2_expert_8_tp_4 [style=dashed]
	stage1_layer2_gate -> stage1_layer2_expert_9_tp_4 [style=dashed]
	stage1_layer2_expert_8_tp_4 -> stage1_layer2_expert_agg
	stage1_layer2_expert_9_tp_4 -> stage1_layer2_expert_agg
	stage1_layer2_gate -> stage1_layer2_expert_10_tp_5 [style=dashed]
	stage1_layer2_gate -> stage1_layer2_expert_11_tp_5 [style=dashed]
	stage1_layer2_expert_10_tp_5 -> stage1_layer2_expert_agg
	stage1_layer2_expert_11_tp_5 -> stage1_layer2_expert_agg
	stage1_layer2_gate -> stage1_layer2_expert_12_tp_6 [style=dashed]
	stage1_layer2_gate -> stage1_layer2_expert_13_tp_6 [style=dashed]
	stage1_layer2_expert_12_tp_6 -> stage1_layer2_expert_agg
	stage1_layer2_expert_13_tp_6 -> stage1_layer2_expert_agg
	stage1_layer2_gate -> stage1_layer2_expert_14_tp_7 [style=dashed]
	stage1_layer2_gate -> stage1_layer2_expert_15_tp_7 [style=dashed]
	stage1_layer2_expert_14_tp_7 -> stage1_layer2_expert_agg
	stage1_layer2_expert_15_tp_7 -> stage1_layer2_expert_agg
	stage1_layer2_expert_agg -> stage1_layer2_expert_allreduce
	stage1_layer2_expert_allreduce -> stage1_layer2_residual2
	stage1_layer2_residual1 -> stage1_layer2_residual2
	stage1_layer2_residual2 -> stage1_layer3_ln1
	stage1_layer3_ln1 -> stage1_layer3_qkv_tp_0
	stage1_layer3_qkv_tp_0 -> stage1_layer3_attn_tp_0
	stage1_layer3_attn_tp_0 -> stage1_layer3_attn_out_tp_0
	stage1_layer3_attn_out_tp_0 -> stage1_layer3_attn_allreduce
	stage1_layer3_ln1 -> stage1_layer3_qkv_tp_1
	stage1_layer3_qkv_tp_1 -> stage1_layer3_attn_tp_1
	stage1_layer3_attn_tp_1 -> stage1_layer3_attn_out_tp_1
	stage1_layer3_attn_out_tp_1 -> stage1_layer3_attn_allreduce
	stage1_layer3_ln1 -> stage1_layer3_qkv_tp_2
	stage1_layer3_qkv_tp_2 -> stage1_layer3_attn_tp_2
	stage1_layer3_attn_tp_2 -> stage1_layer3_attn_out_tp_2
	stage1_layer3_attn_out_tp_2 -> stage1_layer3_attn_allreduce
	stage1_layer3_ln1 -> stage1_layer3_qkv_tp_3
	stage1_layer3_qkv_tp_3 -> stage1_layer3_attn_tp_3
	stage1_layer3_attn_tp_3 -> stage1_layer3_attn_out_tp_3
	stage1_layer3_attn_out_tp_3 -> stage1_layer3_attn_allreduce
	stage1_layer3_ln1 -> stage1_layer3_qkv_tp_4
	stage1_layer3_qkv_tp_4 -> stage1_layer3_attn_tp_4
	stage1_layer3_attn_tp_4 -> stage1_layer3_attn_out_tp_4
	stage1_layer3_attn_out_tp_4 -> stage1_layer3_attn_allreduce
	stage1_layer3_ln1 -> stage1_layer3_qkv_tp_5
	stage1_layer3_qkv_tp_5 -> stage1_layer3_attn_tp_5
	stage1_layer3_attn_tp_5 -> stage1_layer3_attn_out_tp_5
	stage1_layer3_attn_out_tp_5 -> stage1_layer3_attn_allreduce
	stage1_layer3_ln1 -> stage1_layer3_qkv_tp_6
	stage1_layer3_qkv_tp_6 -> stage1_layer3_attn_tp_6
	stage1_layer3_attn_tp_6 -> stage1_layer3_attn_out_tp_6
	stage1_layer3_attn_out_tp_6 -> stage1_layer3_attn_allreduce
	stage1_layer3_ln1 -> stage1_layer3_qkv_tp_7
	stage1_layer3_qkv_tp_7 -> stage1_layer3_attn_tp_7
	stage1_layer3_attn_tp_7 -> stage1_layer3_attn_out_tp_7
	stage1_layer3_attn_out_tp_7 -> stage1_layer3_attn_allreduce
	stage1_layer3_attn_allreduce -> stage1_layer3_residual1
	stage1_layer2_residual2 -> stage1_layer3_residual1
	stage1_layer3_residual1 -> stage1_layer3_ln2
	stage1_layer3_ln2 -> stage1_layer3_gate
	stage1_layer3_gate -> stage1_layer3_expert_0_tp_0 [style=dashed]
	stage1_layer3_gate -> stage1_layer3_expert_1_tp_0 [style=dashed]
	stage1_layer3_expert_0_tp_0 -> stage1_layer3_expert_agg
	stage1_layer3_expert_1_tp_0 -> stage1_layer3_expert_agg
	stage1_layer3_gate -> stage1_layer3_expert_2_tp_1 [style=dashed]
	stage1_layer3_gate -> stage1_layer3_expert_3_tp_1 [style=dashed]
	stage1_layer3_expert_2_tp_1 -> stage1_layer3_expert_agg
	stage1_layer3_expert_3_tp_1 -> stage1_layer3_expert_agg
	stage1_layer3_gate -> stage1_layer3_expert_4_tp_2 [style=dashed]
	stage1_layer3_gate -> stage1_layer3_expert_5_tp_2 [style=dashed]
	stage1_layer3_expert_4_tp_2 -> stage1_layer3_expert_agg
	stage1_layer3_expert_5_tp_2 -> stage1_layer3_expert_agg
	stage1_layer3_gate -> stage1_layer3_expert_6_tp_3 [style=dashed]
	stage1_layer3_gate -> stage1_layer3_expert_7_tp_3 [style=dashed]
	stage1_layer3_expert_6_tp_3 -> stage1_layer3_expert_agg
	stage1_layer3_expert_7_tp_3 -> stage1_layer3_expert_agg
	stage1_layer3_gate -> stage1_layer3_expert_8_tp_4 [style=dashed]
	stage1_layer3_gate -> stage1_layer3_expert_9_tp_4 [style=dashed]
	stage1_layer3_expert_8_tp_4 -> stage1_layer3_expert_agg
	stage1_layer3_expert_9_tp_4 -> stage1_layer3_expert_agg
	stage1_layer3_gate -> stage1_layer3_expert_10_tp_5 [style=dashed]
	stage1_layer3_gate -> stage1_layer3_expert_11_tp_5 [style=dashed]
	stage1_layer3_expert_10_tp_5 -> stage1_layer3_expert_agg
	stage1_layer3_expert_11_tp_5 -> stage1_layer3_expert_agg
	stage1_layer3_gate -> stage1_layer3_expert_12_tp_6 [style=dashed]
	stage1_layer3_gate -> stage1_layer3_expert_13_tp_6 [style=dashed]
	stage1_layer3_expert_12_tp_6 -> stage1_layer3_expert_agg
	stage1_layer3_expert_13_tp_6 -> stage1_layer3_expert_agg
	stage1_layer3_gate -> stage1_layer3_expert_14_tp_7 [style=dashed]
	stage1_layer3_gate -> stage1_layer3_expert_15_tp_7 [style=dashed]
	stage1_layer3_expert_14_tp_7 -> stage1_layer3_expert_agg
	stage1_layer3_expert_15_tp_7 -> stage1_layer3_expert_agg
	stage1_layer3_expert_agg -> stage1_layer3_expert_allreduce
	stage1_layer3_expert_allreduce -> stage1_layer3_residual2
	stage1_layer3_residual1 -> stage1_layer3_residual2
	stage1_layer3_residual2 -> output
}
