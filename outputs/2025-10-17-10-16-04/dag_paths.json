{
  "generated_dags": {
    "ma_separation": {
      "description": "MA Separation method with 12:4 attention:MoE GPU allocation",
      "dot_file": "../outputs/2025-10-17-10-16-04/ma_separation.dot",
      "svg_file": "../outputs/2025-10-17-10-16-04/ma_separation.svg",
      "python_file": "../outputs/2025-10-17-10-16-04/ma_separation_dag.py",
      "configuration": {
        "total_gpus": 16,
        "attention_gpus": 12,
        "moe_gpus": 4,
        "gpus_per_node": 4,
        "attention_gpu_ids": [0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14],
        "moe_gpu_ids": [3, 7, 11, 15],
        "experts_per_gpu": 4,
        "head_distribution": [3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2]
      }
    },
    "baseline_tp8_pp2": {
      "description": "Baseline TP=8, PP=2 configuration with tensor and pipeline parallelism",
      "dot_file": "../outputs/2025-10-17-10-16-04/baseline_tp8_pp2.dot",
      "svg_file": "../outputs/2025-10-17-10-16-04/baseline_tp8_pp2.svg",
      "python_file": "../outputs/2025-10-17-10-16-04/baseline_dag.py",
      "configuration": {
        "total_gpus": 16,
        "tensor_parallel_size": 8,
        "pipeline_parallel_size": 2,
        "stage0_gpus": [0, 1, 2, 3, 4, 5, 6, 7],
        "stage1_gpus": [8, 9, 10, 11, 12, 13, 14, 15],
        "layers_per_stage": 2,
        "heads_per_gpu": 4
      }
    }
  },
  "model_specifications": {
    "architecture": "4-layer MoE transformer",
    "total_parameters": "8.86B",
    "hidden_dim": 4096,
    "attention_heads": 32,
    "head_dim": 128,
    "sequence_length": 2048,
    "batch_size": 1024,
    "experts_per_layer": 16,
    "expert_dim": 16384
  }
}