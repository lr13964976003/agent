// LLM Parallel Strategy DAG
digraph {
	dpi=300 rankdir=TB size="30,40"
	graph [bgcolor=white pad=0.5]
	node [fillcolor=lightblue shape=rectangle style=filled]
	edge [arrowhead=normal penwidth=1.5]
	input [label="Input\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=white penwidth=2 shape=ellipse style=filled]
	embed_0_0 [label="Embedding_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	embed_0_1 [label="Embedding_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	embed_0_2 [label="Embedding_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	embed_0_3 [label="Embedding_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	embed_1_0 [label="Embedding_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	embed_1_1 [label="Embedding_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	embed_1_2 [label="Embedding_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	embed_1_3 [label="Embedding_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	embed_2_0 [label="Embedding_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	embed_2_1 [label="Embedding_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	embed_2_2 [label="Embedding_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	embed_2_3 [label="Embedding_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	embed_3_0 [label="Embedding_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	embed_3_1 [label="Embedding_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	embed_3_2 [label="Embedding_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	embed_3_3 [label="Embedding_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	embed_4_0 [label="Embedding_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	embed_4_1 [label="Embedding_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	embed_4_2 [label="Embedding_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	embed_4_3 [label="Embedding_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	embed_5_0 [label="Embedding_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	embed_5_1 [label="Embedding_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	embed_5_2 [label="Embedding_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	embed_5_3 [label="Embedding_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	embed_6_0 [label="Embedding_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	embed_6_1 [label="Embedding_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	embed_6_2 [label="Embedding_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	embed_6_3 [label="Embedding_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	embed_7_0 [label="Embedding_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	embed_7_1 [label="Embedding_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	embed_7_2 [label="Embedding_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	embed_7_3 [label="Embedding_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	embed_8_0 [label="Embedding_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	embed_8_1 [label="Embedding_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	embed_8_2 [label="Embedding_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	embed_8_3 [label="Embedding_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	embed_9_0 [label="Embedding_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	embed_9_1 [label="Embedding_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	embed_9_2 [label="Embedding_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	embed_9_3 [label="Embedding_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	embed_10_0 [label="Embedding_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	embed_10_1 [label="Embedding_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	embed_10_2 [label="Embedding_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	embed_10_3 [label="Embedding_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	embed_11_0 [label="Embedding_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	embed_11_1 [label="Embedding_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	embed_11_2 [label="Embedding_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	embed_11_3 [label="Embedding_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	embed_12_0 [label="Embedding_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	embed_12_1 [label="Embedding_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	embed_12_2 [label="Embedding_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	embed_12_3 [label="Embedding_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	embed_13_0 [label="Embedding_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	embed_13_1 [label="Embedding_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	embed_13_2 [label="Embedding_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	embed_13_3 [label="Embedding_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	embed_14_0 [label="Embedding_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	embed_14_1 [label="Embedding_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	embed_14_2 [label="Embedding_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	embed_14_3 [label="Embedding_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	embed_15_0 [label="Embedding_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	embed_15_1 [label="Embedding_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	embed_15_2 [label="Embedding_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	embed_15_3 [label="Embedding_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	input -> embed_0_0
	input -> embed_0_1
	input -> embed_0_2
	input -> embed_0_3
	input -> embed_1_0
	input -> embed_1_1
	input -> embed_1_2
	input -> embed_1_3
	input -> embed_2_0
	input -> embed_2_1
	input -> embed_2_2
	input -> embed_2_3
	input -> embed_3_0
	input -> embed_3_1
	input -> embed_3_2
	input -> embed_3_3
	input -> embed_4_0
	input -> embed_4_1
	input -> embed_4_2
	input -> embed_4_3
	input -> embed_5_0
	input -> embed_5_1
	input -> embed_5_2
	input -> embed_5_3
	input -> embed_6_0
	input -> embed_6_1
	input -> embed_6_2
	input -> embed_6_3
	input -> embed_7_0
	input -> embed_7_1
	input -> embed_7_2
	input -> embed_7_3
	input -> embed_8_0
	input -> embed_8_1
	input -> embed_8_2
	input -> embed_8_3
	input -> embed_9_0
	input -> embed_9_1
	input -> embed_9_2
	input -> embed_9_3
	input -> embed_10_0
	input -> embed_10_1
	input -> embed_10_2
	input -> embed_10_3
	input -> embed_11_0
	input -> embed_11_1
	input -> embed_11_2
	input -> embed_11_3
	input -> embed_12_0
	input -> embed_12_1
	input -> embed_12_2
	input -> embed_12_3
	input -> embed_13_0
	input -> embed_13_1
	input -> embed_13_2
	input -> embed_13_3
	input -> embed_14_0
	input -> embed_14_1
	input -> embed_14_2
	input -> embed_14_3
	input -> embed_15_0
	input -> embed_15_1
	input -> embed_15_2
	input -> embed_15_3
	gate_0 [label="Gate_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_1 [label="Gate_1\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_2 [label="Gate_2\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_3 [label="Gate_3\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_4 [label="Gate_4\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_5 [label="Gate_5\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_6 [label="Gate_6\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_7 [label="Gate_7\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_8 [label="Gate_8\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_9 [label="Gate_9\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_10 [label="Gate_10\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_11 [label="Gate_11\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_12 [label="Gate_12\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_13 [label="Gate_13\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_14 [label="Gate_14\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	gate_15 [label="Gate_15\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, top_k=2]" fillcolor=gold penwidth=2 shape=parallelogram style="filled,dashed"]
	input -> gate_0 [penwidth=2 style=dashed]
	input -> gate_1 [penwidth=2 style=dashed]
	input -> gate_2 [penwidth=2 style=dashed]
	input -> gate_3 [penwidth=2 style=dashed]
	input -> gate_4 [penwidth=2 style=dashed]
	input -> gate_5 [penwidth=2 style=dashed]
	input -> gate_6 [penwidth=2 style=dashed]
	input -> gate_7 [penwidth=2 style=dashed]
	input -> gate_8 [penwidth=2 style=dashed]
	input -> gate_9 [penwidth=2 style=dashed]
	input -> gate_10 [penwidth=2 style=dashed]
	input -> gate_11 [penwidth=2 style=dashed]
	input -> gate_12 [penwidth=2 style=dashed]
	input -> gate_13 [penwidth=2 style=dashed]
	input -> gate_14 [penwidth=2 style=dashed]
	input -> gate_15 [penwidth=2 style=dashed]
	qkv_0_0_0 [label="QKV_Proj_0_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_0_0_0 [label="Attention_Scores_0_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_0_0_0 [label="Attention_Softmax_0_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_0_0_0 [label="Attention_Dropout_0_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_0_0_0 [label="Attention_Values_0_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_0_0_0 [label="Attention_Weighted_0_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_0_0_0 [label="Attention_Output_0_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_0_0 [label="Attention_AllReduce_0_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_0_1 [label="QKV_Proj_0_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_0_0_1 [label="Attention_Scores_0_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_0_0_1 [label="Attention_Softmax_0_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_0_0_1 [label="Attention_Dropout_0_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_0_0_1 [label="Attention_Values_0_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_0_0_1 [label="Attention_Weighted_0_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_0_0_1 [label="Attention_Output_0_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_0_0 [label="Attention_AllReduce_0_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_0_2 [label="QKV_Proj_0_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_0_0_2 [label="Attention_Scores_0_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_0_0_2 [label="Attention_Softmax_0_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_0_0_2 [label="Attention_Dropout_0_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_0_0_2 [label="Attention_Values_0_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_0_0_2 [label="Attention_Weighted_0_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_0_0_2 [label="Attention_Output_0_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_0_0 [label="Attention_AllReduce_0_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_0_3 [label="QKV_Proj_0_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_0_0_3 [label="Attention_Scores_0_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_0_0_3 [label="Attention_Softmax_0_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_0_0_3 [label="Attention_Dropout_0_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_0_0_3 [label="Attention_Values_0_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_0_0_3 [label="Attention_Weighted_0_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_0_0_3 [label="Attention_Output_0_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_0_0 [label="Attention_AllReduce_0_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_1_0 [label="QKV_Proj_0_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_0_1_0 [label="Attention_Scores_0_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_0_1_0 [label="Attention_Softmax_0_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_0_1_0 [label="Attention_Dropout_0_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_0_1_0 [label="Attention_Values_0_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_0_1_0 [label="Attention_Weighted_0_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_0_1_0 [label="Attention_Output_0_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_0_1 [label="Attention_AllReduce_0_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_1_1 [label="QKV_Proj_0_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_0_1_1 [label="Attention_Scores_0_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_0_1_1 [label="Attention_Softmax_0_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_0_1_1 [label="Attention_Dropout_0_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_0_1_1 [label="Attention_Values_0_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_0_1_1 [label="Attention_Weighted_0_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_0_1_1 [label="Attention_Output_0_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_0_1 [label="Attention_AllReduce_0_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_1_2 [label="QKV_Proj_0_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_0_1_2 [label="Attention_Scores_0_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_0_1_2 [label="Attention_Softmax_0_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_0_1_2 [label="Attention_Dropout_0_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_0_1_2 [label="Attention_Values_0_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_0_1_2 [label="Attention_Weighted_0_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_0_1_2 [label="Attention_Output_0_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_0_1 [label="Attention_AllReduce_0_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_1_3 [label="QKV_Proj_0_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_0_1_3 [label="Attention_Scores_0_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_0_1_3 [label="Attention_Softmax_0_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_0_1_3 [label="Attention_Dropout_0_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_0_1_3 [label="Attention_Values_0_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_0_1_3 [label="Attention_Weighted_0_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_0_1_3 [label="Attention_Output_0_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_0_1 [label="Attention_AllReduce_0_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_2_0 [label="QKV_Proj_0_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_0_2_0 [label="Attention_Scores_0_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_0_2_0 [label="Attention_Softmax_0_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_0_2_0 [label="Attention_Dropout_0_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_0_2_0 [label="Attention_Values_0_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_0_2_0 [label="Attention_Weighted_0_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_0_2_0 [label="Attention_Output_0_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_0_2 [label="Attention_AllReduce_0_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_2_1 [label="QKV_Proj_0_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_0_2_1 [label="Attention_Scores_0_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_0_2_1 [label="Attention_Softmax_0_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_0_2_1 [label="Attention_Dropout_0_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_0_2_1 [label="Attention_Values_0_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_0_2_1 [label="Attention_Weighted_0_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_0_2_1 [label="Attention_Output_0_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_0_2 [label="Attention_AllReduce_0_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_2_2 [label="QKV_Proj_0_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_0_2_2 [label="Attention_Scores_0_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_0_2_2 [label="Attention_Softmax_0_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_0_2_2 [label="Attention_Dropout_0_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_0_2_2 [label="Attention_Values_0_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_0_2_2 [label="Attention_Weighted_0_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_0_2_2 [label="Attention_Output_0_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_0_2 [label="Attention_AllReduce_0_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_2_3 [label="QKV_Proj_0_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_0_2_3 [label="Attention_Scores_0_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_0_2_3 [label="Attention_Softmax_0_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_0_2_3 [label="Attention_Dropout_0_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_0_2_3 [label="Attention_Values_0_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_0_2_3 [label="Attention_Weighted_0_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_0_2_3 [label="Attention_Output_0_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_0_2 [label="Attention_AllReduce_0_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_3_0 [label="QKV_Proj_0_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_0_3_0 [label="Attention_Scores_0_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_0_3_0 [label="Attention_Softmax_0_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_0_3_0 [label="Attention_Dropout_0_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_0_3_0 [label="Attention_Values_0_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_0_3_0 [label="Attention_Weighted_0_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_0_3_0 [label="Attention_Output_0_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_0_3 [label="Attention_AllReduce_0_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_3_1 [label="QKV_Proj_0_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_0_3_1 [label="Attention_Scores_0_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_0_3_1 [label="Attention_Softmax_0_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_0_3_1 [label="Attention_Dropout_0_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_0_3_1 [label="Attention_Values_0_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_0_3_1 [label="Attention_Weighted_0_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_0_3_1 [label="Attention_Output_0_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_0_3 [label="Attention_AllReduce_0_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_3_2 [label="QKV_Proj_0_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_0_3_2 [label="Attention_Scores_0_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_0_3_2 [label="Attention_Softmax_0_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_0_3_2 [label="Attention_Dropout_0_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_0_3_2 [label="Attention_Values_0_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_0_3_2 [label="Attention_Weighted_0_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_0_3_2 [label="Attention_Output_0_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_0_3 [label="Attention_AllReduce_0_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_3_3 [label="QKV_Proj_0_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_0_3_3 [label="Attention_Scores_0_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_0_3_3 [label="Attention_Softmax_0_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_0_3_3 [label="Attention_Dropout_0_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_0_3_3 [label="Attention_Values_0_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_0_3_3 [label="Attention_Weighted_0_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_0_3_3 [label="Attention_Output_0_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_0_3 [label="Attention_AllReduce_0_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_4_0 [label="QKV_Proj_0_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_0_4_0 [label="Attention_Scores_0_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_0_4_0 [label="Attention_Softmax_0_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_0_4_0 [label="Attention_Dropout_0_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_0_4_0 [label="Attention_Values_0_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_0_4_0 [label="Attention_Weighted_0_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_0_4_0 [label="Attention_Output_0_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_0_4 [label="Attention_AllReduce_0_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_4_1 [label="QKV_Proj_0_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_0_4_1 [label="Attention_Scores_0_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_0_4_1 [label="Attention_Softmax_0_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_0_4_1 [label="Attention_Dropout_0_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_0_4_1 [label="Attention_Values_0_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_0_4_1 [label="Attention_Weighted_0_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_0_4_1 [label="Attention_Output_0_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_0_4 [label="Attention_AllReduce_0_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_4_2 [label="QKV_Proj_0_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_0_4_2 [label="Attention_Scores_0_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_0_4_2 [label="Attention_Softmax_0_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_0_4_2 [label="Attention_Dropout_0_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_0_4_2 [label="Attention_Values_0_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_0_4_2 [label="Attention_Weighted_0_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_0_4_2 [label="Attention_Output_0_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_0_4 [label="Attention_AllReduce_0_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_4_3 [label="QKV_Proj_0_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_0_4_3 [label="Attention_Scores_0_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_0_4_3 [label="Attention_Softmax_0_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_0_4_3 [label="Attention_Dropout_0_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_0_4_3 [label="Attention_Values_0_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_0_4_3 [label="Attention_Weighted_0_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_0_4_3 [label="Attention_Output_0_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_0_4 [label="Attention_AllReduce_0_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_5_0 [label="QKV_Proj_0_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_0_5_0 [label="Attention_Scores_0_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_0_5_0 [label="Attention_Softmax_0_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_0_5_0 [label="Attention_Dropout_0_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_0_5_0 [label="Attention_Values_0_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_0_5_0 [label="Attention_Weighted_0_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_0_5_0 [label="Attention_Output_0_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_0_5 [label="Attention_AllReduce_0_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_5_1 [label="QKV_Proj_0_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_0_5_1 [label="Attention_Scores_0_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_0_5_1 [label="Attention_Softmax_0_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_0_5_1 [label="Attention_Dropout_0_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_0_5_1 [label="Attention_Values_0_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_0_5_1 [label="Attention_Weighted_0_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_0_5_1 [label="Attention_Output_0_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_0_5 [label="Attention_AllReduce_0_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_5_2 [label="QKV_Proj_0_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_0_5_2 [label="Attention_Scores_0_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_0_5_2 [label="Attention_Softmax_0_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_0_5_2 [label="Attention_Dropout_0_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_0_5_2 [label="Attention_Values_0_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_0_5_2 [label="Attention_Weighted_0_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_0_5_2 [label="Attention_Output_0_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_0_5 [label="Attention_AllReduce_0_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_5_3 [label="QKV_Proj_0_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_0_5_3 [label="Attention_Scores_0_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_0_5_3 [label="Attention_Softmax_0_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_0_5_3 [label="Attention_Dropout_0_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_0_5_3 [label="Attention_Values_0_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_0_5_3 [label="Attention_Weighted_0_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_0_5_3 [label="Attention_Output_0_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_0_5 [label="Attention_AllReduce_0_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_6_0 [label="QKV_Proj_0_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_0_6_0 [label="Attention_Scores_0_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_0_6_0 [label="Attention_Softmax_0_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_0_6_0 [label="Attention_Dropout_0_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_0_6_0 [label="Attention_Values_0_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_0_6_0 [label="Attention_Weighted_0_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_0_6_0 [label="Attention_Output_0_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_0_6 [label="Attention_AllReduce_0_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_6_1 [label="QKV_Proj_0_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_0_6_1 [label="Attention_Scores_0_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_0_6_1 [label="Attention_Softmax_0_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_0_6_1 [label="Attention_Dropout_0_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_0_6_1 [label="Attention_Values_0_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_0_6_1 [label="Attention_Weighted_0_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_0_6_1 [label="Attention_Output_0_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_0_6 [label="Attention_AllReduce_0_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_6_2 [label="QKV_Proj_0_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_0_6_2 [label="Attention_Scores_0_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_0_6_2 [label="Attention_Softmax_0_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_0_6_2 [label="Attention_Dropout_0_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_0_6_2 [label="Attention_Values_0_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_0_6_2 [label="Attention_Weighted_0_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_0_6_2 [label="Attention_Output_0_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_0_6 [label="Attention_AllReduce_0_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_6_3 [label="QKV_Proj_0_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_0_6_3 [label="Attention_Scores_0_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_0_6_3 [label="Attention_Softmax_0_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_0_6_3 [label="Attention_Dropout_0_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_0_6_3 [label="Attention_Values_0_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_0_6_3 [label="Attention_Weighted_0_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_0_6_3 [label="Attention_Output_0_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_0_6 [label="Attention_AllReduce_0_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_7_0 [label="QKV_Proj_0_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_0_7_0 [label="Attention_Scores_0_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_0_7_0 [label="Attention_Softmax_0_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_0_7_0 [label="Attention_Dropout_0_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_0_7_0 [label="Attention_Values_0_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_0_7_0 [label="Attention_Weighted_0_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_0_7_0 [label="Attention_Output_0_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_0_7 [label="Attention_AllReduce_0_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_7_1 [label="QKV_Proj_0_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_0_7_1 [label="Attention_Scores_0_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_0_7_1 [label="Attention_Softmax_0_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_0_7_1 [label="Attention_Dropout_0_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_0_7_1 [label="Attention_Values_0_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_0_7_1 [label="Attention_Weighted_0_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_0_7_1 [label="Attention_Output_0_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_0_7 [label="Attention_AllReduce_0_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_7_2 [label="QKV_Proj_0_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_0_7_2 [label="Attention_Scores_0_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_0_7_2 [label="Attention_Softmax_0_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_0_7_2 [label="Attention_Dropout_0_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_0_7_2 [label="Attention_Values_0_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_0_7_2 [label="Attention_Weighted_0_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_0_7_2 [label="Attention_Output_0_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_0_7 [label="Attention_AllReduce_0_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_7_3 [label="QKV_Proj_0_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_0_7_3 [label="Attention_Scores_0_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_0_7_3 [label="Attention_Softmax_0_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_0_7_3 [label="Attention_Dropout_0_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_0_7_3 [label="Attention_Values_0_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_0_7_3 [label="Attention_Weighted_0_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_0_7_3 [label="Attention_Output_0_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_0_7 [label="Attention_AllReduce_0_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_8_0 [label="QKV_Proj_0_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_0_8_0 [label="Attention_Scores_0_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_0_8_0 [label="Attention_Softmax_0_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_0_8_0 [label="Attention_Dropout_0_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_0_8_0 [label="Attention_Values_0_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_0_8_0 [label="Attention_Weighted_0_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_0_8_0 [label="Attention_Output_0_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_0_8 [label="Attention_AllReduce_0_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_8_1 [label="QKV_Proj_0_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_0_8_1 [label="Attention_Scores_0_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_0_8_1 [label="Attention_Softmax_0_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_0_8_1 [label="Attention_Dropout_0_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_0_8_1 [label="Attention_Values_0_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_0_8_1 [label="Attention_Weighted_0_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_0_8_1 [label="Attention_Output_0_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_0_8 [label="Attention_AllReduce_0_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_8_2 [label="QKV_Proj_0_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_0_8_2 [label="Attention_Scores_0_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_0_8_2 [label="Attention_Softmax_0_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_0_8_2 [label="Attention_Dropout_0_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_0_8_2 [label="Attention_Values_0_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_0_8_2 [label="Attention_Weighted_0_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_0_8_2 [label="Attention_Output_0_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_0_8 [label="Attention_AllReduce_0_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_8_3 [label="QKV_Proj_0_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_0_8_3 [label="Attention_Scores_0_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_0_8_3 [label="Attention_Softmax_0_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_0_8_3 [label="Attention_Dropout_0_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_0_8_3 [label="Attention_Values_0_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_0_8_3 [label="Attention_Weighted_0_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_0_8_3 [label="Attention_Output_0_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_0_8 [label="Attention_AllReduce_0_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_9_0 [label="QKV_Proj_0_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_0_9_0 [label="Attention_Scores_0_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_0_9_0 [label="Attention_Softmax_0_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_0_9_0 [label="Attention_Dropout_0_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_0_9_0 [label="Attention_Values_0_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_0_9_0 [label="Attention_Weighted_0_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_0_9_0 [label="Attention_Output_0_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_0_9 [label="Attention_AllReduce_0_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_9_1 [label="QKV_Proj_0_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_0_9_1 [label="Attention_Scores_0_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_0_9_1 [label="Attention_Softmax_0_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_0_9_1 [label="Attention_Dropout_0_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_0_9_1 [label="Attention_Values_0_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_0_9_1 [label="Attention_Weighted_0_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_0_9_1 [label="Attention_Output_0_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_0_9 [label="Attention_AllReduce_0_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_9_2 [label="QKV_Proj_0_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_0_9_2 [label="Attention_Scores_0_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_0_9_2 [label="Attention_Softmax_0_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_0_9_2 [label="Attention_Dropout_0_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_0_9_2 [label="Attention_Values_0_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_0_9_2 [label="Attention_Weighted_0_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_0_9_2 [label="Attention_Output_0_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_0_9 [label="Attention_AllReduce_0_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_9_3 [label="QKV_Proj_0_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_0_9_3 [label="Attention_Scores_0_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_0_9_3 [label="Attention_Softmax_0_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_0_9_3 [label="Attention_Dropout_0_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_0_9_3 [label="Attention_Values_0_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_0_9_3 [label="Attention_Weighted_0_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_0_9_3 [label="Attention_Output_0_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_0_9 [label="Attention_AllReduce_0_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_10_0 [label="QKV_Proj_0_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_0_10_0 [label="Attention_Scores_0_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_0_10_0 [label="Attention_Softmax_0_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_0_10_0 [label="Attention_Dropout_0_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_0_10_0 [label="Attention_Values_0_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_0_10_0 [label="Attention_Weighted_0_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_0_10_0 [label="Attention_Output_0_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_0_10 [label="Attention_AllReduce_0_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_10_1 [label="QKV_Proj_0_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_0_10_1 [label="Attention_Scores_0_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_0_10_1 [label="Attention_Softmax_0_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_0_10_1 [label="Attention_Dropout_0_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_0_10_1 [label="Attention_Values_0_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_0_10_1 [label="Attention_Weighted_0_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_0_10_1 [label="Attention_Output_0_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_0_10 [label="Attention_AllReduce_0_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_10_2 [label="QKV_Proj_0_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_0_10_2 [label="Attention_Scores_0_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_0_10_2 [label="Attention_Softmax_0_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_0_10_2 [label="Attention_Dropout_0_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_0_10_2 [label="Attention_Values_0_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_0_10_2 [label="Attention_Weighted_0_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_0_10_2 [label="Attention_Output_0_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_0_10 [label="Attention_AllReduce_0_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_10_3 [label="QKV_Proj_0_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_0_10_3 [label="Attention_Scores_0_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_0_10_3 [label="Attention_Softmax_0_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_0_10_3 [label="Attention_Dropout_0_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_0_10_3 [label="Attention_Values_0_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_0_10_3 [label="Attention_Weighted_0_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_0_10_3 [label="Attention_Output_0_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_0_10 [label="Attention_AllReduce_0_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_11_0 [label="QKV_Proj_0_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_0_11_0 [label="Attention_Scores_0_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_0_11_0 [label="Attention_Softmax_0_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_0_11_0 [label="Attention_Dropout_0_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_0_11_0 [label="Attention_Values_0_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_0_11_0 [label="Attention_Weighted_0_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_0_11_0 [label="Attention_Output_0_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_0_11 [label="Attention_AllReduce_0_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_11_1 [label="QKV_Proj_0_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_0_11_1 [label="Attention_Scores_0_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_0_11_1 [label="Attention_Softmax_0_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_0_11_1 [label="Attention_Dropout_0_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_0_11_1 [label="Attention_Values_0_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_0_11_1 [label="Attention_Weighted_0_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_0_11_1 [label="Attention_Output_0_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_0_11 [label="Attention_AllReduce_0_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_11_2 [label="QKV_Proj_0_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_0_11_2 [label="Attention_Scores_0_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_0_11_2 [label="Attention_Softmax_0_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_0_11_2 [label="Attention_Dropout_0_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_0_11_2 [label="Attention_Values_0_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_0_11_2 [label="Attention_Weighted_0_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_0_11_2 [label="Attention_Output_0_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_0_11 [label="Attention_AllReduce_0_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_11_3 [label="QKV_Proj_0_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_0_11_3 [label="Attention_Scores_0_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_0_11_3 [label="Attention_Softmax_0_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_0_11_3 [label="Attention_Dropout_0_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_0_11_3 [label="Attention_Values_0_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_0_11_3 [label="Attention_Weighted_0_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_0_11_3 [label="Attention_Output_0_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_0_11 [label="Attention_AllReduce_0_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_12_0 [label="QKV_Proj_0_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_0_12_0 [label="Attention_Scores_0_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_0_12_0 [label="Attention_Softmax_0_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_0_12_0 [label="Attention_Dropout_0_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_0_12_0 [label="Attention_Values_0_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_0_12_0 [label="Attention_Weighted_0_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_0_12_0 [label="Attention_Output_0_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_0_12 [label="Attention_AllReduce_0_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_12_1 [label="QKV_Proj_0_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_0_12_1 [label="Attention_Scores_0_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_0_12_1 [label="Attention_Softmax_0_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_0_12_1 [label="Attention_Dropout_0_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_0_12_1 [label="Attention_Values_0_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_0_12_1 [label="Attention_Weighted_0_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_0_12_1 [label="Attention_Output_0_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_0_12 [label="Attention_AllReduce_0_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_12_2 [label="QKV_Proj_0_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_0_12_2 [label="Attention_Scores_0_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_0_12_2 [label="Attention_Softmax_0_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_0_12_2 [label="Attention_Dropout_0_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_0_12_2 [label="Attention_Values_0_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_0_12_2 [label="Attention_Weighted_0_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_0_12_2 [label="Attention_Output_0_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_0_12 [label="Attention_AllReduce_0_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_12_3 [label="QKV_Proj_0_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_0_12_3 [label="Attention_Scores_0_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_0_12_3 [label="Attention_Softmax_0_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_0_12_3 [label="Attention_Dropout_0_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_0_12_3 [label="Attention_Values_0_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_0_12_3 [label="Attention_Weighted_0_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_0_12_3 [label="Attention_Output_0_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_0_12 [label="Attention_AllReduce_0_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_13_0 [label="QKV_Proj_0_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_0_13_0 [label="Attention_Scores_0_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_0_13_0 [label="Attention_Softmax_0_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_0_13_0 [label="Attention_Dropout_0_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_0_13_0 [label="Attention_Values_0_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_0_13_0 [label="Attention_Weighted_0_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_0_13_0 [label="Attention_Output_0_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_0_13 [label="Attention_AllReduce_0_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_13_1 [label="QKV_Proj_0_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_0_13_1 [label="Attention_Scores_0_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_0_13_1 [label="Attention_Softmax_0_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_0_13_1 [label="Attention_Dropout_0_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_0_13_1 [label="Attention_Values_0_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_0_13_1 [label="Attention_Weighted_0_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_0_13_1 [label="Attention_Output_0_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_0_13 [label="Attention_AllReduce_0_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_13_2 [label="QKV_Proj_0_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_0_13_2 [label="Attention_Scores_0_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_0_13_2 [label="Attention_Softmax_0_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_0_13_2 [label="Attention_Dropout_0_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_0_13_2 [label="Attention_Values_0_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_0_13_2 [label="Attention_Weighted_0_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_0_13_2 [label="Attention_Output_0_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_0_13 [label="Attention_AllReduce_0_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_13_3 [label="QKV_Proj_0_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_0_13_3 [label="Attention_Scores_0_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_0_13_3 [label="Attention_Softmax_0_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_0_13_3 [label="Attention_Dropout_0_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_0_13_3 [label="Attention_Values_0_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_0_13_3 [label="Attention_Weighted_0_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_0_13_3 [label="Attention_Output_0_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_0_13 [label="Attention_AllReduce_0_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_14_0 [label="QKV_Proj_0_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_0_14_0 [label="Attention_Scores_0_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_0_14_0 [label="Attention_Softmax_0_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_0_14_0 [label="Attention_Dropout_0_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_0_14_0 [label="Attention_Values_0_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_0_14_0 [label="Attention_Weighted_0_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_0_14_0 [label="Attention_Output_0_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_0_14 [label="Attention_AllReduce_0_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_14_1 [label="QKV_Proj_0_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_0_14_1 [label="Attention_Scores_0_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_0_14_1 [label="Attention_Softmax_0_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_0_14_1 [label="Attention_Dropout_0_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_0_14_1 [label="Attention_Values_0_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_0_14_1 [label="Attention_Weighted_0_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_0_14_1 [label="Attention_Output_0_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_0_14 [label="Attention_AllReduce_0_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_14_2 [label="QKV_Proj_0_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_0_14_2 [label="Attention_Scores_0_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_0_14_2 [label="Attention_Softmax_0_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_0_14_2 [label="Attention_Dropout_0_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_0_14_2 [label="Attention_Values_0_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_0_14_2 [label="Attention_Weighted_0_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_0_14_2 [label="Attention_Output_0_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_0_14 [label="Attention_AllReduce_0_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_14_3 [label="QKV_Proj_0_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_0_14_3 [label="Attention_Scores_0_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_0_14_3 [label="Attention_Softmax_0_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_0_14_3 [label="Attention_Dropout_0_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_0_14_3 [label="Attention_Values_0_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_0_14_3 [label="Attention_Weighted_0_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_0_14_3 [label="Attention_Output_0_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_0_14 [label="Attention_AllReduce_0_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_15_0 [label="QKV_Proj_0_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_0_15_0 [label="Attention_Scores_0_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_0_15_0 [label="Attention_Softmax_0_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_0_15_0 [label="Attention_Dropout_0_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_0_15_0 [label="Attention_Values_0_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_0_15_0 [label="Attention_Weighted_0_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_0_15_0 [label="Attention_Output_0_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_0_15 [label="Attention_AllReduce_0_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_15_1 [label="QKV_Proj_0_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_0_15_1 [label="Attention_Scores_0_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_0_15_1 [label="Attention_Softmax_0_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_0_15_1 [label="Attention_Dropout_0_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_0_15_1 [label="Attention_Values_0_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_0_15_1 [label="Attention_Weighted_0_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_0_15_1 [label="Attention_Output_0_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_0_15 [label="Attention_AllReduce_0_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_15_2 [label="QKV_Proj_0_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_0_15_2 [label="Attention_Scores_0_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_0_15_2 [label="Attention_Softmax_0_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_0_15_2 [label="Attention_Dropout_0_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_0_15_2 [label="Attention_Values_0_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_0_15_2 [label="Attention_Weighted_0_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_0_15_2 [label="Attention_Output_0_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_0_15 [label="Attention_AllReduce_0_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_0_15_3 [label="QKV_Proj_0_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_0_15_3 [label="Attention_Scores_0_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_0_15_3 [label="Attention_Softmax_0_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_0_15_3 [label="Attention_Dropout_0_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_0_15_3 [label="Attention_Values_0_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_0_15_3 [label="Attention_Weighted_0_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_0_15_3 [label="Attention_Output_0_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_0_15 [label="Attention_AllReduce_0_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_0_0 [label="MLP_Linear1_0_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_0_0_0 [label="GELU_0_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_0_0_0 [label="MLP_Linear2_0_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_0_0_1 [label="MLP_Linear1_0_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_0_0_1 [label="GELU_0_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_0_0_1 [label="MLP_Linear2_0_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_0_0_2 [label="MLP_Linear1_0_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_0_0_2 [label="GELU_0_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_0_0_2 [label="MLP_Linear2_0_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_0_0_3 [label="MLP_Linear1_0_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_0_0_3 [label="GELU_0_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_0_0_3 [label="MLP_Linear2_0_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_0_0 [label="MLP_AllReduce_0_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_1_0 [label="MLP_Linear1_0_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_0_1_0 [label="GELU_0_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_0_1_0 [label="MLP_Linear2_0_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_0_1_1 [label="MLP_Linear1_0_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_0_1_1 [label="GELU_0_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_0_1_1 [label="MLP_Linear2_0_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_0_1_2 [label="MLP_Linear1_0_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_0_1_2 [label="GELU_0_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_0_1_2 [label="MLP_Linear2_0_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_0_1_3 [label="MLP_Linear1_0_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_0_1_3 [label="GELU_0_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_0_1_3 [label="MLP_Linear2_0_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_0_1 [label="MLP_AllReduce_0_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_2_0 [label="MLP_Linear1_0_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_0_2_0 [label="GELU_0_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_0_2_0 [label="MLP_Linear2_0_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_0_2_1 [label="MLP_Linear1_0_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_0_2_1 [label="GELU_0_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_0_2_1 [label="MLP_Linear2_0_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_0_2_2 [label="MLP_Linear1_0_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_0_2_2 [label="GELU_0_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_0_2_2 [label="MLP_Linear2_0_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_0_2_3 [label="MLP_Linear1_0_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_0_2_3 [label="GELU_0_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_0_2_3 [label="MLP_Linear2_0_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_0_2 [label="MLP_AllReduce_0_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_3_0 [label="MLP_Linear1_0_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_0_3_0 [label="GELU_0_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_0_3_0 [label="MLP_Linear2_0_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_0_3_1 [label="MLP_Linear1_0_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_0_3_1 [label="GELU_0_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_0_3_1 [label="MLP_Linear2_0_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_0_3_2 [label="MLP_Linear1_0_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_0_3_2 [label="GELU_0_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_0_3_2 [label="MLP_Linear2_0_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_0_3_3 [label="MLP_Linear1_0_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_0_3_3 [label="GELU_0_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_0_3_3 [label="MLP_Linear2_0_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_0_3 [label="MLP_AllReduce_0_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_4_0 [label="MLP_Linear1_0_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_0_4_0 [label="GELU_0_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_0_4_0 [label="MLP_Linear2_0_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_0_4_1 [label="MLP_Linear1_0_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_0_4_1 [label="GELU_0_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_0_4_1 [label="MLP_Linear2_0_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_0_4_2 [label="MLP_Linear1_0_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_0_4_2 [label="GELU_0_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_0_4_2 [label="MLP_Linear2_0_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_0_4_3 [label="MLP_Linear1_0_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_0_4_3 [label="GELU_0_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_0_4_3 [label="MLP_Linear2_0_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_0_4 [label="MLP_AllReduce_0_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_5_0 [label="MLP_Linear1_0_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_0_5_0 [label="GELU_0_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_0_5_0 [label="MLP_Linear2_0_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_0_5_1 [label="MLP_Linear1_0_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_0_5_1 [label="GELU_0_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_0_5_1 [label="MLP_Linear2_0_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_0_5_2 [label="MLP_Linear1_0_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_0_5_2 [label="GELU_0_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_0_5_2 [label="MLP_Linear2_0_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_0_5_3 [label="MLP_Linear1_0_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_0_5_3 [label="GELU_0_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_0_5_3 [label="MLP_Linear2_0_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_0_5 [label="MLP_AllReduce_0_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_6_0 [label="MLP_Linear1_0_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_0_6_0 [label="GELU_0_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_0_6_0 [label="MLP_Linear2_0_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_0_6_1 [label="MLP_Linear1_0_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_0_6_1 [label="GELU_0_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_0_6_1 [label="MLP_Linear2_0_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_0_6_2 [label="MLP_Linear1_0_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_0_6_2 [label="GELU_0_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_0_6_2 [label="MLP_Linear2_0_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_0_6_3 [label="MLP_Linear1_0_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_0_6_3 [label="GELU_0_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_0_6_3 [label="MLP_Linear2_0_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_0_6 [label="MLP_AllReduce_0_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_7_0 [label="MLP_Linear1_0_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_0_7_0 [label="GELU_0_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_0_7_0 [label="MLP_Linear2_0_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_0_7_1 [label="MLP_Linear1_0_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_0_7_1 [label="GELU_0_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_0_7_1 [label="MLP_Linear2_0_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_0_7_2 [label="MLP_Linear1_0_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_0_7_2 [label="GELU_0_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_0_7_2 [label="MLP_Linear2_0_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_0_7_3 [label="MLP_Linear1_0_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_0_7_3 [label="GELU_0_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_0_7_3 [label="MLP_Linear2_0_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_0_7 [label="MLP_AllReduce_0_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_8_0 [label="MLP_Linear1_0_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_0_8_0 [label="GELU_0_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_0_8_0 [label="MLP_Linear2_0_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_0_8_1 [label="MLP_Linear1_0_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_0_8_1 [label="GELU_0_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_0_8_1 [label="MLP_Linear2_0_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_0_8_2 [label="MLP_Linear1_0_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_0_8_2 [label="GELU_0_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_0_8_2 [label="MLP_Linear2_0_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_0_8_3 [label="MLP_Linear1_0_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_0_8_3 [label="GELU_0_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_0_8_3 [label="MLP_Linear2_0_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_0_8 [label="MLP_AllReduce_0_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_9_0 [label="MLP_Linear1_0_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_0_9_0 [label="GELU_0_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_0_9_0 [label="MLP_Linear2_0_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_0_9_1 [label="MLP_Linear1_0_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_0_9_1 [label="GELU_0_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_0_9_1 [label="MLP_Linear2_0_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_0_9_2 [label="MLP_Linear1_0_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_0_9_2 [label="GELU_0_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_0_9_2 [label="MLP_Linear2_0_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_0_9_3 [label="MLP_Linear1_0_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_0_9_3 [label="GELU_0_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_0_9_3 [label="MLP_Linear2_0_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_0_9 [label="MLP_AllReduce_0_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_10_0 [label="MLP_Linear1_0_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_0_10_0 [label="GELU_0_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_0_10_0 [label="MLP_Linear2_0_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_0_10_1 [label="MLP_Linear1_0_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_0_10_1 [label="GELU_0_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_0_10_1 [label="MLP_Linear2_0_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_0_10_2 [label="MLP_Linear1_0_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_0_10_2 [label="GELU_0_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_0_10_2 [label="MLP_Linear2_0_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_0_10_3 [label="MLP_Linear1_0_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_0_10_3 [label="GELU_0_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_0_10_3 [label="MLP_Linear2_0_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_0_10 [label="MLP_AllReduce_0_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_11_0 [label="MLP_Linear1_0_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_0_11_0 [label="GELU_0_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_0_11_0 [label="MLP_Linear2_0_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_0_11_1 [label="MLP_Linear1_0_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_0_11_1 [label="GELU_0_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_0_11_1 [label="MLP_Linear2_0_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_0_11_2 [label="MLP_Linear1_0_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_0_11_2 [label="GELU_0_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_0_11_2 [label="MLP_Linear2_0_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_0_11_3 [label="MLP_Linear1_0_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_0_11_3 [label="GELU_0_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_0_11_3 [label="MLP_Linear2_0_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_0_11 [label="MLP_AllReduce_0_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_12_0 [label="MLP_Linear1_0_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_0_12_0 [label="GELU_0_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_0_12_0 [label="MLP_Linear2_0_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_0_12_1 [label="MLP_Linear1_0_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_0_12_1 [label="GELU_0_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_0_12_1 [label="MLP_Linear2_0_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_0_12_2 [label="MLP_Linear1_0_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_0_12_2 [label="GELU_0_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_0_12_2 [label="MLP_Linear2_0_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_0_12_3 [label="MLP_Linear1_0_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_0_12_3 [label="GELU_0_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_0_12_3 [label="MLP_Linear2_0_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_0_12 [label="MLP_AllReduce_0_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_13_0 [label="MLP_Linear1_0_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_0_13_0 [label="GELU_0_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_0_13_0 [label="MLP_Linear2_0_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_0_13_1 [label="MLP_Linear1_0_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_0_13_1 [label="GELU_0_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_0_13_1 [label="MLP_Linear2_0_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_0_13_2 [label="MLP_Linear1_0_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_0_13_2 [label="GELU_0_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_0_13_2 [label="MLP_Linear2_0_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_0_13_3 [label="MLP_Linear1_0_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_0_13_3 [label="GELU_0_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_0_13_3 [label="MLP_Linear2_0_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_0_13 [label="MLP_AllReduce_0_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_14_0 [label="MLP_Linear1_0_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_0_14_0 [label="GELU_0_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_0_14_0 [label="MLP_Linear2_0_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_0_14_1 [label="MLP_Linear1_0_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_0_14_1 [label="GELU_0_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_0_14_1 [label="MLP_Linear2_0_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_0_14_2 [label="MLP_Linear1_0_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_0_14_2 [label="GELU_0_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_0_14_2 [label="MLP_Linear2_0_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_0_14_3 [label="MLP_Linear1_0_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_0_14_3 [label="GELU_0_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_0_14_3 [label="MLP_Linear2_0_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_0_14 [label="MLP_AllReduce_0_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_0_15_0 [label="MLP_Linear1_0_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_0_15_0 [label="GELU_0_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_0_15_0 [label="MLP_Linear2_0_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_0_15_1 [label="MLP_Linear1_0_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_0_15_1 [label="GELU_0_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_0_15_1 [label="MLP_Linear2_0_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_0_15_2 [label="MLP_Linear1_0_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_0_15_2 [label="GELU_0_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_0_15_2 [label="MLP_Linear2_0_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_0_15_3 [label="MLP_Linear1_0_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_0_15_3 [label="GELU_0_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_0_15_3 [label="MLP_Linear2_0_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_0_15 [label="MLP_AllReduce_0_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_0_0 [label="Expert_Route_0_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_1 [label="Expert_Route_0_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_2 [label="Expert_Route_0_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_3 [label="Expert_Route_0_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_4 [label="Expert_Route_0_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_5 [label="Expert_Route_0_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_6 [label="Expert_Route_0_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_7 [label="Expert_Route_0_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_8 [label="Expert_Route_0_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_9 [label="Expert_Route_0_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_10 [label="Expert_Route_0_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_11 [label="Expert_Route_0_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_12 [label="Expert_Route_0_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_13 [label="Expert_Route_0_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_14 [label="Expert_Route_0_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_0_15 [label="Expert_Route_0_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_1_0_0 [label="QKV_Proj_1_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_1_0_0 [label="Attention_Scores_1_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_1_0_0 [label="Attention_Softmax_1_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_1_0_0 [label="Attention_Dropout_1_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_1_0_0 [label="Attention_Values_1_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_1_0_0 [label="Attention_Weighted_1_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_1_0_0 [label="Attention_Output_1_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_1_0 [label="Attention_AllReduce_1_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_0_1 [label="QKV_Proj_1_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_1_0_1 [label="Attention_Scores_1_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_1_0_1 [label="Attention_Softmax_1_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_1_0_1 [label="Attention_Dropout_1_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_1_0_1 [label="Attention_Values_1_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_1_0_1 [label="Attention_Weighted_1_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_1_0_1 [label="Attention_Output_1_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_1_0 [label="Attention_AllReduce_1_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_0_2 [label="QKV_Proj_1_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_1_0_2 [label="Attention_Scores_1_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_1_0_2 [label="Attention_Softmax_1_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_1_0_2 [label="Attention_Dropout_1_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_1_0_2 [label="Attention_Values_1_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_1_0_2 [label="Attention_Weighted_1_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_1_0_2 [label="Attention_Output_1_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_1_0 [label="Attention_AllReduce_1_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_0_3 [label="QKV_Proj_1_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_1_0_3 [label="Attention_Scores_1_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_1_0_3 [label="Attention_Softmax_1_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_1_0_3 [label="Attention_Dropout_1_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_1_0_3 [label="Attention_Values_1_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_1_0_3 [label="Attention_Weighted_1_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_1_0_3 [label="Attention_Output_1_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_1_0 [label="Attention_AllReduce_1_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_1_0 [label="QKV_Proj_1_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_1_1_0 [label="Attention_Scores_1_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_1_1_0 [label="Attention_Softmax_1_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_1_1_0 [label="Attention_Dropout_1_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_1_1_0 [label="Attention_Values_1_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_1_1_0 [label="Attention_Weighted_1_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_1_1_0 [label="Attention_Output_1_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_1_1 [label="Attention_AllReduce_1_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_1_1 [label="QKV_Proj_1_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_1_1_1 [label="Attention_Scores_1_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_1_1_1 [label="Attention_Softmax_1_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_1_1_1 [label="Attention_Dropout_1_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_1_1_1 [label="Attention_Values_1_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_1_1_1 [label="Attention_Weighted_1_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_1_1_1 [label="Attention_Output_1_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_1_1 [label="Attention_AllReduce_1_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_1_2 [label="QKV_Proj_1_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_1_1_2 [label="Attention_Scores_1_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_1_1_2 [label="Attention_Softmax_1_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_1_1_2 [label="Attention_Dropout_1_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_1_1_2 [label="Attention_Values_1_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_1_1_2 [label="Attention_Weighted_1_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_1_1_2 [label="Attention_Output_1_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_1_1 [label="Attention_AllReduce_1_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_1_3 [label="QKV_Proj_1_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_1_1_3 [label="Attention_Scores_1_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_1_1_3 [label="Attention_Softmax_1_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_1_1_3 [label="Attention_Dropout_1_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_1_1_3 [label="Attention_Values_1_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_1_1_3 [label="Attention_Weighted_1_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_1_1_3 [label="Attention_Output_1_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_1_1 [label="Attention_AllReduce_1_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_2_0 [label="QKV_Proj_1_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_1_2_0 [label="Attention_Scores_1_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_1_2_0 [label="Attention_Softmax_1_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_1_2_0 [label="Attention_Dropout_1_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_1_2_0 [label="Attention_Values_1_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_1_2_0 [label="Attention_Weighted_1_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_1_2_0 [label="Attention_Output_1_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_1_2 [label="Attention_AllReduce_1_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_2_1 [label="QKV_Proj_1_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_1_2_1 [label="Attention_Scores_1_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_1_2_1 [label="Attention_Softmax_1_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_1_2_1 [label="Attention_Dropout_1_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_1_2_1 [label="Attention_Values_1_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_1_2_1 [label="Attention_Weighted_1_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_1_2_1 [label="Attention_Output_1_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_1_2 [label="Attention_AllReduce_1_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_2_2 [label="QKV_Proj_1_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_1_2_2 [label="Attention_Scores_1_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_1_2_2 [label="Attention_Softmax_1_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_1_2_2 [label="Attention_Dropout_1_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_1_2_2 [label="Attention_Values_1_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_1_2_2 [label="Attention_Weighted_1_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_1_2_2 [label="Attention_Output_1_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_1_2 [label="Attention_AllReduce_1_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_2_3 [label="QKV_Proj_1_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_1_2_3 [label="Attention_Scores_1_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_1_2_3 [label="Attention_Softmax_1_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_1_2_3 [label="Attention_Dropout_1_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_1_2_3 [label="Attention_Values_1_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_1_2_3 [label="Attention_Weighted_1_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_1_2_3 [label="Attention_Output_1_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_1_2 [label="Attention_AllReduce_1_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_3_0 [label="QKV_Proj_1_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_1_3_0 [label="Attention_Scores_1_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_1_3_0 [label="Attention_Softmax_1_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_1_3_0 [label="Attention_Dropout_1_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_1_3_0 [label="Attention_Values_1_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_1_3_0 [label="Attention_Weighted_1_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_1_3_0 [label="Attention_Output_1_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_1_3 [label="Attention_AllReduce_1_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_3_1 [label="QKV_Proj_1_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_1_3_1 [label="Attention_Scores_1_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_1_3_1 [label="Attention_Softmax_1_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_1_3_1 [label="Attention_Dropout_1_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_1_3_1 [label="Attention_Values_1_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_1_3_1 [label="Attention_Weighted_1_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_1_3_1 [label="Attention_Output_1_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_1_3 [label="Attention_AllReduce_1_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_3_2 [label="QKV_Proj_1_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_1_3_2 [label="Attention_Scores_1_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_1_3_2 [label="Attention_Softmax_1_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_1_3_2 [label="Attention_Dropout_1_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_1_3_2 [label="Attention_Values_1_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_1_3_2 [label="Attention_Weighted_1_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_1_3_2 [label="Attention_Output_1_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_1_3 [label="Attention_AllReduce_1_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_3_3 [label="QKV_Proj_1_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_1_3_3 [label="Attention_Scores_1_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_1_3_3 [label="Attention_Softmax_1_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_1_3_3 [label="Attention_Dropout_1_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_1_3_3 [label="Attention_Values_1_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_1_3_3 [label="Attention_Weighted_1_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_1_3_3 [label="Attention_Output_1_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_1_3 [label="Attention_AllReduce_1_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_4_0 [label="QKV_Proj_1_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_1_4_0 [label="Attention_Scores_1_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_1_4_0 [label="Attention_Softmax_1_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_1_4_0 [label="Attention_Dropout_1_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_1_4_0 [label="Attention_Values_1_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_1_4_0 [label="Attention_Weighted_1_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_1_4_0 [label="Attention_Output_1_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_1_4 [label="Attention_AllReduce_1_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_4_1 [label="QKV_Proj_1_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_1_4_1 [label="Attention_Scores_1_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_1_4_1 [label="Attention_Softmax_1_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_1_4_1 [label="Attention_Dropout_1_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_1_4_1 [label="Attention_Values_1_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_1_4_1 [label="Attention_Weighted_1_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_1_4_1 [label="Attention_Output_1_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_1_4 [label="Attention_AllReduce_1_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_4_2 [label="QKV_Proj_1_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_1_4_2 [label="Attention_Scores_1_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_1_4_2 [label="Attention_Softmax_1_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_1_4_2 [label="Attention_Dropout_1_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_1_4_2 [label="Attention_Values_1_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_1_4_2 [label="Attention_Weighted_1_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_1_4_2 [label="Attention_Output_1_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_1_4 [label="Attention_AllReduce_1_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_4_3 [label="QKV_Proj_1_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_1_4_3 [label="Attention_Scores_1_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_1_4_3 [label="Attention_Softmax_1_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_1_4_3 [label="Attention_Dropout_1_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_1_4_3 [label="Attention_Values_1_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_1_4_3 [label="Attention_Weighted_1_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_1_4_3 [label="Attention_Output_1_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_1_4 [label="Attention_AllReduce_1_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_5_0 [label="QKV_Proj_1_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_1_5_0 [label="Attention_Scores_1_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_1_5_0 [label="Attention_Softmax_1_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_1_5_0 [label="Attention_Dropout_1_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_1_5_0 [label="Attention_Values_1_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_1_5_0 [label="Attention_Weighted_1_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_1_5_0 [label="Attention_Output_1_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_1_5 [label="Attention_AllReduce_1_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_5_1 [label="QKV_Proj_1_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_1_5_1 [label="Attention_Scores_1_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_1_5_1 [label="Attention_Softmax_1_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_1_5_1 [label="Attention_Dropout_1_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_1_5_1 [label="Attention_Values_1_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_1_5_1 [label="Attention_Weighted_1_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_1_5_1 [label="Attention_Output_1_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_1_5 [label="Attention_AllReduce_1_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_5_2 [label="QKV_Proj_1_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_1_5_2 [label="Attention_Scores_1_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_1_5_2 [label="Attention_Softmax_1_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_1_5_2 [label="Attention_Dropout_1_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_1_5_2 [label="Attention_Values_1_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_1_5_2 [label="Attention_Weighted_1_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_1_5_2 [label="Attention_Output_1_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_1_5 [label="Attention_AllReduce_1_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_5_3 [label="QKV_Proj_1_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_1_5_3 [label="Attention_Scores_1_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_1_5_3 [label="Attention_Softmax_1_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_1_5_3 [label="Attention_Dropout_1_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_1_5_3 [label="Attention_Values_1_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_1_5_3 [label="Attention_Weighted_1_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_1_5_3 [label="Attention_Output_1_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_1_5 [label="Attention_AllReduce_1_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_6_0 [label="QKV_Proj_1_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_1_6_0 [label="Attention_Scores_1_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_1_6_0 [label="Attention_Softmax_1_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_1_6_0 [label="Attention_Dropout_1_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_1_6_0 [label="Attention_Values_1_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_1_6_0 [label="Attention_Weighted_1_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_1_6_0 [label="Attention_Output_1_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_1_6 [label="Attention_AllReduce_1_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_6_1 [label="QKV_Proj_1_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_1_6_1 [label="Attention_Scores_1_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_1_6_1 [label="Attention_Softmax_1_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_1_6_1 [label="Attention_Dropout_1_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_1_6_1 [label="Attention_Values_1_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_1_6_1 [label="Attention_Weighted_1_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_1_6_1 [label="Attention_Output_1_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_1_6 [label="Attention_AllReduce_1_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_6_2 [label="QKV_Proj_1_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_1_6_2 [label="Attention_Scores_1_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_1_6_2 [label="Attention_Softmax_1_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_1_6_2 [label="Attention_Dropout_1_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_1_6_2 [label="Attention_Values_1_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_1_6_2 [label="Attention_Weighted_1_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_1_6_2 [label="Attention_Output_1_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_1_6 [label="Attention_AllReduce_1_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_6_3 [label="QKV_Proj_1_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_1_6_3 [label="Attention_Scores_1_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_1_6_3 [label="Attention_Softmax_1_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_1_6_3 [label="Attention_Dropout_1_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_1_6_3 [label="Attention_Values_1_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_1_6_3 [label="Attention_Weighted_1_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_1_6_3 [label="Attention_Output_1_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_1_6 [label="Attention_AllReduce_1_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_7_0 [label="QKV_Proj_1_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_1_7_0 [label="Attention_Scores_1_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_1_7_0 [label="Attention_Softmax_1_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_1_7_0 [label="Attention_Dropout_1_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_1_7_0 [label="Attention_Values_1_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_1_7_0 [label="Attention_Weighted_1_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_1_7_0 [label="Attention_Output_1_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_1_7 [label="Attention_AllReduce_1_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_7_1 [label="QKV_Proj_1_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_1_7_1 [label="Attention_Scores_1_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_1_7_1 [label="Attention_Softmax_1_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_1_7_1 [label="Attention_Dropout_1_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_1_7_1 [label="Attention_Values_1_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_1_7_1 [label="Attention_Weighted_1_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_1_7_1 [label="Attention_Output_1_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_1_7 [label="Attention_AllReduce_1_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_7_2 [label="QKV_Proj_1_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_1_7_2 [label="Attention_Scores_1_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_1_7_2 [label="Attention_Softmax_1_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_1_7_2 [label="Attention_Dropout_1_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_1_7_2 [label="Attention_Values_1_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_1_7_2 [label="Attention_Weighted_1_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_1_7_2 [label="Attention_Output_1_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_1_7 [label="Attention_AllReduce_1_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_7_3 [label="QKV_Proj_1_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_1_7_3 [label="Attention_Scores_1_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_1_7_3 [label="Attention_Softmax_1_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_1_7_3 [label="Attention_Dropout_1_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_1_7_3 [label="Attention_Values_1_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_1_7_3 [label="Attention_Weighted_1_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_1_7_3 [label="Attention_Output_1_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_1_7 [label="Attention_AllReduce_1_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_8_0 [label="QKV_Proj_1_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_1_8_0 [label="Attention_Scores_1_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_1_8_0 [label="Attention_Softmax_1_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_1_8_0 [label="Attention_Dropout_1_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_1_8_0 [label="Attention_Values_1_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_1_8_0 [label="Attention_Weighted_1_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_1_8_0 [label="Attention_Output_1_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_1_8 [label="Attention_AllReduce_1_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_8_1 [label="QKV_Proj_1_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_1_8_1 [label="Attention_Scores_1_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_1_8_1 [label="Attention_Softmax_1_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_1_8_1 [label="Attention_Dropout_1_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_1_8_1 [label="Attention_Values_1_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_1_8_1 [label="Attention_Weighted_1_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_1_8_1 [label="Attention_Output_1_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_1_8 [label="Attention_AllReduce_1_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_8_2 [label="QKV_Proj_1_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_1_8_2 [label="Attention_Scores_1_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_1_8_2 [label="Attention_Softmax_1_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_1_8_2 [label="Attention_Dropout_1_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_1_8_2 [label="Attention_Values_1_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_1_8_2 [label="Attention_Weighted_1_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_1_8_2 [label="Attention_Output_1_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_1_8 [label="Attention_AllReduce_1_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_8_3 [label="QKV_Proj_1_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_1_8_3 [label="Attention_Scores_1_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_1_8_3 [label="Attention_Softmax_1_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_1_8_3 [label="Attention_Dropout_1_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_1_8_3 [label="Attention_Values_1_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_1_8_3 [label="Attention_Weighted_1_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_1_8_3 [label="Attention_Output_1_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_1_8 [label="Attention_AllReduce_1_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_9_0 [label="QKV_Proj_1_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_1_9_0 [label="Attention_Scores_1_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_1_9_0 [label="Attention_Softmax_1_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_1_9_0 [label="Attention_Dropout_1_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_1_9_0 [label="Attention_Values_1_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_1_9_0 [label="Attention_Weighted_1_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_1_9_0 [label="Attention_Output_1_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_1_9 [label="Attention_AllReduce_1_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_9_1 [label="QKV_Proj_1_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_1_9_1 [label="Attention_Scores_1_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_1_9_1 [label="Attention_Softmax_1_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_1_9_1 [label="Attention_Dropout_1_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_1_9_1 [label="Attention_Values_1_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_1_9_1 [label="Attention_Weighted_1_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_1_9_1 [label="Attention_Output_1_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_1_9 [label="Attention_AllReduce_1_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_9_2 [label="QKV_Proj_1_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_1_9_2 [label="Attention_Scores_1_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_1_9_2 [label="Attention_Softmax_1_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_1_9_2 [label="Attention_Dropout_1_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_1_9_2 [label="Attention_Values_1_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_1_9_2 [label="Attention_Weighted_1_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_1_9_2 [label="Attention_Output_1_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_1_9 [label="Attention_AllReduce_1_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_9_3 [label="QKV_Proj_1_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_1_9_3 [label="Attention_Scores_1_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_1_9_3 [label="Attention_Softmax_1_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_1_9_3 [label="Attention_Dropout_1_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_1_9_3 [label="Attention_Values_1_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_1_9_3 [label="Attention_Weighted_1_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_1_9_3 [label="Attention_Output_1_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_1_9 [label="Attention_AllReduce_1_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_10_0 [label="QKV_Proj_1_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_1_10_0 [label="Attention_Scores_1_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_1_10_0 [label="Attention_Softmax_1_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_1_10_0 [label="Attention_Dropout_1_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_1_10_0 [label="Attention_Values_1_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_1_10_0 [label="Attention_Weighted_1_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_1_10_0 [label="Attention_Output_1_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_1_10 [label="Attention_AllReduce_1_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_10_1 [label="QKV_Proj_1_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_1_10_1 [label="Attention_Scores_1_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_1_10_1 [label="Attention_Softmax_1_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_1_10_1 [label="Attention_Dropout_1_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_1_10_1 [label="Attention_Values_1_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_1_10_1 [label="Attention_Weighted_1_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_1_10_1 [label="Attention_Output_1_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_1_10 [label="Attention_AllReduce_1_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_10_2 [label="QKV_Proj_1_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_1_10_2 [label="Attention_Scores_1_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_1_10_2 [label="Attention_Softmax_1_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_1_10_2 [label="Attention_Dropout_1_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_1_10_2 [label="Attention_Values_1_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_1_10_2 [label="Attention_Weighted_1_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_1_10_2 [label="Attention_Output_1_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_1_10 [label="Attention_AllReduce_1_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_10_3 [label="QKV_Proj_1_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_1_10_3 [label="Attention_Scores_1_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_1_10_3 [label="Attention_Softmax_1_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_1_10_3 [label="Attention_Dropout_1_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_1_10_3 [label="Attention_Values_1_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_1_10_3 [label="Attention_Weighted_1_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_1_10_3 [label="Attention_Output_1_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_1_10 [label="Attention_AllReduce_1_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_11_0 [label="QKV_Proj_1_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_1_11_0 [label="Attention_Scores_1_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_1_11_0 [label="Attention_Softmax_1_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_1_11_0 [label="Attention_Dropout_1_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_1_11_0 [label="Attention_Values_1_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_1_11_0 [label="Attention_Weighted_1_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_1_11_0 [label="Attention_Output_1_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_1_11 [label="Attention_AllReduce_1_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_11_1 [label="QKV_Proj_1_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_1_11_1 [label="Attention_Scores_1_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_1_11_1 [label="Attention_Softmax_1_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_1_11_1 [label="Attention_Dropout_1_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_1_11_1 [label="Attention_Values_1_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_1_11_1 [label="Attention_Weighted_1_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_1_11_1 [label="Attention_Output_1_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_1_11 [label="Attention_AllReduce_1_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_11_2 [label="QKV_Proj_1_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_1_11_2 [label="Attention_Scores_1_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_1_11_2 [label="Attention_Softmax_1_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_1_11_2 [label="Attention_Dropout_1_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_1_11_2 [label="Attention_Values_1_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_1_11_2 [label="Attention_Weighted_1_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_1_11_2 [label="Attention_Output_1_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_1_11 [label="Attention_AllReduce_1_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_11_3 [label="QKV_Proj_1_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_1_11_3 [label="Attention_Scores_1_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_1_11_3 [label="Attention_Softmax_1_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_1_11_3 [label="Attention_Dropout_1_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_1_11_3 [label="Attention_Values_1_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_1_11_3 [label="Attention_Weighted_1_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_1_11_3 [label="Attention_Output_1_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_1_11 [label="Attention_AllReduce_1_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_12_0 [label="QKV_Proj_1_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_1_12_0 [label="Attention_Scores_1_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_1_12_0 [label="Attention_Softmax_1_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_1_12_0 [label="Attention_Dropout_1_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_1_12_0 [label="Attention_Values_1_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_1_12_0 [label="Attention_Weighted_1_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_1_12_0 [label="Attention_Output_1_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_1_12 [label="Attention_AllReduce_1_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_12_1 [label="QKV_Proj_1_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_1_12_1 [label="Attention_Scores_1_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_1_12_1 [label="Attention_Softmax_1_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_1_12_1 [label="Attention_Dropout_1_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_1_12_1 [label="Attention_Values_1_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_1_12_1 [label="Attention_Weighted_1_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_1_12_1 [label="Attention_Output_1_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_1_12 [label="Attention_AllReduce_1_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_12_2 [label="QKV_Proj_1_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_1_12_2 [label="Attention_Scores_1_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_1_12_2 [label="Attention_Softmax_1_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_1_12_2 [label="Attention_Dropout_1_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_1_12_2 [label="Attention_Values_1_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_1_12_2 [label="Attention_Weighted_1_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_1_12_2 [label="Attention_Output_1_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_1_12 [label="Attention_AllReduce_1_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_12_3 [label="QKV_Proj_1_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_1_12_3 [label="Attention_Scores_1_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_1_12_3 [label="Attention_Softmax_1_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_1_12_3 [label="Attention_Dropout_1_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_1_12_3 [label="Attention_Values_1_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_1_12_3 [label="Attention_Weighted_1_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_1_12_3 [label="Attention_Output_1_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_1_12 [label="Attention_AllReduce_1_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_13_0 [label="QKV_Proj_1_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_1_13_0 [label="Attention_Scores_1_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_1_13_0 [label="Attention_Softmax_1_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_1_13_0 [label="Attention_Dropout_1_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_1_13_0 [label="Attention_Values_1_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_1_13_0 [label="Attention_Weighted_1_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_1_13_0 [label="Attention_Output_1_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_1_13 [label="Attention_AllReduce_1_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_13_1 [label="QKV_Proj_1_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_1_13_1 [label="Attention_Scores_1_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_1_13_1 [label="Attention_Softmax_1_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_1_13_1 [label="Attention_Dropout_1_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_1_13_1 [label="Attention_Values_1_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_1_13_1 [label="Attention_Weighted_1_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_1_13_1 [label="Attention_Output_1_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_1_13 [label="Attention_AllReduce_1_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_13_2 [label="QKV_Proj_1_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_1_13_2 [label="Attention_Scores_1_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_1_13_2 [label="Attention_Softmax_1_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_1_13_2 [label="Attention_Dropout_1_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_1_13_2 [label="Attention_Values_1_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_1_13_2 [label="Attention_Weighted_1_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_1_13_2 [label="Attention_Output_1_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_1_13 [label="Attention_AllReduce_1_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_13_3 [label="QKV_Proj_1_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_1_13_3 [label="Attention_Scores_1_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_1_13_3 [label="Attention_Softmax_1_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_1_13_3 [label="Attention_Dropout_1_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_1_13_3 [label="Attention_Values_1_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_1_13_3 [label="Attention_Weighted_1_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_1_13_3 [label="Attention_Output_1_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_1_13 [label="Attention_AllReduce_1_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_14_0 [label="QKV_Proj_1_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_1_14_0 [label="Attention_Scores_1_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_1_14_0 [label="Attention_Softmax_1_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_1_14_0 [label="Attention_Dropout_1_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_1_14_0 [label="Attention_Values_1_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_1_14_0 [label="Attention_Weighted_1_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_1_14_0 [label="Attention_Output_1_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_1_14 [label="Attention_AllReduce_1_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_14_1 [label="QKV_Proj_1_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_1_14_1 [label="Attention_Scores_1_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_1_14_1 [label="Attention_Softmax_1_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_1_14_1 [label="Attention_Dropout_1_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_1_14_1 [label="Attention_Values_1_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_1_14_1 [label="Attention_Weighted_1_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_1_14_1 [label="Attention_Output_1_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_1_14 [label="Attention_AllReduce_1_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_14_2 [label="QKV_Proj_1_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_1_14_2 [label="Attention_Scores_1_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_1_14_2 [label="Attention_Softmax_1_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_1_14_2 [label="Attention_Dropout_1_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_1_14_2 [label="Attention_Values_1_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_1_14_2 [label="Attention_Weighted_1_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_1_14_2 [label="Attention_Output_1_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_1_14 [label="Attention_AllReduce_1_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_14_3 [label="QKV_Proj_1_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_1_14_3 [label="Attention_Scores_1_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_1_14_3 [label="Attention_Softmax_1_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_1_14_3 [label="Attention_Dropout_1_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_1_14_3 [label="Attention_Values_1_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_1_14_3 [label="Attention_Weighted_1_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_1_14_3 [label="Attention_Output_1_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_1_14 [label="Attention_AllReduce_1_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_15_0 [label="QKV_Proj_1_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_1_15_0 [label="Attention_Scores_1_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_1_15_0 [label="Attention_Softmax_1_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_1_15_0 [label="Attention_Dropout_1_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_1_15_0 [label="Attention_Values_1_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_1_15_0 [label="Attention_Weighted_1_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_1_15_0 [label="Attention_Output_1_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_1_15 [label="Attention_AllReduce_1_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_15_1 [label="QKV_Proj_1_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_1_15_1 [label="Attention_Scores_1_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_1_15_1 [label="Attention_Softmax_1_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_1_15_1 [label="Attention_Dropout_1_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_1_15_1 [label="Attention_Values_1_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_1_15_1 [label="Attention_Weighted_1_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_1_15_1 [label="Attention_Output_1_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_1_15 [label="Attention_AllReduce_1_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_15_2 [label="QKV_Proj_1_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_1_15_2 [label="Attention_Scores_1_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_1_15_2 [label="Attention_Softmax_1_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_1_15_2 [label="Attention_Dropout_1_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_1_15_2 [label="Attention_Values_1_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_1_15_2 [label="Attention_Weighted_1_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_1_15_2 [label="Attention_Output_1_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_1_15 [label="Attention_AllReduce_1_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_1_15_3 [label="QKV_Proj_1_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_1_15_3 [label="Attention_Scores_1_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_1_15_3 [label="Attention_Softmax_1_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_1_15_3 [label="Attention_Dropout_1_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_1_15_3 [label="Attention_Values_1_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_1_15_3 [label="Attention_Weighted_1_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_1_15_3 [label="Attention_Output_1_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_1_15 [label="Attention_AllReduce_1_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_0_0 [label="MLP_Linear1_1_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_1_0_0 [label="GELU_1_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_1_0_0 [label="MLP_Linear2_1_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_1_0_1 [label="MLP_Linear1_1_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_1_0_1 [label="GELU_1_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_1_0_1 [label="MLP_Linear2_1_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_1_0_2 [label="MLP_Linear1_1_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_1_0_2 [label="GELU_1_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_1_0_2 [label="MLP_Linear2_1_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_1_0_3 [label="MLP_Linear1_1_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_1_0_3 [label="GELU_1_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_1_0_3 [label="MLP_Linear2_1_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_1_0 [label="MLP_AllReduce_1_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_1_0 [label="MLP_Linear1_1_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_1_1_0 [label="GELU_1_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_1_1_0 [label="MLP_Linear2_1_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_1_1_1 [label="MLP_Linear1_1_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_1_1_1 [label="GELU_1_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_1_1_1 [label="MLP_Linear2_1_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_1_1_2 [label="MLP_Linear1_1_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_1_1_2 [label="GELU_1_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_1_1_2 [label="MLP_Linear2_1_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_1_1_3 [label="MLP_Linear1_1_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_1_1_3 [label="GELU_1_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_1_1_3 [label="MLP_Linear2_1_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_1_1 [label="MLP_AllReduce_1_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_2_0 [label="MLP_Linear1_1_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_1_2_0 [label="GELU_1_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_1_2_0 [label="MLP_Linear2_1_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_1_2_1 [label="MLP_Linear1_1_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_1_2_1 [label="GELU_1_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_1_2_1 [label="MLP_Linear2_1_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_1_2_2 [label="MLP_Linear1_1_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_1_2_2 [label="GELU_1_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_1_2_2 [label="MLP_Linear2_1_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_1_2_3 [label="MLP_Linear1_1_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_1_2_3 [label="GELU_1_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_1_2_3 [label="MLP_Linear2_1_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_1_2 [label="MLP_AllReduce_1_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_3_0 [label="MLP_Linear1_1_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_1_3_0 [label="GELU_1_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_1_3_0 [label="MLP_Linear2_1_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_1_3_1 [label="MLP_Linear1_1_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_1_3_1 [label="GELU_1_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_1_3_1 [label="MLP_Linear2_1_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_1_3_2 [label="MLP_Linear1_1_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_1_3_2 [label="GELU_1_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_1_3_2 [label="MLP_Linear2_1_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_1_3_3 [label="MLP_Linear1_1_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_1_3_3 [label="GELU_1_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_1_3_3 [label="MLP_Linear2_1_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_1_3 [label="MLP_AllReduce_1_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_4_0 [label="MLP_Linear1_1_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_1_4_0 [label="GELU_1_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_1_4_0 [label="MLP_Linear2_1_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_1_4_1 [label="MLP_Linear1_1_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_1_4_1 [label="GELU_1_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_1_4_1 [label="MLP_Linear2_1_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_1_4_2 [label="MLP_Linear1_1_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_1_4_2 [label="GELU_1_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_1_4_2 [label="MLP_Linear2_1_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_1_4_3 [label="MLP_Linear1_1_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_1_4_3 [label="GELU_1_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_1_4_3 [label="MLP_Linear2_1_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_1_4 [label="MLP_AllReduce_1_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_5_0 [label="MLP_Linear1_1_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_1_5_0 [label="GELU_1_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_1_5_0 [label="MLP_Linear2_1_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_1_5_1 [label="MLP_Linear1_1_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_1_5_1 [label="GELU_1_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_1_5_1 [label="MLP_Linear2_1_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_1_5_2 [label="MLP_Linear1_1_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_1_5_2 [label="GELU_1_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_1_5_2 [label="MLP_Linear2_1_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_1_5_3 [label="MLP_Linear1_1_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_1_5_3 [label="GELU_1_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_1_5_3 [label="MLP_Linear2_1_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_1_5 [label="MLP_AllReduce_1_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_6_0 [label="MLP_Linear1_1_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_1_6_0 [label="GELU_1_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_1_6_0 [label="MLP_Linear2_1_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_1_6_1 [label="MLP_Linear1_1_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_1_6_1 [label="GELU_1_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_1_6_1 [label="MLP_Linear2_1_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_1_6_2 [label="MLP_Linear1_1_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_1_6_2 [label="GELU_1_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_1_6_2 [label="MLP_Linear2_1_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_1_6_3 [label="MLP_Linear1_1_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_1_6_3 [label="GELU_1_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_1_6_3 [label="MLP_Linear2_1_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_1_6 [label="MLP_AllReduce_1_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_7_0 [label="MLP_Linear1_1_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_1_7_0 [label="GELU_1_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_1_7_0 [label="MLP_Linear2_1_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_1_7_1 [label="MLP_Linear1_1_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_1_7_1 [label="GELU_1_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_1_7_1 [label="MLP_Linear2_1_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_1_7_2 [label="MLP_Linear1_1_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_1_7_2 [label="GELU_1_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_1_7_2 [label="MLP_Linear2_1_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_1_7_3 [label="MLP_Linear1_1_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_1_7_3 [label="GELU_1_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_1_7_3 [label="MLP_Linear2_1_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_1_7 [label="MLP_AllReduce_1_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_8_0 [label="MLP_Linear1_1_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_1_8_0 [label="GELU_1_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_1_8_0 [label="MLP_Linear2_1_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_1_8_1 [label="MLP_Linear1_1_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_1_8_1 [label="GELU_1_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_1_8_1 [label="MLP_Linear2_1_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_1_8_2 [label="MLP_Linear1_1_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_1_8_2 [label="GELU_1_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_1_8_2 [label="MLP_Linear2_1_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_1_8_3 [label="MLP_Linear1_1_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_1_8_3 [label="GELU_1_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_1_8_3 [label="MLP_Linear2_1_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_1_8 [label="MLP_AllReduce_1_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_9_0 [label="MLP_Linear1_1_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_1_9_0 [label="GELU_1_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_1_9_0 [label="MLP_Linear2_1_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_1_9_1 [label="MLP_Linear1_1_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_1_9_1 [label="GELU_1_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_1_9_1 [label="MLP_Linear2_1_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_1_9_2 [label="MLP_Linear1_1_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_1_9_2 [label="GELU_1_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_1_9_2 [label="MLP_Linear2_1_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_1_9_3 [label="MLP_Linear1_1_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_1_9_3 [label="GELU_1_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_1_9_3 [label="MLP_Linear2_1_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_1_9 [label="MLP_AllReduce_1_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_10_0 [label="MLP_Linear1_1_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_1_10_0 [label="GELU_1_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_1_10_0 [label="MLP_Linear2_1_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_1_10_1 [label="MLP_Linear1_1_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_1_10_1 [label="GELU_1_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_1_10_1 [label="MLP_Linear2_1_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_1_10_2 [label="MLP_Linear1_1_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_1_10_2 [label="GELU_1_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_1_10_2 [label="MLP_Linear2_1_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_1_10_3 [label="MLP_Linear1_1_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_1_10_3 [label="GELU_1_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_1_10_3 [label="MLP_Linear2_1_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_1_10 [label="MLP_AllReduce_1_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_11_0 [label="MLP_Linear1_1_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_1_11_0 [label="GELU_1_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_1_11_0 [label="MLP_Linear2_1_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_1_11_1 [label="MLP_Linear1_1_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_1_11_1 [label="GELU_1_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_1_11_1 [label="MLP_Linear2_1_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_1_11_2 [label="MLP_Linear1_1_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_1_11_2 [label="GELU_1_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_1_11_2 [label="MLP_Linear2_1_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_1_11_3 [label="MLP_Linear1_1_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_1_11_3 [label="GELU_1_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_1_11_3 [label="MLP_Linear2_1_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_1_11 [label="MLP_AllReduce_1_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_12_0 [label="MLP_Linear1_1_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_1_12_0 [label="GELU_1_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_1_12_0 [label="MLP_Linear2_1_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_1_12_1 [label="MLP_Linear1_1_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_1_12_1 [label="GELU_1_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_1_12_1 [label="MLP_Linear2_1_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_1_12_2 [label="MLP_Linear1_1_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_1_12_2 [label="GELU_1_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_1_12_2 [label="MLP_Linear2_1_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_1_12_3 [label="MLP_Linear1_1_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_1_12_3 [label="GELU_1_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_1_12_3 [label="MLP_Linear2_1_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_1_12 [label="MLP_AllReduce_1_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_13_0 [label="MLP_Linear1_1_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_1_13_0 [label="GELU_1_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_1_13_0 [label="MLP_Linear2_1_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_1_13_1 [label="MLP_Linear1_1_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_1_13_1 [label="GELU_1_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_1_13_1 [label="MLP_Linear2_1_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_1_13_2 [label="MLP_Linear1_1_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_1_13_2 [label="GELU_1_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_1_13_2 [label="MLP_Linear2_1_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_1_13_3 [label="MLP_Linear1_1_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_1_13_3 [label="GELU_1_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_1_13_3 [label="MLP_Linear2_1_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_1_13 [label="MLP_AllReduce_1_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_14_0 [label="MLP_Linear1_1_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_1_14_0 [label="GELU_1_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_1_14_0 [label="MLP_Linear2_1_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_1_14_1 [label="MLP_Linear1_1_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_1_14_1 [label="GELU_1_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_1_14_1 [label="MLP_Linear2_1_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_1_14_2 [label="MLP_Linear1_1_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_1_14_2 [label="GELU_1_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_1_14_2 [label="MLP_Linear2_1_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_1_14_3 [label="MLP_Linear1_1_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_1_14_3 [label="GELU_1_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_1_14_3 [label="MLP_Linear2_1_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_1_14 [label="MLP_AllReduce_1_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_1_15_0 [label="MLP_Linear1_1_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_1_15_0 [label="GELU_1_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_1_15_0 [label="MLP_Linear2_1_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_1_15_1 [label="MLP_Linear1_1_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_1_15_1 [label="GELU_1_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_1_15_1 [label="MLP_Linear2_1_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_1_15_2 [label="MLP_Linear1_1_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_1_15_2 [label="GELU_1_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_1_15_2 [label="MLP_Linear2_1_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_1_15_3 [label="MLP_Linear1_1_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_1_15_3 [label="GELU_1_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_1_15_3 [label="MLP_Linear2_1_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_1_15 [label="MLP_AllReduce_1_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_1_0 [label="Expert_Route_1_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_1 [label="Expert_Route_1_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_2 [label="Expert_Route_1_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_3 [label="Expert_Route_1_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_4 [label="Expert_Route_1_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_5 [label="Expert_Route_1_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_6 [label="Expert_Route_1_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_7 [label="Expert_Route_1_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_8 [label="Expert_Route_1_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_9 [label="Expert_Route_1_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_10 [label="Expert_Route_1_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_11 [label="Expert_Route_1_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_12 [label="Expert_Route_1_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_13 [label="Expert_Route_1_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_14 [label="Expert_Route_1_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_1_15 [label="Expert_Route_1_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_2_0_0 [label="QKV_Proj_2_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_2_0_0 [label="Attention_Scores_2_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_2_0_0 [label="Attention_Softmax_2_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_2_0_0 [label="Attention_Dropout_2_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_2_0_0 [label="Attention_Values_2_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_2_0_0 [label="Attention_Weighted_2_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_2_0_0 [label="Attention_Output_2_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_2_0 [label="Attention_AllReduce_2_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_0_1 [label="QKV_Proj_2_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_2_0_1 [label="Attention_Scores_2_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_2_0_1 [label="Attention_Softmax_2_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_2_0_1 [label="Attention_Dropout_2_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_2_0_1 [label="Attention_Values_2_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_2_0_1 [label="Attention_Weighted_2_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_2_0_1 [label="Attention_Output_2_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_2_0 [label="Attention_AllReduce_2_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_0_2 [label="QKV_Proj_2_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_2_0_2 [label="Attention_Scores_2_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_2_0_2 [label="Attention_Softmax_2_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_2_0_2 [label="Attention_Dropout_2_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_2_0_2 [label="Attention_Values_2_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_2_0_2 [label="Attention_Weighted_2_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_2_0_2 [label="Attention_Output_2_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_2_0 [label="Attention_AllReduce_2_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_0_3 [label="QKV_Proj_2_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_2_0_3 [label="Attention_Scores_2_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_2_0_3 [label="Attention_Softmax_2_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_2_0_3 [label="Attention_Dropout_2_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_2_0_3 [label="Attention_Values_2_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_2_0_3 [label="Attention_Weighted_2_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_2_0_3 [label="Attention_Output_2_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_2_0 [label="Attention_AllReduce_2_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_1_0 [label="QKV_Proj_2_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_2_1_0 [label="Attention_Scores_2_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_2_1_0 [label="Attention_Softmax_2_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_2_1_0 [label="Attention_Dropout_2_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_2_1_0 [label="Attention_Values_2_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_2_1_0 [label="Attention_Weighted_2_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_2_1_0 [label="Attention_Output_2_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_2_1 [label="Attention_AllReduce_2_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_1_1 [label="QKV_Proj_2_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_2_1_1 [label="Attention_Scores_2_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_2_1_1 [label="Attention_Softmax_2_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_2_1_1 [label="Attention_Dropout_2_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_2_1_1 [label="Attention_Values_2_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_2_1_1 [label="Attention_Weighted_2_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_2_1_1 [label="Attention_Output_2_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_2_1 [label="Attention_AllReduce_2_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_1_2 [label="QKV_Proj_2_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_2_1_2 [label="Attention_Scores_2_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_2_1_2 [label="Attention_Softmax_2_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_2_1_2 [label="Attention_Dropout_2_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_2_1_2 [label="Attention_Values_2_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_2_1_2 [label="Attention_Weighted_2_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_2_1_2 [label="Attention_Output_2_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_2_1 [label="Attention_AllReduce_2_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_1_3 [label="QKV_Proj_2_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_2_1_3 [label="Attention_Scores_2_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_2_1_3 [label="Attention_Softmax_2_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_2_1_3 [label="Attention_Dropout_2_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_2_1_3 [label="Attention_Values_2_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_2_1_3 [label="Attention_Weighted_2_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_2_1_3 [label="Attention_Output_2_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_2_1 [label="Attention_AllReduce_2_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_2_0 [label="QKV_Proj_2_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_2_2_0 [label="Attention_Scores_2_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_2_2_0 [label="Attention_Softmax_2_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_2_2_0 [label="Attention_Dropout_2_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_2_2_0 [label="Attention_Values_2_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_2_2_0 [label="Attention_Weighted_2_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_2_2_0 [label="Attention_Output_2_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_2_2 [label="Attention_AllReduce_2_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_2_1 [label="QKV_Proj_2_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_2_2_1 [label="Attention_Scores_2_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_2_2_1 [label="Attention_Softmax_2_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_2_2_1 [label="Attention_Dropout_2_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_2_2_1 [label="Attention_Values_2_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_2_2_1 [label="Attention_Weighted_2_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_2_2_1 [label="Attention_Output_2_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_2_2 [label="Attention_AllReduce_2_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_2_2 [label="QKV_Proj_2_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_2_2_2 [label="Attention_Scores_2_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_2_2_2 [label="Attention_Softmax_2_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_2_2_2 [label="Attention_Dropout_2_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_2_2_2 [label="Attention_Values_2_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_2_2_2 [label="Attention_Weighted_2_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_2_2_2 [label="Attention_Output_2_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_2_2 [label="Attention_AllReduce_2_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_2_3 [label="QKV_Proj_2_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_2_2_3 [label="Attention_Scores_2_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_2_2_3 [label="Attention_Softmax_2_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_2_2_3 [label="Attention_Dropout_2_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_2_2_3 [label="Attention_Values_2_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_2_2_3 [label="Attention_Weighted_2_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_2_2_3 [label="Attention_Output_2_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_2_2 [label="Attention_AllReduce_2_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_3_0 [label="QKV_Proj_2_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_2_3_0 [label="Attention_Scores_2_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_2_3_0 [label="Attention_Softmax_2_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_2_3_0 [label="Attention_Dropout_2_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_2_3_0 [label="Attention_Values_2_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_2_3_0 [label="Attention_Weighted_2_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_2_3_0 [label="Attention_Output_2_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_2_3 [label="Attention_AllReduce_2_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_3_1 [label="QKV_Proj_2_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_2_3_1 [label="Attention_Scores_2_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_2_3_1 [label="Attention_Softmax_2_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_2_3_1 [label="Attention_Dropout_2_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_2_3_1 [label="Attention_Values_2_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_2_3_1 [label="Attention_Weighted_2_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_2_3_1 [label="Attention_Output_2_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_2_3 [label="Attention_AllReduce_2_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_3_2 [label="QKV_Proj_2_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_2_3_2 [label="Attention_Scores_2_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_2_3_2 [label="Attention_Softmax_2_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_2_3_2 [label="Attention_Dropout_2_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_2_3_2 [label="Attention_Values_2_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_2_3_2 [label="Attention_Weighted_2_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_2_3_2 [label="Attention_Output_2_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_2_3 [label="Attention_AllReduce_2_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_3_3 [label="QKV_Proj_2_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_2_3_3 [label="Attention_Scores_2_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_2_3_3 [label="Attention_Softmax_2_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_2_3_3 [label="Attention_Dropout_2_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_2_3_3 [label="Attention_Values_2_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_2_3_3 [label="Attention_Weighted_2_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_2_3_3 [label="Attention_Output_2_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_2_3 [label="Attention_AllReduce_2_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_4_0 [label="QKV_Proj_2_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_2_4_0 [label="Attention_Scores_2_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_2_4_0 [label="Attention_Softmax_2_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_2_4_0 [label="Attention_Dropout_2_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_2_4_0 [label="Attention_Values_2_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_2_4_0 [label="Attention_Weighted_2_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_2_4_0 [label="Attention_Output_2_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_2_4 [label="Attention_AllReduce_2_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_4_1 [label="QKV_Proj_2_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_2_4_1 [label="Attention_Scores_2_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_2_4_1 [label="Attention_Softmax_2_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_2_4_1 [label="Attention_Dropout_2_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_2_4_1 [label="Attention_Values_2_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_2_4_1 [label="Attention_Weighted_2_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_2_4_1 [label="Attention_Output_2_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_2_4 [label="Attention_AllReduce_2_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_4_2 [label="QKV_Proj_2_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_2_4_2 [label="Attention_Scores_2_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_2_4_2 [label="Attention_Softmax_2_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_2_4_2 [label="Attention_Dropout_2_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_2_4_2 [label="Attention_Values_2_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_2_4_2 [label="Attention_Weighted_2_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_2_4_2 [label="Attention_Output_2_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_2_4 [label="Attention_AllReduce_2_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_4_3 [label="QKV_Proj_2_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_2_4_3 [label="Attention_Scores_2_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_2_4_3 [label="Attention_Softmax_2_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_2_4_3 [label="Attention_Dropout_2_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_2_4_3 [label="Attention_Values_2_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_2_4_3 [label="Attention_Weighted_2_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_2_4_3 [label="Attention_Output_2_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_2_4 [label="Attention_AllReduce_2_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_5_0 [label="QKV_Proj_2_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_2_5_0 [label="Attention_Scores_2_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_2_5_0 [label="Attention_Softmax_2_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_2_5_0 [label="Attention_Dropout_2_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_2_5_0 [label="Attention_Values_2_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_2_5_0 [label="Attention_Weighted_2_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_2_5_0 [label="Attention_Output_2_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_2_5 [label="Attention_AllReduce_2_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_5_1 [label="QKV_Proj_2_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_2_5_1 [label="Attention_Scores_2_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_2_5_1 [label="Attention_Softmax_2_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_2_5_1 [label="Attention_Dropout_2_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_2_5_1 [label="Attention_Values_2_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_2_5_1 [label="Attention_Weighted_2_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_2_5_1 [label="Attention_Output_2_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_2_5 [label="Attention_AllReduce_2_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_5_2 [label="QKV_Proj_2_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_2_5_2 [label="Attention_Scores_2_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_2_5_2 [label="Attention_Softmax_2_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_2_5_2 [label="Attention_Dropout_2_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_2_5_2 [label="Attention_Values_2_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_2_5_2 [label="Attention_Weighted_2_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_2_5_2 [label="Attention_Output_2_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_2_5 [label="Attention_AllReduce_2_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_5_3 [label="QKV_Proj_2_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_2_5_3 [label="Attention_Scores_2_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_2_5_3 [label="Attention_Softmax_2_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_2_5_3 [label="Attention_Dropout_2_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_2_5_3 [label="Attention_Values_2_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_2_5_3 [label="Attention_Weighted_2_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_2_5_3 [label="Attention_Output_2_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_2_5 [label="Attention_AllReduce_2_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_6_0 [label="QKV_Proj_2_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_2_6_0 [label="Attention_Scores_2_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_2_6_0 [label="Attention_Softmax_2_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_2_6_0 [label="Attention_Dropout_2_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_2_6_0 [label="Attention_Values_2_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_2_6_0 [label="Attention_Weighted_2_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_2_6_0 [label="Attention_Output_2_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_2_6 [label="Attention_AllReduce_2_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_6_1 [label="QKV_Proj_2_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_2_6_1 [label="Attention_Scores_2_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_2_6_1 [label="Attention_Softmax_2_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_2_6_1 [label="Attention_Dropout_2_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_2_6_1 [label="Attention_Values_2_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_2_6_1 [label="Attention_Weighted_2_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_2_6_1 [label="Attention_Output_2_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_2_6 [label="Attention_AllReduce_2_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_6_2 [label="QKV_Proj_2_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_2_6_2 [label="Attention_Scores_2_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_2_6_2 [label="Attention_Softmax_2_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_2_6_2 [label="Attention_Dropout_2_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_2_6_2 [label="Attention_Values_2_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_2_6_2 [label="Attention_Weighted_2_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_2_6_2 [label="Attention_Output_2_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_2_6 [label="Attention_AllReduce_2_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_6_3 [label="QKV_Proj_2_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_2_6_3 [label="Attention_Scores_2_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_2_6_3 [label="Attention_Softmax_2_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_2_6_3 [label="Attention_Dropout_2_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_2_6_3 [label="Attention_Values_2_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_2_6_3 [label="Attention_Weighted_2_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_2_6_3 [label="Attention_Output_2_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_2_6 [label="Attention_AllReduce_2_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_7_0 [label="QKV_Proj_2_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_2_7_0 [label="Attention_Scores_2_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_2_7_0 [label="Attention_Softmax_2_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_2_7_0 [label="Attention_Dropout_2_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_2_7_0 [label="Attention_Values_2_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_2_7_0 [label="Attention_Weighted_2_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_2_7_0 [label="Attention_Output_2_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_2_7 [label="Attention_AllReduce_2_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_7_1 [label="QKV_Proj_2_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_2_7_1 [label="Attention_Scores_2_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_2_7_1 [label="Attention_Softmax_2_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_2_7_1 [label="Attention_Dropout_2_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_2_7_1 [label="Attention_Values_2_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_2_7_1 [label="Attention_Weighted_2_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_2_7_1 [label="Attention_Output_2_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_2_7 [label="Attention_AllReduce_2_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_7_2 [label="QKV_Proj_2_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_2_7_2 [label="Attention_Scores_2_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_2_7_2 [label="Attention_Softmax_2_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_2_7_2 [label="Attention_Dropout_2_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_2_7_2 [label="Attention_Values_2_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_2_7_2 [label="Attention_Weighted_2_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_2_7_2 [label="Attention_Output_2_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_2_7 [label="Attention_AllReduce_2_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_7_3 [label="QKV_Proj_2_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_2_7_3 [label="Attention_Scores_2_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_2_7_3 [label="Attention_Softmax_2_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_2_7_3 [label="Attention_Dropout_2_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_2_7_3 [label="Attention_Values_2_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_2_7_3 [label="Attention_Weighted_2_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_2_7_3 [label="Attention_Output_2_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_2_7 [label="Attention_AllReduce_2_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_8_0 [label="QKV_Proj_2_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_2_8_0 [label="Attention_Scores_2_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_2_8_0 [label="Attention_Softmax_2_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_2_8_0 [label="Attention_Dropout_2_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_2_8_0 [label="Attention_Values_2_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_2_8_0 [label="Attention_Weighted_2_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_2_8_0 [label="Attention_Output_2_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_2_8 [label="Attention_AllReduce_2_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_8_1 [label="QKV_Proj_2_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_2_8_1 [label="Attention_Scores_2_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_2_8_1 [label="Attention_Softmax_2_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_2_8_1 [label="Attention_Dropout_2_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_2_8_1 [label="Attention_Values_2_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_2_8_1 [label="Attention_Weighted_2_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_2_8_1 [label="Attention_Output_2_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_2_8 [label="Attention_AllReduce_2_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_8_2 [label="QKV_Proj_2_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_2_8_2 [label="Attention_Scores_2_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_2_8_2 [label="Attention_Softmax_2_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_2_8_2 [label="Attention_Dropout_2_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_2_8_2 [label="Attention_Values_2_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_2_8_2 [label="Attention_Weighted_2_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_2_8_2 [label="Attention_Output_2_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_2_8 [label="Attention_AllReduce_2_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_8_3 [label="QKV_Proj_2_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_2_8_3 [label="Attention_Scores_2_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_2_8_3 [label="Attention_Softmax_2_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_2_8_3 [label="Attention_Dropout_2_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_2_8_3 [label="Attention_Values_2_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_2_8_3 [label="Attention_Weighted_2_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_2_8_3 [label="Attention_Output_2_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_2_8 [label="Attention_AllReduce_2_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_9_0 [label="QKV_Proj_2_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_2_9_0 [label="Attention_Scores_2_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_2_9_0 [label="Attention_Softmax_2_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_2_9_0 [label="Attention_Dropout_2_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_2_9_0 [label="Attention_Values_2_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_2_9_0 [label="Attention_Weighted_2_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_2_9_0 [label="Attention_Output_2_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_2_9 [label="Attention_AllReduce_2_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_9_1 [label="QKV_Proj_2_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_2_9_1 [label="Attention_Scores_2_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_2_9_1 [label="Attention_Softmax_2_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_2_9_1 [label="Attention_Dropout_2_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_2_9_1 [label="Attention_Values_2_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_2_9_1 [label="Attention_Weighted_2_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_2_9_1 [label="Attention_Output_2_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_2_9 [label="Attention_AllReduce_2_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_9_2 [label="QKV_Proj_2_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_2_9_2 [label="Attention_Scores_2_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_2_9_2 [label="Attention_Softmax_2_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_2_9_2 [label="Attention_Dropout_2_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_2_9_2 [label="Attention_Values_2_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_2_9_2 [label="Attention_Weighted_2_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_2_9_2 [label="Attention_Output_2_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_2_9 [label="Attention_AllReduce_2_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_9_3 [label="QKV_Proj_2_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_2_9_3 [label="Attention_Scores_2_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_2_9_3 [label="Attention_Softmax_2_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_2_9_3 [label="Attention_Dropout_2_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_2_9_3 [label="Attention_Values_2_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_2_9_3 [label="Attention_Weighted_2_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_2_9_3 [label="Attention_Output_2_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_2_9 [label="Attention_AllReduce_2_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_10_0 [label="QKV_Proj_2_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_2_10_0 [label="Attention_Scores_2_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_2_10_0 [label="Attention_Softmax_2_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_2_10_0 [label="Attention_Dropout_2_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_2_10_0 [label="Attention_Values_2_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_2_10_0 [label="Attention_Weighted_2_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_2_10_0 [label="Attention_Output_2_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_2_10 [label="Attention_AllReduce_2_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_10_1 [label="QKV_Proj_2_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_2_10_1 [label="Attention_Scores_2_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_2_10_1 [label="Attention_Softmax_2_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_2_10_1 [label="Attention_Dropout_2_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_2_10_1 [label="Attention_Values_2_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_2_10_1 [label="Attention_Weighted_2_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_2_10_1 [label="Attention_Output_2_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_2_10 [label="Attention_AllReduce_2_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_10_2 [label="QKV_Proj_2_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_2_10_2 [label="Attention_Scores_2_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_2_10_2 [label="Attention_Softmax_2_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_2_10_2 [label="Attention_Dropout_2_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_2_10_2 [label="Attention_Values_2_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_2_10_2 [label="Attention_Weighted_2_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_2_10_2 [label="Attention_Output_2_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_2_10 [label="Attention_AllReduce_2_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_10_3 [label="QKV_Proj_2_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_2_10_3 [label="Attention_Scores_2_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_2_10_3 [label="Attention_Softmax_2_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_2_10_3 [label="Attention_Dropout_2_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_2_10_3 [label="Attention_Values_2_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_2_10_3 [label="Attention_Weighted_2_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_2_10_3 [label="Attention_Output_2_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_2_10 [label="Attention_AllReduce_2_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_11_0 [label="QKV_Proj_2_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_2_11_0 [label="Attention_Scores_2_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_2_11_0 [label="Attention_Softmax_2_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_2_11_0 [label="Attention_Dropout_2_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_2_11_0 [label="Attention_Values_2_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_2_11_0 [label="Attention_Weighted_2_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_2_11_0 [label="Attention_Output_2_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_2_11 [label="Attention_AllReduce_2_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_11_1 [label="QKV_Proj_2_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_2_11_1 [label="Attention_Scores_2_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_2_11_1 [label="Attention_Softmax_2_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_2_11_1 [label="Attention_Dropout_2_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_2_11_1 [label="Attention_Values_2_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_2_11_1 [label="Attention_Weighted_2_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_2_11_1 [label="Attention_Output_2_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_2_11 [label="Attention_AllReduce_2_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_11_2 [label="QKV_Proj_2_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_2_11_2 [label="Attention_Scores_2_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_2_11_2 [label="Attention_Softmax_2_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_2_11_2 [label="Attention_Dropout_2_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_2_11_2 [label="Attention_Values_2_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_2_11_2 [label="Attention_Weighted_2_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_2_11_2 [label="Attention_Output_2_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_2_11 [label="Attention_AllReduce_2_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_11_3 [label="QKV_Proj_2_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_2_11_3 [label="Attention_Scores_2_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_2_11_3 [label="Attention_Softmax_2_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_2_11_3 [label="Attention_Dropout_2_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_2_11_3 [label="Attention_Values_2_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_2_11_3 [label="Attention_Weighted_2_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_2_11_3 [label="Attention_Output_2_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_2_11 [label="Attention_AllReduce_2_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_12_0 [label="QKV_Proj_2_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_2_12_0 [label="Attention_Scores_2_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_2_12_0 [label="Attention_Softmax_2_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_2_12_0 [label="Attention_Dropout_2_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_2_12_0 [label="Attention_Values_2_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_2_12_0 [label="Attention_Weighted_2_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_2_12_0 [label="Attention_Output_2_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_2_12 [label="Attention_AllReduce_2_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_12_1 [label="QKV_Proj_2_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_2_12_1 [label="Attention_Scores_2_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_2_12_1 [label="Attention_Softmax_2_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_2_12_1 [label="Attention_Dropout_2_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_2_12_1 [label="Attention_Values_2_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_2_12_1 [label="Attention_Weighted_2_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_2_12_1 [label="Attention_Output_2_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_2_12 [label="Attention_AllReduce_2_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_12_2 [label="QKV_Proj_2_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_2_12_2 [label="Attention_Scores_2_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_2_12_2 [label="Attention_Softmax_2_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_2_12_2 [label="Attention_Dropout_2_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_2_12_2 [label="Attention_Values_2_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_2_12_2 [label="Attention_Weighted_2_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_2_12_2 [label="Attention_Output_2_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_2_12 [label="Attention_AllReduce_2_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_12_3 [label="QKV_Proj_2_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_2_12_3 [label="Attention_Scores_2_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_2_12_3 [label="Attention_Softmax_2_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_2_12_3 [label="Attention_Dropout_2_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_2_12_3 [label="Attention_Values_2_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_2_12_3 [label="Attention_Weighted_2_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_2_12_3 [label="Attention_Output_2_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_2_12 [label="Attention_AllReduce_2_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_13_0 [label="QKV_Proj_2_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_2_13_0 [label="Attention_Scores_2_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_2_13_0 [label="Attention_Softmax_2_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_2_13_0 [label="Attention_Dropout_2_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_2_13_0 [label="Attention_Values_2_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_2_13_0 [label="Attention_Weighted_2_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_2_13_0 [label="Attention_Output_2_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_2_13 [label="Attention_AllReduce_2_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_13_1 [label="QKV_Proj_2_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_2_13_1 [label="Attention_Scores_2_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_2_13_1 [label="Attention_Softmax_2_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_2_13_1 [label="Attention_Dropout_2_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_2_13_1 [label="Attention_Values_2_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_2_13_1 [label="Attention_Weighted_2_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_2_13_1 [label="Attention_Output_2_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_2_13 [label="Attention_AllReduce_2_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_13_2 [label="QKV_Proj_2_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_2_13_2 [label="Attention_Scores_2_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_2_13_2 [label="Attention_Softmax_2_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_2_13_2 [label="Attention_Dropout_2_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_2_13_2 [label="Attention_Values_2_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_2_13_2 [label="Attention_Weighted_2_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_2_13_2 [label="Attention_Output_2_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_2_13 [label="Attention_AllReduce_2_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_13_3 [label="QKV_Proj_2_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_2_13_3 [label="Attention_Scores_2_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_2_13_3 [label="Attention_Softmax_2_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_2_13_3 [label="Attention_Dropout_2_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_2_13_3 [label="Attention_Values_2_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_2_13_3 [label="Attention_Weighted_2_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_2_13_3 [label="Attention_Output_2_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_2_13 [label="Attention_AllReduce_2_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_14_0 [label="QKV_Proj_2_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_2_14_0 [label="Attention_Scores_2_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_2_14_0 [label="Attention_Softmax_2_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_2_14_0 [label="Attention_Dropout_2_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_2_14_0 [label="Attention_Values_2_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_2_14_0 [label="Attention_Weighted_2_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_2_14_0 [label="Attention_Output_2_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_2_14 [label="Attention_AllReduce_2_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_14_1 [label="QKV_Proj_2_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_2_14_1 [label="Attention_Scores_2_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_2_14_1 [label="Attention_Softmax_2_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_2_14_1 [label="Attention_Dropout_2_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_2_14_1 [label="Attention_Values_2_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_2_14_1 [label="Attention_Weighted_2_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_2_14_1 [label="Attention_Output_2_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_2_14 [label="Attention_AllReduce_2_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_14_2 [label="QKV_Proj_2_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_2_14_2 [label="Attention_Scores_2_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_2_14_2 [label="Attention_Softmax_2_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_2_14_2 [label="Attention_Dropout_2_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_2_14_2 [label="Attention_Values_2_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_2_14_2 [label="Attention_Weighted_2_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_2_14_2 [label="Attention_Output_2_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_2_14 [label="Attention_AllReduce_2_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_14_3 [label="QKV_Proj_2_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_2_14_3 [label="Attention_Scores_2_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_2_14_3 [label="Attention_Softmax_2_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_2_14_3 [label="Attention_Dropout_2_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_2_14_3 [label="Attention_Values_2_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_2_14_3 [label="Attention_Weighted_2_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_2_14_3 [label="Attention_Output_2_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_2_14 [label="Attention_AllReduce_2_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_15_0 [label="QKV_Proj_2_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_2_15_0 [label="Attention_Scores_2_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_2_15_0 [label="Attention_Softmax_2_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_2_15_0 [label="Attention_Dropout_2_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_2_15_0 [label="Attention_Values_2_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_2_15_0 [label="Attention_Weighted_2_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_2_15_0 [label="Attention_Output_2_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_2_15 [label="Attention_AllReduce_2_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_15_1 [label="QKV_Proj_2_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_2_15_1 [label="Attention_Scores_2_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_2_15_1 [label="Attention_Softmax_2_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_2_15_1 [label="Attention_Dropout_2_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_2_15_1 [label="Attention_Values_2_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_2_15_1 [label="Attention_Weighted_2_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_2_15_1 [label="Attention_Output_2_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_2_15 [label="Attention_AllReduce_2_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_15_2 [label="QKV_Proj_2_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_2_15_2 [label="Attention_Scores_2_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_2_15_2 [label="Attention_Softmax_2_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_2_15_2 [label="Attention_Dropout_2_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_2_15_2 [label="Attention_Values_2_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_2_15_2 [label="Attention_Weighted_2_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_2_15_2 [label="Attention_Output_2_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_2_15 [label="Attention_AllReduce_2_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_2_15_3 [label="QKV_Proj_2_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_2_15_3 [label="Attention_Scores_2_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_2_15_3 [label="Attention_Softmax_2_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_2_15_3 [label="Attention_Dropout_2_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_2_15_3 [label="Attention_Values_2_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_2_15_3 [label="Attention_Weighted_2_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_2_15_3 [label="Attention_Output_2_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_2_15 [label="Attention_AllReduce_2_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_0_0 [label="MLP_Linear1_2_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_2_0_0 [label="GELU_2_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_2_0_0 [label="MLP_Linear2_2_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_2_0_1 [label="MLP_Linear1_2_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_2_0_1 [label="GELU_2_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_2_0_1 [label="MLP_Linear2_2_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_2_0_2 [label="MLP_Linear1_2_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_2_0_2 [label="GELU_2_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_2_0_2 [label="MLP_Linear2_2_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_2_0_3 [label="MLP_Linear1_2_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_2_0_3 [label="GELU_2_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_2_0_3 [label="MLP_Linear2_2_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_2_0 [label="MLP_AllReduce_2_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_1_0 [label="MLP_Linear1_2_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_2_1_0 [label="GELU_2_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_2_1_0 [label="MLP_Linear2_2_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_2_1_1 [label="MLP_Linear1_2_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_2_1_1 [label="GELU_2_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_2_1_1 [label="MLP_Linear2_2_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_2_1_2 [label="MLP_Linear1_2_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_2_1_2 [label="GELU_2_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_2_1_2 [label="MLP_Linear2_2_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_2_1_3 [label="MLP_Linear1_2_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_2_1_3 [label="GELU_2_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_2_1_3 [label="MLP_Linear2_2_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_2_1 [label="MLP_AllReduce_2_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_2_0 [label="MLP_Linear1_2_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_2_2_0 [label="GELU_2_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_2_2_0 [label="MLP_Linear2_2_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_2_2_1 [label="MLP_Linear1_2_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_2_2_1 [label="GELU_2_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_2_2_1 [label="MLP_Linear2_2_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_2_2_2 [label="MLP_Linear1_2_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_2_2_2 [label="GELU_2_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_2_2_2 [label="MLP_Linear2_2_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_2_2_3 [label="MLP_Linear1_2_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_2_2_3 [label="GELU_2_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_2_2_3 [label="MLP_Linear2_2_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_2_2 [label="MLP_AllReduce_2_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_3_0 [label="MLP_Linear1_2_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_2_3_0 [label="GELU_2_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_2_3_0 [label="MLP_Linear2_2_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_2_3_1 [label="MLP_Linear1_2_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_2_3_1 [label="GELU_2_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_2_3_1 [label="MLP_Linear2_2_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_2_3_2 [label="MLP_Linear1_2_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_2_3_2 [label="GELU_2_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_2_3_2 [label="MLP_Linear2_2_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_2_3_3 [label="MLP_Linear1_2_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_2_3_3 [label="GELU_2_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_2_3_3 [label="MLP_Linear2_2_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_2_3 [label="MLP_AllReduce_2_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_4_0 [label="MLP_Linear1_2_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_2_4_0 [label="GELU_2_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_2_4_0 [label="MLP_Linear2_2_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_2_4_1 [label="MLP_Linear1_2_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_2_4_1 [label="GELU_2_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_2_4_1 [label="MLP_Linear2_2_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_2_4_2 [label="MLP_Linear1_2_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_2_4_2 [label="GELU_2_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_2_4_2 [label="MLP_Linear2_2_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_2_4_3 [label="MLP_Linear1_2_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_2_4_3 [label="GELU_2_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_2_4_3 [label="MLP_Linear2_2_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_2_4 [label="MLP_AllReduce_2_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_5_0 [label="MLP_Linear1_2_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_2_5_0 [label="GELU_2_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_2_5_0 [label="MLP_Linear2_2_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_2_5_1 [label="MLP_Linear1_2_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_2_5_1 [label="GELU_2_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_2_5_1 [label="MLP_Linear2_2_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_2_5_2 [label="MLP_Linear1_2_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_2_5_2 [label="GELU_2_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_2_5_2 [label="MLP_Linear2_2_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_2_5_3 [label="MLP_Linear1_2_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_2_5_3 [label="GELU_2_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_2_5_3 [label="MLP_Linear2_2_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_2_5 [label="MLP_AllReduce_2_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_6_0 [label="MLP_Linear1_2_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_2_6_0 [label="GELU_2_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_2_6_0 [label="MLP_Linear2_2_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_2_6_1 [label="MLP_Linear1_2_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_2_6_1 [label="GELU_2_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_2_6_1 [label="MLP_Linear2_2_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_2_6_2 [label="MLP_Linear1_2_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_2_6_2 [label="GELU_2_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_2_6_2 [label="MLP_Linear2_2_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_2_6_3 [label="MLP_Linear1_2_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_2_6_3 [label="GELU_2_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_2_6_3 [label="MLP_Linear2_2_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_2_6 [label="MLP_AllReduce_2_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_7_0 [label="MLP_Linear1_2_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_2_7_0 [label="GELU_2_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_2_7_0 [label="MLP_Linear2_2_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_2_7_1 [label="MLP_Linear1_2_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_2_7_1 [label="GELU_2_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_2_7_1 [label="MLP_Linear2_2_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_2_7_2 [label="MLP_Linear1_2_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_2_7_2 [label="GELU_2_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_2_7_2 [label="MLP_Linear2_2_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_2_7_3 [label="MLP_Linear1_2_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_2_7_3 [label="GELU_2_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_2_7_3 [label="MLP_Linear2_2_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_2_7 [label="MLP_AllReduce_2_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_8_0 [label="MLP_Linear1_2_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_2_8_0 [label="GELU_2_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_2_8_0 [label="MLP_Linear2_2_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_2_8_1 [label="MLP_Linear1_2_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_2_8_1 [label="GELU_2_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_2_8_1 [label="MLP_Linear2_2_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_2_8_2 [label="MLP_Linear1_2_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_2_8_2 [label="GELU_2_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_2_8_2 [label="MLP_Linear2_2_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_2_8_3 [label="MLP_Linear1_2_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_2_8_3 [label="GELU_2_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_2_8_3 [label="MLP_Linear2_2_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_2_8 [label="MLP_AllReduce_2_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_9_0 [label="MLP_Linear1_2_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_2_9_0 [label="GELU_2_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_2_9_0 [label="MLP_Linear2_2_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_2_9_1 [label="MLP_Linear1_2_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_2_9_1 [label="GELU_2_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_2_9_1 [label="MLP_Linear2_2_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_2_9_2 [label="MLP_Linear1_2_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_2_9_2 [label="GELU_2_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_2_9_2 [label="MLP_Linear2_2_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_2_9_3 [label="MLP_Linear1_2_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_2_9_3 [label="GELU_2_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_2_9_3 [label="MLP_Linear2_2_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_2_9 [label="MLP_AllReduce_2_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_10_0 [label="MLP_Linear1_2_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_2_10_0 [label="GELU_2_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_2_10_0 [label="MLP_Linear2_2_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_2_10_1 [label="MLP_Linear1_2_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_2_10_1 [label="GELU_2_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_2_10_1 [label="MLP_Linear2_2_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_2_10_2 [label="MLP_Linear1_2_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_2_10_2 [label="GELU_2_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_2_10_2 [label="MLP_Linear2_2_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_2_10_3 [label="MLP_Linear1_2_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_2_10_3 [label="GELU_2_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_2_10_3 [label="MLP_Linear2_2_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_2_10 [label="MLP_AllReduce_2_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_11_0 [label="MLP_Linear1_2_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_2_11_0 [label="GELU_2_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_2_11_0 [label="MLP_Linear2_2_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_2_11_1 [label="MLP_Linear1_2_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_2_11_1 [label="GELU_2_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_2_11_1 [label="MLP_Linear2_2_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_2_11_2 [label="MLP_Linear1_2_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_2_11_2 [label="GELU_2_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_2_11_2 [label="MLP_Linear2_2_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_2_11_3 [label="MLP_Linear1_2_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_2_11_3 [label="GELU_2_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_2_11_3 [label="MLP_Linear2_2_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_2_11 [label="MLP_AllReduce_2_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_12_0 [label="MLP_Linear1_2_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_2_12_0 [label="GELU_2_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_2_12_0 [label="MLP_Linear2_2_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_2_12_1 [label="MLP_Linear1_2_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_2_12_1 [label="GELU_2_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_2_12_1 [label="MLP_Linear2_2_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_2_12_2 [label="MLP_Linear1_2_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_2_12_2 [label="GELU_2_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_2_12_2 [label="MLP_Linear2_2_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_2_12_3 [label="MLP_Linear1_2_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_2_12_3 [label="GELU_2_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_2_12_3 [label="MLP_Linear2_2_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_2_12 [label="MLP_AllReduce_2_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_13_0 [label="MLP_Linear1_2_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_2_13_0 [label="GELU_2_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_2_13_0 [label="MLP_Linear2_2_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_2_13_1 [label="MLP_Linear1_2_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_2_13_1 [label="GELU_2_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_2_13_1 [label="MLP_Linear2_2_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_2_13_2 [label="MLP_Linear1_2_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_2_13_2 [label="GELU_2_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_2_13_2 [label="MLP_Linear2_2_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_2_13_3 [label="MLP_Linear1_2_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_2_13_3 [label="GELU_2_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_2_13_3 [label="MLP_Linear2_2_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_2_13 [label="MLP_AllReduce_2_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_14_0 [label="MLP_Linear1_2_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_2_14_0 [label="GELU_2_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_2_14_0 [label="MLP_Linear2_2_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_2_14_1 [label="MLP_Linear1_2_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_2_14_1 [label="GELU_2_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_2_14_1 [label="MLP_Linear2_2_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_2_14_2 [label="MLP_Linear1_2_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_2_14_2 [label="GELU_2_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_2_14_2 [label="MLP_Linear2_2_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_2_14_3 [label="MLP_Linear1_2_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_2_14_3 [label="GELU_2_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_2_14_3 [label="MLP_Linear2_2_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_2_14 [label="MLP_AllReduce_2_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_2_15_0 [label="MLP_Linear1_2_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_2_15_0 [label="GELU_2_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_2_15_0 [label="MLP_Linear2_2_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_2_15_1 [label="MLP_Linear1_2_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_2_15_1 [label="GELU_2_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_2_15_1 [label="MLP_Linear2_2_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_2_15_2 [label="MLP_Linear1_2_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_2_15_2 [label="GELU_2_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_2_15_2 [label="MLP_Linear2_2_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_2_15_3 [label="MLP_Linear1_2_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_2_15_3 [label="GELU_2_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_2_15_3 [label="MLP_Linear2_2_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_2_15 [label="MLP_AllReduce_2_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_2_0 [label="Expert_Route_2_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_1 [label="Expert_Route_2_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_2 [label="Expert_Route_2_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_3 [label="Expert_Route_2_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_4 [label="Expert_Route_2_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_5 [label="Expert_Route_2_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_6 [label="Expert_Route_2_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_7 [label="Expert_Route_2_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_8 [label="Expert_Route_2_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_9 [label="Expert_Route_2_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_10 [label="Expert_Route_2_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_11 [label="Expert_Route_2_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_12 [label="Expert_Route_2_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_13 [label="Expert_Route_2_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_14 [label="Expert_Route_2_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_2_15 [label="Expert_Route_2_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_3_0_0 [label="QKV_Proj_3_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_3_0_0 [label="Attention_Scores_3_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_3_0_0 [label="Attention_Softmax_3_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_3_0_0 [label="Attention_Dropout_3_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_3_0_0 [label="Attention_Values_3_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_3_0_0 [label="Attention_Weighted_3_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_3_0_0 [label="Attention_Output_3_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_3_0 [label="Attention_AllReduce_3_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_0_1 [label="QKV_Proj_3_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_3_0_1 [label="Attention_Scores_3_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_3_0_1 [label="Attention_Softmax_3_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_3_0_1 [label="Attention_Dropout_3_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_3_0_1 [label="Attention_Values_3_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_3_0_1 [label="Attention_Weighted_3_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_3_0_1 [label="Attention_Output_3_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_3_0 [label="Attention_AllReduce_3_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_0_2 [label="QKV_Proj_3_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_3_0_2 [label="Attention_Scores_3_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_3_0_2 [label="Attention_Softmax_3_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_3_0_2 [label="Attention_Dropout_3_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_3_0_2 [label="Attention_Values_3_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_3_0_2 [label="Attention_Weighted_3_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_3_0_2 [label="Attention_Output_3_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_3_0 [label="Attention_AllReduce_3_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_0_3 [label="QKV_Proj_3_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_3_0_3 [label="Attention_Scores_3_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_3_0_3 [label="Attention_Softmax_3_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_3_0_3 [label="Attention_Dropout_3_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_3_0_3 [label="Attention_Values_3_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_3_0_3 [label="Attention_Weighted_3_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_3_0_3 [label="Attention_Output_3_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_3_0 [label="Attention_AllReduce_3_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_1_0 [label="QKV_Proj_3_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_3_1_0 [label="Attention_Scores_3_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_3_1_0 [label="Attention_Softmax_3_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_3_1_0 [label="Attention_Dropout_3_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_3_1_0 [label="Attention_Values_3_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_3_1_0 [label="Attention_Weighted_3_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_3_1_0 [label="Attention_Output_3_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_3_1 [label="Attention_AllReduce_3_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_1_1 [label="QKV_Proj_3_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_3_1_1 [label="Attention_Scores_3_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_3_1_1 [label="Attention_Softmax_3_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_3_1_1 [label="Attention_Dropout_3_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_3_1_1 [label="Attention_Values_3_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_3_1_1 [label="Attention_Weighted_3_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_3_1_1 [label="Attention_Output_3_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_3_1 [label="Attention_AllReduce_3_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_1_2 [label="QKV_Proj_3_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_3_1_2 [label="Attention_Scores_3_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_3_1_2 [label="Attention_Softmax_3_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_3_1_2 [label="Attention_Dropout_3_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_3_1_2 [label="Attention_Values_3_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_3_1_2 [label="Attention_Weighted_3_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_3_1_2 [label="Attention_Output_3_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_3_1 [label="Attention_AllReduce_3_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_1_3 [label="QKV_Proj_3_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_3_1_3 [label="Attention_Scores_3_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_3_1_3 [label="Attention_Softmax_3_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_3_1_3 [label="Attention_Dropout_3_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_3_1_3 [label="Attention_Values_3_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_3_1_3 [label="Attention_Weighted_3_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_3_1_3 [label="Attention_Output_3_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_3_1 [label="Attention_AllReduce_3_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_2_0 [label="QKV_Proj_3_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_3_2_0 [label="Attention_Scores_3_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_3_2_0 [label="Attention_Softmax_3_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_3_2_0 [label="Attention_Dropout_3_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_3_2_0 [label="Attention_Values_3_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_3_2_0 [label="Attention_Weighted_3_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_3_2_0 [label="Attention_Output_3_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_3_2 [label="Attention_AllReduce_3_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_2_1 [label="QKV_Proj_3_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_3_2_1 [label="Attention_Scores_3_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_3_2_1 [label="Attention_Softmax_3_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_3_2_1 [label="Attention_Dropout_3_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_3_2_1 [label="Attention_Values_3_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_3_2_1 [label="Attention_Weighted_3_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_3_2_1 [label="Attention_Output_3_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_3_2 [label="Attention_AllReduce_3_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_2_2 [label="QKV_Proj_3_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_3_2_2 [label="Attention_Scores_3_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_3_2_2 [label="Attention_Softmax_3_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_3_2_2 [label="Attention_Dropout_3_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_3_2_2 [label="Attention_Values_3_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_3_2_2 [label="Attention_Weighted_3_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_3_2_2 [label="Attention_Output_3_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_3_2 [label="Attention_AllReduce_3_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_2_3 [label="QKV_Proj_3_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_3_2_3 [label="Attention_Scores_3_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_3_2_3 [label="Attention_Softmax_3_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_3_2_3 [label="Attention_Dropout_3_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_3_2_3 [label="Attention_Values_3_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_3_2_3 [label="Attention_Weighted_3_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_3_2_3 [label="Attention_Output_3_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_3_2 [label="Attention_AllReduce_3_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_3_0 [label="QKV_Proj_3_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_3_3_0 [label="Attention_Scores_3_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_3_3_0 [label="Attention_Softmax_3_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_3_3_0 [label="Attention_Dropout_3_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_3_3_0 [label="Attention_Values_3_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_3_3_0 [label="Attention_Weighted_3_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_3_3_0 [label="Attention_Output_3_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_3_3 [label="Attention_AllReduce_3_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_3_1 [label="QKV_Proj_3_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_3_3_1 [label="Attention_Scores_3_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_3_3_1 [label="Attention_Softmax_3_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_3_3_1 [label="Attention_Dropout_3_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_3_3_1 [label="Attention_Values_3_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_3_3_1 [label="Attention_Weighted_3_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_3_3_1 [label="Attention_Output_3_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_3_3 [label="Attention_AllReduce_3_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_3_2 [label="QKV_Proj_3_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_3_3_2 [label="Attention_Scores_3_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_3_3_2 [label="Attention_Softmax_3_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_3_3_2 [label="Attention_Dropout_3_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_3_3_2 [label="Attention_Values_3_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_3_3_2 [label="Attention_Weighted_3_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_3_3_2 [label="Attention_Output_3_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_3_3 [label="Attention_AllReduce_3_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_3_3 [label="QKV_Proj_3_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_3_3_3 [label="Attention_Scores_3_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_3_3_3 [label="Attention_Softmax_3_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_3_3_3 [label="Attention_Dropout_3_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_3_3_3 [label="Attention_Values_3_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_3_3_3 [label="Attention_Weighted_3_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_3_3_3 [label="Attention_Output_3_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_3_3 [label="Attention_AllReduce_3_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_4_0 [label="QKV_Proj_3_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_3_4_0 [label="Attention_Scores_3_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_3_4_0 [label="Attention_Softmax_3_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_3_4_0 [label="Attention_Dropout_3_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_3_4_0 [label="Attention_Values_3_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_3_4_0 [label="Attention_Weighted_3_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_3_4_0 [label="Attention_Output_3_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_3_4 [label="Attention_AllReduce_3_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_4_1 [label="QKV_Proj_3_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_3_4_1 [label="Attention_Scores_3_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_3_4_1 [label="Attention_Softmax_3_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_3_4_1 [label="Attention_Dropout_3_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_3_4_1 [label="Attention_Values_3_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_3_4_1 [label="Attention_Weighted_3_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_3_4_1 [label="Attention_Output_3_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_3_4 [label="Attention_AllReduce_3_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_4_2 [label="QKV_Proj_3_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_3_4_2 [label="Attention_Scores_3_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_3_4_2 [label="Attention_Softmax_3_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_3_4_2 [label="Attention_Dropout_3_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_3_4_2 [label="Attention_Values_3_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_3_4_2 [label="Attention_Weighted_3_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_3_4_2 [label="Attention_Output_3_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_3_4 [label="Attention_AllReduce_3_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_4_3 [label="QKV_Proj_3_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_3_4_3 [label="Attention_Scores_3_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_3_4_3 [label="Attention_Softmax_3_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_3_4_3 [label="Attention_Dropout_3_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_3_4_3 [label="Attention_Values_3_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_3_4_3 [label="Attention_Weighted_3_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_3_4_3 [label="Attention_Output_3_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_3_4 [label="Attention_AllReduce_3_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_5_0 [label="QKV_Proj_3_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_3_5_0 [label="Attention_Scores_3_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_3_5_0 [label="Attention_Softmax_3_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_3_5_0 [label="Attention_Dropout_3_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_3_5_0 [label="Attention_Values_3_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_3_5_0 [label="Attention_Weighted_3_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_3_5_0 [label="Attention_Output_3_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_3_5 [label="Attention_AllReduce_3_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_5_1 [label="QKV_Proj_3_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_3_5_1 [label="Attention_Scores_3_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_3_5_1 [label="Attention_Softmax_3_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_3_5_1 [label="Attention_Dropout_3_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_3_5_1 [label="Attention_Values_3_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_3_5_1 [label="Attention_Weighted_3_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_3_5_1 [label="Attention_Output_3_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_3_5 [label="Attention_AllReduce_3_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_5_2 [label="QKV_Proj_3_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_3_5_2 [label="Attention_Scores_3_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_3_5_2 [label="Attention_Softmax_3_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_3_5_2 [label="Attention_Dropout_3_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_3_5_2 [label="Attention_Values_3_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_3_5_2 [label="Attention_Weighted_3_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_3_5_2 [label="Attention_Output_3_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_3_5 [label="Attention_AllReduce_3_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_5_3 [label="QKV_Proj_3_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_3_5_3 [label="Attention_Scores_3_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_3_5_3 [label="Attention_Softmax_3_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_3_5_3 [label="Attention_Dropout_3_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_3_5_3 [label="Attention_Values_3_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_3_5_3 [label="Attention_Weighted_3_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_3_5_3 [label="Attention_Output_3_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_3_5 [label="Attention_AllReduce_3_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_6_0 [label="QKV_Proj_3_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_3_6_0 [label="Attention_Scores_3_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_3_6_0 [label="Attention_Softmax_3_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_3_6_0 [label="Attention_Dropout_3_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_3_6_0 [label="Attention_Values_3_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_3_6_0 [label="Attention_Weighted_3_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_3_6_0 [label="Attention_Output_3_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_3_6 [label="Attention_AllReduce_3_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_6_1 [label="QKV_Proj_3_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_3_6_1 [label="Attention_Scores_3_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_3_6_1 [label="Attention_Softmax_3_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_3_6_1 [label="Attention_Dropout_3_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_3_6_1 [label="Attention_Values_3_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_3_6_1 [label="Attention_Weighted_3_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_3_6_1 [label="Attention_Output_3_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_3_6 [label="Attention_AllReduce_3_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_6_2 [label="QKV_Proj_3_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_3_6_2 [label="Attention_Scores_3_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_3_6_2 [label="Attention_Softmax_3_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_3_6_2 [label="Attention_Dropout_3_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_3_6_2 [label="Attention_Values_3_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_3_6_2 [label="Attention_Weighted_3_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_3_6_2 [label="Attention_Output_3_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_3_6 [label="Attention_AllReduce_3_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_6_3 [label="QKV_Proj_3_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_3_6_3 [label="Attention_Scores_3_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_3_6_3 [label="Attention_Softmax_3_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_3_6_3 [label="Attention_Dropout_3_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_3_6_3 [label="Attention_Values_3_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_3_6_3 [label="Attention_Weighted_3_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_3_6_3 [label="Attention_Output_3_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_3_6 [label="Attention_AllReduce_3_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_7_0 [label="QKV_Proj_3_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_3_7_0 [label="Attention_Scores_3_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_3_7_0 [label="Attention_Softmax_3_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_3_7_0 [label="Attention_Dropout_3_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_3_7_0 [label="Attention_Values_3_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_3_7_0 [label="Attention_Weighted_3_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_3_7_0 [label="Attention_Output_3_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_3_7 [label="Attention_AllReduce_3_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_7_1 [label="QKV_Proj_3_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_3_7_1 [label="Attention_Scores_3_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_3_7_1 [label="Attention_Softmax_3_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_3_7_1 [label="Attention_Dropout_3_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_3_7_1 [label="Attention_Values_3_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_3_7_1 [label="Attention_Weighted_3_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_3_7_1 [label="Attention_Output_3_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_3_7 [label="Attention_AllReduce_3_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_7_2 [label="QKV_Proj_3_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_3_7_2 [label="Attention_Scores_3_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_3_7_2 [label="Attention_Softmax_3_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_3_7_2 [label="Attention_Dropout_3_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_3_7_2 [label="Attention_Values_3_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_3_7_2 [label="Attention_Weighted_3_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_3_7_2 [label="Attention_Output_3_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_3_7 [label="Attention_AllReduce_3_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_7_3 [label="QKV_Proj_3_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_3_7_3 [label="Attention_Scores_3_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_3_7_3 [label="Attention_Softmax_3_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_3_7_3 [label="Attention_Dropout_3_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_3_7_3 [label="Attention_Values_3_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_3_7_3 [label="Attention_Weighted_3_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_3_7_3 [label="Attention_Output_3_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_3_7 [label="Attention_AllReduce_3_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_8_0 [label="QKV_Proj_3_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_3_8_0 [label="Attention_Scores_3_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_3_8_0 [label="Attention_Softmax_3_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_3_8_0 [label="Attention_Dropout_3_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_3_8_0 [label="Attention_Values_3_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_3_8_0 [label="Attention_Weighted_3_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_3_8_0 [label="Attention_Output_3_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_3_8 [label="Attention_AllReduce_3_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_8_1 [label="QKV_Proj_3_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_3_8_1 [label="Attention_Scores_3_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_3_8_1 [label="Attention_Softmax_3_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_3_8_1 [label="Attention_Dropout_3_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_3_8_1 [label="Attention_Values_3_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_3_8_1 [label="Attention_Weighted_3_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_3_8_1 [label="Attention_Output_3_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_3_8 [label="Attention_AllReduce_3_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_8_2 [label="QKV_Proj_3_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_3_8_2 [label="Attention_Scores_3_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_3_8_2 [label="Attention_Softmax_3_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_3_8_2 [label="Attention_Dropout_3_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_3_8_2 [label="Attention_Values_3_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_3_8_2 [label="Attention_Weighted_3_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_3_8_2 [label="Attention_Output_3_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_3_8 [label="Attention_AllReduce_3_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_8_3 [label="QKV_Proj_3_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_3_8_3 [label="Attention_Scores_3_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_3_8_3 [label="Attention_Softmax_3_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_3_8_3 [label="Attention_Dropout_3_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_3_8_3 [label="Attention_Values_3_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_3_8_3 [label="Attention_Weighted_3_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_3_8_3 [label="Attention_Output_3_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_3_8 [label="Attention_AllReduce_3_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_9_0 [label="QKV_Proj_3_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_3_9_0 [label="Attention_Scores_3_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_3_9_0 [label="Attention_Softmax_3_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_3_9_0 [label="Attention_Dropout_3_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_3_9_0 [label="Attention_Values_3_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_3_9_0 [label="Attention_Weighted_3_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_3_9_0 [label="Attention_Output_3_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_3_9 [label="Attention_AllReduce_3_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_9_1 [label="QKV_Proj_3_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_3_9_1 [label="Attention_Scores_3_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_3_9_1 [label="Attention_Softmax_3_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_3_9_1 [label="Attention_Dropout_3_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_3_9_1 [label="Attention_Values_3_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_3_9_1 [label="Attention_Weighted_3_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_3_9_1 [label="Attention_Output_3_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_3_9 [label="Attention_AllReduce_3_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_9_2 [label="QKV_Proj_3_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_3_9_2 [label="Attention_Scores_3_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_3_9_2 [label="Attention_Softmax_3_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_3_9_2 [label="Attention_Dropout_3_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_3_9_2 [label="Attention_Values_3_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_3_9_2 [label="Attention_Weighted_3_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_3_9_2 [label="Attention_Output_3_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_3_9 [label="Attention_AllReduce_3_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_9_3 [label="QKV_Proj_3_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_3_9_3 [label="Attention_Scores_3_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_3_9_3 [label="Attention_Softmax_3_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_3_9_3 [label="Attention_Dropout_3_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_3_9_3 [label="Attention_Values_3_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_3_9_3 [label="Attention_Weighted_3_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_3_9_3 [label="Attention_Output_3_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_3_9 [label="Attention_AllReduce_3_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_10_0 [label="QKV_Proj_3_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_3_10_0 [label="Attention_Scores_3_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_3_10_0 [label="Attention_Softmax_3_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_3_10_0 [label="Attention_Dropout_3_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_3_10_0 [label="Attention_Values_3_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_3_10_0 [label="Attention_Weighted_3_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_3_10_0 [label="Attention_Output_3_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_3_10 [label="Attention_AllReduce_3_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_10_1 [label="QKV_Proj_3_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_3_10_1 [label="Attention_Scores_3_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_3_10_1 [label="Attention_Softmax_3_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_3_10_1 [label="Attention_Dropout_3_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_3_10_1 [label="Attention_Values_3_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_3_10_1 [label="Attention_Weighted_3_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_3_10_1 [label="Attention_Output_3_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_3_10 [label="Attention_AllReduce_3_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_10_2 [label="QKV_Proj_3_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_3_10_2 [label="Attention_Scores_3_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_3_10_2 [label="Attention_Softmax_3_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_3_10_2 [label="Attention_Dropout_3_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_3_10_2 [label="Attention_Values_3_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_3_10_2 [label="Attention_Weighted_3_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_3_10_2 [label="Attention_Output_3_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_3_10 [label="Attention_AllReduce_3_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_10_3 [label="QKV_Proj_3_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_3_10_3 [label="Attention_Scores_3_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_3_10_3 [label="Attention_Softmax_3_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_3_10_3 [label="Attention_Dropout_3_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_3_10_3 [label="Attention_Values_3_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_3_10_3 [label="Attention_Weighted_3_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_3_10_3 [label="Attention_Output_3_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_3_10 [label="Attention_AllReduce_3_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_11_0 [label="QKV_Proj_3_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_3_11_0 [label="Attention_Scores_3_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_3_11_0 [label="Attention_Softmax_3_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_3_11_0 [label="Attention_Dropout_3_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_3_11_0 [label="Attention_Values_3_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_3_11_0 [label="Attention_Weighted_3_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_3_11_0 [label="Attention_Output_3_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_3_11 [label="Attention_AllReduce_3_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_11_1 [label="QKV_Proj_3_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_3_11_1 [label="Attention_Scores_3_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_3_11_1 [label="Attention_Softmax_3_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_3_11_1 [label="Attention_Dropout_3_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_3_11_1 [label="Attention_Values_3_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_3_11_1 [label="Attention_Weighted_3_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_3_11_1 [label="Attention_Output_3_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_3_11 [label="Attention_AllReduce_3_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_11_2 [label="QKV_Proj_3_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_3_11_2 [label="Attention_Scores_3_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_3_11_2 [label="Attention_Softmax_3_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_3_11_2 [label="Attention_Dropout_3_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_3_11_2 [label="Attention_Values_3_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_3_11_2 [label="Attention_Weighted_3_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_3_11_2 [label="Attention_Output_3_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_3_11 [label="Attention_AllReduce_3_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_11_3 [label="QKV_Proj_3_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_3_11_3 [label="Attention_Scores_3_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_3_11_3 [label="Attention_Softmax_3_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_3_11_3 [label="Attention_Dropout_3_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_3_11_3 [label="Attention_Values_3_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_3_11_3 [label="Attention_Weighted_3_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_3_11_3 [label="Attention_Output_3_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_3_11 [label="Attention_AllReduce_3_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_12_0 [label="QKV_Proj_3_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_3_12_0 [label="Attention_Scores_3_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_3_12_0 [label="Attention_Softmax_3_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_3_12_0 [label="Attention_Dropout_3_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_3_12_0 [label="Attention_Values_3_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_3_12_0 [label="Attention_Weighted_3_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_3_12_0 [label="Attention_Output_3_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_3_12 [label="Attention_AllReduce_3_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_12_1 [label="QKV_Proj_3_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_3_12_1 [label="Attention_Scores_3_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_3_12_1 [label="Attention_Softmax_3_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_3_12_1 [label="Attention_Dropout_3_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_3_12_1 [label="Attention_Values_3_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_3_12_1 [label="Attention_Weighted_3_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_3_12_1 [label="Attention_Output_3_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_3_12 [label="Attention_AllReduce_3_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_12_2 [label="QKV_Proj_3_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_3_12_2 [label="Attention_Scores_3_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_3_12_2 [label="Attention_Softmax_3_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_3_12_2 [label="Attention_Dropout_3_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_3_12_2 [label="Attention_Values_3_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_3_12_2 [label="Attention_Weighted_3_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_3_12_2 [label="Attention_Output_3_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_3_12 [label="Attention_AllReduce_3_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_12_3 [label="QKV_Proj_3_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_3_12_3 [label="Attention_Scores_3_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_3_12_3 [label="Attention_Softmax_3_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_3_12_3 [label="Attention_Dropout_3_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_3_12_3 [label="Attention_Values_3_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_3_12_3 [label="Attention_Weighted_3_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_3_12_3 [label="Attention_Output_3_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_3_12 [label="Attention_AllReduce_3_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_13_0 [label="QKV_Proj_3_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_3_13_0 [label="Attention_Scores_3_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_3_13_0 [label="Attention_Softmax_3_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_3_13_0 [label="Attention_Dropout_3_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_3_13_0 [label="Attention_Values_3_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_3_13_0 [label="Attention_Weighted_3_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_3_13_0 [label="Attention_Output_3_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_3_13 [label="Attention_AllReduce_3_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_13_1 [label="QKV_Proj_3_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_3_13_1 [label="Attention_Scores_3_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_3_13_1 [label="Attention_Softmax_3_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_3_13_1 [label="Attention_Dropout_3_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_3_13_1 [label="Attention_Values_3_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_3_13_1 [label="Attention_Weighted_3_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_3_13_1 [label="Attention_Output_3_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_3_13 [label="Attention_AllReduce_3_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_13_2 [label="QKV_Proj_3_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_3_13_2 [label="Attention_Scores_3_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_3_13_2 [label="Attention_Softmax_3_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_3_13_2 [label="Attention_Dropout_3_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_3_13_2 [label="Attention_Values_3_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_3_13_2 [label="Attention_Weighted_3_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_3_13_2 [label="Attention_Output_3_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_3_13 [label="Attention_AllReduce_3_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_13_3 [label="QKV_Proj_3_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_3_13_3 [label="Attention_Scores_3_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_3_13_3 [label="Attention_Softmax_3_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_3_13_3 [label="Attention_Dropout_3_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_3_13_3 [label="Attention_Values_3_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_3_13_3 [label="Attention_Weighted_3_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_3_13_3 [label="Attention_Output_3_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_3_13 [label="Attention_AllReduce_3_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_14_0 [label="QKV_Proj_3_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_3_14_0 [label="Attention_Scores_3_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_3_14_0 [label="Attention_Softmax_3_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_3_14_0 [label="Attention_Dropout_3_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_3_14_0 [label="Attention_Values_3_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_3_14_0 [label="Attention_Weighted_3_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_3_14_0 [label="Attention_Output_3_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_3_14 [label="Attention_AllReduce_3_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_14_1 [label="QKV_Proj_3_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_3_14_1 [label="Attention_Scores_3_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_3_14_1 [label="Attention_Softmax_3_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_3_14_1 [label="Attention_Dropout_3_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_3_14_1 [label="Attention_Values_3_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_3_14_1 [label="Attention_Weighted_3_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_3_14_1 [label="Attention_Output_3_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_3_14 [label="Attention_AllReduce_3_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_14_2 [label="QKV_Proj_3_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_3_14_2 [label="Attention_Scores_3_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_3_14_2 [label="Attention_Softmax_3_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_3_14_2 [label="Attention_Dropout_3_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_3_14_2 [label="Attention_Values_3_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_3_14_2 [label="Attention_Weighted_3_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_3_14_2 [label="Attention_Output_3_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_3_14 [label="Attention_AllReduce_3_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_14_3 [label="QKV_Proj_3_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_3_14_3 [label="Attention_Scores_3_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_3_14_3 [label="Attention_Softmax_3_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_3_14_3 [label="Attention_Dropout_3_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_3_14_3 [label="Attention_Values_3_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_3_14_3 [label="Attention_Weighted_3_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_3_14_3 [label="Attention_Output_3_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_3_14 [label="Attention_AllReduce_3_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_15_0 [label="QKV_Proj_3_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_3_15_0 [label="Attention_Scores_3_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_3_15_0 [label="Attention_Softmax_3_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_3_15_0 [label="Attention_Dropout_3_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_3_15_0 [label="Attention_Values_3_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_3_15_0 [label="Attention_Weighted_3_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_3_15_0 [label="Attention_Output_3_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_3_15 [label="Attention_AllReduce_3_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_15_1 [label="QKV_Proj_3_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_3_15_1 [label="Attention_Scores_3_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_3_15_1 [label="Attention_Softmax_3_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_3_15_1 [label="Attention_Dropout_3_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_3_15_1 [label="Attention_Values_3_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_3_15_1 [label="Attention_Weighted_3_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_3_15_1 [label="Attention_Output_3_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_3_15 [label="Attention_AllReduce_3_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_15_2 [label="QKV_Proj_3_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_3_15_2 [label="Attention_Scores_3_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_3_15_2 [label="Attention_Softmax_3_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_3_15_2 [label="Attention_Dropout_3_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_3_15_2 [label="Attention_Values_3_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_3_15_2 [label="Attention_Weighted_3_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_3_15_2 [label="Attention_Output_3_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_3_15 [label="Attention_AllReduce_3_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_3_15_3 [label="QKV_Proj_3_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_3_15_3 [label="Attention_Scores_3_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_3_15_3 [label="Attention_Softmax_3_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_3_15_3 [label="Attention_Dropout_3_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_3_15_3 [label="Attention_Values_3_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_3_15_3 [label="Attention_Weighted_3_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_3_15_3 [label="Attention_Output_3_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_3_15 [label="Attention_AllReduce_3_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_0_0 [label="MLP_Linear1_3_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_3_0_0 [label="GELU_3_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_3_0_0 [label="MLP_Linear2_3_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_3_0_1 [label="MLP_Linear1_3_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_3_0_1 [label="GELU_3_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_3_0_1 [label="MLP_Linear2_3_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_3_0_2 [label="MLP_Linear1_3_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_3_0_2 [label="GELU_3_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_3_0_2 [label="MLP_Linear2_3_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_3_0_3 [label="MLP_Linear1_3_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_3_0_3 [label="GELU_3_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_3_0_3 [label="MLP_Linear2_3_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_3_0 [label="MLP_AllReduce_3_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_1_0 [label="MLP_Linear1_3_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_3_1_0 [label="GELU_3_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_3_1_0 [label="MLP_Linear2_3_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_3_1_1 [label="MLP_Linear1_3_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_3_1_1 [label="GELU_3_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_3_1_1 [label="MLP_Linear2_3_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_3_1_2 [label="MLP_Linear1_3_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_3_1_2 [label="GELU_3_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_3_1_2 [label="MLP_Linear2_3_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_3_1_3 [label="MLP_Linear1_3_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_3_1_3 [label="GELU_3_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_3_1_3 [label="MLP_Linear2_3_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_3_1 [label="MLP_AllReduce_3_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_2_0 [label="MLP_Linear1_3_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_3_2_0 [label="GELU_3_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_3_2_0 [label="MLP_Linear2_3_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_3_2_1 [label="MLP_Linear1_3_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_3_2_1 [label="GELU_3_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_3_2_1 [label="MLP_Linear2_3_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_3_2_2 [label="MLP_Linear1_3_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_3_2_2 [label="GELU_3_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_3_2_2 [label="MLP_Linear2_3_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_3_2_3 [label="MLP_Linear1_3_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_3_2_3 [label="GELU_3_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_3_2_3 [label="MLP_Linear2_3_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_3_2 [label="MLP_AllReduce_3_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_3_0 [label="MLP_Linear1_3_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_3_3_0 [label="GELU_3_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_3_3_0 [label="MLP_Linear2_3_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_3_3_1 [label="MLP_Linear1_3_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_3_3_1 [label="GELU_3_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_3_3_1 [label="MLP_Linear2_3_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_3_3_2 [label="MLP_Linear1_3_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_3_3_2 [label="GELU_3_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_3_3_2 [label="MLP_Linear2_3_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_3_3_3 [label="MLP_Linear1_3_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_3_3_3 [label="GELU_3_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_3_3_3 [label="MLP_Linear2_3_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_3_3 [label="MLP_AllReduce_3_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_4_0 [label="MLP_Linear1_3_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_3_4_0 [label="GELU_3_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_3_4_0 [label="MLP_Linear2_3_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_3_4_1 [label="MLP_Linear1_3_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_3_4_1 [label="GELU_3_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_3_4_1 [label="MLP_Linear2_3_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_3_4_2 [label="MLP_Linear1_3_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_3_4_2 [label="GELU_3_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_3_4_2 [label="MLP_Linear2_3_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_3_4_3 [label="MLP_Linear1_3_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_3_4_3 [label="GELU_3_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_3_4_3 [label="MLP_Linear2_3_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_3_4 [label="MLP_AllReduce_3_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_5_0 [label="MLP_Linear1_3_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_3_5_0 [label="GELU_3_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_3_5_0 [label="MLP_Linear2_3_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_3_5_1 [label="MLP_Linear1_3_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_3_5_1 [label="GELU_3_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_3_5_1 [label="MLP_Linear2_3_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_3_5_2 [label="MLP_Linear1_3_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_3_5_2 [label="GELU_3_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_3_5_2 [label="MLP_Linear2_3_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_3_5_3 [label="MLP_Linear1_3_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_3_5_3 [label="GELU_3_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_3_5_3 [label="MLP_Linear2_3_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_3_5 [label="MLP_AllReduce_3_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_6_0 [label="MLP_Linear1_3_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_3_6_0 [label="GELU_3_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_3_6_0 [label="MLP_Linear2_3_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_3_6_1 [label="MLP_Linear1_3_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_3_6_1 [label="GELU_3_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_3_6_1 [label="MLP_Linear2_3_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_3_6_2 [label="MLP_Linear1_3_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_3_6_2 [label="GELU_3_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_3_6_2 [label="MLP_Linear2_3_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_3_6_3 [label="MLP_Linear1_3_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_3_6_3 [label="GELU_3_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_3_6_3 [label="MLP_Linear2_3_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_3_6 [label="MLP_AllReduce_3_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_7_0 [label="MLP_Linear1_3_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_3_7_0 [label="GELU_3_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_3_7_0 [label="MLP_Linear2_3_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_3_7_1 [label="MLP_Linear1_3_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_3_7_1 [label="GELU_3_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_3_7_1 [label="MLP_Linear2_3_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_3_7_2 [label="MLP_Linear1_3_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_3_7_2 [label="GELU_3_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_3_7_2 [label="MLP_Linear2_3_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_3_7_3 [label="MLP_Linear1_3_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_3_7_3 [label="GELU_3_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_3_7_3 [label="MLP_Linear2_3_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_3_7 [label="MLP_AllReduce_3_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_8_0 [label="MLP_Linear1_3_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_3_8_0 [label="GELU_3_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_3_8_0 [label="MLP_Linear2_3_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_3_8_1 [label="MLP_Linear1_3_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_3_8_1 [label="GELU_3_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_3_8_1 [label="MLP_Linear2_3_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_3_8_2 [label="MLP_Linear1_3_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_3_8_2 [label="GELU_3_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_3_8_2 [label="MLP_Linear2_3_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_3_8_3 [label="MLP_Linear1_3_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_3_8_3 [label="GELU_3_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_3_8_3 [label="MLP_Linear2_3_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_3_8 [label="MLP_AllReduce_3_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_9_0 [label="MLP_Linear1_3_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_3_9_0 [label="GELU_3_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_3_9_0 [label="MLP_Linear2_3_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_3_9_1 [label="MLP_Linear1_3_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_3_9_1 [label="GELU_3_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_3_9_1 [label="MLP_Linear2_3_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_3_9_2 [label="MLP_Linear1_3_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_3_9_2 [label="GELU_3_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_3_9_2 [label="MLP_Linear2_3_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_3_9_3 [label="MLP_Linear1_3_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_3_9_3 [label="GELU_3_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_3_9_3 [label="MLP_Linear2_3_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_3_9 [label="MLP_AllReduce_3_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_10_0 [label="MLP_Linear1_3_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_3_10_0 [label="GELU_3_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_3_10_0 [label="MLP_Linear2_3_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_3_10_1 [label="MLP_Linear1_3_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_3_10_1 [label="GELU_3_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_3_10_1 [label="MLP_Linear2_3_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_3_10_2 [label="MLP_Linear1_3_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_3_10_2 [label="GELU_3_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_3_10_2 [label="MLP_Linear2_3_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_3_10_3 [label="MLP_Linear1_3_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_3_10_3 [label="GELU_3_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_3_10_3 [label="MLP_Linear2_3_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_3_10 [label="MLP_AllReduce_3_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_11_0 [label="MLP_Linear1_3_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_3_11_0 [label="GELU_3_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_3_11_0 [label="MLP_Linear2_3_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_3_11_1 [label="MLP_Linear1_3_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_3_11_1 [label="GELU_3_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_3_11_1 [label="MLP_Linear2_3_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_3_11_2 [label="MLP_Linear1_3_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_3_11_2 [label="GELU_3_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_3_11_2 [label="MLP_Linear2_3_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_3_11_3 [label="MLP_Linear1_3_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_3_11_3 [label="GELU_3_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_3_11_3 [label="MLP_Linear2_3_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_3_11 [label="MLP_AllReduce_3_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_12_0 [label="MLP_Linear1_3_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_3_12_0 [label="GELU_3_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_3_12_0 [label="MLP_Linear2_3_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_3_12_1 [label="MLP_Linear1_3_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_3_12_1 [label="GELU_3_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_3_12_1 [label="MLP_Linear2_3_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_3_12_2 [label="MLP_Linear1_3_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_3_12_2 [label="GELU_3_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_3_12_2 [label="MLP_Linear2_3_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_3_12_3 [label="MLP_Linear1_3_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_3_12_3 [label="GELU_3_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_3_12_3 [label="MLP_Linear2_3_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_3_12 [label="MLP_AllReduce_3_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_13_0 [label="MLP_Linear1_3_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_3_13_0 [label="GELU_3_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_3_13_0 [label="MLP_Linear2_3_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_3_13_1 [label="MLP_Linear1_3_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_3_13_1 [label="GELU_3_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_3_13_1 [label="MLP_Linear2_3_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_3_13_2 [label="MLP_Linear1_3_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_3_13_2 [label="GELU_3_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_3_13_2 [label="MLP_Linear2_3_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_3_13_3 [label="MLP_Linear1_3_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_3_13_3 [label="GELU_3_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_3_13_3 [label="MLP_Linear2_3_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_3_13 [label="MLP_AllReduce_3_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_14_0 [label="MLP_Linear1_3_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_3_14_0 [label="GELU_3_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_3_14_0 [label="MLP_Linear2_3_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_3_14_1 [label="MLP_Linear1_3_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_3_14_1 [label="GELU_3_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_3_14_1 [label="MLP_Linear2_3_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_3_14_2 [label="MLP_Linear1_3_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_3_14_2 [label="GELU_3_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_3_14_2 [label="MLP_Linear2_3_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_3_14_3 [label="MLP_Linear1_3_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_3_14_3 [label="GELU_3_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_3_14_3 [label="MLP_Linear2_3_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_3_14 [label="MLP_AllReduce_3_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_3_15_0 [label="MLP_Linear1_3_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_3_15_0 [label="GELU_3_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_3_15_0 [label="MLP_Linear2_3_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_3_15_1 [label="MLP_Linear1_3_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_3_15_1 [label="GELU_3_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_3_15_1 [label="MLP_Linear2_3_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_3_15_2 [label="MLP_Linear1_3_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_3_15_2 [label="GELU_3_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_3_15_2 [label="MLP_Linear2_3_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_3_15_3 [label="MLP_Linear1_3_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_3_15_3 [label="GELU_3_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_3_15_3 [label="MLP_Linear2_3_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_3_15 [label="MLP_AllReduce_3_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_3_0 [label="Expert_Route_3_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_1 [label="Expert_Route_3_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_2 [label="Expert_Route_3_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_3 [label="Expert_Route_3_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_4 [label="Expert_Route_3_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_5 [label="Expert_Route_3_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_6 [label="Expert_Route_3_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_7 [label="Expert_Route_3_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_8 [label="Expert_Route_3_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_9 [label="Expert_Route_3_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_10 [label="Expert_Route_3_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_11 [label="Expert_Route_3_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_12 [label="Expert_Route_3_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_13 [label="Expert_Route_3_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_14 [label="Expert_Route_3_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_3_15 [label="Expert_Route_3_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_4_0_0 [label="QKV_Proj_4_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_4_0_0 [label="Attention_Scores_4_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_4_0_0 [label="Attention_Softmax_4_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_4_0_0 [label="Attention_Dropout_4_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_4_0_0 [label="Attention_Values_4_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_4_0_0 [label="Attention_Weighted_4_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_4_0_0 [label="Attention_Output_4_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_4_0 [label="Attention_AllReduce_4_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_0_1 [label="QKV_Proj_4_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_4_0_1 [label="Attention_Scores_4_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_4_0_1 [label="Attention_Softmax_4_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_4_0_1 [label="Attention_Dropout_4_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_4_0_1 [label="Attention_Values_4_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_4_0_1 [label="Attention_Weighted_4_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_4_0_1 [label="Attention_Output_4_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_4_0 [label="Attention_AllReduce_4_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_0_2 [label="QKV_Proj_4_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_4_0_2 [label="Attention_Scores_4_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_4_0_2 [label="Attention_Softmax_4_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_4_0_2 [label="Attention_Dropout_4_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_4_0_2 [label="Attention_Values_4_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_4_0_2 [label="Attention_Weighted_4_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_4_0_2 [label="Attention_Output_4_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_4_0 [label="Attention_AllReduce_4_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_0_3 [label="QKV_Proj_4_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_4_0_3 [label="Attention_Scores_4_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_4_0_3 [label="Attention_Softmax_4_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_4_0_3 [label="Attention_Dropout_4_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_4_0_3 [label="Attention_Values_4_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_4_0_3 [label="Attention_Weighted_4_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_4_0_3 [label="Attention_Output_4_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_4_0 [label="Attention_AllReduce_4_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_1_0 [label="QKV_Proj_4_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_4_1_0 [label="Attention_Scores_4_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_4_1_0 [label="Attention_Softmax_4_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_4_1_0 [label="Attention_Dropout_4_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_4_1_0 [label="Attention_Values_4_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_4_1_0 [label="Attention_Weighted_4_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_4_1_0 [label="Attention_Output_4_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_4_1 [label="Attention_AllReduce_4_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_1_1 [label="QKV_Proj_4_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_4_1_1 [label="Attention_Scores_4_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_4_1_1 [label="Attention_Softmax_4_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_4_1_1 [label="Attention_Dropout_4_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_4_1_1 [label="Attention_Values_4_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_4_1_1 [label="Attention_Weighted_4_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_4_1_1 [label="Attention_Output_4_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_4_1 [label="Attention_AllReduce_4_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_1_2 [label="QKV_Proj_4_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_4_1_2 [label="Attention_Scores_4_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_4_1_2 [label="Attention_Softmax_4_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_4_1_2 [label="Attention_Dropout_4_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_4_1_2 [label="Attention_Values_4_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_4_1_2 [label="Attention_Weighted_4_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_4_1_2 [label="Attention_Output_4_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_4_1 [label="Attention_AllReduce_4_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_1_3 [label="QKV_Proj_4_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_4_1_3 [label="Attention_Scores_4_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_4_1_3 [label="Attention_Softmax_4_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_4_1_3 [label="Attention_Dropout_4_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_4_1_3 [label="Attention_Values_4_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_4_1_3 [label="Attention_Weighted_4_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_4_1_3 [label="Attention_Output_4_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_4_1 [label="Attention_AllReduce_4_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_2_0 [label="QKV_Proj_4_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_4_2_0 [label="Attention_Scores_4_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_4_2_0 [label="Attention_Softmax_4_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_4_2_0 [label="Attention_Dropout_4_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_4_2_0 [label="Attention_Values_4_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_4_2_0 [label="Attention_Weighted_4_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_4_2_0 [label="Attention_Output_4_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_4_2 [label="Attention_AllReduce_4_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_2_1 [label="QKV_Proj_4_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_4_2_1 [label="Attention_Scores_4_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_4_2_1 [label="Attention_Softmax_4_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_4_2_1 [label="Attention_Dropout_4_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_4_2_1 [label="Attention_Values_4_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_4_2_1 [label="Attention_Weighted_4_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_4_2_1 [label="Attention_Output_4_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_4_2 [label="Attention_AllReduce_4_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_2_2 [label="QKV_Proj_4_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_4_2_2 [label="Attention_Scores_4_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_4_2_2 [label="Attention_Softmax_4_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_4_2_2 [label="Attention_Dropout_4_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_4_2_2 [label="Attention_Values_4_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_4_2_2 [label="Attention_Weighted_4_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_4_2_2 [label="Attention_Output_4_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_4_2 [label="Attention_AllReduce_4_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_2_3 [label="QKV_Proj_4_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_4_2_3 [label="Attention_Scores_4_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_4_2_3 [label="Attention_Softmax_4_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_4_2_3 [label="Attention_Dropout_4_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_4_2_3 [label="Attention_Values_4_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_4_2_3 [label="Attention_Weighted_4_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_4_2_3 [label="Attention_Output_4_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_4_2 [label="Attention_AllReduce_4_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_3_0 [label="QKV_Proj_4_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_4_3_0 [label="Attention_Scores_4_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_4_3_0 [label="Attention_Softmax_4_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_4_3_0 [label="Attention_Dropout_4_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_4_3_0 [label="Attention_Values_4_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_4_3_0 [label="Attention_Weighted_4_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_4_3_0 [label="Attention_Output_4_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_4_3 [label="Attention_AllReduce_4_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_3_1 [label="QKV_Proj_4_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_4_3_1 [label="Attention_Scores_4_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_4_3_1 [label="Attention_Softmax_4_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_4_3_1 [label="Attention_Dropout_4_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_4_3_1 [label="Attention_Values_4_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_4_3_1 [label="Attention_Weighted_4_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_4_3_1 [label="Attention_Output_4_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_4_3 [label="Attention_AllReduce_4_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_3_2 [label="QKV_Proj_4_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_4_3_2 [label="Attention_Scores_4_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_4_3_2 [label="Attention_Softmax_4_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_4_3_2 [label="Attention_Dropout_4_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_4_3_2 [label="Attention_Values_4_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_4_3_2 [label="Attention_Weighted_4_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_4_3_2 [label="Attention_Output_4_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_4_3 [label="Attention_AllReduce_4_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_3_3 [label="QKV_Proj_4_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_4_3_3 [label="Attention_Scores_4_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_4_3_3 [label="Attention_Softmax_4_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_4_3_3 [label="Attention_Dropout_4_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_4_3_3 [label="Attention_Values_4_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_4_3_3 [label="Attention_Weighted_4_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_4_3_3 [label="Attention_Output_4_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_4_3 [label="Attention_AllReduce_4_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_4_0 [label="QKV_Proj_4_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_4_4_0 [label="Attention_Scores_4_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_4_4_0 [label="Attention_Softmax_4_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_4_4_0 [label="Attention_Dropout_4_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_4_4_0 [label="Attention_Values_4_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_4_4_0 [label="Attention_Weighted_4_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_4_4_0 [label="Attention_Output_4_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_4_4 [label="Attention_AllReduce_4_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_4_1 [label="QKV_Proj_4_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_4_4_1 [label="Attention_Scores_4_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_4_4_1 [label="Attention_Softmax_4_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_4_4_1 [label="Attention_Dropout_4_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_4_4_1 [label="Attention_Values_4_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_4_4_1 [label="Attention_Weighted_4_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_4_4_1 [label="Attention_Output_4_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_4_4 [label="Attention_AllReduce_4_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_4_2 [label="QKV_Proj_4_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_4_4_2 [label="Attention_Scores_4_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_4_4_2 [label="Attention_Softmax_4_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_4_4_2 [label="Attention_Dropout_4_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_4_4_2 [label="Attention_Values_4_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_4_4_2 [label="Attention_Weighted_4_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_4_4_2 [label="Attention_Output_4_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_4_4 [label="Attention_AllReduce_4_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_4_3 [label="QKV_Proj_4_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_4_4_3 [label="Attention_Scores_4_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_4_4_3 [label="Attention_Softmax_4_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_4_4_3 [label="Attention_Dropout_4_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_4_4_3 [label="Attention_Values_4_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_4_4_3 [label="Attention_Weighted_4_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_4_4_3 [label="Attention_Output_4_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_4_4 [label="Attention_AllReduce_4_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_5_0 [label="QKV_Proj_4_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_4_5_0 [label="Attention_Scores_4_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_4_5_0 [label="Attention_Softmax_4_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_4_5_0 [label="Attention_Dropout_4_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_4_5_0 [label="Attention_Values_4_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_4_5_0 [label="Attention_Weighted_4_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_4_5_0 [label="Attention_Output_4_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_4_5 [label="Attention_AllReduce_4_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_5_1 [label="QKV_Proj_4_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_4_5_1 [label="Attention_Scores_4_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_4_5_1 [label="Attention_Softmax_4_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_4_5_1 [label="Attention_Dropout_4_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_4_5_1 [label="Attention_Values_4_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_4_5_1 [label="Attention_Weighted_4_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_4_5_1 [label="Attention_Output_4_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_4_5 [label="Attention_AllReduce_4_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_5_2 [label="QKV_Proj_4_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_4_5_2 [label="Attention_Scores_4_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_4_5_2 [label="Attention_Softmax_4_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_4_5_2 [label="Attention_Dropout_4_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_4_5_2 [label="Attention_Values_4_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_4_5_2 [label="Attention_Weighted_4_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_4_5_2 [label="Attention_Output_4_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_4_5 [label="Attention_AllReduce_4_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_5_3 [label="QKV_Proj_4_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_4_5_3 [label="Attention_Scores_4_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_4_5_3 [label="Attention_Softmax_4_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_4_5_3 [label="Attention_Dropout_4_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_4_5_3 [label="Attention_Values_4_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_4_5_3 [label="Attention_Weighted_4_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_4_5_3 [label="Attention_Output_4_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_4_5 [label="Attention_AllReduce_4_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_6_0 [label="QKV_Proj_4_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_4_6_0 [label="Attention_Scores_4_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_4_6_0 [label="Attention_Softmax_4_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_4_6_0 [label="Attention_Dropout_4_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_4_6_0 [label="Attention_Values_4_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_4_6_0 [label="Attention_Weighted_4_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_4_6_0 [label="Attention_Output_4_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_4_6 [label="Attention_AllReduce_4_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_6_1 [label="QKV_Proj_4_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_4_6_1 [label="Attention_Scores_4_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_4_6_1 [label="Attention_Softmax_4_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_4_6_1 [label="Attention_Dropout_4_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_4_6_1 [label="Attention_Values_4_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_4_6_1 [label="Attention_Weighted_4_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_4_6_1 [label="Attention_Output_4_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_4_6 [label="Attention_AllReduce_4_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_6_2 [label="QKV_Proj_4_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_4_6_2 [label="Attention_Scores_4_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_4_6_2 [label="Attention_Softmax_4_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_4_6_2 [label="Attention_Dropout_4_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_4_6_2 [label="Attention_Values_4_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_4_6_2 [label="Attention_Weighted_4_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_4_6_2 [label="Attention_Output_4_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_4_6 [label="Attention_AllReduce_4_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_6_3 [label="QKV_Proj_4_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_4_6_3 [label="Attention_Scores_4_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_4_6_3 [label="Attention_Softmax_4_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_4_6_3 [label="Attention_Dropout_4_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_4_6_3 [label="Attention_Values_4_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_4_6_3 [label="Attention_Weighted_4_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_4_6_3 [label="Attention_Output_4_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_4_6 [label="Attention_AllReduce_4_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_7_0 [label="QKV_Proj_4_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_4_7_0 [label="Attention_Scores_4_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_4_7_0 [label="Attention_Softmax_4_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_4_7_0 [label="Attention_Dropout_4_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_4_7_0 [label="Attention_Values_4_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_4_7_0 [label="Attention_Weighted_4_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_4_7_0 [label="Attention_Output_4_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_4_7 [label="Attention_AllReduce_4_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_7_1 [label="QKV_Proj_4_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_4_7_1 [label="Attention_Scores_4_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_4_7_1 [label="Attention_Softmax_4_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_4_7_1 [label="Attention_Dropout_4_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_4_7_1 [label="Attention_Values_4_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_4_7_1 [label="Attention_Weighted_4_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_4_7_1 [label="Attention_Output_4_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_4_7 [label="Attention_AllReduce_4_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_7_2 [label="QKV_Proj_4_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_4_7_2 [label="Attention_Scores_4_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_4_7_2 [label="Attention_Softmax_4_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_4_7_2 [label="Attention_Dropout_4_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_4_7_2 [label="Attention_Values_4_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_4_7_2 [label="Attention_Weighted_4_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_4_7_2 [label="Attention_Output_4_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_4_7 [label="Attention_AllReduce_4_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_7_3 [label="QKV_Proj_4_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_4_7_3 [label="Attention_Scores_4_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_4_7_3 [label="Attention_Softmax_4_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_4_7_3 [label="Attention_Dropout_4_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_4_7_3 [label="Attention_Values_4_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_4_7_3 [label="Attention_Weighted_4_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_4_7_3 [label="Attention_Output_4_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_4_7 [label="Attention_AllReduce_4_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_8_0 [label="QKV_Proj_4_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_4_8_0 [label="Attention_Scores_4_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_4_8_0 [label="Attention_Softmax_4_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_4_8_0 [label="Attention_Dropout_4_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_4_8_0 [label="Attention_Values_4_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_4_8_0 [label="Attention_Weighted_4_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_4_8_0 [label="Attention_Output_4_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_4_8 [label="Attention_AllReduce_4_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_8_1 [label="QKV_Proj_4_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_4_8_1 [label="Attention_Scores_4_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_4_8_1 [label="Attention_Softmax_4_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_4_8_1 [label="Attention_Dropout_4_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_4_8_1 [label="Attention_Values_4_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_4_8_1 [label="Attention_Weighted_4_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_4_8_1 [label="Attention_Output_4_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_4_8 [label="Attention_AllReduce_4_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_8_2 [label="QKV_Proj_4_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_4_8_2 [label="Attention_Scores_4_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_4_8_2 [label="Attention_Softmax_4_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_4_8_2 [label="Attention_Dropout_4_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_4_8_2 [label="Attention_Values_4_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_4_8_2 [label="Attention_Weighted_4_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_4_8_2 [label="Attention_Output_4_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_4_8 [label="Attention_AllReduce_4_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_8_3 [label="QKV_Proj_4_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_4_8_3 [label="Attention_Scores_4_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_4_8_3 [label="Attention_Softmax_4_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_4_8_3 [label="Attention_Dropout_4_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_4_8_3 [label="Attention_Values_4_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_4_8_3 [label="Attention_Weighted_4_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_4_8_3 [label="Attention_Output_4_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_4_8 [label="Attention_AllReduce_4_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_9_0 [label="QKV_Proj_4_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_4_9_0 [label="Attention_Scores_4_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_4_9_0 [label="Attention_Softmax_4_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_4_9_0 [label="Attention_Dropout_4_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_4_9_0 [label="Attention_Values_4_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_4_9_0 [label="Attention_Weighted_4_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_4_9_0 [label="Attention_Output_4_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_4_9 [label="Attention_AllReduce_4_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_9_1 [label="QKV_Proj_4_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_4_9_1 [label="Attention_Scores_4_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_4_9_1 [label="Attention_Softmax_4_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_4_9_1 [label="Attention_Dropout_4_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_4_9_1 [label="Attention_Values_4_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_4_9_1 [label="Attention_Weighted_4_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_4_9_1 [label="Attention_Output_4_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_4_9 [label="Attention_AllReduce_4_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_9_2 [label="QKV_Proj_4_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_4_9_2 [label="Attention_Scores_4_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_4_9_2 [label="Attention_Softmax_4_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_4_9_2 [label="Attention_Dropout_4_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_4_9_2 [label="Attention_Values_4_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_4_9_2 [label="Attention_Weighted_4_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_4_9_2 [label="Attention_Output_4_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_4_9 [label="Attention_AllReduce_4_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_9_3 [label="QKV_Proj_4_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_4_9_3 [label="Attention_Scores_4_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_4_9_3 [label="Attention_Softmax_4_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_4_9_3 [label="Attention_Dropout_4_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_4_9_3 [label="Attention_Values_4_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_4_9_3 [label="Attention_Weighted_4_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_4_9_3 [label="Attention_Output_4_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_4_9 [label="Attention_AllReduce_4_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_10_0 [label="QKV_Proj_4_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_4_10_0 [label="Attention_Scores_4_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_4_10_0 [label="Attention_Softmax_4_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_4_10_0 [label="Attention_Dropout_4_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_4_10_0 [label="Attention_Values_4_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_4_10_0 [label="Attention_Weighted_4_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_4_10_0 [label="Attention_Output_4_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_4_10 [label="Attention_AllReduce_4_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_10_1 [label="QKV_Proj_4_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_4_10_1 [label="Attention_Scores_4_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_4_10_1 [label="Attention_Softmax_4_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_4_10_1 [label="Attention_Dropout_4_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_4_10_1 [label="Attention_Values_4_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_4_10_1 [label="Attention_Weighted_4_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_4_10_1 [label="Attention_Output_4_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_4_10 [label="Attention_AllReduce_4_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_10_2 [label="QKV_Proj_4_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_4_10_2 [label="Attention_Scores_4_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_4_10_2 [label="Attention_Softmax_4_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_4_10_2 [label="Attention_Dropout_4_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_4_10_2 [label="Attention_Values_4_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_4_10_2 [label="Attention_Weighted_4_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_4_10_2 [label="Attention_Output_4_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_4_10 [label="Attention_AllReduce_4_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_10_3 [label="QKV_Proj_4_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_4_10_3 [label="Attention_Scores_4_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_4_10_3 [label="Attention_Softmax_4_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_4_10_3 [label="Attention_Dropout_4_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_4_10_3 [label="Attention_Values_4_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_4_10_3 [label="Attention_Weighted_4_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_4_10_3 [label="Attention_Output_4_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_4_10 [label="Attention_AllReduce_4_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_11_0 [label="QKV_Proj_4_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_4_11_0 [label="Attention_Scores_4_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_4_11_0 [label="Attention_Softmax_4_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_4_11_0 [label="Attention_Dropout_4_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_4_11_0 [label="Attention_Values_4_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_4_11_0 [label="Attention_Weighted_4_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_4_11_0 [label="Attention_Output_4_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_4_11 [label="Attention_AllReduce_4_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_11_1 [label="QKV_Proj_4_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_4_11_1 [label="Attention_Scores_4_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_4_11_1 [label="Attention_Softmax_4_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_4_11_1 [label="Attention_Dropout_4_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_4_11_1 [label="Attention_Values_4_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_4_11_1 [label="Attention_Weighted_4_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_4_11_1 [label="Attention_Output_4_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_4_11 [label="Attention_AllReduce_4_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_11_2 [label="QKV_Proj_4_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_4_11_2 [label="Attention_Scores_4_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_4_11_2 [label="Attention_Softmax_4_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_4_11_2 [label="Attention_Dropout_4_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_4_11_2 [label="Attention_Values_4_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_4_11_2 [label="Attention_Weighted_4_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_4_11_2 [label="Attention_Output_4_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_4_11 [label="Attention_AllReduce_4_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_11_3 [label="QKV_Proj_4_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_4_11_3 [label="Attention_Scores_4_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_4_11_3 [label="Attention_Softmax_4_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_4_11_3 [label="Attention_Dropout_4_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_4_11_3 [label="Attention_Values_4_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_4_11_3 [label="Attention_Weighted_4_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_4_11_3 [label="Attention_Output_4_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_4_11 [label="Attention_AllReduce_4_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_12_0 [label="QKV_Proj_4_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_4_12_0 [label="Attention_Scores_4_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_4_12_0 [label="Attention_Softmax_4_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_4_12_0 [label="Attention_Dropout_4_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_4_12_0 [label="Attention_Values_4_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_4_12_0 [label="Attention_Weighted_4_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_4_12_0 [label="Attention_Output_4_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_4_12 [label="Attention_AllReduce_4_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_12_1 [label="QKV_Proj_4_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_4_12_1 [label="Attention_Scores_4_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_4_12_1 [label="Attention_Softmax_4_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_4_12_1 [label="Attention_Dropout_4_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_4_12_1 [label="Attention_Values_4_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_4_12_1 [label="Attention_Weighted_4_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_4_12_1 [label="Attention_Output_4_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_4_12 [label="Attention_AllReduce_4_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_12_2 [label="QKV_Proj_4_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_4_12_2 [label="Attention_Scores_4_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_4_12_2 [label="Attention_Softmax_4_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_4_12_2 [label="Attention_Dropout_4_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_4_12_2 [label="Attention_Values_4_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_4_12_2 [label="Attention_Weighted_4_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_4_12_2 [label="Attention_Output_4_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_4_12 [label="Attention_AllReduce_4_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_12_3 [label="QKV_Proj_4_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_4_12_3 [label="Attention_Scores_4_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_4_12_3 [label="Attention_Softmax_4_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_4_12_3 [label="Attention_Dropout_4_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_4_12_3 [label="Attention_Values_4_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_4_12_3 [label="Attention_Weighted_4_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_4_12_3 [label="Attention_Output_4_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_4_12 [label="Attention_AllReduce_4_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_13_0 [label="QKV_Proj_4_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_4_13_0 [label="Attention_Scores_4_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_4_13_0 [label="Attention_Softmax_4_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_4_13_0 [label="Attention_Dropout_4_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_4_13_0 [label="Attention_Values_4_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_4_13_0 [label="Attention_Weighted_4_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_4_13_0 [label="Attention_Output_4_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_4_13 [label="Attention_AllReduce_4_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_13_1 [label="QKV_Proj_4_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_4_13_1 [label="Attention_Scores_4_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_4_13_1 [label="Attention_Softmax_4_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_4_13_1 [label="Attention_Dropout_4_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_4_13_1 [label="Attention_Values_4_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_4_13_1 [label="Attention_Weighted_4_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_4_13_1 [label="Attention_Output_4_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_4_13 [label="Attention_AllReduce_4_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_13_2 [label="QKV_Proj_4_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_4_13_2 [label="Attention_Scores_4_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_4_13_2 [label="Attention_Softmax_4_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_4_13_2 [label="Attention_Dropout_4_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_4_13_2 [label="Attention_Values_4_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_4_13_2 [label="Attention_Weighted_4_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_4_13_2 [label="Attention_Output_4_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_4_13 [label="Attention_AllReduce_4_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_13_3 [label="QKV_Proj_4_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_4_13_3 [label="Attention_Scores_4_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_4_13_3 [label="Attention_Softmax_4_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_4_13_3 [label="Attention_Dropout_4_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_4_13_3 [label="Attention_Values_4_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_4_13_3 [label="Attention_Weighted_4_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_4_13_3 [label="Attention_Output_4_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_4_13 [label="Attention_AllReduce_4_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_14_0 [label="QKV_Proj_4_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_4_14_0 [label="Attention_Scores_4_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_4_14_0 [label="Attention_Softmax_4_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_4_14_0 [label="Attention_Dropout_4_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_4_14_0 [label="Attention_Values_4_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_4_14_0 [label="Attention_Weighted_4_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_4_14_0 [label="Attention_Output_4_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_4_14 [label="Attention_AllReduce_4_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_14_1 [label="QKV_Proj_4_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_4_14_1 [label="Attention_Scores_4_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_4_14_1 [label="Attention_Softmax_4_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_4_14_1 [label="Attention_Dropout_4_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_4_14_1 [label="Attention_Values_4_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_4_14_1 [label="Attention_Weighted_4_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_4_14_1 [label="Attention_Output_4_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_4_14 [label="Attention_AllReduce_4_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_14_2 [label="QKV_Proj_4_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_4_14_2 [label="Attention_Scores_4_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_4_14_2 [label="Attention_Softmax_4_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_4_14_2 [label="Attention_Dropout_4_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_4_14_2 [label="Attention_Values_4_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_4_14_2 [label="Attention_Weighted_4_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_4_14_2 [label="Attention_Output_4_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_4_14 [label="Attention_AllReduce_4_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_14_3 [label="QKV_Proj_4_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_4_14_3 [label="Attention_Scores_4_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_4_14_3 [label="Attention_Softmax_4_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_4_14_3 [label="Attention_Dropout_4_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_4_14_3 [label="Attention_Values_4_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_4_14_3 [label="Attention_Weighted_4_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_4_14_3 [label="Attention_Output_4_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_4_14 [label="Attention_AllReduce_4_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_15_0 [label="QKV_Proj_4_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_4_15_0 [label="Attention_Scores_4_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_4_15_0 [label="Attention_Softmax_4_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_4_15_0 [label="Attention_Dropout_4_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_4_15_0 [label="Attention_Values_4_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_4_15_0 [label="Attention_Weighted_4_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_4_15_0 [label="Attention_Output_4_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_4_15 [label="Attention_AllReduce_4_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_15_1 [label="QKV_Proj_4_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_4_15_1 [label="Attention_Scores_4_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_4_15_1 [label="Attention_Softmax_4_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_4_15_1 [label="Attention_Dropout_4_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_4_15_1 [label="Attention_Values_4_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_4_15_1 [label="Attention_Weighted_4_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_4_15_1 [label="Attention_Output_4_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_4_15 [label="Attention_AllReduce_4_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_15_2 [label="QKV_Proj_4_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_4_15_2 [label="Attention_Scores_4_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_4_15_2 [label="Attention_Softmax_4_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_4_15_2 [label="Attention_Dropout_4_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_4_15_2 [label="Attention_Values_4_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_4_15_2 [label="Attention_Weighted_4_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_4_15_2 [label="Attention_Output_4_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_4_15 [label="Attention_AllReduce_4_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_4_15_3 [label="QKV_Proj_4_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_4_15_3 [label="Attention_Scores_4_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_4_15_3 [label="Attention_Softmax_4_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_4_15_3 [label="Attention_Dropout_4_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_4_15_3 [label="Attention_Values_4_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_4_15_3 [label="Attention_Weighted_4_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_4_15_3 [label="Attention_Output_4_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_4_15 [label="Attention_AllReduce_4_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_0_0 [label="MLP_Linear1_4_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_4_0_0 [label="GELU_4_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_4_0_0 [label="MLP_Linear2_4_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_4_0_1 [label="MLP_Linear1_4_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_4_0_1 [label="GELU_4_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_4_0_1 [label="MLP_Linear2_4_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_4_0_2 [label="MLP_Linear1_4_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_4_0_2 [label="GELU_4_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_4_0_2 [label="MLP_Linear2_4_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_4_0_3 [label="MLP_Linear1_4_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_4_0_3 [label="GELU_4_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_4_0_3 [label="MLP_Linear2_4_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_4_0 [label="MLP_AllReduce_4_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_1_0 [label="MLP_Linear1_4_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_4_1_0 [label="GELU_4_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_4_1_0 [label="MLP_Linear2_4_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_4_1_1 [label="MLP_Linear1_4_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_4_1_1 [label="GELU_4_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_4_1_1 [label="MLP_Linear2_4_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_4_1_2 [label="MLP_Linear1_4_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_4_1_2 [label="GELU_4_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_4_1_2 [label="MLP_Linear2_4_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_4_1_3 [label="MLP_Linear1_4_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_4_1_3 [label="GELU_4_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_4_1_3 [label="MLP_Linear2_4_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_4_1 [label="MLP_AllReduce_4_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_2_0 [label="MLP_Linear1_4_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_4_2_0 [label="GELU_4_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_4_2_0 [label="MLP_Linear2_4_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_4_2_1 [label="MLP_Linear1_4_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_4_2_1 [label="GELU_4_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_4_2_1 [label="MLP_Linear2_4_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_4_2_2 [label="MLP_Linear1_4_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_4_2_2 [label="GELU_4_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_4_2_2 [label="MLP_Linear2_4_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_4_2_3 [label="MLP_Linear1_4_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_4_2_3 [label="GELU_4_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_4_2_3 [label="MLP_Linear2_4_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_4_2 [label="MLP_AllReduce_4_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_3_0 [label="MLP_Linear1_4_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_4_3_0 [label="GELU_4_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_4_3_0 [label="MLP_Linear2_4_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_4_3_1 [label="MLP_Linear1_4_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_4_3_1 [label="GELU_4_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_4_3_1 [label="MLP_Linear2_4_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_4_3_2 [label="MLP_Linear1_4_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_4_3_2 [label="GELU_4_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_4_3_2 [label="MLP_Linear2_4_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_4_3_3 [label="MLP_Linear1_4_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_4_3_3 [label="GELU_4_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_4_3_3 [label="MLP_Linear2_4_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_4_3 [label="MLP_AllReduce_4_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_4_0 [label="MLP_Linear1_4_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_4_4_0 [label="GELU_4_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_4_4_0 [label="MLP_Linear2_4_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_4_4_1 [label="MLP_Linear1_4_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_4_4_1 [label="GELU_4_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_4_4_1 [label="MLP_Linear2_4_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_4_4_2 [label="MLP_Linear1_4_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_4_4_2 [label="GELU_4_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_4_4_2 [label="MLP_Linear2_4_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_4_4_3 [label="MLP_Linear1_4_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_4_4_3 [label="GELU_4_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_4_4_3 [label="MLP_Linear2_4_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_4_4 [label="MLP_AllReduce_4_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_5_0 [label="MLP_Linear1_4_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_4_5_0 [label="GELU_4_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_4_5_0 [label="MLP_Linear2_4_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_4_5_1 [label="MLP_Linear1_4_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_4_5_1 [label="GELU_4_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_4_5_1 [label="MLP_Linear2_4_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_4_5_2 [label="MLP_Linear1_4_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_4_5_2 [label="GELU_4_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_4_5_2 [label="MLP_Linear2_4_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_4_5_3 [label="MLP_Linear1_4_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_4_5_3 [label="GELU_4_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_4_5_3 [label="MLP_Linear2_4_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_4_5 [label="MLP_AllReduce_4_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_6_0 [label="MLP_Linear1_4_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_4_6_0 [label="GELU_4_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_4_6_0 [label="MLP_Linear2_4_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_4_6_1 [label="MLP_Linear1_4_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_4_6_1 [label="GELU_4_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_4_6_1 [label="MLP_Linear2_4_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_4_6_2 [label="MLP_Linear1_4_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_4_6_2 [label="GELU_4_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_4_6_2 [label="MLP_Linear2_4_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_4_6_3 [label="MLP_Linear1_4_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_4_6_3 [label="GELU_4_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_4_6_3 [label="MLP_Linear2_4_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_4_6 [label="MLP_AllReduce_4_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_7_0 [label="MLP_Linear1_4_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_4_7_0 [label="GELU_4_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_4_7_0 [label="MLP_Linear2_4_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_4_7_1 [label="MLP_Linear1_4_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_4_7_1 [label="GELU_4_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_4_7_1 [label="MLP_Linear2_4_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_4_7_2 [label="MLP_Linear1_4_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_4_7_2 [label="GELU_4_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_4_7_2 [label="MLP_Linear2_4_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_4_7_3 [label="MLP_Linear1_4_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_4_7_3 [label="GELU_4_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_4_7_3 [label="MLP_Linear2_4_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_4_7 [label="MLP_AllReduce_4_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_8_0 [label="MLP_Linear1_4_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_4_8_0 [label="GELU_4_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_4_8_0 [label="MLP_Linear2_4_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_4_8_1 [label="MLP_Linear1_4_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_4_8_1 [label="GELU_4_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_4_8_1 [label="MLP_Linear2_4_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_4_8_2 [label="MLP_Linear1_4_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_4_8_2 [label="GELU_4_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_4_8_2 [label="MLP_Linear2_4_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_4_8_3 [label="MLP_Linear1_4_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_4_8_3 [label="GELU_4_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_4_8_3 [label="MLP_Linear2_4_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_4_8 [label="MLP_AllReduce_4_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_9_0 [label="MLP_Linear1_4_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_4_9_0 [label="GELU_4_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_4_9_0 [label="MLP_Linear2_4_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_4_9_1 [label="MLP_Linear1_4_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_4_9_1 [label="GELU_4_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_4_9_1 [label="MLP_Linear2_4_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_4_9_2 [label="MLP_Linear1_4_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_4_9_2 [label="GELU_4_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_4_9_2 [label="MLP_Linear2_4_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_4_9_3 [label="MLP_Linear1_4_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_4_9_3 [label="GELU_4_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_4_9_3 [label="MLP_Linear2_4_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_4_9 [label="MLP_AllReduce_4_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_10_0 [label="MLP_Linear1_4_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_4_10_0 [label="GELU_4_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_4_10_0 [label="MLP_Linear2_4_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_4_10_1 [label="MLP_Linear1_4_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_4_10_1 [label="GELU_4_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_4_10_1 [label="MLP_Linear2_4_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_4_10_2 [label="MLP_Linear1_4_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_4_10_2 [label="GELU_4_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_4_10_2 [label="MLP_Linear2_4_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_4_10_3 [label="MLP_Linear1_4_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_4_10_3 [label="GELU_4_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_4_10_3 [label="MLP_Linear2_4_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_4_10 [label="MLP_AllReduce_4_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_11_0 [label="MLP_Linear1_4_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_4_11_0 [label="GELU_4_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_4_11_0 [label="MLP_Linear2_4_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_4_11_1 [label="MLP_Linear1_4_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_4_11_1 [label="GELU_4_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_4_11_1 [label="MLP_Linear2_4_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_4_11_2 [label="MLP_Linear1_4_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_4_11_2 [label="GELU_4_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_4_11_2 [label="MLP_Linear2_4_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_4_11_3 [label="MLP_Linear1_4_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_4_11_3 [label="GELU_4_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_4_11_3 [label="MLP_Linear2_4_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_4_11 [label="MLP_AllReduce_4_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_12_0 [label="MLP_Linear1_4_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_4_12_0 [label="GELU_4_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_4_12_0 [label="MLP_Linear2_4_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_4_12_1 [label="MLP_Linear1_4_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_4_12_1 [label="GELU_4_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_4_12_1 [label="MLP_Linear2_4_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_4_12_2 [label="MLP_Linear1_4_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_4_12_2 [label="GELU_4_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_4_12_2 [label="MLP_Linear2_4_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_4_12_3 [label="MLP_Linear1_4_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_4_12_3 [label="GELU_4_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_4_12_3 [label="MLP_Linear2_4_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_4_12 [label="MLP_AllReduce_4_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_13_0 [label="MLP_Linear1_4_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_4_13_0 [label="GELU_4_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_4_13_0 [label="MLP_Linear2_4_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_4_13_1 [label="MLP_Linear1_4_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_4_13_1 [label="GELU_4_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_4_13_1 [label="MLP_Linear2_4_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_4_13_2 [label="MLP_Linear1_4_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_4_13_2 [label="GELU_4_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_4_13_2 [label="MLP_Linear2_4_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_4_13_3 [label="MLP_Linear1_4_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_4_13_3 [label="GELU_4_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_4_13_3 [label="MLP_Linear2_4_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_4_13 [label="MLP_AllReduce_4_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_14_0 [label="MLP_Linear1_4_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_4_14_0 [label="GELU_4_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_4_14_0 [label="MLP_Linear2_4_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_4_14_1 [label="MLP_Linear1_4_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_4_14_1 [label="GELU_4_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_4_14_1 [label="MLP_Linear2_4_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_4_14_2 [label="MLP_Linear1_4_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_4_14_2 [label="GELU_4_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_4_14_2 [label="MLP_Linear2_4_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_4_14_3 [label="MLP_Linear1_4_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_4_14_3 [label="GELU_4_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_4_14_3 [label="MLP_Linear2_4_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_4_14 [label="MLP_AllReduce_4_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_4_15_0 [label="MLP_Linear1_4_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_4_15_0 [label="GELU_4_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_4_15_0 [label="MLP_Linear2_4_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_4_15_1 [label="MLP_Linear1_4_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_4_15_1 [label="GELU_4_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_4_15_1 [label="MLP_Linear2_4_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_4_15_2 [label="MLP_Linear1_4_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_4_15_2 [label="GELU_4_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_4_15_2 [label="MLP_Linear2_4_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_4_15_3 [label="MLP_Linear1_4_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_4_15_3 [label="GELU_4_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_4_15_3 [label="MLP_Linear2_4_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_4_15 [label="MLP_AllReduce_4_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_4_0 [label="Expert_Route_4_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_1 [label="Expert_Route_4_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_2 [label="Expert_Route_4_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_3 [label="Expert_Route_4_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_4 [label="Expert_Route_4_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_5 [label="Expert_Route_4_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_6 [label="Expert_Route_4_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_7 [label="Expert_Route_4_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_8 [label="Expert_Route_4_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_9 [label="Expert_Route_4_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_10 [label="Expert_Route_4_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_11 [label="Expert_Route_4_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_12 [label="Expert_Route_4_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_13 [label="Expert_Route_4_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_14 [label="Expert_Route_4_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_4_15 [label="Expert_Route_4_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_5_0_0 [label="QKV_Proj_5_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_5_0_0 [label="Attention_Scores_5_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_5_0_0 [label="Attention_Softmax_5_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_5_0_0 [label="Attention_Dropout_5_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_5_0_0 [label="Attention_Values_5_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_5_0_0 [label="Attention_Weighted_5_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_5_0_0 [label="Attention_Output_5_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_5_0 [label="Attention_AllReduce_5_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_0_1 [label="QKV_Proj_5_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_5_0_1 [label="Attention_Scores_5_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_5_0_1 [label="Attention_Softmax_5_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_5_0_1 [label="Attention_Dropout_5_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_5_0_1 [label="Attention_Values_5_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_5_0_1 [label="Attention_Weighted_5_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_5_0_1 [label="Attention_Output_5_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_5_0 [label="Attention_AllReduce_5_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_0_2 [label="QKV_Proj_5_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_5_0_2 [label="Attention_Scores_5_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_5_0_2 [label="Attention_Softmax_5_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_5_0_2 [label="Attention_Dropout_5_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_5_0_2 [label="Attention_Values_5_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_5_0_2 [label="Attention_Weighted_5_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_5_0_2 [label="Attention_Output_5_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_5_0 [label="Attention_AllReduce_5_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_0_3 [label="QKV_Proj_5_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_5_0_3 [label="Attention_Scores_5_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_5_0_3 [label="Attention_Softmax_5_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_5_0_3 [label="Attention_Dropout_5_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_5_0_3 [label="Attention_Values_5_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_5_0_3 [label="Attention_Weighted_5_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_5_0_3 [label="Attention_Output_5_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_5_0 [label="Attention_AllReduce_5_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_1_0 [label="QKV_Proj_5_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_5_1_0 [label="Attention_Scores_5_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_5_1_0 [label="Attention_Softmax_5_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_5_1_0 [label="Attention_Dropout_5_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_5_1_0 [label="Attention_Values_5_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_5_1_0 [label="Attention_Weighted_5_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_5_1_0 [label="Attention_Output_5_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_5_1 [label="Attention_AllReduce_5_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_1_1 [label="QKV_Proj_5_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_5_1_1 [label="Attention_Scores_5_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_5_1_1 [label="Attention_Softmax_5_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_5_1_1 [label="Attention_Dropout_5_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_5_1_1 [label="Attention_Values_5_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_5_1_1 [label="Attention_Weighted_5_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_5_1_1 [label="Attention_Output_5_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_5_1 [label="Attention_AllReduce_5_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_1_2 [label="QKV_Proj_5_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_5_1_2 [label="Attention_Scores_5_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_5_1_2 [label="Attention_Softmax_5_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_5_1_2 [label="Attention_Dropout_5_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_5_1_2 [label="Attention_Values_5_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_5_1_2 [label="Attention_Weighted_5_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_5_1_2 [label="Attention_Output_5_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_5_1 [label="Attention_AllReduce_5_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_1_3 [label="QKV_Proj_5_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_5_1_3 [label="Attention_Scores_5_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_5_1_3 [label="Attention_Softmax_5_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_5_1_3 [label="Attention_Dropout_5_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_5_1_3 [label="Attention_Values_5_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_5_1_3 [label="Attention_Weighted_5_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_5_1_3 [label="Attention_Output_5_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_5_1 [label="Attention_AllReduce_5_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_2_0 [label="QKV_Proj_5_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_5_2_0 [label="Attention_Scores_5_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_5_2_0 [label="Attention_Softmax_5_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_5_2_0 [label="Attention_Dropout_5_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_5_2_0 [label="Attention_Values_5_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_5_2_0 [label="Attention_Weighted_5_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_5_2_0 [label="Attention_Output_5_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_5_2 [label="Attention_AllReduce_5_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_2_1 [label="QKV_Proj_5_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_5_2_1 [label="Attention_Scores_5_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_5_2_1 [label="Attention_Softmax_5_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_5_2_1 [label="Attention_Dropout_5_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_5_2_1 [label="Attention_Values_5_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_5_2_1 [label="Attention_Weighted_5_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_5_2_1 [label="Attention_Output_5_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_5_2 [label="Attention_AllReduce_5_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_2_2 [label="QKV_Proj_5_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_5_2_2 [label="Attention_Scores_5_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_5_2_2 [label="Attention_Softmax_5_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_5_2_2 [label="Attention_Dropout_5_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_5_2_2 [label="Attention_Values_5_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_5_2_2 [label="Attention_Weighted_5_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_5_2_2 [label="Attention_Output_5_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_5_2 [label="Attention_AllReduce_5_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_2_3 [label="QKV_Proj_5_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_5_2_3 [label="Attention_Scores_5_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_5_2_3 [label="Attention_Softmax_5_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_5_2_3 [label="Attention_Dropout_5_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_5_2_3 [label="Attention_Values_5_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_5_2_3 [label="Attention_Weighted_5_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_5_2_3 [label="Attention_Output_5_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_5_2 [label="Attention_AllReduce_5_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_3_0 [label="QKV_Proj_5_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_5_3_0 [label="Attention_Scores_5_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_5_3_0 [label="Attention_Softmax_5_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_5_3_0 [label="Attention_Dropout_5_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_5_3_0 [label="Attention_Values_5_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_5_3_0 [label="Attention_Weighted_5_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_5_3_0 [label="Attention_Output_5_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_5_3 [label="Attention_AllReduce_5_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_3_1 [label="QKV_Proj_5_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_5_3_1 [label="Attention_Scores_5_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_5_3_1 [label="Attention_Softmax_5_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_5_3_1 [label="Attention_Dropout_5_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_5_3_1 [label="Attention_Values_5_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_5_3_1 [label="Attention_Weighted_5_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_5_3_1 [label="Attention_Output_5_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_5_3 [label="Attention_AllReduce_5_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_3_2 [label="QKV_Proj_5_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_5_3_2 [label="Attention_Scores_5_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_5_3_2 [label="Attention_Softmax_5_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_5_3_2 [label="Attention_Dropout_5_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_5_3_2 [label="Attention_Values_5_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_5_3_2 [label="Attention_Weighted_5_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_5_3_2 [label="Attention_Output_5_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_5_3 [label="Attention_AllReduce_5_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_3_3 [label="QKV_Proj_5_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_5_3_3 [label="Attention_Scores_5_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_5_3_3 [label="Attention_Softmax_5_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_5_3_3 [label="Attention_Dropout_5_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_5_3_3 [label="Attention_Values_5_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_5_3_3 [label="Attention_Weighted_5_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_5_3_3 [label="Attention_Output_5_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_5_3 [label="Attention_AllReduce_5_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_4_0 [label="QKV_Proj_5_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_5_4_0 [label="Attention_Scores_5_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_5_4_0 [label="Attention_Softmax_5_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_5_4_0 [label="Attention_Dropout_5_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_5_4_0 [label="Attention_Values_5_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_5_4_0 [label="Attention_Weighted_5_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_5_4_0 [label="Attention_Output_5_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_5_4 [label="Attention_AllReduce_5_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_4_1 [label="QKV_Proj_5_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_5_4_1 [label="Attention_Scores_5_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_5_4_1 [label="Attention_Softmax_5_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_5_4_1 [label="Attention_Dropout_5_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_5_4_1 [label="Attention_Values_5_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_5_4_1 [label="Attention_Weighted_5_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_5_4_1 [label="Attention_Output_5_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_5_4 [label="Attention_AllReduce_5_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_4_2 [label="QKV_Proj_5_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_5_4_2 [label="Attention_Scores_5_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_5_4_2 [label="Attention_Softmax_5_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_5_4_2 [label="Attention_Dropout_5_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_5_4_2 [label="Attention_Values_5_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_5_4_2 [label="Attention_Weighted_5_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_5_4_2 [label="Attention_Output_5_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_5_4 [label="Attention_AllReduce_5_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_4_3 [label="QKV_Proj_5_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_5_4_3 [label="Attention_Scores_5_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_5_4_3 [label="Attention_Softmax_5_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_5_4_3 [label="Attention_Dropout_5_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_5_4_3 [label="Attention_Values_5_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_5_4_3 [label="Attention_Weighted_5_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_5_4_3 [label="Attention_Output_5_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_5_4 [label="Attention_AllReduce_5_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_5_0 [label="QKV_Proj_5_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_5_5_0 [label="Attention_Scores_5_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_5_5_0 [label="Attention_Softmax_5_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_5_5_0 [label="Attention_Dropout_5_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_5_5_0 [label="Attention_Values_5_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_5_5_0 [label="Attention_Weighted_5_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_5_5_0 [label="Attention_Output_5_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_5_5 [label="Attention_AllReduce_5_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_5_1 [label="QKV_Proj_5_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_5_5_1 [label="Attention_Scores_5_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_5_5_1 [label="Attention_Softmax_5_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_5_5_1 [label="Attention_Dropout_5_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_5_5_1 [label="Attention_Values_5_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_5_5_1 [label="Attention_Weighted_5_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_5_5_1 [label="Attention_Output_5_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_5_5 [label="Attention_AllReduce_5_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_5_2 [label="QKV_Proj_5_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_5_5_2 [label="Attention_Scores_5_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_5_5_2 [label="Attention_Softmax_5_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_5_5_2 [label="Attention_Dropout_5_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_5_5_2 [label="Attention_Values_5_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_5_5_2 [label="Attention_Weighted_5_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_5_5_2 [label="Attention_Output_5_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_5_5 [label="Attention_AllReduce_5_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_5_3 [label="QKV_Proj_5_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_5_5_3 [label="Attention_Scores_5_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_5_5_3 [label="Attention_Softmax_5_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_5_5_3 [label="Attention_Dropout_5_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_5_5_3 [label="Attention_Values_5_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_5_5_3 [label="Attention_Weighted_5_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_5_5_3 [label="Attention_Output_5_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_5_5 [label="Attention_AllReduce_5_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_6_0 [label="QKV_Proj_5_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_5_6_0 [label="Attention_Scores_5_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_5_6_0 [label="Attention_Softmax_5_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_5_6_0 [label="Attention_Dropout_5_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_5_6_0 [label="Attention_Values_5_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_5_6_0 [label="Attention_Weighted_5_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_5_6_0 [label="Attention_Output_5_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_5_6 [label="Attention_AllReduce_5_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_6_1 [label="QKV_Proj_5_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_5_6_1 [label="Attention_Scores_5_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_5_6_1 [label="Attention_Softmax_5_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_5_6_1 [label="Attention_Dropout_5_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_5_6_1 [label="Attention_Values_5_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_5_6_1 [label="Attention_Weighted_5_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_5_6_1 [label="Attention_Output_5_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_5_6 [label="Attention_AllReduce_5_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_6_2 [label="QKV_Proj_5_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_5_6_2 [label="Attention_Scores_5_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_5_6_2 [label="Attention_Softmax_5_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_5_6_2 [label="Attention_Dropout_5_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_5_6_2 [label="Attention_Values_5_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_5_6_2 [label="Attention_Weighted_5_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_5_6_2 [label="Attention_Output_5_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_5_6 [label="Attention_AllReduce_5_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_6_3 [label="QKV_Proj_5_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_5_6_3 [label="Attention_Scores_5_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_5_6_3 [label="Attention_Softmax_5_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_5_6_3 [label="Attention_Dropout_5_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_5_6_3 [label="Attention_Values_5_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_5_6_3 [label="Attention_Weighted_5_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_5_6_3 [label="Attention_Output_5_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_5_6 [label="Attention_AllReduce_5_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_7_0 [label="QKV_Proj_5_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_5_7_0 [label="Attention_Scores_5_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_5_7_0 [label="Attention_Softmax_5_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_5_7_0 [label="Attention_Dropout_5_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_5_7_0 [label="Attention_Values_5_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_5_7_0 [label="Attention_Weighted_5_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_5_7_0 [label="Attention_Output_5_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_5_7 [label="Attention_AllReduce_5_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_7_1 [label="QKV_Proj_5_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_5_7_1 [label="Attention_Scores_5_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_5_7_1 [label="Attention_Softmax_5_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_5_7_1 [label="Attention_Dropout_5_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_5_7_1 [label="Attention_Values_5_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_5_7_1 [label="Attention_Weighted_5_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_5_7_1 [label="Attention_Output_5_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_5_7 [label="Attention_AllReduce_5_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_7_2 [label="QKV_Proj_5_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_5_7_2 [label="Attention_Scores_5_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_5_7_2 [label="Attention_Softmax_5_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_5_7_2 [label="Attention_Dropout_5_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_5_7_2 [label="Attention_Values_5_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_5_7_2 [label="Attention_Weighted_5_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_5_7_2 [label="Attention_Output_5_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_5_7 [label="Attention_AllReduce_5_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_7_3 [label="QKV_Proj_5_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_5_7_3 [label="Attention_Scores_5_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_5_7_3 [label="Attention_Softmax_5_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_5_7_3 [label="Attention_Dropout_5_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_5_7_3 [label="Attention_Values_5_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_5_7_3 [label="Attention_Weighted_5_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_5_7_3 [label="Attention_Output_5_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_5_7 [label="Attention_AllReduce_5_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_8_0 [label="QKV_Proj_5_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_5_8_0 [label="Attention_Scores_5_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_5_8_0 [label="Attention_Softmax_5_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_5_8_0 [label="Attention_Dropout_5_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_5_8_0 [label="Attention_Values_5_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_5_8_0 [label="Attention_Weighted_5_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_5_8_0 [label="Attention_Output_5_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_5_8 [label="Attention_AllReduce_5_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_8_1 [label="QKV_Proj_5_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_5_8_1 [label="Attention_Scores_5_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_5_8_1 [label="Attention_Softmax_5_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_5_8_1 [label="Attention_Dropout_5_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_5_8_1 [label="Attention_Values_5_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_5_8_1 [label="Attention_Weighted_5_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_5_8_1 [label="Attention_Output_5_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_5_8 [label="Attention_AllReduce_5_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_8_2 [label="QKV_Proj_5_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_5_8_2 [label="Attention_Scores_5_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_5_8_2 [label="Attention_Softmax_5_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_5_8_2 [label="Attention_Dropout_5_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_5_8_2 [label="Attention_Values_5_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_5_8_2 [label="Attention_Weighted_5_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_5_8_2 [label="Attention_Output_5_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_5_8 [label="Attention_AllReduce_5_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_8_3 [label="QKV_Proj_5_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_5_8_3 [label="Attention_Scores_5_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_5_8_3 [label="Attention_Softmax_5_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_5_8_3 [label="Attention_Dropout_5_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_5_8_3 [label="Attention_Values_5_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_5_8_3 [label="Attention_Weighted_5_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_5_8_3 [label="Attention_Output_5_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_5_8 [label="Attention_AllReduce_5_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_9_0 [label="QKV_Proj_5_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_5_9_0 [label="Attention_Scores_5_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_5_9_0 [label="Attention_Softmax_5_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_5_9_0 [label="Attention_Dropout_5_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_5_9_0 [label="Attention_Values_5_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_5_9_0 [label="Attention_Weighted_5_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_5_9_0 [label="Attention_Output_5_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_5_9 [label="Attention_AllReduce_5_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_9_1 [label="QKV_Proj_5_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_5_9_1 [label="Attention_Scores_5_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_5_9_1 [label="Attention_Softmax_5_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_5_9_1 [label="Attention_Dropout_5_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_5_9_1 [label="Attention_Values_5_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_5_9_1 [label="Attention_Weighted_5_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_5_9_1 [label="Attention_Output_5_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_5_9 [label="Attention_AllReduce_5_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_9_2 [label="QKV_Proj_5_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_5_9_2 [label="Attention_Scores_5_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_5_9_2 [label="Attention_Softmax_5_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_5_9_2 [label="Attention_Dropout_5_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_5_9_2 [label="Attention_Values_5_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_5_9_2 [label="Attention_Weighted_5_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_5_9_2 [label="Attention_Output_5_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_5_9 [label="Attention_AllReduce_5_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_9_3 [label="QKV_Proj_5_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_5_9_3 [label="Attention_Scores_5_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_5_9_3 [label="Attention_Softmax_5_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_5_9_3 [label="Attention_Dropout_5_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_5_9_3 [label="Attention_Values_5_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_5_9_3 [label="Attention_Weighted_5_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_5_9_3 [label="Attention_Output_5_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_5_9 [label="Attention_AllReduce_5_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_10_0 [label="QKV_Proj_5_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_5_10_0 [label="Attention_Scores_5_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_5_10_0 [label="Attention_Softmax_5_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_5_10_0 [label="Attention_Dropout_5_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_5_10_0 [label="Attention_Values_5_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_5_10_0 [label="Attention_Weighted_5_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_5_10_0 [label="Attention_Output_5_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_5_10 [label="Attention_AllReduce_5_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_10_1 [label="QKV_Proj_5_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_5_10_1 [label="Attention_Scores_5_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_5_10_1 [label="Attention_Softmax_5_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_5_10_1 [label="Attention_Dropout_5_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_5_10_1 [label="Attention_Values_5_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_5_10_1 [label="Attention_Weighted_5_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_5_10_1 [label="Attention_Output_5_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_5_10 [label="Attention_AllReduce_5_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_10_2 [label="QKV_Proj_5_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_5_10_2 [label="Attention_Scores_5_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_5_10_2 [label="Attention_Softmax_5_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_5_10_2 [label="Attention_Dropout_5_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_5_10_2 [label="Attention_Values_5_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_5_10_2 [label="Attention_Weighted_5_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_5_10_2 [label="Attention_Output_5_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_5_10 [label="Attention_AllReduce_5_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_10_3 [label="QKV_Proj_5_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_5_10_3 [label="Attention_Scores_5_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_5_10_3 [label="Attention_Softmax_5_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_5_10_3 [label="Attention_Dropout_5_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_5_10_3 [label="Attention_Values_5_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_5_10_3 [label="Attention_Weighted_5_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_5_10_3 [label="Attention_Output_5_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_5_10 [label="Attention_AllReduce_5_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_11_0 [label="QKV_Proj_5_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_5_11_0 [label="Attention_Scores_5_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_5_11_0 [label="Attention_Softmax_5_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_5_11_0 [label="Attention_Dropout_5_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_5_11_0 [label="Attention_Values_5_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_5_11_0 [label="Attention_Weighted_5_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_5_11_0 [label="Attention_Output_5_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_5_11 [label="Attention_AllReduce_5_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_11_1 [label="QKV_Proj_5_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_5_11_1 [label="Attention_Scores_5_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_5_11_1 [label="Attention_Softmax_5_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_5_11_1 [label="Attention_Dropout_5_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_5_11_1 [label="Attention_Values_5_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_5_11_1 [label="Attention_Weighted_5_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_5_11_1 [label="Attention_Output_5_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_5_11 [label="Attention_AllReduce_5_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_11_2 [label="QKV_Proj_5_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_5_11_2 [label="Attention_Scores_5_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_5_11_2 [label="Attention_Softmax_5_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_5_11_2 [label="Attention_Dropout_5_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_5_11_2 [label="Attention_Values_5_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_5_11_2 [label="Attention_Weighted_5_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_5_11_2 [label="Attention_Output_5_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_5_11 [label="Attention_AllReduce_5_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_11_3 [label="QKV_Proj_5_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_5_11_3 [label="Attention_Scores_5_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_5_11_3 [label="Attention_Softmax_5_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_5_11_3 [label="Attention_Dropout_5_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_5_11_3 [label="Attention_Values_5_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_5_11_3 [label="Attention_Weighted_5_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_5_11_3 [label="Attention_Output_5_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_5_11 [label="Attention_AllReduce_5_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_12_0 [label="QKV_Proj_5_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_5_12_0 [label="Attention_Scores_5_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_5_12_0 [label="Attention_Softmax_5_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_5_12_0 [label="Attention_Dropout_5_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_5_12_0 [label="Attention_Values_5_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_5_12_0 [label="Attention_Weighted_5_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_5_12_0 [label="Attention_Output_5_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_5_12 [label="Attention_AllReduce_5_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_12_1 [label="QKV_Proj_5_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_5_12_1 [label="Attention_Scores_5_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_5_12_1 [label="Attention_Softmax_5_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_5_12_1 [label="Attention_Dropout_5_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_5_12_1 [label="Attention_Values_5_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_5_12_1 [label="Attention_Weighted_5_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_5_12_1 [label="Attention_Output_5_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_5_12 [label="Attention_AllReduce_5_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_12_2 [label="QKV_Proj_5_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_5_12_2 [label="Attention_Scores_5_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_5_12_2 [label="Attention_Softmax_5_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_5_12_2 [label="Attention_Dropout_5_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_5_12_2 [label="Attention_Values_5_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_5_12_2 [label="Attention_Weighted_5_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_5_12_2 [label="Attention_Output_5_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_5_12 [label="Attention_AllReduce_5_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_12_3 [label="QKV_Proj_5_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_5_12_3 [label="Attention_Scores_5_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_5_12_3 [label="Attention_Softmax_5_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_5_12_3 [label="Attention_Dropout_5_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_5_12_3 [label="Attention_Values_5_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_5_12_3 [label="Attention_Weighted_5_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_5_12_3 [label="Attention_Output_5_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_5_12 [label="Attention_AllReduce_5_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_13_0 [label="QKV_Proj_5_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_5_13_0 [label="Attention_Scores_5_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_5_13_0 [label="Attention_Softmax_5_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_5_13_0 [label="Attention_Dropout_5_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_5_13_0 [label="Attention_Values_5_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_5_13_0 [label="Attention_Weighted_5_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_5_13_0 [label="Attention_Output_5_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_5_13 [label="Attention_AllReduce_5_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_13_1 [label="QKV_Proj_5_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_5_13_1 [label="Attention_Scores_5_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_5_13_1 [label="Attention_Softmax_5_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_5_13_1 [label="Attention_Dropout_5_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_5_13_1 [label="Attention_Values_5_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_5_13_1 [label="Attention_Weighted_5_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_5_13_1 [label="Attention_Output_5_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_5_13 [label="Attention_AllReduce_5_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_13_2 [label="QKV_Proj_5_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_5_13_2 [label="Attention_Scores_5_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_5_13_2 [label="Attention_Softmax_5_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_5_13_2 [label="Attention_Dropout_5_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_5_13_2 [label="Attention_Values_5_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_5_13_2 [label="Attention_Weighted_5_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_5_13_2 [label="Attention_Output_5_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_5_13 [label="Attention_AllReduce_5_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_13_3 [label="QKV_Proj_5_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_5_13_3 [label="Attention_Scores_5_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_5_13_3 [label="Attention_Softmax_5_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_5_13_3 [label="Attention_Dropout_5_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_5_13_3 [label="Attention_Values_5_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_5_13_3 [label="Attention_Weighted_5_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_5_13_3 [label="Attention_Output_5_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_5_13 [label="Attention_AllReduce_5_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_14_0 [label="QKV_Proj_5_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_5_14_0 [label="Attention_Scores_5_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_5_14_0 [label="Attention_Softmax_5_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_5_14_0 [label="Attention_Dropout_5_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_5_14_0 [label="Attention_Values_5_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_5_14_0 [label="Attention_Weighted_5_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_5_14_0 [label="Attention_Output_5_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_5_14 [label="Attention_AllReduce_5_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_14_1 [label="QKV_Proj_5_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_5_14_1 [label="Attention_Scores_5_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_5_14_1 [label="Attention_Softmax_5_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_5_14_1 [label="Attention_Dropout_5_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_5_14_1 [label="Attention_Values_5_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_5_14_1 [label="Attention_Weighted_5_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_5_14_1 [label="Attention_Output_5_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_5_14 [label="Attention_AllReduce_5_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_14_2 [label="QKV_Proj_5_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_5_14_2 [label="Attention_Scores_5_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_5_14_2 [label="Attention_Softmax_5_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_5_14_2 [label="Attention_Dropout_5_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_5_14_2 [label="Attention_Values_5_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_5_14_2 [label="Attention_Weighted_5_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_5_14_2 [label="Attention_Output_5_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_5_14 [label="Attention_AllReduce_5_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_14_3 [label="QKV_Proj_5_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_5_14_3 [label="Attention_Scores_5_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_5_14_3 [label="Attention_Softmax_5_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_5_14_3 [label="Attention_Dropout_5_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_5_14_3 [label="Attention_Values_5_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_5_14_3 [label="Attention_Weighted_5_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_5_14_3 [label="Attention_Output_5_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_5_14 [label="Attention_AllReduce_5_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_15_0 [label="QKV_Proj_5_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_5_15_0 [label="Attention_Scores_5_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_5_15_0 [label="Attention_Softmax_5_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_5_15_0 [label="Attention_Dropout_5_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_5_15_0 [label="Attention_Values_5_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_5_15_0 [label="Attention_Weighted_5_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_5_15_0 [label="Attention_Output_5_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_5_15 [label="Attention_AllReduce_5_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_15_1 [label="QKV_Proj_5_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_5_15_1 [label="Attention_Scores_5_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_5_15_1 [label="Attention_Softmax_5_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_5_15_1 [label="Attention_Dropout_5_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_5_15_1 [label="Attention_Values_5_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_5_15_1 [label="Attention_Weighted_5_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_5_15_1 [label="Attention_Output_5_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_5_15 [label="Attention_AllReduce_5_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_15_2 [label="QKV_Proj_5_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_5_15_2 [label="Attention_Scores_5_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_5_15_2 [label="Attention_Softmax_5_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_5_15_2 [label="Attention_Dropout_5_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_5_15_2 [label="Attention_Values_5_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_5_15_2 [label="Attention_Weighted_5_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_5_15_2 [label="Attention_Output_5_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_5_15 [label="Attention_AllReduce_5_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_5_15_3 [label="QKV_Proj_5_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_5_15_3 [label="Attention_Scores_5_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_5_15_3 [label="Attention_Softmax_5_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_5_15_3 [label="Attention_Dropout_5_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_5_15_3 [label="Attention_Values_5_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_5_15_3 [label="Attention_Weighted_5_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_5_15_3 [label="Attention_Output_5_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_5_15 [label="Attention_AllReduce_5_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_0_0 [label="MLP_Linear1_5_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_5_0_0 [label="GELU_5_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_5_0_0 [label="MLP_Linear2_5_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_5_0_1 [label="MLP_Linear1_5_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_5_0_1 [label="GELU_5_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_5_0_1 [label="MLP_Linear2_5_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_5_0_2 [label="MLP_Linear1_5_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_5_0_2 [label="GELU_5_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_5_0_2 [label="MLP_Linear2_5_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_5_0_3 [label="MLP_Linear1_5_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_5_0_3 [label="GELU_5_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_5_0_3 [label="MLP_Linear2_5_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_5_0 [label="MLP_AllReduce_5_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_1_0 [label="MLP_Linear1_5_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_5_1_0 [label="GELU_5_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_5_1_0 [label="MLP_Linear2_5_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_5_1_1 [label="MLP_Linear1_5_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_5_1_1 [label="GELU_5_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_5_1_1 [label="MLP_Linear2_5_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_5_1_2 [label="MLP_Linear1_5_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_5_1_2 [label="GELU_5_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_5_1_2 [label="MLP_Linear2_5_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_5_1_3 [label="MLP_Linear1_5_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_5_1_3 [label="GELU_5_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_5_1_3 [label="MLP_Linear2_5_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_5_1 [label="MLP_AllReduce_5_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_2_0 [label="MLP_Linear1_5_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_5_2_0 [label="GELU_5_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_5_2_0 [label="MLP_Linear2_5_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_5_2_1 [label="MLP_Linear1_5_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_5_2_1 [label="GELU_5_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_5_2_1 [label="MLP_Linear2_5_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_5_2_2 [label="MLP_Linear1_5_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_5_2_2 [label="GELU_5_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_5_2_2 [label="MLP_Linear2_5_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_5_2_3 [label="MLP_Linear1_5_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_5_2_3 [label="GELU_5_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_5_2_3 [label="MLP_Linear2_5_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_5_2 [label="MLP_AllReduce_5_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_3_0 [label="MLP_Linear1_5_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_5_3_0 [label="GELU_5_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_5_3_0 [label="MLP_Linear2_5_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_5_3_1 [label="MLP_Linear1_5_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_5_3_1 [label="GELU_5_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_5_3_1 [label="MLP_Linear2_5_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_5_3_2 [label="MLP_Linear1_5_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_5_3_2 [label="GELU_5_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_5_3_2 [label="MLP_Linear2_5_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_5_3_3 [label="MLP_Linear1_5_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_5_3_3 [label="GELU_5_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_5_3_3 [label="MLP_Linear2_5_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_5_3 [label="MLP_AllReduce_5_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_4_0 [label="MLP_Linear1_5_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_5_4_0 [label="GELU_5_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_5_4_0 [label="MLP_Linear2_5_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_5_4_1 [label="MLP_Linear1_5_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_5_4_1 [label="GELU_5_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_5_4_1 [label="MLP_Linear2_5_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_5_4_2 [label="MLP_Linear1_5_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_5_4_2 [label="GELU_5_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_5_4_2 [label="MLP_Linear2_5_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_5_4_3 [label="MLP_Linear1_5_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_5_4_3 [label="GELU_5_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_5_4_3 [label="MLP_Linear2_5_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_5_4 [label="MLP_AllReduce_5_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_5_0 [label="MLP_Linear1_5_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_5_5_0 [label="GELU_5_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_5_5_0 [label="MLP_Linear2_5_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_5_5_1 [label="MLP_Linear1_5_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_5_5_1 [label="GELU_5_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_5_5_1 [label="MLP_Linear2_5_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_5_5_2 [label="MLP_Linear1_5_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_5_5_2 [label="GELU_5_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_5_5_2 [label="MLP_Linear2_5_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_5_5_3 [label="MLP_Linear1_5_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_5_5_3 [label="GELU_5_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_5_5_3 [label="MLP_Linear2_5_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_5_5 [label="MLP_AllReduce_5_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_6_0 [label="MLP_Linear1_5_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_5_6_0 [label="GELU_5_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_5_6_0 [label="MLP_Linear2_5_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_5_6_1 [label="MLP_Linear1_5_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_5_6_1 [label="GELU_5_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_5_6_1 [label="MLP_Linear2_5_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_5_6_2 [label="MLP_Linear1_5_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_5_6_2 [label="GELU_5_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_5_6_2 [label="MLP_Linear2_5_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_5_6_3 [label="MLP_Linear1_5_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_5_6_3 [label="GELU_5_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_5_6_3 [label="MLP_Linear2_5_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_5_6 [label="MLP_AllReduce_5_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_7_0 [label="MLP_Linear1_5_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_5_7_0 [label="GELU_5_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_5_7_0 [label="MLP_Linear2_5_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_5_7_1 [label="MLP_Linear1_5_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_5_7_1 [label="GELU_5_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_5_7_1 [label="MLP_Linear2_5_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_5_7_2 [label="MLP_Linear1_5_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_5_7_2 [label="GELU_5_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_5_7_2 [label="MLP_Linear2_5_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_5_7_3 [label="MLP_Linear1_5_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_5_7_3 [label="GELU_5_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_5_7_3 [label="MLP_Linear2_5_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_5_7 [label="MLP_AllReduce_5_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_8_0 [label="MLP_Linear1_5_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_5_8_0 [label="GELU_5_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_5_8_0 [label="MLP_Linear2_5_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_5_8_1 [label="MLP_Linear1_5_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_5_8_1 [label="GELU_5_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_5_8_1 [label="MLP_Linear2_5_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_5_8_2 [label="MLP_Linear1_5_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_5_8_2 [label="GELU_5_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_5_8_2 [label="MLP_Linear2_5_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_5_8_3 [label="MLP_Linear1_5_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_5_8_3 [label="GELU_5_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_5_8_3 [label="MLP_Linear2_5_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_5_8 [label="MLP_AllReduce_5_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_9_0 [label="MLP_Linear1_5_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_5_9_0 [label="GELU_5_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_5_9_0 [label="MLP_Linear2_5_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_5_9_1 [label="MLP_Linear1_5_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_5_9_1 [label="GELU_5_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_5_9_1 [label="MLP_Linear2_5_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_5_9_2 [label="MLP_Linear1_5_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_5_9_2 [label="GELU_5_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_5_9_2 [label="MLP_Linear2_5_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_5_9_3 [label="MLP_Linear1_5_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_5_9_3 [label="GELU_5_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_5_9_3 [label="MLP_Linear2_5_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_5_9 [label="MLP_AllReduce_5_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_10_0 [label="MLP_Linear1_5_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_5_10_0 [label="GELU_5_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_5_10_0 [label="MLP_Linear2_5_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_5_10_1 [label="MLP_Linear1_5_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_5_10_1 [label="GELU_5_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_5_10_1 [label="MLP_Linear2_5_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_5_10_2 [label="MLP_Linear1_5_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_5_10_2 [label="GELU_5_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_5_10_2 [label="MLP_Linear2_5_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_5_10_3 [label="MLP_Linear1_5_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_5_10_3 [label="GELU_5_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_5_10_3 [label="MLP_Linear2_5_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_5_10 [label="MLP_AllReduce_5_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_11_0 [label="MLP_Linear1_5_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_5_11_0 [label="GELU_5_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_5_11_0 [label="MLP_Linear2_5_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_5_11_1 [label="MLP_Linear1_5_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_5_11_1 [label="GELU_5_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_5_11_1 [label="MLP_Linear2_5_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_5_11_2 [label="MLP_Linear1_5_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_5_11_2 [label="GELU_5_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_5_11_2 [label="MLP_Linear2_5_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_5_11_3 [label="MLP_Linear1_5_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_5_11_3 [label="GELU_5_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_5_11_3 [label="MLP_Linear2_5_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_5_11 [label="MLP_AllReduce_5_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_12_0 [label="MLP_Linear1_5_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_5_12_0 [label="GELU_5_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_5_12_0 [label="MLP_Linear2_5_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_5_12_1 [label="MLP_Linear1_5_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_5_12_1 [label="GELU_5_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_5_12_1 [label="MLP_Linear2_5_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_5_12_2 [label="MLP_Linear1_5_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_5_12_2 [label="GELU_5_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_5_12_2 [label="MLP_Linear2_5_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_5_12_3 [label="MLP_Linear1_5_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_5_12_3 [label="GELU_5_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_5_12_3 [label="MLP_Linear2_5_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_5_12 [label="MLP_AllReduce_5_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_13_0 [label="MLP_Linear1_5_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_5_13_0 [label="GELU_5_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_5_13_0 [label="MLP_Linear2_5_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_5_13_1 [label="MLP_Linear1_5_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_5_13_1 [label="GELU_5_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_5_13_1 [label="MLP_Linear2_5_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_5_13_2 [label="MLP_Linear1_5_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_5_13_2 [label="GELU_5_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_5_13_2 [label="MLP_Linear2_5_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_5_13_3 [label="MLP_Linear1_5_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_5_13_3 [label="GELU_5_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_5_13_3 [label="MLP_Linear2_5_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_5_13 [label="MLP_AllReduce_5_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_14_0 [label="MLP_Linear1_5_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_5_14_0 [label="GELU_5_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_5_14_0 [label="MLP_Linear2_5_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_5_14_1 [label="MLP_Linear1_5_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_5_14_1 [label="GELU_5_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_5_14_1 [label="MLP_Linear2_5_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_5_14_2 [label="MLP_Linear1_5_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_5_14_2 [label="GELU_5_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_5_14_2 [label="MLP_Linear2_5_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_5_14_3 [label="MLP_Linear1_5_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_5_14_3 [label="GELU_5_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_5_14_3 [label="MLP_Linear2_5_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_5_14 [label="MLP_AllReduce_5_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_5_15_0 [label="MLP_Linear1_5_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_5_15_0 [label="GELU_5_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_5_15_0 [label="MLP_Linear2_5_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_5_15_1 [label="MLP_Linear1_5_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_5_15_1 [label="GELU_5_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_5_15_1 [label="MLP_Linear2_5_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_5_15_2 [label="MLP_Linear1_5_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_5_15_2 [label="GELU_5_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_5_15_2 [label="MLP_Linear2_5_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_5_15_3 [label="MLP_Linear1_5_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_5_15_3 [label="GELU_5_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_5_15_3 [label="MLP_Linear2_5_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_5_15 [label="MLP_AllReduce_5_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_5_0 [label="Expert_Route_5_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_1 [label="Expert_Route_5_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_2 [label="Expert_Route_5_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_3 [label="Expert_Route_5_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_4 [label="Expert_Route_5_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_5 [label="Expert_Route_5_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_6 [label="Expert_Route_5_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_7 [label="Expert_Route_5_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_8 [label="Expert_Route_5_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_9 [label="Expert_Route_5_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_10 [label="Expert_Route_5_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_11 [label="Expert_Route_5_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_12 [label="Expert_Route_5_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_13 [label="Expert_Route_5_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_14 [label="Expert_Route_5_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_5_15 [label="Expert_Route_5_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_6_0_0 [label="QKV_Proj_6_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_6_0_0 [label="Attention_Scores_6_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_6_0_0 [label="Attention_Softmax_6_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_6_0_0 [label="Attention_Dropout_6_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_6_0_0 [label="Attention_Values_6_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_6_0_0 [label="Attention_Weighted_6_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_6_0_0 [label="Attention_Output_6_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_6_0 [label="Attention_AllReduce_6_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_0_1 [label="QKV_Proj_6_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_6_0_1 [label="Attention_Scores_6_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_6_0_1 [label="Attention_Softmax_6_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_6_0_1 [label="Attention_Dropout_6_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_6_0_1 [label="Attention_Values_6_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_6_0_1 [label="Attention_Weighted_6_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_6_0_1 [label="Attention_Output_6_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_6_0 [label="Attention_AllReduce_6_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_0_2 [label="QKV_Proj_6_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_6_0_2 [label="Attention_Scores_6_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_6_0_2 [label="Attention_Softmax_6_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_6_0_2 [label="Attention_Dropout_6_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_6_0_2 [label="Attention_Values_6_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_6_0_2 [label="Attention_Weighted_6_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_6_0_2 [label="Attention_Output_6_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_6_0 [label="Attention_AllReduce_6_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_0_3 [label="QKV_Proj_6_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_6_0_3 [label="Attention_Scores_6_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_6_0_3 [label="Attention_Softmax_6_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_6_0_3 [label="Attention_Dropout_6_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_6_0_3 [label="Attention_Values_6_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_6_0_3 [label="Attention_Weighted_6_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_6_0_3 [label="Attention_Output_6_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_6_0 [label="Attention_AllReduce_6_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_1_0 [label="QKV_Proj_6_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_6_1_0 [label="Attention_Scores_6_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_6_1_0 [label="Attention_Softmax_6_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_6_1_0 [label="Attention_Dropout_6_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_6_1_0 [label="Attention_Values_6_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_6_1_0 [label="Attention_Weighted_6_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_6_1_0 [label="Attention_Output_6_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_6_1 [label="Attention_AllReduce_6_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_1_1 [label="QKV_Proj_6_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_6_1_1 [label="Attention_Scores_6_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_6_1_1 [label="Attention_Softmax_6_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_6_1_1 [label="Attention_Dropout_6_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_6_1_1 [label="Attention_Values_6_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_6_1_1 [label="Attention_Weighted_6_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_6_1_1 [label="Attention_Output_6_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_6_1 [label="Attention_AllReduce_6_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_1_2 [label="QKV_Proj_6_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_6_1_2 [label="Attention_Scores_6_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_6_1_2 [label="Attention_Softmax_6_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_6_1_2 [label="Attention_Dropout_6_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_6_1_2 [label="Attention_Values_6_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_6_1_2 [label="Attention_Weighted_6_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_6_1_2 [label="Attention_Output_6_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_6_1 [label="Attention_AllReduce_6_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_1_3 [label="QKV_Proj_6_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_6_1_3 [label="Attention_Scores_6_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_6_1_3 [label="Attention_Softmax_6_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_6_1_3 [label="Attention_Dropout_6_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_6_1_3 [label="Attention_Values_6_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_6_1_3 [label="Attention_Weighted_6_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_6_1_3 [label="Attention_Output_6_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_6_1 [label="Attention_AllReduce_6_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_2_0 [label="QKV_Proj_6_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_6_2_0 [label="Attention_Scores_6_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_6_2_0 [label="Attention_Softmax_6_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_6_2_0 [label="Attention_Dropout_6_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_6_2_0 [label="Attention_Values_6_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_6_2_0 [label="Attention_Weighted_6_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_6_2_0 [label="Attention_Output_6_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_6_2 [label="Attention_AllReduce_6_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_2_1 [label="QKV_Proj_6_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_6_2_1 [label="Attention_Scores_6_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_6_2_1 [label="Attention_Softmax_6_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_6_2_1 [label="Attention_Dropout_6_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_6_2_1 [label="Attention_Values_6_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_6_2_1 [label="Attention_Weighted_6_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_6_2_1 [label="Attention_Output_6_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_6_2 [label="Attention_AllReduce_6_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_2_2 [label="QKV_Proj_6_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_6_2_2 [label="Attention_Scores_6_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_6_2_2 [label="Attention_Softmax_6_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_6_2_2 [label="Attention_Dropout_6_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_6_2_2 [label="Attention_Values_6_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_6_2_2 [label="Attention_Weighted_6_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_6_2_2 [label="Attention_Output_6_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_6_2 [label="Attention_AllReduce_6_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_2_3 [label="QKV_Proj_6_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_6_2_3 [label="Attention_Scores_6_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_6_2_3 [label="Attention_Softmax_6_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_6_2_3 [label="Attention_Dropout_6_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_6_2_3 [label="Attention_Values_6_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_6_2_3 [label="Attention_Weighted_6_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_6_2_3 [label="Attention_Output_6_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_6_2 [label="Attention_AllReduce_6_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_3_0 [label="QKV_Proj_6_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_6_3_0 [label="Attention_Scores_6_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_6_3_0 [label="Attention_Softmax_6_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_6_3_0 [label="Attention_Dropout_6_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_6_3_0 [label="Attention_Values_6_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_6_3_0 [label="Attention_Weighted_6_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_6_3_0 [label="Attention_Output_6_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_6_3 [label="Attention_AllReduce_6_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_3_1 [label="QKV_Proj_6_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_6_3_1 [label="Attention_Scores_6_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_6_3_1 [label="Attention_Softmax_6_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_6_3_1 [label="Attention_Dropout_6_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_6_3_1 [label="Attention_Values_6_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_6_3_1 [label="Attention_Weighted_6_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_6_3_1 [label="Attention_Output_6_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_6_3 [label="Attention_AllReduce_6_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_3_2 [label="QKV_Proj_6_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_6_3_2 [label="Attention_Scores_6_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_6_3_2 [label="Attention_Softmax_6_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_6_3_2 [label="Attention_Dropout_6_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_6_3_2 [label="Attention_Values_6_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_6_3_2 [label="Attention_Weighted_6_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_6_3_2 [label="Attention_Output_6_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_6_3 [label="Attention_AllReduce_6_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_3_3 [label="QKV_Proj_6_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_6_3_3 [label="Attention_Scores_6_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_6_3_3 [label="Attention_Softmax_6_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_6_3_3 [label="Attention_Dropout_6_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_6_3_3 [label="Attention_Values_6_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_6_3_3 [label="Attention_Weighted_6_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_6_3_3 [label="Attention_Output_6_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_6_3 [label="Attention_AllReduce_6_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_4_0 [label="QKV_Proj_6_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_6_4_0 [label="Attention_Scores_6_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_6_4_0 [label="Attention_Softmax_6_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_6_4_0 [label="Attention_Dropout_6_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_6_4_0 [label="Attention_Values_6_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_6_4_0 [label="Attention_Weighted_6_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_6_4_0 [label="Attention_Output_6_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_6_4 [label="Attention_AllReduce_6_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_4_1 [label="QKV_Proj_6_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_6_4_1 [label="Attention_Scores_6_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_6_4_1 [label="Attention_Softmax_6_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_6_4_1 [label="Attention_Dropout_6_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_6_4_1 [label="Attention_Values_6_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_6_4_1 [label="Attention_Weighted_6_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_6_4_1 [label="Attention_Output_6_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_6_4 [label="Attention_AllReduce_6_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_4_2 [label="QKV_Proj_6_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_6_4_2 [label="Attention_Scores_6_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_6_4_2 [label="Attention_Softmax_6_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_6_4_2 [label="Attention_Dropout_6_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_6_4_2 [label="Attention_Values_6_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_6_4_2 [label="Attention_Weighted_6_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_6_4_2 [label="Attention_Output_6_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_6_4 [label="Attention_AllReduce_6_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_4_3 [label="QKV_Proj_6_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_6_4_3 [label="Attention_Scores_6_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_6_4_3 [label="Attention_Softmax_6_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_6_4_3 [label="Attention_Dropout_6_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_6_4_3 [label="Attention_Values_6_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_6_4_3 [label="Attention_Weighted_6_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_6_4_3 [label="Attention_Output_6_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_6_4 [label="Attention_AllReduce_6_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_5_0 [label="QKV_Proj_6_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_6_5_0 [label="Attention_Scores_6_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_6_5_0 [label="Attention_Softmax_6_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_6_5_0 [label="Attention_Dropout_6_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_6_5_0 [label="Attention_Values_6_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_6_5_0 [label="Attention_Weighted_6_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_6_5_0 [label="Attention_Output_6_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_6_5 [label="Attention_AllReduce_6_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_5_1 [label="QKV_Proj_6_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_6_5_1 [label="Attention_Scores_6_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_6_5_1 [label="Attention_Softmax_6_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_6_5_1 [label="Attention_Dropout_6_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_6_5_1 [label="Attention_Values_6_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_6_5_1 [label="Attention_Weighted_6_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_6_5_1 [label="Attention_Output_6_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_6_5 [label="Attention_AllReduce_6_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_5_2 [label="QKV_Proj_6_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_6_5_2 [label="Attention_Scores_6_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_6_5_2 [label="Attention_Softmax_6_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_6_5_2 [label="Attention_Dropout_6_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_6_5_2 [label="Attention_Values_6_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_6_5_2 [label="Attention_Weighted_6_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_6_5_2 [label="Attention_Output_6_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_6_5 [label="Attention_AllReduce_6_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_5_3 [label="QKV_Proj_6_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_6_5_3 [label="Attention_Scores_6_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_6_5_3 [label="Attention_Softmax_6_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_6_5_3 [label="Attention_Dropout_6_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_6_5_3 [label="Attention_Values_6_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_6_5_3 [label="Attention_Weighted_6_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_6_5_3 [label="Attention_Output_6_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_6_5 [label="Attention_AllReduce_6_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_6_0 [label="QKV_Proj_6_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_6_6_0 [label="Attention_Scores_6_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_6_6_0 [label="Attention_Softmax_6_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_6_6_0 [label="Attention_Dropout_6_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_6_6_0 [label="Attention_Values_6_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_6_6_0 [label="Attention_Weighted_6_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_6_6_0 [label="Attention_Output_6_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_6_6 [label="Attention_AllReduce_6_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_6_1 [label="QKV_Proj_6_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_6_6_1 [label="Attention_Scores_6_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_6_6_1 [label="Attention_Softmax_6_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_6_6_1 [label="Attention_Dropout_6_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_6_6_1 [label="Attention_Values_6_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_6_6_1 [label="Attention_Weighted_6_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_6_6_1 [label="Attention_Output_6_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_6_6 [label="Attention_AllReduce_6_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_6_2 [label="QKV_Proj_6_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_6_6_2 [label="Attention_Scores_6_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_6_6_2 [label="Attention_Softmax_6_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_6_6_2 [label="Attention_Dropout_6_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_6_6_2 [label="Attention_Values_6_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_6_6_2 [label="Attention_Weighted_6_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_6_6_2 [label="Attention_Output_6_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_6_6 [label="Attention_AllReduce_6_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_6_3 [label="QKV_Proj_6_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_6_6_3 [label="Attention_Scores_6_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_6_6_3 [label="Attention_Softmax_6_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_6_6_3 [label="Attention_Dropout_6_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_6_6_3 [label="Attention_Values_6_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_6_6_3 [label="Attention_Weighted_6_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_6_6_3 [label="Attention_Output_6_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_6_6 [label="Attention_AllReduce_6_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_7_0 [label="QKV_Proj_6_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_6_7_0 [label="Attention_Scores_6_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_6_7_0 [label="Attention_Softmax_6_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_6_7_0 [label="Attention_Dropout_6_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_6_7_0 [label="Attention_Values_6_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_6_7_0 [label="Attention_Weighted_6_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_6_7_0 [label="Attention_Output_6_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_6_7 [label="Attention_AllReduce_6_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_7_1 [label="QKV_Proj_6_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_6_7_1 [label="Attention_Scores_6_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_6_7_1 [label="Attention_Softmax_6_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_6_7_1 [label="Attention_Dropout_6_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_6_7_1 [label="Attention_Values_6_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_6_7_1 [label="Attention_Weighted_6_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_6_7_1 [label="Attention_Output_6_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_6_7 [label="Attention_AllReduce_6_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_7_2 [label="QKV_Proj_6_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_6_7_2 [label="Attention_Scores_6_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_6_7_2 [label="Attention_Softmax_6_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_6_7_2 [label="Attention_Dropout_6_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_6_7_2 [label="Attention_Values_6_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_6_7_2 [label="Attention_Weighted_6_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_6_7_2 [label="Attention_Output_6_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_6_7 [label="Attention_AllReduce_6_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_7_3 [label="QKV_Proj_6_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_6_7_3 [label="Attention_Scores_6_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_6_7_3 [label="Attention_Softmax_6_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_6_7_3 [label="Attention_Dropout_6_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_6_7_3 [label="Attention_Values_6_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_6_7_3 [label="Attention_Weighted_6_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_6_7_3 [label="Attention_Output_6_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_6_7 [label="Attention_AllReduce_6_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_8_0 [label="QKV_Proj_6_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_6_8_0 [label="Attention_Scores_6_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_6_8_0 [label="Attention_Softmax_6_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_6_8_0 [label="Attention_Dropout_6_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_6_8_0 [label="Attention_Values_6_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_6_8_0 [label="Attention_Weighted_6_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_6_8_0 [label="Attention_Output_6_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_6_8 [label="Attention_AllReduce_6_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_8_1 [label="QKV_Proj_6_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_6_8_1 [label="Attention_Scores_6_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_6_8_1 [label="Attention_Softmax_6_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_6_8_1 [label="Attention_Dropout_6_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_6_8_1 [label="Attention_Values_6_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_6_8_1 [label="Attention_Weighted_6_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_6_8_1 [label="Attention_Output_6_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_6_8 [label="Attention_AllReduce_6_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_8_2 [label="QKV_Proj_6_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_6_8_2 [label="Attention_Scores_6_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_6_8_2 [label="Attention_Softmax_6_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_6_8_2 [label="Attention_Dropout_6_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_6_8_2 [label="Attention_Values_6_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_6_8_2 [label="Attention_Weighted_6_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_6_8_2 [label="Attention_Output_6_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_6_8 [label="Attention_AllReduce_6_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_8_3 [label="QKV_Proj_6_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_6_8_3 [label="Attention_Scores_6_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_6_8_3 [label="Attention_Softmax_6_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_6_8_3 [label="Attention_Dropout_6_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_6_8_3 [label="Attention_Values_6_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_6_8_3 [label="Attention_Weighted_6_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_6_8_3 [label="Attention_Output_6_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_6_8 [label="Attention_AllReduce_6_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_9_0 [label="QKV_Proj_6_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_6_9_0 [label="Attention_Scores_6_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_6_9_0 [label="Attention_Softmax_6_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_6_9_0 [label="Attention_Dropout_6_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_6_9_0 [label="Attention_Values_6_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_6_9_0 [label="Attention_Weighted_6_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_6_9_0 [label="Attention_Output_6_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_6_9 [label="Attention_AllReduce_6_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_9_1 [label="QKV_Proj_6_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_6_9_1 [label="Attention_Scores_6_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_6_9_1 [label="Attention_Softmax_6_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_6_9_1 [label="Attention_Dropout_6_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_6_9_1 [label="Attention_Values_6_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_6_9_1 [label="Attention_Weighted_6_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_6_9_1 [label="Attention_Output_6_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_6_9 [label="Attention_AllReduce_6_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_9_2 [label="QKV_Proj_6_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_6_9_2 [label="Attention_Scores_6_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_6_9_2 [label="Attention_Softmax_6_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_6_9_2 [label="Attention_Dropout_6_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_6_9_2 [label="Attention_Values_6_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_6_9_2 [label="Attention_Weighted_6_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_6_9_2 [label="Attention_Output_6_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_6_9 [label="Attention_AllReduce_6_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_9_3 [label="QKV_Proj_6_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_6_9_3 [label="Attention_Scores_6_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_6_9_3 [label="Attention_Softmax_6_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_6_9_3 [label="Attention_Dropout_6_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_6_9_3 [label="Attention_Values_6_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_6_9_3 [label="Attention_Weighted_6_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_6_9_3 [label="Attention_Output_6_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_6_9 [label="Attention_AllReduce_6_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_10_0 [label="QKV_Proj_6_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_6_10_0 [label="Attention_Scores_6_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_6_10_0 [label="Attention_Softmax_6_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_6_10_0 [label="Attention_Dropout_6_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_6_10_0 [label="Attention_Values_6_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_6_10_0 [label="Attention_Weighted_6_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_6_10_0 [label="Attention_Output_6_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_6_10 [label="Attention_AllReduce_6_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_10_1 [label="QKV_Proj_6_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_6_10_1 [label="Attention_Scores_6_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_6_10_1 [label="Attention_Softmax_6_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_6_10_1 [label="Attention_Dropout_6_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_6_10_1 [label="Attention_Values_6_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_6_10_1 [label="Attention_Weighted_6_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_6_10_1 [label="Attention_Output_6_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_6_10 [label="Attention_AllReduce_6_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_10_2 [label="QKV_Proj_6_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_6_10_2 [label="Attention_Scores_6_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_6_10_2 [label="Attention_Softmax_6_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_6_10_2 [label="Attention_Dropout_6_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_6_10_2 [label="Attention_Values_6_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_6_10_2 [label="Attention_Weighted_6_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_6_10_2 [label="Attention_Output_6_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_6_10 [label="Attention_AllReduce_6_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_10_3 [label="QKV_Proj_6_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_6_10_3 [label="Attention_Scores_6_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_6_10_3 [label="Attention_Softmax_6_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_6_10_3 [label="Attention_Dropout_6_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_6_10_3 [label="Attention_Values_6_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_6_10_3 [label="Attention_Weighted_6_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_6_10_3 [label="Attention_Output_6_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_6_10 [label="Attention_AllReduce_6_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_11_0 [label="QKV_Proj_6_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_6_11_0 [label="Attention_Scores_6_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_6_11_0 [label="Attention_Softmax_6_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_6_11_0 [label="Attention_Dropout_6_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_6_11_0 [label="Attention_Values_6_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_6_11_0 [label="Attention_Weighted_6_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_6_11_0 [label="Attention_Output_6_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_6_11 [label="Attention_AllReduce_6_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_11_1 [label="QKV_Proj_6_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_6_11_1 [label="Attention_Scores_6_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_6_11_1 [label="Attention_Softmax_6_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_6_11_1 [label="Attention_Dropout_6_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_6_11_1 [label="Attention_Values_6_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_6_11_1 [label="Attention_Weighted_6_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_6_11_1 [label="Attention_Output_6_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_6_11 [label="Attention_AllReduce_6_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_11_2 [label="QKV_Proj_6_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_6_11_2 [label="Attention_Scores_6_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_6_11_2 [label="Attention_Softmax_6_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_6_11_2 [label="Attention_Dropout_6_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_6_11_2 [label="Attention_Values_6_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_6_11_2 [label="Attention_Weighted_6_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_6_11_2 [label="Attention_Output_6_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_6_11 [label="Attention_AllReduce_6_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_11_3 [label="QKV_Proj_6_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_6_11_3 [label="Attention_Scores_6_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_6_11_3 [label="Attention_Softmax_6_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_6_11_3 [label="Attention_Dropout_6_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_6_11_3 [label="Attention_Values_6_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_6_11_3 [label="Attention_Weighted_6_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_6_11_3 [label="Attention_Output_6_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_6_11 [label="Attention_AllReduce_6_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_12_0 [label="QKV_Proj_6_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_6_12_0 [label="Attention_Scores_6_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_6_12_0 [label="Attention_Softmax_6_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_6_12_0 [label="Attention_Dropout_6_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_6_12_0 [label="Attention_Values_6_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_6_12_0 [label="Attention_Weighted_6_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_6_12_0 [label="Attention_Output_6_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_6_12 [label="Attention_AllReduce_6_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_12_1 [label="QKV_Proj_6_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_6_12_1 [label="Attention_Scores_6_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_6_12_1 [label="Attention_Softmax_6_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_6_12_1 [label="Attention_Dropout_6_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_6_12_1 [label="Attention_Values_6_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_6_12_1 [label="Attention_Weighted_6_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_6_12_1 [label="Attention_Output_6_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_6_12 [label="Attention_AllReduce_6_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_12_2 [label="QKV_Proj_6_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_6_12_2 [label="Attention_Scores_6_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_6_12_2 [label="Attention_Softmax_6_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_6_12_2 [label="Attention_Dropout_6_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_6_12_2 [label="Attention_Values_6_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_6_12_2 [label="Attention_Weighted_6_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_6_12_2 [label="Attention_Output_6_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_6_12 [label="Attention_AllReduce_6_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_12_3 [label="QKV_Proj_6_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_6_12_3 [label="Attention_Scores_6_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_6_12_3 [label="Attention_Softmax_6_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_6_12_3 [label="Attention_Dropout_6_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_6_12_3 [label="Attention_Values_6_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_6_12_3 [label="Attention_Weighted_6_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_6_12_3 [label="Attention_Output_6_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_6_12 [label="Attention_AllReduce_6_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_13_0 [label="QKV_Proj_6_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_6_13_0 [label="Attention_Scores_6_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_6_13_0 [label="Attention_Softmax_6_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_6_13_0 [label="Attention_Dropout_6_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_6_13_0 [label="Attention_Values_6_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_6_13_0 [label="Attention_Weighted_6_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_6_13_0 [label="Attention_Output_6_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_6_13 [label="Attention_AllReduce_6_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_13_1 [label="QKV_Proj_6_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_6_13_1 [label="Attention_Scores_6_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_6_13_1 [label="Attention_Softmax_6_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_6_13_1 [label="Attention_Dropout_6_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_6_13_1 [label="Attention_Values_6_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_6_13_1 [label="Attention_Weighted_6_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_6_13_1 [label="Attention_Output_6_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_6_13 [label="Attention_AllReduce_6_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_13_2 [label="QKV_Proj_6_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_6_13_2 [label="Attention_Scores_6_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_6_13_2 [label="Attention_Softmax_6_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_6_13_2 [label="Attention_Dropout_6_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_6_13_2 [label="Attention_Values_6_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_6_13_2 [label="Attention_Weighted_6_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_6_13_2 [label="Attention_Output_6_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_6_13 [label="Attention_AllReduce_6_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_13_3 [label="QKV_Proj_6_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_6_13_3 [label="Attention_Scores_6_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_6_13_3 [label="Attention_Softmax_6_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_6_13_3 [label="Attention_Dropout_6_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_6_13_3 [label="Attention_Values_6_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_6_13_3 [label="Attention_Weighted_6_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_6_13_3 [label="Attention_Output_6_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_6_13 [label="Attention_AllReduce_6_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_14_0 [label="QKV_Proj_6_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_6_14_0 [label="Attention_Scores_6_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_6_14_0 [label="Attention_Softmax_6_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_6_14_0 [label="Attention_Dropout_6_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_6_14_0 [label="Attention_Values_6_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_6_14_0 [label="Attention_Weighted_6_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_6_14_0 [label="Attention_Output_6_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_6_14 [label="Attention_AllReduce_6_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_14_1 [label="QKV_Proj_6_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_6_14_1 [label="Attention_Scores_6_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_6_14_1 [label="Attention_Softmax_6_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_6_14_1 [label="Attention_Dropout_6_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_6_14_1 [label="Attention_Values_6_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_6_14_1 [label="Attention_Weighted_6_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_6_14_1 [label="Attention_Output_6_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_6_14 [label="Attention_AllReduce_6_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_14_2 [label="QKV_Proj_6_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_6_14_2 [label="Attention_Scores_6_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_6_14_2 [label="Attention_Softmax_6_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_6_14_2 [label="Attention_Dropout_6_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_6_14_2 [label="Attention_Values_6_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_6_14_2 [label="Attention_Weighted_6_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_6_14_2 [label="Attention_Output_6_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_6_14 [label="Attention_AllReduce_6_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_14_3 [label="QKV_Proj_6_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_6_14_3 [label="Attention_Scores_6_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_6_14_3 [label="Attention_Softmax_6_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_6_14_3 [label="Attention_Dropout_6_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_6_14_3 [label="Attention_Values_6_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_6_14_3 [label="Attention_Weighted_6_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_6_14_3 [label="Attention_Output_6_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_6_14 [label="Attention_AllReduce_6_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_15_0 [label="QKV_Proj_6_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_6_15_0 [label="Attention_Scores_6_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_6_15_0 [label="Attention_Softmax_6_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_6_15_0 [label="Attention_Dropout_6_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_6_15_0 [label="Attention_Values_6_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_6_15_0 [label="Attention_Weighted_6_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_6_15_0 [label="Attention_Output_6_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_6_15 [label="Attention_AllReduce_6_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_15_1 [label="QKV_Proj_6_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_6_15_1 [label="Attention_Scores_6_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_6_15_1 [label="Attention_Softmax_6_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_6_15_1 [label="Attention_Dropout_6_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_6_15_1 [label="Attention_Values_6_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_6_15_1 [label="Attention_Weighted_6_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_6_15_1 [label="Attention_Output_6_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_6_15 [label="Attention_AllReduce_6_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_15_2 [label="QKV_Proj_6_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_6_15_2 [label="Attention_Scores_6_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_6_15_2 [label="Attention_Softmax_6_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_6_15_2 [label="Attention_Dropout_6_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_6_15_2 [label="Attention_Values_6_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_6_15_2 [label="Attention_Weighted_6_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_6_15_2 [label="Attention_Output_6_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_6_15 [label="Attention_AllReduce_6_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_6_15_3 [label="QKV_Proj_6_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_6_15_3 [label="Attention_Scores_6_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_6_15_3 [label="Attention_Softmax_6_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_6_15_3 [label="Attention_Dropout_6_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_6_15_3 [label="Attention_Values_6_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_6_15_3 [label="Attention_Weighted_6_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_6_15_3 [label="Attention_Output_6_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_6_15 [label="Attention_AllReduce_6_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_0_0 [label="MLP_Linear1_6_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_6_0_0 [label="GELU_6_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_6_0_0 [label="MLP_Linear2_6_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_6_0_1 [label="MLP_Linear1_6_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_6_0_1 [label="GELU_6_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_6_0_1 [label="MLP_Linear2_6_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_6_0_2 [label="MLP_Linear1_6_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_6_0_2 [label="GELU_6_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_6_0_2 [label="MLP_Linear2_6_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_6_0_3 [label="MLP_Linear1_6_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_6_0_3 [label="GELU_6_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_6_0_3 [label="MLP_Linear2_6_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_6_0 [label="MLP_AllReduce_6_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_1_0 [label="MLP_Linear1_6_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_6_1_0 [label="GELU_6_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_6_1_0 [label="MLP_Linear2_6_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_6_1_1 [label="MLP_Linear1_6_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_6_1_1 [label="GELU_6_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_6_1_1 [label="MLP_Linear2_6_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_6_1_2 [label="MLP_Linear1_6_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_6_1_2 [label="GELU_6_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_6_1_2 [label="MLP_Linear2_6_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_6_1_3 [label="MLP_Linear1_6_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_6_1_3 [label="GELU_6_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_6_1_3 [label="MLP_Linear2_6_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_6_1 [label="MLP_AllReduce_6_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_2_0 [label="MLP_Linear1_6_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_6_2_0 [label="GELU_6_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_6_2_0 [label="MLP_Linear2_6_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_6_2_1 [label="MLP_Linear1_6_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_6_2_1 [label="GELU_6_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_6_2_1 [label="MLP_Linear2_6_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_6_2_2 [label="MLP_Linear1_6_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_6_2_2 [label="GELU_6_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_6_2_2 [label="MLP_Linear2_6_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_6_2_3 [label="MLP_Linear1_6_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_6_2_3 [label="GELU_6_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_6_2_3 [label="MLP_Linear2_6_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_6_2 [label="MLP_AllReduce_6_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_3_0 [label="MLP_Linear1_6_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_6_3_0 [label="GELU_6_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_6_3_0 [label="MLP_Linear2_6_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_6_3_1 [label="MLP_Linear1_6_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_6_3_1 [label="GELU_6_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_6_3_1 [label="MLP_Linear2_6_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_6_3_2 [label="MLP_Linear1_6_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_6_3_2 [label="GELU_6_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_6_3_2 [label="MLP_Linear2_6_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_6_3_3 [label="MLP_Linear1_6_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_6_3_3 [label="GELU_6_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_6_3_3 [label="MLP_Linear2_6_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_6_3 [label="MLP_AllReduce_6_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_4_0 [label="MLP_Linear1_6_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_6_4_0 [label="GELU_6_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_6_4_0 [label="MLP_Linear2_6_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_6_4_1 [label="MLP_Linear1_6_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_6_4_1 [label="GELU_6_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_6_4_1 [label="MLP_Linear2_6_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_6_4_2 [label="MLP_Linear1_6_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_6_4_2 [label="GELU_6_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_6_4_2 [label="MLP_Linear2_6_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_6_4_3 [label="MLP_Linear1_6_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_6_4_3 [label="GELU_6_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_6_4_3 [label="MLP_Linear2_6_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_6_4 [label="MLP_AllReduce_6_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_5_0 [label="MLP_Linear1_6_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_6_5_0 [label="GELU_6_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_6_5_0 [label="MLP_Linear2_6_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_6_5_1 [label="MLP_Linear1_6_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_6_5_1 [label="GELU_6_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_6_5_1 [label="MLP_Linear2_6_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_6_5_2 [label="MLP_Linear1_6_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_6_5_2 [label="GELU_6_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_6_5_2 [label="MLP_Linear2_6_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_6_5_3 [label="MLP_Linear1_6_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_6_5_3 [label="GELU_6_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_6_5_3 [label="MLP_Linear2_6_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_6_5 [label="MLP_AllReduce_6_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_6_0 [label="MLP_Linear1_6_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_6_6_0 [label="GELU_6_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_6_6_0 [label="MLP_Linear2_6_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_6_6_1 [label="MLP_Linear1_6_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_6_6_1 [label="GELU_6_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_6_6_1 [label="MLP_Linear2_6_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_6_6_2 [label="MLP_Linear1_6_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_6_6_2 [label="GELU_6_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_6_6_2 [label="MLP_Linear2_6_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_6_6_3 [label="MLP_Linear1_6_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_6_6_3 [label="GELU_6_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_6_6_3 [label="MLP_Linear2_6_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_6_6 [label="MLP_AllReduce_6_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_7_0 [label="MLP_Linear1_6_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_6_7_0 [label="GELU_6_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_6_7_0 [label="MLP_Linear2_6_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_6_7_1 [label="MLP_Linear1_6_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_6_7_1 [label="GELU_6_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_6_7_1 [label="MLP_Linear2_6_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_6_7_2 [label="MLP_Linear1_6_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_6_7_2 [label="GELU_6_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_6_7_2 [label="MLP_Linear2_6_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_6_7_3 [label="MLP_Linear1_6_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_6_7_3 [label="GELU_6_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_6_7_3 [label="MLP_Linear2_6_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_6_7 [label="MLP_AllReduce_6_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_8_0 [label="MLP_Linear1_6_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_6_8_0 [label="GELU_6_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_6_8_0 [label="MLP_Linear2_6_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_6_8_1 [label="MLP_Linear1_6_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_6_8_1 [label="GELU_6_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_6_8_1 [label="MLP_Linear2_6_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_6_8_2 [label="MLP_Linear1_6_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_6_8_2 [label="GELU_6_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_6_8_2 [label="MLP_Linear2_6_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_6_8_3 [label="MLP_Linear1_6_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_6_8_3 [label="GELU_6_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_6_8_3 [label="MLP_Linear2_6_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_6_8 [label="MLP_AllReduce_6_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_9_0 [label="MLP_Linear1_6_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_6_9_0 [label="GELU_6_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_6_9_0 [label="MLP_Linear2_6_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_6_9_1 [label="MLP_Linear1_6_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_6_9_1 [label="GELU_6_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_6_9_1 [label="MLP_Linear2_6_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_6_9_2 [label="MLP_Linear1_6_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_6_9_2 [label="GELU_6_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_6_9_2 [label="MLP_Linear2_6_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_6_9_3 [label="MLP_Linear1_6_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_6_9_3 [label="GELU_6_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_6_9_3 [label="MLP_Linear2_6_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_6_9 [label="MLP_AllReduce_6_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_10_0 [label="MLP_Linear1_6_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_6_10_0 [label="GELU_6_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_6_10_0 [label="MLP_Linear2_6_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_6_10_1 [label="MLP_Linear1_6_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_6_10_1 [label="GELU_6_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_6_10_1 [label="MLP_Linear2_6_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_6_10_2 [label="MLP_Linear1_6_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_6_10_2 [label="GELU_6_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_6_10_2 [label="MLP_Linear2_6_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_6_10_3 [label="MLP_Linear1_6_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_6_10_3 [label="GELU_6_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_6_10_3 [label="MLP_Linear2_6_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_6_10 [label="MLP_AllReduce_6_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_11_0 [label="MLP_Linear1_6_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_6_11_0 [label="GELU_6_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_6_11_0 [label="MLP_Linear2_6_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_6_11_1 [label="MLP_Linear1_6_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_6_11_1 [label="GELU_6_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_6_11_1 [label="MLP_Linear2_6_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_6_11_2 [label="MLP_Linear1_6_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_6_11_2 [label="GELU_6_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_6_11_2 [label="MLP_Linear2_6_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_6_11_3 [label="MLP_Linear1_6_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_6_11_3 [label="GELU_6_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_6_11_3 [label="MLP_Linear2_6_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_6_11 [label="MLP_AllReduce_6_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_12_0 [label="MLP_Linear1_6_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_6_12_0 [label="GELU_6_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_6_12_0 [label="MLP_Linear2_6_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_6_12_1 [label="MLP_Linear1_6_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_6_12_1 [label="GELU_6_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_6_12_1 [label="MLP_Linear2_6_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_6_12_2 [label="MLP_Linear1_6_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_6_12_2 [label="GELU_6_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_6_12_2 [label="MLP_Linear2_6_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_6_12_3 [label="MLP_Linear1_6_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_6_12_3 [label="GELU_6_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_6_12_3 [label="MLP_Linear2_6_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_6_12 [label="MLP_AllReduce_6_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_13_0 [label="MLP_Linear1_6_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_6_13_0 [label="GELU_6_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_6_13_0 [label="MLP_Linear2_6_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_6_13_1 [label="MLP_Linear1_6_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_6_13_1 [label="GELU_6_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_6_13_1 [label="MLP_Linear2_6_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_6_13_2 [label="MLP_Linear1_6_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_6_13_2 [label="GELU_6_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_6_13_2 [label="MLP_Linear2_6_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_6_13_3 [label="MLP_Linear1_6_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_6_13_3 [label="GELU_6_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_6_13_3 [label="MLP_Linear2_6_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_6_13 [label="MLP_AllReduce_6_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_14_0 [label="MLP_Linear1_6_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_6_14_0 [label="GELU_6_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_6_14_0 [label="MLP_Linear2_6_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_6_14_1 [label="MLP_Linear1_6_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_6_14_1 [label="GELU_6_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_6_14_1 [label="MLP_Linear2_6_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_6_14_2 [label="MLP_Linear1_6_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_6_14_2 [label="GELU_6_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_6_14_2 [label="MLP_Linear2_6_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_6_14_3 [label="MLP_Linear1_6_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_6_14_3 [label="GELU_6_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_6_14_3 [label="MLP_Linear2_6_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_6_14 [label="MLP_AllReduce_6_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_6_15_0 [label="MLP_Linear1_6_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_6_15_0 [label="GELU_6_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_6_15_0 [label="MLP_Linear2_6_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_6_15_1 [label="MLP_Linear1_6_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_6_15_1 [label="GELU_6_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_6_15_1 [label="MLP_Linear2_6_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_6_15_2 [label="MLP_Linear1_6_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_6_15_2 [label="GELU_6_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_6_15_2 [label="MLP_Linear2_6_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_6_15_3 [label="MLP_Linear1_6_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_6_15_3 [label="GELU_6_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_6_15_3 [label="MLP_Linear2_6_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_6_15 [label="MLP_AllReduce_6_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_6_0 [label="Expert_Route_6_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_1 [label="Expert_Route_6_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_2 [label="Expert_Route_6_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_3 [label="Expert_Route_6_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_4 [label="Expert_Route_6_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_5 [label="Expert_Route_6_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_6 [label="Expert_Route_6_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_7 [label="Expert_Route_6_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_8 [label="Expert_Route_6_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_9 [label="Expert_Route_6_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_10 [label="Expert_Route_6_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_11 [label="Expert_Route_6_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_12 [label="Expert_Route_6_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_13 [label="Expert_Route_6_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_14 [label="Expert_Route_6_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_6_15 [label="Expert_Route_6_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_7_0_0 [label="QKV_Proj_7_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_7_0_0 [label="Attention_Scores_7_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_7_0_0 [label="Attention_Softmax_7_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_7_0_0 [label="Attention_Dropout_7_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_7_0_0 [label="Attention_Values_7_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_7_0_0 [label="Attention_Weighted_7_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_7_0_0 [label="Attention_Output_7_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_7_0 [label="Attention_AllReduce_7_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_0_1 [label="QKV_Proj_7_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_7_0_1 [label="Attention_Scores_7_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_7_0_1 [label="Attention_Softmax_7_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_7_0_1 [label="Attention_Dropout_7_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_7_0_1 [label="Attention_Values_7_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_7_0_1 [label="Attention_Weighted_7_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_7_0_1 [label="Attention_Output_7_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_7_0 [label="Attention_AllReduce_7_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_0_2 [label="QKV_Proj_7_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_7_0_2 [label="Attention_Scores_7_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_7_0_2 [label="Attention_Softmax_7_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_7_0_2 [label="Attention_Dropout_7_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_7_0_2 [label="Attention_Values_7_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_7_0_2 [label="Attention_Weighted_7_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_7_0_2 [label="Attention_Output_7_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_7_0 [label="Attention_AllReduce_7_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_0_3 [label="QKV_Proj_7_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_7_0_3 [label="Attention_Scores_7_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_7_0_3 [label="Attention_Softmax_7_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_7_0_3 [label="Attention_Dropout_7_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_7_0_3 [label="Attention_Values_7_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_7_0_3 [label="Attention_Weighted_7_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_7_0_3 [label="Attention_Output_7_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_7_0 [label="Attention_AllReduce_7_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_1_0 [label="QKV_Proj_7_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_7_1_0 [label="Attention_Scores_7_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_7_1_0 [label="Attention_Softmax_7_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_7_1_0 [label="Attention_Dropout_7_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_7_1_0 [label="Attention_Values_7_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_7_1_0 [label="Attention_Weighted_7_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_7_1_0 [label="Attention_Output_7_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_7_1 [label="Attention_AllReduce_7_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_1_1 [label="QKV_Proj_7_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_7_1_1 [label="Attention_Scores_7_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_7_1_1 [label="Attention_Softmax_7_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_7_1_1 [label="Attention_Dropout_7_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_7_1_1 [label="Attention_Values_7_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_7_1_1 [label="Attention_Weighted_7_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_7_1_1 [label="Attention_Output_7_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_7_1 [label="Attention_AllReduce_7_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_1_2 [label="QKV_Proj_7_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_7_1_2 [label="Attention_Scores_7_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_7_1_2 [label="Attention_Softmax_7_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_7_1_2 [label="Attention_Dropout_7_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_7_1_2 [label="Attention_Values_7_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_7_1_2 [label="Attention_Weighted_7_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_7_1_2 [label="Attention_Output_7_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_7_1 [label="Attention_AllReduce_7_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_1_3 [label="QKV_Proj_7_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_7_1_3 [label="Attention_Scores_7_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_7_1_3 [label="Attention_Softmax_7_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_7_1_3 [label="Attention_Dropout_7_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_7_1_3 [label="Attention_Values_7_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_7_1_3 [label="Attention_Weighted_7_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_7_1_3 [label="Attention_Output_7_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_7_1 [label="Attention_AllReduce_7_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_2_0 [label="QKV_Proj_7_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_7_2_0 [label="Attention_Scores_7_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_7_2_0 [label="Attention_Softmax_7_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_7_2_0 [label="Attention_Dropout_7_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_7_2_0 [label="Attention_Values_7_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_7_2_0 [label="Attention_Weighted_7_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_7_2_0 [label="Attention_Output_7_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_7_2 [label="Attention_AllReduce_7_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_2_1 [label="QKV_Proj_7_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_7_2_1 [label="Attention_Scores_7_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_7_2_1 [label="Attention_Softmax_7_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_7_2_1 [label="Attention_Dropout_7_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_7_2_1 [label="Attention_Values_7_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_7_2_1 [label="Attention_Weighted_7_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_7_2_1 [label="Attention_Output_7_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_7_2 [label="Attention_AllReduce_7_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_2_2 [label="QKV_Proj_7_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_7_2_2 [label="Attention_Scores_7_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_7_2_2 [label="Attention_Softmax_7_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_7_2_2 [label="Attention_Dropout_7_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_7_2_2 [label="Attention_Values_7_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_7_2_2 [label="Attention_Weighted_7_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_7_2_2 [label="Attention_Output_7_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_7_2 [label="Attention_AllReduce_7_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_2_3 [label="QKV_Proj_7_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_7_2_3 [label="Attention_Scores_7_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_7_2_3 [label="Attention_Softmax_7_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_7_2_3 [label="Attention_Dropout_7_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_7_2_3 [label="Attention_Values_7_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_7_2_3 [label="Attention_Weighted_7_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_7_2_3 [label="Attention_Output_7_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_7_2 [label="Attention_AllReduce_7_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_3_0 [label="QKV_Proj_7_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_7_3_0 [label="Attention_Scores_7_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_7_3_0 [label="Attention_Softmax_7_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_7_3_0 [label="Attention_Dropout_7_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_7_3_0 [label="Attention_Values_7_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_7_3_0 [label="Attention_Weighted_7_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_7_3_0 [label="Attention_Output_7_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_7_3 [label="Attention_AllReduce_7_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_3_1 [label="QKV_Proj_7_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_7_3_1 [label="Attention_Scores_7_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_7_3_1 [label="Attention_Softmax_7_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_7_3_1 [label="Attention_Dropout_7_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_7_3_1 [label="Attention_Values_7_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_7_3_1 [label="Attention_Weighted_7_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_7_3_1 [label="Attention_Output_7_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_7_3 [label="Attention_AllReduce_7_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_3_2 [label="QKV_Proj_7_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_7_3_2 [label="Attention_Scores_7_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_7_3_2 [label="Attention_Softmax_7_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_7_3_2 [label="Attention_Dropout_7_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_7_3_2 [label="Attention_Values_7_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_7_3_2 [label="Attention_Weighted_7_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_7_3_2 [label="Attention_Output_7_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_7_3 [label="Attention_AllReduce_7_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_3_3 [label="QKV_Proj_7_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_7_3_3 [label="Attention_Scores_7_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_7_3_3 [label="Attention_Softmax_7_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_7_3_3 [label="Attention_Dropout_7_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_7_3_3 [label="Attention_Values_7_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_7_3_3 [label="Attention_Weighted_7_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_7_3_3 [label="Attention_Output_7_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_7_3 [label="Attention_AllReduce_7_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_4_0 [label="QKV_Proj_7_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_7_4_0 [label="Attention_Scores_7_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_7_4_0 [label="Attention_Softmax_7_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_7_4_0 [label="Attention_Dropout_7_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_7_4_0 [label="Attention_Values_7_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_7_4_0 [label="Attention_Weighted_7_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_7_4_0 [label="Attention_Output_7_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_7_4 [label="Attention_AllReduce_7_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_4_1 [label="QKV_Proj_7_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_7_4_1 [label="Attention_Scores_7_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_7_4_1 [label="Attention_Softmax_7_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_7_4_1 [label="Attention_Dropout_7_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_7_4_1 [label="Attention_Values_7_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_7_4_1 [label="Attention_Weighted_7_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_7_4_1 [label="Attention_Output_7_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_7_4 [label="Attention_AllReduce_7_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_4_2 [label="QKV_Proj_7_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_7_4_2 [label="Attention_Scores_7_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_7_4_2 [label="Attention_Softmax_7_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_7_4_2 [label="Attention_Dropout_7_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_7_4_2 [label="Attention_Values_7_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_7_4_2 [label="Attention_Weighted_7_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_7_4_2 [label="Attention_Output_7_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_7_4 [label="Attention_AllReduce_7_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_4_3 [label="QKV_Proj_7_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_7_4_3 [label="Attention_Scores_7_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_7_4_3 [label="Attention_Softmax_7_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_7_4_3 [label="Attention_Dropout_7_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_7_4_3 [label="Attention_Values_7_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_7_4_3 [label="Attention_Weighted_7_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_7_4_3 [label="Attention_Output_7_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_7_4 [label="Attention_AllReduce_7_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_5_0 [label="QKV_Proj_7_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_7_5_0 [label="Attention_Scores_7_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_7_5_0 [label="Attention_Softmax_7_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_7_5_0 [label="Attention_Dropout_7_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_7_5_0 [label="Attention_Values_7_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_7_5_0 [label="Attention_Weighted_7_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_7_5_0 [label="Attention_Output_7_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_7_5 [label="Attention_AllReduce_7_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_5_1 [label="QKV_Proj_7_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_7_5_1 [label="Attention_Scores_7_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_7_5_1 [label="Attention_Softmax_7_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_7_5_1 [label="Attention_Dropout_7_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_7_5_1 [label="Attention_Values_7_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_7_5_1 [label="Attention_Weighted_7_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_7_5_1 [label="Attention_Output_7_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_7_5 [label="Attention_AllReduce_7_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_5_2 [label="QKV_Proj_7_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_7_5_2 [label="Attention_Scores_7_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_7_5_2 [label="Attention_Softmax_7_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_7_5_2 [label="Attention_Dropout_7_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_7_5_2 [label="Attention_Values_7_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_7_5_2 [label="Attention_Weighted_7_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_7_5_2 [label="Attention_Output_7_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_7_5 [label="Attention_AllReduce_7_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_5_3 [label="QKV_Proj_7_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_7_5_3 [label="Attention_Scores_7_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_7_5_3 [label="Attention_Softmax_7_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_7_5_3 [label="Attention_Dropout_7_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_7_5_3 [label="Attention_Values_7_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_7_5_3 [label="Attention_Weighted_7_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_7_5_3 [label="Attention_Output_7_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_7_5 [label="Attention_AllReduce_7_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_6_0 [label="QKV_Proj_7_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_7_6_0 [label="Attention_Scores_7_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_7_6_0 [label="Attention_Softmax_7_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_7_6_0 [label="Attention_Dropout_7_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_7_6_0 [label="Attention_Values_7_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_7_6_0 [label="Attention_Weighted_7_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_7_6_0 [label="Attention_Output_7_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_7_6 [label="Attention_AllReduce_7_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_6_1 [label="QKV_Proj_7_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_7_6_1 [label="Attention_Scores_7_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_7_6_1 [label="Attention_Softmax_7_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_7_6_1 [label="Attention_Dropout_7_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_7_6_1 [label="Attention_Values_7_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_7_6_1 [label="Attention_Weighted_7_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_7_6_1 [label="Attention_Output_7_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_7_6 [label="Attention_AllReduce_7_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_6_2 [label="QKV_Proj_7_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_7_6_2 [label="Attention_Scores_7_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_7_6_2 [label="Attention_Softmax_7_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_7_6_2 [label="Attention_Dropout_7_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_7_6_2 [label="Attention_Values_7_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_7_6_2 [label="Attention_Weighted_7_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_7_6_2 [label="Attention_Output_7_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_7_6 [label="Attention_AllReduce_7_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_6_3 [label="QKV_Proj_7_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_7_6_3 [label="Attention_Scores_7_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_7_6_3 [label="Attention_Softmax_7_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_7_6_3 [label="Attention_Dropout_7_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_7_6_3 [label="Attention_Values_7_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_7_6_3 [label="Attention_Weighted_7_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_7_6_3 [label="Attention_Output_7_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_7_6 [label="Attention_AllReduce_7_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_7_0 [label="QKV_Proj_7_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_7_7_0 [label="Attention_Scores_7_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_7_7_0 [label="Attention_Softmax_7_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_7_7_0 [label="Attention_Dropout_7_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_7_7_0 [label="Attention_Values_7_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_7_7_0 [label="Attention_Weighted_7_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_7_7_0 [label="Attention_Output_7_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_7_7 [label="Attention_AllReduce_7_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_7_1 [label="QKV_Proj_7_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_7_7_1 [label="Attention_Scores_7_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_7_7_1 [label="Attention_Softmax_7_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_7_7_1 [label="Attention_Dropout_7_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_7_7_1 [label="Attention_Values_7_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_7_7_1 [label="Attention_Weighted_7_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_7_7_1 [label="Attention_Output_7_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_7_7 [label="Attention_AllReduce_7_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_7_2 [label="QKV_Proj_7_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_7_7_2 [label="Attention_Scores_7_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_7_7_2 [label="Attention_Softmax_7_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_7_7_2 [label="Attention_Dropout_7_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_7_7_2 [label="Attention_Values_7_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_7_7_2 [label="Attention_Weighted_7_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_7_7_2 [label="Attention_Output_7_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_7_7 [label="Attention_AllReduce_7_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_7_3 [label="QKV_Proj_7_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_7_7_3 [label="Attention_Scores_7_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_7_7_3 [label="Attention_Softmax_7_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_7_7_3 [label="Attention_Dropout_7_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_7_7_3 [label="Attention_Values_7_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_7_7_3 [label="Attention_Weighted_7_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_7_7_3 [label="Attention_Output_7_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_7_7 [label="Attention_AllReduce_7_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_8_0 [label="QKV_Proj_7_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_7_8_0 [label="Attention_Scores_7_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_7_8_0 [label="Attention_Softmax_7_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_7_8_0 [label="Attention_Dropout_7_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_7_8_0 [label="Attention_Values_7_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_7_8_0 [label="Attention_Weighted_7_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_7_8_0 [label="Attention_Output_7_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_7_8 [label="Attention_AllReduce_7_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_8_1 [label="QKV_Proj_7_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_7_8_1 [label="Attention_Scores_7_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_7_8_1 [label="Attention_Softmax_7_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_7_8_1 [label="Attention_Dropout_7_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_7_8_1 [label="Attention_Values_7_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_7_8_1 [label="Attention_Weighted_7_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_7_8_1 [label="Attention_Output_7_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_7_8 [label="Attention_AllReduce_7_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_8_2 [label="QKV_Proj_7_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_7_8_2 [label="Attention_Scores_7_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_7_8_2 [label="Attention_Softmax_7_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_7_8_2 [label="Attention_Dropout_7_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_7_8_2 [label="Attention_Values_7_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_7_8_2 [label="Attention_Weighted_7_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_7_8_2 [label="Attention_Output_7_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_7_8 [label="Attention_AllReduce_7_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_8_3 [label="QKV_Proj_7_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_7_8_3 [label="Attention_Scores_7_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_7_8_3 [label="Attention_Softmax_7_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_7_8_3 [label="Attention_Dropout_7_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_7_8_3 [label="Attention_Values_7_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_7_8_3 [label="Attention_Weighted_7_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_7_8_3 [label="Attention_Output_7_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_7_8 [label="Attention_AllReduce_7_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_9_0 [label="QKV_Proj_7_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_7_9_0 [label="Attention_Scores_7_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_7_9_0 [label="Attention_Softmax_7_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_7_9_0 [label="Attention_Dropout_7_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_7_9_0 [label="Attention_Values_7_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_7_9_0 [label="Attention_Weighted_7_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_7_9_0 [label="Attention_Output_7_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_7_9 [label="Attention_AllReduce_7_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_9_1 [label="QKV_Proj_7_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_7_9_1 [label="Attention_Scores_7_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_7_9_1 [label="Attention_Softmax_7_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_7_9_1 [label="Attention_Dropout_7_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_7_9_1 [label="Attention_Values_7_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_7_9_1 [label="Attention_Weighted_7_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_7_9_1 [label="Attention_Output_7_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_7_9 [label="Attention_AllReduce_7_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_9_2 [label="QKV_Proj_7_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_7_9_2 [label="Attention_Scores_7_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_7_9_2 [label="Attention_Softmax_7_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_7_9_2 [label="Attention_Dropout_7_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_7_9_2 [label="Attention_Values_7_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_7_9_2 [label="Attention_Weighted_7_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_7_9_2 [label="Attention_Output_7_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_7_9 [label="Attention_AllReduce_7_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_9_3 [label="QKV_Proj_7_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_7_9_3 [label="Attention_Scores_7_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_7_9_3 [label="Attention_Softmax_7_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_7_9_3 [label="Attention_Dropout_7_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_7_9_3 [label="Attention_Values_7_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_7_9_3 [label="Attention_Weighted_7_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_7_9_3 [label="Attention_Output_7_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_7_9 [label="Attention_AllReduce_7_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_10_0 [label="QKV_Proj_7_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_7_10_0 [label="Attention_Scores_7_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_7_10_0 [label="Attention_Softmax_7_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_7_10_0 [label="Attention_Dropout_7_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_7_10_0 [label="Attention_Values_7_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_7_10_0 [label="Attention_Weighted_7_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_7_10_0 [label="Attention_Output_7_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_7_10 [label="Attention_AllReduce_7_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_10_1 [label="QKV_Proj_7_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_7_10_1 [label="Attention_Scores_7_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_7_10_1 [label="Attention_Softmax_7_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_7_10_1 [label="Attention_Dropout_7_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_7_10_1 [label="Attention_Values_7_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_7_10_1 [label="Attention_Weighted_7_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_7_10_1 [label="Attention_Output_7_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_7_10 [label="Attention_AllReduce_7_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_10_2 [label="QKV_Proj_7_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_7_10_2 [label="Attention_Scores_7_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_7_10_2 [label="Attention_Softmax_7_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_7_10_2 [label="Attention_Dropout_7_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_7_10_2 [label="Attention_Values_7_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_7_10_2 [label="Attention_Weighted_7_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_7_10_2 [label="Attention_Output_7_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_7_10 [label="Attention_AllReduce_7_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_10_3 [label="QKV_Proj_7_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_7_10_3 [label="Attention_Scores_7_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_7_10_3 [label="Attention_Softmax_7_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_7_10_3 [label="Attention_Dropout_7_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_7_10_3 [label="Attention_Values_7_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_7_10_3 [label="Attention_Weighted_7_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_7_10_3 [label="Attention_Output_7_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_7_10 [label="Attention_AllReduce_7_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_11_0 [label="QKV_Proj_7_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_7_11_0 [label="Attention_Scores_7_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_7_11_0 [label="Attention_Softmax_7_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_7_11_0 [label="Attention_Dropout_7_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_7_11_0 [label="Attention_Values_7_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_7_11_0 [label="Attention_Weighted_7_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_7_11_0 [label="Attention_Output_7_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_7_11 [label="Attention_AllReduce_7_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_11_1 [label="QKV_Proj_7_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_7_11_1 [label="Attention_Scores_7_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_7_11_1 [label="Attention_Softmax_7_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_7_11_1 [label="Attention_Dropout_7_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_7_11_1 [label="Attention_Values_7_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_7_11_1 [label="Attention_Weighted_7_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_7_11_1 [label="Attention_Output_7_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_7_11 [label="Attention_AllReduce_7_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_11_2 [label="QKV_Proj_7_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_7_11_2 [label="Attention_Scores_7_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_7_11_2 [label="Attention_Softmax_7_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_7_11_2 [label="Attention_Dropout_7_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_7_11_2 [label="Attention_Values_7_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_7_11_2 [label="Attention_Weighted_7_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_7_11_2 [label="Attention_Output_7_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_7_11 [label="Attention_AllReduce_7_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_11_3 [label="QKV_Proj_7_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_7_11_3 [label="Attention_Scores_7_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_7_11_3 [label="Attention_Softmax_7_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_7_11_3 [label="Attention_Dropout_7_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_7_11_3 [label="Attention_Values_7_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_7_11_3 [label="Attention_Weighted_7_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_7_11_3 [label="Attention_Output_7_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_7_11 [label="Attention_AllReduce_7_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_12_0 [label="QKV_Proj_7_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_7_12_0 [label="Attention_Scores_7_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_7_12_0 [label="Attention_Softmax_7_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_7_12_0 [label="Attention_Dropout_7_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_7_12_0 [label="Attention_Values_7_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_7_12_0 [label="Attention_Weighted_7_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_7_12_0 [label="Attention_Output_7_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_7_12 [label="Attention_AllReduce_7_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_12_1 [label="QKV_Proj_7_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_7_12_1 [label="Attention_Scores_7_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_7_12_1 [label="Attention_Softmax_7_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_7_12_1 [label="Attention_Dropout_7_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_7_12_1 [label="Attention_Values_7_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_7_12_1 [label="Attention_Weighted_7_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_7_12_1 [label="Attention_Output_7_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_7_12 [label="Attention_AllReduce_7_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_12_2 [label="QKV_Proj_7_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_7_12_2 [label="Attention_Scores_7_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_7_12_2 [label="Attention_Softmax_7_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_7_12_2 [label="Attention_Dropout_7_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_7_12_2 [label="Attention_Values_7_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_7_12_2 [label="Attention_Weighted_7_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_7_12_2 [label="Attention_Output_7_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_7_12 [label="Attention_AllReduce_7_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_12_3 [label="QKV_Proj_7_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_7_12_3 [label="Attention_Scores_7_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_7_12_3 [label="Attention_Softmax_7_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_7_12_3 [label="Attention_Dropout_7_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_7_12_3 [label="Attention_Values_7_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_7_12_3 [label="Attention_Weighted_7_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_7_12_3 [label="Attention_Output_7_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_7_12 [label="Attention_AllReduce_7_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_13_0 [label="QKV_Proj_7_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_7_13_0 [label="Attention_Scores_7_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_7_13_0 [label="Attention_Softmax_7_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_7_13_0 [label="Attention_Dropout_7_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_7_13_0 [label="Attention_Values_7_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_7_13_0 [label="Attention_Weighted_7_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_7_13_0 [label="Attention_Output_7_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_7_13 [label="Attention_AllReduce_7_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_13_1 [label="QKV_Proj_7_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_7_13_1 [label="Attention_Scores_7_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_7_13_1 [label="Attention_Softmax_7_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_7_13_1 [label="Attention_Dropout_7_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_7_13_1 [label="Attention_Values_7_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_7_13_1 [label="Attention_Weighted_7_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_7_13_1 [label="Attention_Output_7_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_7_13 [label="Attention_AllReduce_7_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_13_2 [label="QKV_Proj_7_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_7_13_2 [label="Attention_Scores_7_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_7_13_2 [label="Attention_Softmax_7_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_7_13_2 [label="Attention_Dropout_7_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_7_13_2 [label="Attention_Values_7_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_7_13_2 [label="Attention_Weighted_7_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_7_13_2 [label="Attention_Output_7_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_7_13 [label="Attention_AllReduce_7_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_13_3 [label="QKV_Proj_7_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_7_13_3 [label="Attention_Scores_7_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_7_13_3 [label="Attention_Softmax_7_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_7_13_3 [label="Attention_Dropout_7_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_7_13_3 [label="Attention_Values_7_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_7_13_3 [label="Attention_Weighted_7_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_7_13_3 [label="Attention_Output_7_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_7_13 [label="Attention_AllReduce_7_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_14_0 [label="QKV_Proj_7_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_7_14_0 [label="Attention_Scores_7_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_7_14_0 [label="Attention_Softmax_7_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_7_14_0 [label="Attention_Dropout_7_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_7_14_0 [label="Attention_Values_7_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_7_14_0 [label="Attention_Weighted_7_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_7_14_0 [label="Attention_Output_7_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_7_14 [label="Attention_AllReduce_7_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_14_1 [label="QKV_Proj_7_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_7_14_1 [label="Attention_Scores_7_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_7_14_1 [label="Attention_Softmax_7_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_7_14_1 [label="Attention_Dropout_7_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_7_14_1 [label="Attention_Values_7_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_7_14_1 [label="Attention_Weighted_7_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_7_14_1 [label="Attention_Output_7_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_7_14 [label="Attention_AllReduce_7_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_14_2 [label="QKV_Proj_7_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_7_14_2 [label="Attention_Scores_7_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_7_14_2 [label="Attention_Softmax_7_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_7_14_2 [label="Attention_Dropout_7_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_7_14_2 [label="Attention_Values_7_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_7_14_2 [label="Attention_Weighted_7_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_7_14_2 [label="Attention_Output_7_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_7_14 [label="Attention_AllReduce_7_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_14_3 [label="QKV_Proj_7_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_7_14_3 [label="Attention_Scores_7_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_7_14_3 [label="Attention_Softmax_7_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_7_14_3 [label="Attention_Dropout_7_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_7_14_3 [label="Attention_Values_7_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_7_14_3 [label="Attention_Weighted_7_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_7_14_3 [label="Attention_Output_7_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_7_14 [label="Attention_AllReduce_7_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_15_0 [label="QKV_Proj_7_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_7_15_0 [label="Attention_Scores_7_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_7_15_0 [label="Attention_Softmax_7_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_7_15_0 [label="Attention_Dropout_7_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_7_15_0 [label="Attention_Values_7_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_7_15_0 [label="Attention_Weighted_7_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_7_15_0 [label="Attention_Output_7_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_7_15 [label="Attention_AllReduce_7_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_15_1 [label="QKV_Proj_7_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_7_15_1 [label="Attention_Scores_7_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_7_15_1 [label="Attention_Softmax_7_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_7_15_1 [label="Attention_Dropout_7_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_7_15_1 [label="Attention_Values_7_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_7_15_1 [label="Attention_Weighted_7_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_7_15_1 [label="Attention_Output_7_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_7_15 [label="Attention_AllReduce_7_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_15_2 [label="QKV_Proj_7_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_7_15_2 [label="Attention_Scores_7_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_7_15_2 [label="Attention_Softmax_7_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_7_15_2 [label="Attention_Dropout_7_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_7_15_2 [label="Attention_Values_7_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_7_15_2 [label="Attention_Weighted_7_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_7_15_2 [label="Attention_Output_7_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_7_15 [label="Attention_AllReduce_7_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_7_15_3 [label="QKV_Proj_7_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_7_15_3 [label="Attention_Scores_7_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_7_15_3 [label="Attention_Softmax_7_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_7_15_3 [label="Attention_Dropout_7_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_7_15_3 [label="Attention_Values_7_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_7_15_3 [label="Attention_Weighted_7_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_7_15_3 [label="Attention_Output_7_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_7_15 [label="Attention_AllReduce_7_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_0_0 [label="MLP_Linear1_7_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_7_0_0 [label="GELU_7_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_7_0_0 [label="MLP_Linear2_7_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_7_0_1 [label="MLP_Linear1_7_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_7_0_1 [label="GELU_7_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_7_0_1 [label="MLP_Linear2_7_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_7_0_2 [label="MLP_Linear1_7_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_7_0_2 [label="GELU_7_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_7_0_2 [label="MLP_Linear2_7_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_7_0_3 [label="MLP_Linear1_7_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_7_0_3 [label="GELU_7_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_7_0_3 [label="MLP_Linear2_7_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_7_0 [label="MLP_AllReduce_7_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_1_0 [label="MLP_Linear1_7_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_7_1_0 [label="GELU_7_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_7_1_0 [label="MLP_Linear2_7_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_7_1_1 [label="MLP_Linear1_7_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_7_1_1 [label="GELU_7_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_7_1_1 [label="MLP_Linear2_7_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_7_1_2 [label="MLP_Linear1_7_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_7_1_2 [label="GELU_7_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_7_1_2 [label="MLP_Linear2_7_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_7_1_3 [label="MLP_Linear1_7_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_7_1_3 [label="GELU_7_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_7_1_3 [label="MLP_Linear2_7_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_7_1 [label="MLP_AllReduce_7_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_2_0 [label="MLP_Linear1_7_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_7_2_0 [label="GELU_7_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_7_2_0 [label="MLP_Linear2_7_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_7_2_1 [label="MLP_Linear1_7_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_7_2_1 [label="GELU_7_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_7_2_1 [label="MLP_Linear2_7_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_7_2_2 [label="MLP_Linear1_7_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_7_2_2 [label="GELU_7_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_7_2_2 [label="MLP_Linear2_7_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_7_2_3 [label="MLP_Linear1_7_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_7_2_3 [label="GELU_7_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_7_2_3 [label="MLP_Linear2_7_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_7_2 [label="MLP_AllReduce_7_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_3_0 [label="MLP_Linear1_7_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_7_3_0 [label="GELU_7_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_7_3_0 [label="MLP_Linear2_7_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_7_3_1 [label="MLP_Linear1_7_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_7_3_1 [label="GELU_7_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_7_3_1 [label="MLP_Linear2_7_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_7_3_2 [label="MLP_Linear1_7_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_7_3_2 [label="GELU_7_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_7_3_2 [label="MLP_Linear2_7_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_7_3_3 [label="MLP_Linear1_7_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_7_3_3 [label="GELU_7_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_7_3_3 [label="MLP_Linear2_7_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_7_3 [label="MLP_AllReduce_7_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_4_0 [label="MLP_Linear1_7_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_7_4_0 [label="GELU_7_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_7_4_0 [label="MLP_Linear2_7_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_7_4_1 [label="MLP_Linear1_7_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_7_4_1 [label="GELU_7_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_7_4_1 [label="MLP_Linear2_7_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_7_4_2 [label="MLP_Linear1_7_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_7_4_2 [label="GELU_7_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_7_4_2 [label="MLP_Linear2_7_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_7_4_3 [label="MLP_Linear1_7_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_7_4_3 [label="GELU_7_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_7_4_3 [label="MLP_Linear2_7_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_7_4 [label="MLP_AllReduce_7_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_5_0 [label="MLP_Linear1_7_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_7_5_0 [label="GELU_7_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_7_5_0 [label="MLP_Linear2_7_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_7_5_1 [label="MLP_Linear1_7_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_7_5_1 [label="GELU_7_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_7_5_1 [label="MLP_Linear2_7_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_7_5_2 [label="MLP_Linear1_7_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_7_5_2 [label="GELU_7_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_7_5_2 [label="MLP_Linear2_7_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_7_5_3 [label="MLP_Linear1_7_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_7_5_3 [label="GELU_7_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_7_5_3 [label="MLP_Linear2_7_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_7_5 [label="MLP_AllReduce_7_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_6_0 [label="MLP_Linear1_7_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_7_6_0 [label="GELU_7_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_7_6_0 [label="MLP_Linear2_7_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_7_6_1 [label="MLP_Linear1_7_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_7_6_1 [label="GELU_7_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_7_6_1 [label="MLP_Linear2_7_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_7_6_2 [label="MLP_Linear1_7_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_7_6_2 [label="GELU_7_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_7_6_2 [label="MLP_Linear2_7_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_7_6_3 [label="MLP_Linear1_7_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_7_6_3 [label="GELU_7_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_7_6_3 [label="MLP_Linear2_7_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_7_6 [label="MLP_AllReduce_7_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_7_0 [label="MLP_Linear1_7_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_7_7_0 [label="GELU_7_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_7_7_0 [label="MLP_Linear2_7_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_7_7_1 [label="MLP_Linear1_7_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_7_7_1 [label="GELU_7_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_7_7_1 [label="MLP_Linear2_7_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_7_7_2 [label="MLP_Linear1_7_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_7_7_2 [label="GELU_7_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_7_7_2 [label="MLP_Linear2_7_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_7_7_3 [label="MLP_Linear1_7_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_7_7_3 [label="GELU_7_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_7_7_3 [label="MLP_Linear2_7_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_7_7 [label="MLP_AllReduce_7_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_8_0 [label="MLP_Linear1_7_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_7_8_0 [label="GELU_7_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_7_8_0 [label="MLP_Linear2_7_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_7_8_1 [label="MLP_Linear1_7_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_7_8_1 [label="GELU_7_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_7_8_1 [label="MLP_Linear2_7_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_7_8_2 [label="MLP_Linear1_7_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_7_8_2 [label="GELU_7_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_7_8_2 [label="MLP_Linear2_7_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_7_8_3 [label="MLP_Linear1_7_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_7_8_3 [label="GELU_7_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_7_8_3 [label="MLP_Linear2_7_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_7_8 [label="MLP_AllReduce_7_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_9_0 [label="MLP_Linear1_7_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_7_9_0 [label="GELU_7_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_7_9_0 [label="MLP_Linear2_7_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_7_9_1 [label="MLP_Linear1_7_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_7_9_1 [label="GELU_7_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_7_9_1 [label="MLP_Linear2_7_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_7_9_2 [label="MLP_Linear1_7_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_7_9_2 [label="GELU_7_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_7_9_2 [label="MLP_Linear2_7_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_7_9_3 [label="MLP_Linear1_7_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_7_9_3 [label="GELU_7_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_7_9_3 [label="MLP_Linear2_7_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_7_9 [label="MLP_AllReduce_7_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_10_0 [label="MLP_Linear1_7_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_7_10_0 [label="GELU_7_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_7_10_0 [label="MLP_Linear2_7_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_7_10_1 [label="MLP_Linear1_7_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_7_10_1 [label="GELU_7_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_7_10_1 [label="MLP_Linear2_7_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_7_10_2 [label="MLP_Linear1_7_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_7_10_2 [label="GELU_7_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_7_10_2 [label="MLP_Linear2_7_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_7_10_3 [label="MLP_Linear1_7_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_7_10_3 [label="GELU_7_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_7_10_3 [label="MLP_Linear2_7_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_7_10 [label="MLP_AllReduce_7_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_11_0 [label="MLP_Linear1_7_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_7_11_0 [label="GELU_7_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_7_11_0 [label="MLP_Linear2_7_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_7_11_1 [label="MLP_Linear1_7_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_7_11_1 [label="GELU_7_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_7_11_1 [label="MLP_Linear2_7_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_7_11_2 [label="MLP_Linear1_7_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_7_11_2 [label="GELU_7_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_7_11_2 [label="MLP_Linear2_7_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_7_11_3 [label="MLP_Linear1_7_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_7_11_3 [label="GELU_7_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_7_11_3 [label="MLP_Linear2_7_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_7_11 [label="MLP_AllReduce_7_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_12_0 [label="MLP_Linear1_7_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_7_12_0 [label="GELU_7_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_7_12_0 [label="MLP_Linear2_7_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_7_12_1 [label="MLP_Linear1_7_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_7_12_1 [label="GELU_7_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_7_12_1 [label="MLP_Linear2_7_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_7_12_2 [label="MLP_Linear1_7_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_7_12_2 [label="GELU_7_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_7_12_2 [label="MLP_Linear2_7_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_7_12_3 [label="MLP_Linear1_7_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_7_12_3 [label="GELU_7_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_7_12_3 [label="MLP_Linear2_7_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_7_12 [label="MLP_AllReduce_7_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_13_0 [label="MLP_Linear1_7_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_7_13_0 [label="GELU_7_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_7_13_0 [label="MLP_Linear2_7_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_7_13_1 [label="MLP_Linear1_7_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_7_13_1 [label="GELU_7_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_7_13_1 [label="MLP_Linear2_7_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_7_13_2 [label="MLP_Linear1_7_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_7_13_2 [label="GELU_7_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_7_13_2 [label="MLP_Linear2_7_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_7_13_3 [label="MLP_Linear1_7_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_7_13_3 [label="GELU_7_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_7_13_3 [label="MLP_Linear2_7_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_7_13 [label="MLP_AllReduce_7_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_14_0 [label="MLP_Linear1_7_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_7_14_0 [label="GELU_7_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_7_14_0 [label="MLP_Linear2_7_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_7_14_1 [label="MLP_Linear1_7_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_7_14_1 [label="GELU_7_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_7_14_1 [label="MLP_Linear2_7_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_7_14_2 [label="MLP_Linear1_7_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_7_14_2 [label="GELU_7_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_7_14_2 [label="MLP_Linear2_7_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_7_14_3 [label="MLP_Linear1_7_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_7_14_3 [label="GELU_7_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_7_14_3 [label="MLP_Linear2_7_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_7_14 [label="MLP_AllReduce_7_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_7_15_0 [label="MLP_Linear1_7_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_7_15_0 [label="GELU_7_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_7_15_0 [label="MLP_Linear2_7_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_7_15_1 [label="MLP_Linear1_7_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_7_15_1 [label="GELU_7_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_7_15_1 [label="MLP_Linear2_7_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_7_15_2 [label="MLP_Linear1_7_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_7_15_2 [label="GELU_7_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_7_15_2 [label="MLP_Linear2_7_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_7_15_3 [label="MLP_Linear1_7_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_7_15_3 [label="GELU_7_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_7_15_3 [label="MLP_Linear2_7_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_7_15 [label="MLP_AllReduce_7_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_7_0 [label="Expert_Route_7_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_1 [label="Expert_Route_7_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_2 [label="Expert_Route_7_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_3 [label="Expert_Route_7_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_4 [label="Expert_Route_7_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_5 [label="Expert_Route_7_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_6 [label="Expert_Route_7_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_7 [label="Expert_Route_7_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_8 [label="Expert_Route_7_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_9 [label="Expert_Route_7_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_10 [label="Expert_Route_7_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_11 [label="Expert_Route_7_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_12 [label="Expert_Route_7_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_13 [label="Expert_Route_7_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_14 [label="Expert_Route_7_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_7_15 [label="Expert_Route_7_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_8_0_0 [label="QKV_Proj_8_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_8_0_0 [label="Attention_Scores_8_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_8_0_0 [label="Attention_Softmax_8_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_8_0_0 [label="Attention_Dropout_8_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_8_0_0 [label="Attention_Values_8_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_8_0_0 [label="Attention_Weighted_8_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_8_0_0 [label="Attention_Output_8_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_8_0 [label="Attention_AllReduce_8_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_0_1 [label="QKV_Proj_8_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_8_0_1 [label="Attention_Scores_8_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_8_0_1 [label="Attention_Softmax_8_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_8_0_1 [label="Attention_Dropout_8_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_8_0_1 [label="Attention_Values_8_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_8_0_1 [label="Attention_Weighted_8_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_8_0_1 [label="Attention_Output_8_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_8_0 [label="Attention_AllReduce_8_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_0_2 [label="QKV_Proj_8_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_8_0_2 [label="Attention_Scores_8_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_8_0_2 [label="Attention_Softmax_8_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_8_0_2 [label="Attention_Dropout_8_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_8_0_2 [label="Attention_Values_8_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_8_0_2 [label="Attention_Weighted_8_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_8_0_2 [label="Attention_Output_8_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_8_0 [label="Attention_AllReduce_8_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_0_3 [label="QKV_Proj_8_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_8_0_3 [label="Attention_Scores_8_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_8_0_3 [label="Attention_Softmax_8_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_8_0_3 [label="Attention_Dropout_8_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_8_0_3 [label="Attention_Values_8_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_8_0_3 [label="Attention_Weighted_8_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_8_0_3 [label="Attention_Output_8_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_8_0 [label="Attention_AllReduce_8_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_1_0 [label="QKV_Proj_8_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_8_1_0 [label="Attention_Scores_8_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_8_1_0 [label="Attention_Softmax_8_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_8_1_0 [label="Attention_Dropout_8_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_8_1_0 [label="Attention_Values_8_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_8_1_0 [label="Attention_Weighted_8_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_8_1_0 [label="Attention_Output_8_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_8_1 [label="Attention_AllReduce_8_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_1_1 [label="QKV_Proj_8_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_8_1_1 [label="Attention_Scores_8_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_8_1_1 [label="Attention_Softmax_8_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_8_1_1 [label="Attention_Dropout_8_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_8_1_1 [label="Attention_Values_8_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_8_1_1 [label="Attention_Weighted_8_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_8_1_1 [label="Attention_Output_8_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_8_1 [label="Attention_AllReduce_8_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_1_2 [label="QKV_Proj_8_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_8_1_2 [label="Attention_Scores_8_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_8_1_2 [label="Attention_Softmax_8_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_8_1_2 [label="Attention_Dropout_8_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_8_1_2 [label="Attention_Values_8_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_8_1_2 [label="Attention_Weighted_8_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_8_1_2 [label="Attention_Output_8_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_8_1 [label="Attention_AllReduce_8_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_1_3 [label="QKV_Proj_8_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_8_1_3 [label="Attention_Scores_8_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_8_1_3 [label="Attention_Softmax_8_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_8_1_3 [label="Attention_Dropout_8_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_8_1_3 [label="Attention_Values_8_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_8_1_3 [label="Attention_Weighted_8_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_8_1_3 [label="Attention_Output_8_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_8_1 [label="Attention_AllReduce_8_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_2_0 [label="QKV_Proj_8_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_8_2_0 [label="Attention_Scores_8_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_8_2_0 [label="Attention_Softmax_8_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_8_2_0 [label="Attention_Dropout_8_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_8_2_0 [label="Attention_Values_8_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_8_2_0 [label="Attention_Weighted_8_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_8_2_0 [label="Attention_Output_8_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_8_2 [label="Attention_AllReduce_8_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_2_1 [label="QKV_Proj_8_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_8_2_1 [label="Attention_Scores_8_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_8_2_1 [label="Attention_Softmax_8_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_8_2_1 [label="Attention_Dropout_8_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_8_2_1 [label="Attention_Values_8_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_8_2_1 [label="Attention_Weighted_8_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_8_2_1 [label="Attention_Output_8_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_8_2 [label="Attention_AllReduce_8_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_2_2 [label="QKV_Proj_8_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_8_2_2 [label="Attention_Scores_8_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_8_2_2 [label="Attention_Softmax_8_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_8_2_2 [label="Attention_Dropout_8_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_8_2_2 [label="Attention_Values_8_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_8_2_2 [label="Attention_Weighted_8_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_8_2_2 [label="Attention_Output_8_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_8_2 [label="Attention_AllReduce_8_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_2_3 [label="QKV_Proj_8_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_8_2_3 [label="Attention_Scores_8_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_8_2_3 [label="Attention_Softmax_8_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_8_2_3 [label="Attention_Dropout_8_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_8_2_3 [label="Attention_Values_8_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_8_2_3 [label="Attention_Weighted_8_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_8_2_3 [label="Attention_Output_8_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_8_2 [label="Attention_AllReduce_8_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_3_0 [label="QKV_Proj_8_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_8_3_0 [label="Attention_Scores_8_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_8_3_0 [label="Attention_Softmax_8_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_8_3_0 [label="Attention_Dropout_8_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_8_3_0 [label="Attention_Values_8_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_8_3_0 [label="Attention_Weighted_8_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_8_3_0 [label="Attention_Output_8_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_8_3 [label="Attention_AllReduce_8_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_3_1 [label="QKV_Proj_8_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_8_3_1 [label="Attention_Scores_8_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_8_3_1 [label="Attention_Softmax_8_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_8_3_1 [label="Attention_Dropout_8_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_8_3_1 [label="Attention_Values_8_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_8_3_1 [label="Attention_Weighted_8_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_8_3_1 [label="Attention_Output_8_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_8_3 [label="Attention_AllReduce_8_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_3_2 [label="QKV_Proj_8_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_8_3_2 [label="Attention_Scores_8_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_8_3_2 [label="Attention_Softmax_8_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_8_3_2 [label="Attention_Dropout_8_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_8_3_2 [label="Attention_Values_8_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_8_3_2 [label="Attention_Weighted_8_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_8_3_2 [label="Attention_Output_8_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_8_3 [label="Attention_AllReduce_8_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_3_3 [label="QKV_Proj_8_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_8_3_3 [label="Attention_Scores_8_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_8_3_3 [label="Attention_Softmax_8_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_8_3_3 [label="Attention_Dropout_8_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_8_3_3 [label="Attention_Values_8_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_8_3_3 [label="Attention_Weighted_8_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_8_3_3 [label="Attention_Output_8_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_8_3 [label="Attention_AllReduce_8_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_4_0 [label="QKV_Proj_8_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_8_4_0 [label="Attention_Scores_8_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_8_4_0 [label="Attention_Softmax_8_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_8_4_0 [label="Attention_Dropout_8_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_8_4_0 [label="Attention_Values_8_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_8_4_0 [label="Attention_Weighted_8_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_8_4_0 [label="Attention_Output_8_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_8_4 [label="Attention_AllReduce_8_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_4_1 [label="QKV_Proj_8_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_8_4_1 [label="Attention_Scores_8_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_8_4_1 [label="Attention_Softmax_8_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_8_4_1 [label="Attention_Dropout_8_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_8_4_1 [label="Attention_Values_8_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_8_4_1 [label="Attention_Weighted_8_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_8_4_1 [label="Attention_Output_8_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_8_4 [label="Attention_AllReduce_8_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_4_2 [label="QKV_Proj_8_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_8_4_2 [label="Attention_Scores_8_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_8_4_2 [label="Attention_Softmax_8_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_8_4_2 [label="Attention_Dropout_8_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_8_4_2 [label="Attention_Values_8_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_8_4_2 [label="Attention_Weighted_8_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_8_4_2 [label="Attention_Output_8_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_8_4 [label="Attention_AllReduce_8_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_4_3 [label="QKV_Proj_8_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_8_4_3 [label="Attention_Scores_8_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_8_4_3 [label="Attention_Softmax_8_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_8_4_3 [label="Attention_Dropout_8_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_8_4_3 [label="Attention_Values_8_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_8_4_3 [label="Attention_Weighted_8_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_8_4_3 [label="Attention_Output_8_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_8_4 [label="Attention_AllReduce_8_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_5_0 [label="QKV_Proj_8_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_8_5_0 [label="Attention_Scores_8_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_8_5_0 [label="Attention_Softmax_8_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_8_5_0 [label="Attention_Dropout_8_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_8_5_0 [label="Attention_Values_8_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_8_5_0 [label="Attention_Weighted_8_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_8_5_0 [label="Attention_Output_8_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_8_5 [label="Attention_AllReduce_8_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_5_1 [label="QKV_Proj_8_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_8_5_1 [label="Attention_Scores_8_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_8_5_1 [label="Attention_Softmax_8_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_8_5_1 [label="Attention_Dropout_8_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_8_5_1 [label="Attention_Values_8_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_8_5_1 [label="Attention_Weighted_8_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_8_5_1 [label="Attention_Output_8_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_8_5 [label="Attention_AllReduce_8_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_5_2 [label="QKV_Proj_8_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_8_5_2 [label="Attention_Scores_8_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_8_5_2 [label="Attention_Softmax_8_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_8_5_2 [label="Attention_Dropout_8_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_8_5_2 [label="Attention_Values_8_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_8_5_2 [label="Attention_Weighted_8_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_8_5_2 [label="Attention_Output_8_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_8_5 [label="Attention_AllReduce_8_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_5_3 [label="QKV_Proj_8_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_8_5_3 [label="Attention_Scores_8_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_8_5_3 [label="Attention_Softmax_8_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_8_5_3 [label="Attention_Dropout_8_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_8_5_3 [label="Attention_Values_8_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_8_5_3 [label="Attention_Weighted_8_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_8_5_3 [label="Attention_Output_8_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_8_5 [label="Attention_AllReduce_8_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_6_0 [label="QKV_Proj_8_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_8_6_0 [label="Attention_Scores_8_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_8_6_0 [label="Attention_Softmax_8_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_8_6_0 [label="Attention_Dropout_8_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_8_6_0 [label="Attention_Values_8_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_8_6_0 [label="Attention_Weighted_8_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_8_6_0 [label="Attention_Output_8_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_8_6 [label="Attention_AllReduce_8_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_6_1 [label="QKV_Proj_8_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_8_6_1 [label="Attention_Scores_8_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_8_6_1 [label="Attention_Softmax_8_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_8_6_1 [label="Attention_Dropout_8_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_8_6_1 [label="Attention_Values_8_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_8_6_1 [label="Attention_Weighted_8_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_8_6_1 [label="Attention_Output_8_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_8_6 [label="Attention_AllReduce_8_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_6_2 [label="QKV_Proj_8_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_8_6_2 [label="Attention_Scores_8_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_8_6_2 [label="Attention_Softmax_8_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_8_6_2 [label="Attention_Dropout_8_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_8_6_2 [label="Attention_Values_8_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_8_6_2 [label="Attention_Weighted_8_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_8_6_2 [label="Attention_Output_8_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_8_6 [label="Attention_AllReduce_8_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_6_3 [label="QKV_Proj_8_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_8_6_3 [label="Attention_Scores_8_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_8_6_3 [label="Attention_Softmax_8_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_8_6_3 [label="Attention_Dropout_8_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_8_6_3 [label="Attention_Values_8_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_8_6_3 [label="Attention_Weighted_8_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_8_6_3 [label="Attention_Output_8_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_8_6 [label="Attention_AllReduce_8_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_7_0 [label="QKV_Proj_8_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_8_7_0 [label="Attention_Scores_8_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_8_7_0 [label="Attention_Softmax_8_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_8_7_0 [label="Attention_Dropout_8_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_8_7_0 [label="Attention_Values_8_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_8_7_0 [label="Attention_Weighted_8_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_8_7_0 [label="Attention_Output_8_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_8_7 [label="Attention_AllReduce_8_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_7_1 [label="QKV_Proj_8_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_8_7_1 [label="Attention_Scores_8_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_8_7_1 [label="Attention_Softmax_8_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_8_7_1 [label="Attention_Dropout_8_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_8_7_1 [label="Attention_Values_8_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_8_7_1 [label="Attention_Weighted_8_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_8_7_1 [label="Attention_Output_8_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_8_7 [label="Attention_AllReduce_8_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_7_2 [label="QKV_Proj_8_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_8_7_2 [label="Attention_Scores_8_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_8_7_2 [label="Attention_Softmax_8_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_8_7_2 [label="Attention_Dropout_8_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_8_7_2 [label="Attention_Values_8_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_8_7_2 [label="Attention_Weighted_8_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_8_7_2 [label="Attention_Output_8_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_8_7 [label="Attention_AllReduce_8_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_7_3 [label="QKV_Proj_8_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_8_7_3 [label="Attention_Scores_8_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_8_7_3 [label="Attention_Softmax_8_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_8_7_3 [label="Attention_Dropout_8_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_8_7_3 [label="Attention_Values_8_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_8_7_3 [label="Attention_Weighted_8_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_8_7_3 [label="Attention_Output_8_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_8_7 [label="Attention_AllReduce_8_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_8_0 [label="QKV_Proj_8_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_8_8_0 [label="Attention_Scores_8_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_8_8_0 [label="Attention_Softmax_8_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_8_8_0 [label="Attention_Dropout_8_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_8_8_0 [label="Attention_Values_8_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_8_8_0 [label="Attention_Weighted_8_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_8_8_0 [label="Attention_Output_8_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_8_8 [label="Attention_AllReduce_8_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_8_1 [label="QKV_Proj_8_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_8_8_1 [label="Attention_Scores_8_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_8_8_1 [label="Attention_Softmax_8_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_8_8_1 [label="Attention_Dropout_8_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_8_8_1 [label="Attention_Values_8_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_8_8_1 [label="Attention_Weighted_8_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_8_8_1 [label="Attention_Output_8_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_8_8 [label="Attention_AllReduce_8_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_8_2 [label="QKV_Proj_8_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_8_8_2 [label="Attention_Scores_8_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_8_8_2 [label="Attention_Softmax_8_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_8_8_2 [label="Attention_Dropout_8_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_8_8_2 [label="Attention_Values_8_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_8_8_2 [label="Attention_Weighted_8_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_8_8_2 [label="Attention_Output_8_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_8_8 [label="Attention_AllReduce_8_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_8_3 [label="QKV_Proj_8_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_8_8_3 [label="Attention_Scores_8_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_8_8_3 [label="Attention_Softmax_8_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_8_8_3 [label="Attention_Dropout_8_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_8_8_3 [label="Attention_Values_8_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_8_8_3 [label="Attention_Weighted_8_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_8_8_3 [label="Attention_Output_8_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_8_8 [label="Attention_AllReduce_8_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_9_0 [label="QKV_Proj_8_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_8_9_0 [label="Attention_Scores_8_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_8_9_0 [label="Attention_Softmax_8_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_8_9_0 [label="Attention_Dropout_8_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_8_9_0 [label="Attention_Values_8_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_8_9_0 [label="Attention_Weighted_8_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_8_9_0 [label="Attention_Output_8_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_8_9 [label="Attention_AllReduce_8_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_9_1 [label="QKV_Proj_8_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_8_9_1 [label="Attention_Scores_8_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_8_9_1 [label="Attention_Softmax_8_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_8_9_1 [label="Attention_Dropout_8_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_8_9_1 [label="Attention_Values_8_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_8_9_1 [label="Attention_Weighted_8_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_8_9_1 [label="Attention_Output_8_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_8_9 [label="Attention_AllReduce_8_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_9_2 [label="QKV_Proj_8_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_8_9_2 [label="Attention_Scores_8_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_8_9_2 [label="Attention_Softmax_8_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_8_9_2 [label="Attention_Dropout_8_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_8_9_2 [label="Attention_Values_8_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_8_9_2 [label="Attention_Weighted_8_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_8_9_2 [label="Attention_Output_8_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_8_9 [label="Attention_AllReduce_8_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_9_3 [label="QKV_Proj_8_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_8_9_3 [label="Attention_Scores_8_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_8_9_3 [label="Attention_Softmax_8_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_8_9_3 [label="Attention_Dropout_8_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_8_9_3 [label="Attention_Values_8_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_8_9_3 [label="Attention_Weighted_8_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_8_9_3 [label="Attention_Output_8_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_8_9 [label="Attention_AllReduce_8_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_10_0 [label="QKV_Proj_8_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_8_10_0 [label="Attention_Scores_8_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_8_10_0 [label="Attention_Softmax_8_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_8_10_0 [label="Attention_Dropout_8_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_8_10_0 [label="Attention_Values_8_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_8_10_0 [label="Attention_Weighted_8_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_8_10_0 [label="Attention_Output_8_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_8_10 [label="Attention_AllReduce_8_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_10_1 [label="QKV_Proj_8_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_8_10_1 [label="Attention_Scores_8_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_8_10_1 [label="Attention_Softmax_8_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_8_10_1 [label="Attention_Dropout_8_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_8_10_1 [label="Attention_Values_8_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_8_10_1 [label="Attention_Weighted_8_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_8_10_1 [label="Attention_Output_8_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_8_10 [label="Attention_AllReduce_8_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_10_2 [label="QKV_Proj_8_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_8_10_2 [label="Attention_Scores_8_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_8_10_2 [label="Attention_Softmax_8_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_8_10_2 [label="Attention_Dropout_8_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_8_10_2 [label="Attention_Values_8_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_8_10_2 [label="Attention_Weighted_8_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_8_10_2 [label="Attention_Output_8_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_8_10 [label="Attention_AllReduce_8_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_10_3 [label="QKV_Proj_8_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_8_10_3 [label="Attention_Scores_8_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_8_10_3 [label="Attention_Softmax_8_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_8_10_3 [label="Attention_Dropout_8_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_8_10_3 [label="Attention_Values_8_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_8_10_3 [label="Attention_Weighted_8_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_8_10_3 [label="Attention_Output_8_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_8_10 [label="Attention_AllReduce_8_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_11_0 [label="QKV_Proj_8_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_8_11_0 [label="Attention_Scores_8_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_8_11_0 [label="Attention_Softmax_8_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_8_11_0 [label="Attention_Dropout_8_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_8_11_0 [label="Attention_Values_8_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_8_11_0 [label="Attention_Weighted_8_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_8_11_0 [label="Attention_Output_8_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_8_11 [label="Attention_AllReduce_8_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_11_1 [label="QKV_Proj_8_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_8_11_1 [label="Attention_Scores_8_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_8_11_1 [label="Attention_Softmax_8_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_8_11_1 [label="Attention_Dropout_8_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_8_11_1 [label="Attention_Values_8_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_8_11_1 [label="Attention_Weighted_8_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_8_11_1 [label="Attention_Output_8_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_8_11 [label="Attention_AllReduce_8_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_11_2 [label="QKV_Proj_8_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_8_11_2 [label="Attention_Scores_8_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_8_11_2 [label="Attention_Softmax_8_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_8_11_2 [label="Attention_Dropout_8_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_8_11_2 [label="Attention_Values_8_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_8_11_2 [label="Attention_Weighted_8_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_8_11_2 [label="Attention_Output_8_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_8_11 [label="Attention_AllReduce_8_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_11_3 [label="QKV_Proj_8_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_8_11_3 [label="Attention_Scores_8_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_8_11_3 [label="Attention_Softmax_8_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_8_11_3 [label="Attention_Dropout_8_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_8_11_3 [label="Attention_Values_8_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_8_11_3 [label="Attention_Weighted_8_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_8_11_3 [label="Attention_Output_8_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_8_11 [label="Attention_AllReduce_8_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_12_0 [label="QKV_Proj_8_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_8_12_0 [label="Attention_Scores_8_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_8_12_0 [label="Attention_Softmax_8_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_8_12_0 [label="Attention_Dropout_8_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_8_12_0 [label="Attention_Values_8_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_8_12_0 [label="Attention_Weighted_8_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_8_12_0 [label="Attention_Output_8_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_8_12 [label="Attention_AllReduce_8_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_12_1 [label="QKV_Proj_8_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_8_12_1 [label="Attention_Scores_8_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_8_12_1 [label="Attention_Softmax_8_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_8_12_1 [label="Attention_Dropout_8_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_8_12_1 [label="Attention_Values_8_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_8_12_1 [label="Attention_Weighted_8_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_8_12_1 [label="Attention_Output_8_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_8_12 [label="Attention_AllReduce_8_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_12_2 [label="QKV_Proj_8_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_8_12_2 [label="Attention_Scores_8_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_8_12_2 [label="Attention_Softmax_8_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_8_12_2 [label="Attention_Dropout_8_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_8_12_2 [label="Attention_Values_8_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_8_12_2 [label="Attention_Weighted_8_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_8_12_2 [label="Attention_Output_8_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_8_12 [label="Attention_AllReduce_8_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_12_3 [label="QKV_Proj_8_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_8_12_3 [label="Attention_Scores_8_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_8_12_3 [label="Attention_Softmax_8_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_8_12_3 [label="Attention_Dropout_8_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_8_12_3 [label="Attention_Values_8_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_8_12_3 [label="Attention_Weighted_8_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_8_12_3 [label="Attention_Output_8_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_8_12 [label="Attention_AllReduce_8_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_13_0 [label="QKV_Proj_8_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_8_13_0 [label="Attention_Scores_8_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_8_13_0 [label="Attention_Softmax_8_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_8_13_0 [label="Attention_Dropout_8_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_8_13_0 [label="Attention_Values_8_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_8_13_0 [label="Attention_Weighted_8_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_8_13_0 [label="Attention_Output_8_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_8_13 [label="Attention_AllReduce_8_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_13_1 [label="QKV_Proj_8_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_8_13_1 [label="Attention_Scores_8_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_8_13_1 [label="Attention_Softmax_8_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_8_13_1 [label="Attention_Dropout_8_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_8_13_1 [label="Attention_Values_8_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_8_13_1 [label="Attention_Weighted_8_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_8_13_1 [label="Attention_Output_8_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_8_13 [label="Attention_AllReduce_8_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_13_2 [label="QKV_Proj_8_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_8_13_2 [label="Attention_Scores_8_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_8_13_2 [label="Attention_Softmax_8_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_8_13_2 [label="Attention_Dropout_8_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_8_13_2 [label="Attention_Values_8_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_8_13_2 [label="Attention_Weighted_8_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_8_13_2 [label="Attention_Output_8_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_8_13 [label="Attention_AllReduce_8_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_13_3 [label="QKV_Proj_8_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_8_13_3 [label="Attention_Scores_8_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_8_13_3 [label="Attention_Softmax_8_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_8_13_3 [label="Attention_Dropout_8_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_8_13_3 [label="Attention_Values_8_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_8_13_3 [label="Attention_Weighted_8_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_8_13_3 [label="Attention_Output_8_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_8_13 [label="Attention_AllReduce_8_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_14_0 [label="QKV_Proj_8_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_8_14_0 [label="Attention_Scores_8_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_8_14_0 [label="Attention_Softmax_8_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_8_14_0 [label="Attention_Dropout_8_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_8_14_0 [label="Attention_Values_8_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_8_14_0 [label="Attention_Weighted_8_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_8_14_0 [label="Attention_Output_8_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_8_14 [label="Attention_AllReduce_8_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_14_1 [label="QKV_Proj_8_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_8_14_1 [label="Attention_Scores_8_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_8_14_1 [label="Attention_Softmax_8_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_8_14_1 [label="Attention_Dropout_8_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_8_14_1 [label="Attention_Values_8_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_8_14_1 [label="Attention_Weighted_8_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_8_14_1 [label="Attention_Output_8_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_8_14 [label="Attention_AllReduce_8_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_14_2 [label="QKV_Proj_8_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_8_14_2 [label="Attention_Scores_8_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_8_14_2 [label="Attention_Softmax_8_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_8_14_2 [label="Attention_Dropout_8_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_8_14_2 [label="Attention_Values_8_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_8_14_2 [label="Attention_Weighted_8_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_8_14_2 [label="Attention_Output_8_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_8_14 [label="Attention_AllReduce_8_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_14_3 [label="QKV_Proj_8_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_8_14_3 [label="Attention_Scores_8_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_8_14_3 [label="Attention_Softmax_8_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_8_14_3 [label="Attention_Dropout_8_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_8_14_3 [label="Attention_Values_8_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_8_14_3 [label="Attention_Weighted_8_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_8_14_3 [label="Attention_Output_8_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_8_14 [label="Attention_AllReduce_8_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_15_0 [label="QKV_Proj_8_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_8_15_0 [label="Attention_Scores_8_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_8_15_0 [label="Attention_Softmax_8_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_8_15_0 [label="Attention_Dropout_8_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_8_15_0 [label="Attention_Values_8_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_8_15_0 [label="Attention_Weighted_8_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_8_15_0 [label="Attention_Output_8_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_8_15 [label="Attention_AllReduce_8_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_15_1 [label="QKV_Proj_8_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_8_15_1 [label="Attention_Scores_8_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_8_15_1 [label="Attention_Softmax_8_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_8_15_1 [label="Attention_Dropout_8_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_8_15_1 [label="Attention_Values_8_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_8_15_1 [label="Attention_Weighted_8_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_8_15_1 [label="Attention_Output_8_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_8_15 [label="Attention_AllReduce_8_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_15_2 [label="QKV_Proj_8_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_8_15_2 [label="Attention_Scores_8_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_8_15_2 [label="Attention_Softmax_8_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_8_15_2 [label="Attention_Dropout_8_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_8_15_2 [label="Attention_Values_8_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_8_15_2 [label="Attention_Weighted_8_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_8_15_2 [label="Attention_Output_8_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_8_15 [label="Attention_AllReduce_8_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_8_15_3 [label="QKV_Proj_8_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_8_15_3 [label="Attention_Scores_8_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_8_15_3 [label="Attention_Softmax_8_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_8_15_3 [label="Attention_Dropout_8_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_8_15_3 [label="Attention_Values_8_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_8_15_3 [label="Attention_Weighted_8_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_8_15_3 [label="Attention_Output_8_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_8_15 [label="Attention_AllReduce_8_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_0_0 [label="MLP_Linear1_8_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_8_0_0 [label="GELU_8_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_8_0_0 [label="MLP_Linear2_8_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_8_0_1 [label="MLP_Linear1_8_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_8_0_1 [label="GELU_8_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_8_0_1 [label="MLP_Linear2_8_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_8_0_2 [label="MLP_Linear1_8_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_8_0_2 [label="GELU_8_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_8_0_2 [label="MLP_Linear2_8_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_8_0_3 [label="MLP_Linear1_8_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_8_0_3 [label="GELU_8_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_8_0_3 [label="MLP_Linear2_8_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_8_0 [label="MLP_AllReduce_8_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_1_0 [label="MLP_Linear1_8_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_8_1_0 [label="GELU_8_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_8_1_0 [label="MLP_Linear2_8_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_8_1_1 [label="MLP_Linear1_8_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_8_1_1 [label="GELU_8_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_8_1_1 [label="MLP_Linear2_8_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_8_1_2 [label="MLP_Linear1_8_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_8_1_2 [label="GELU_8_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_8_1_2 [label="MLP_Linear2_8_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_8_1_3 [label="MLP_Linear1_8_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_8_1_3 [label="GELU_8_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_8_1_3 [label="MLP_Linear2_8_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_8_1 [label="MLP_AllReduce_8_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_2_0 [label="MLP_Linear1_8_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_8_2_0 [label="GELU_8_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_8_2_0 [label="MLP_Linear2_8_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_8_2_1 [label="MLP_Linear1_8_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_8_2_1 [label="GELU_8_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_8_2_1 [label="MLP_Linear2_8_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_8_2_2 [label="MLP_Linear1_8_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_8_2_2 [label="GELU_8_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_8_2_2 [label="MLP_Linear2_8_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_8_2_3 [label="MLP_Linear1_8_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_8_2_3 [label="GELU_8_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_8_2_3 [label="MLP_Linear2_8_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_8_2 [label="MLP_AllReduce_8_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_3_0 [label="MLP_Linear1_8_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_8_3_0 [label="GELU_8_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_8_3_0 [label="MLP_Linear2_8_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_8_3_1 [label="MLP_Linear1_8_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_8_3_1 [label="GELU_8_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_8_3_1 [label="MLP_Linear2_8_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_8_3_2 [label="MLP_Linear1_8_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_8_3_2 [label="GELU_8_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_8_3_2 [label="MLP_Linear2_8_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_8_3_3 [label="MLP_Linear1_8_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_8_3_3 [label="GELU_8_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_8_3_3 [label="MLP_Linear2_8_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_8_3 [label="MLP_AllReduce_8_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_4_0 [label="MLP_Linear1_8_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_8_4_0 [label="GELU_8_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_8_4_0 [label="MLP_Linear2_8_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_8_4_1 [label="MLP_Linear1_8_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_8_4_1 [label="GELU_8_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_8_4_1 [label="MLP_Linear2_8_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_8_4_2 [label="MLP_Linear1_8_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_8_4_2 [label="GELU_8_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_8_4_2 [label="MLP_Linear2_8_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_8_4_3 [label="MLP_Linear1_8_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_8_4_3 [label="GELU_8_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_8_4_3 [label="MLP_Linear2_8_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_8_4 [label="MLP_AllReduce_8_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_5_0 [label="MLP_Linear1_8_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_8_5_0 [label="GELU_8_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_8_5_0 [label="MLP_Linear2_8_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_8_5_1 [label="MLP_Linear1_8_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_8_5_1 [label="GELU_8_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_8_5_1 [label="MLP_Linear2_8_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_8_5_2 [label="MLP_Linear1_8_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_8_5_2 [label="GELU_8_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_8_5_2 [label="MLP_Linear2_8_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_8_5_3 [label="MLP_Linear1_8_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_8_5_3 [label="GELU_8_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_8_5_3 [label="MLP_Linear2_8_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_8_5 [label="MLP_AllReduce_8_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_6_0 [label="MLP_Linear1_8_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_8_6_0 [label="GELU_8_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_8_6_0 [label="MLP_Linear2_8_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_8_6_1 [label="MLP_Linear1_8_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_8_6_1 [label="GELU_8_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_8_6_1 [label="MLP_Linear2_8_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_8_6_2 [label="MLP_Linear1_8_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_8_6_2 [label="GELU_8_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_8_6_2 [label="MLP_Linear2_8_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_8_6_3 [label="MLP_Linear1_8_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_8_6_3 [label="GELU_8_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_8_6_3 [label="MLP_Linear2_8_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_8_6 [label="MLP_AllReduce_8_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_7_0 [label="MLP_Linear1_8_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_8_7_0 [label="GELU_8_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_8_7_0 [label="MLP_Linear2_8_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_8_7_1 [label="MLP_Linear1_8_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_8_7_1 [label="GELU_8_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_8_7_1 [label="MLP_Linear2_8_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_8_7_2 [label="MLP_Linear1_8_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_8_7_2 [label="GELU_8_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_8_7_2 [label="MLP_Linear2_8_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_8_7_3 [label="MLP_Linear1_8_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_8_7_3 [label="GELU_8_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_8_7_3 [label="MLP_Linear2_8_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_8_7 [label="MLP_AllReduce_8_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_8_0 [label="MLP_Linear1_8_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_8_8_0 [label="GELU_8_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_8_8_0 [label="MLP_Linear2_8_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_8_8_1 [label="MLP_Linear1_8_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_8_8_1 [label="GELU_8_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_8_8_1 [label="MLP_Linear2_8_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_8_8_2 [label="MLP_Linear1_8_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_8_8_2 [label="GELU_8_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_8_8_2 [label="MLP_Linear2_8_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_8_8_3 [label="MLP_Linear1_8_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_8_8_3 [label="GELU_8_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_8_8_3 [label="MLP_Linear2_8_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_8_8 [label="MLP_AllReduce_8_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_9_0 [label="MLP_Linear1_8_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_8_9_0 [label="GELU_8_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_8_9_0 [label="MLP_Linear2_8_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_8_9_1 [label="MLP_Linear1_8_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_8_9_1 [label="GELU_8_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_8_9_1 [label="MLP_Linear2_8_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_8_9_2 [label="MLP_Linear1_8_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_8_9_2 [label="GELU_8_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_8_9_2 [label="MLP_Linear2_8_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_8_9_3 [label="MLP_Linear1_8_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_8_9_3 [label="GELU_8_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_8_9_3 [label="MLP_Linear2_8_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_8_9 [label="MLP_AllReduce_8_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_10_0 [label="MLP_Linear1_8_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_8_10_0 [label="GELU_8_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_8_10_0 [label="MLP_Linear2_8_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_8_10_1 [label="MLP_Linear1_8_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_8_10_1 [label="GELU_8_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_8_10_1 [label="MLP_Linear2_8_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_8_10_2 [label="MLP_Linear1_8_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_8_10_2 [label="GELU_8_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_8_10_2 [label="MLP_Linear2_8_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_8_10_3 [label="MLP_Linear1_8_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_8_10_3 [label="GELU_8_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_8_10_3 [label="MLP_Linear2_8_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_8_10 [label="MLP_AllReduce_8_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_11_0 [label="MLP_Linear1_8_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_8_11_0 [label="GELU_8_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_8_11_0 [label="MLP_Linear2_8_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_8_11_1 [label="MLP_Linear1_8_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_8_11_1 [label="GELU_8_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_8_11_1 [label="MLP_Linear2_8_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_8_11_2 [label="MLP_Linear1_8_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_8_11_2 [label="GELU_8_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_8_11_2 [label="MLP_Linear2_8_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_8_11_3 [label="MLP_Linear1_8_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_8_11_3 [label="GELU_8_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_8_11_3 [label="MLP_Linear2_8_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_8_11 [label="MLP_AllReduce_8_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_12_0 [label="MLP_Linear1_8_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_8_12_0 [label="GELU_8_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_8_12_0 [label="MLP_Linear2_8_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_8_12_1 [label="MLP_Linear1_8_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_8_12_1 [label="GELU_8_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_8_12_1 [label="MLP_Linear2_8_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_8_12_2 [label="MLP_Linear1_8_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_8_12_2 [label="GELU_8_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_8_12_2 [label="MLP_Linear2_8_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_8_12_3 [label="MLP_Linear1_8_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_8_12_3 [label="GELU_8_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_8_12_3 [label="MLP_Linear2_8_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_8_12 [label="MLP_AllReduce_8_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_13_0 [label="MLP_Linear1_8_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_8_13_0 [label="GELU_8_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_8_13_0 [label="MLP_Linear2_8_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_8_13_1 [label="MLP_Linear1_8_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_8_13_1 [label="GELU_8_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_8_13_1 [label="MLP_Linear2_8_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_8_13_2 [label="MLP_Linear1_8_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_8_13_2 [label="GELU_8_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_8_13_2 [label="MLP_Linear2_8_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_8_13_3 [label="MLP_Linear1_8_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_8_13_3 [label="GELU_8_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_8_13_3 [label="MLP_Linear2_8_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_8_13 [label="MLP_AllReduce_8_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_14_0 [label="MLP_Linear1_8_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_8_14_0 [label="GELU_8_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_8_14_0 [label="MLP_Linear2_8_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_8_14_1 [label="MLP_Linear1_8_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_8_14_1 [label="GELU_8_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_8_14_1 [label="MLP_Linear2_8_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_8_14_2 [label="MLP_Linear1_8_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_8_14_2 [label="GELU_8_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_8_14_2 [label="MLP_Linear2_8_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_8_14_3 [label="MLP_Linear1_8_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_8_14_3 [label="GELU_8_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_8_14_3 [label="MLP_Linear2_8_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_8_14 [label="MLP_AllReduce_8_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_8_15_0 [label="MLP_Linear1_8_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_8_15_0 [label="GELU_8_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_8_15_0 [label="MLP_Linear2_8_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_8_15_1 [label="MLP_Linear1_8_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_8_15_1 [label="GELU_8_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_8_15_1 [label="MLP_Linear2_8_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_8_15_2 [label="MLP_Linear1_8_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_8_15_2 [label="GELU_8_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_8_15_2 [label="MLP_Linear2_8_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_8_15_3 [label="MLP_Linear1_8_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_8_15_3 [label="GELU_8_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_8_15_3 [label="MLP_Linear2_8_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_8_15 [label="MLP_AllReduce_8_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_8_0 [label="Expert_Route_8_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_1 [label="Expert_Route_8_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_2 [label="Expert_Route_8_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_3 [label="Expert_Route_8_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_4 [label="Expert_Route_8_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_5 [label="Expert_Route_8_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_6 [label="Expert_Route_8_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_7 [label="Expert_Route_8_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_8 [label="Expert_Route_8_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_9 [label="Expert_Route_8_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_10 [label="Expert_Route_8_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_11 [label="Expert_Route_8_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_12 [label="Expert_Route_8_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_13 [label="Expert_Route_8_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_14 [label="Expert_Route_8_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_8_15 [label="Expert_Route_8_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_9_0_0 [label="QKV_Proj_9_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_9_0_0 [label="Attention_Scores_9_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_9_0_0 [label="Attention_Softmax_9_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_9_0_0 [label="Attention_Dropout_9_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_9_0_0 [label="Attention_Values_9_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_9_0_0 [label="Attention_Weighted_9_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_9_0_0 [label="Attention_Output_9_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_9_0 [label="Attention_AllReduce_9_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_0_1 [label="QKV_Proj_9_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_9_0_1 [label="Attention_Scores_9_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_9_0_1 [label="Attention_Softmax_9_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_9_0_1 [label="Attention_Dropout_9_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_9_0_1 [label="Attention_Values_9_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_9_0_1 [label="Attention_Weighted_9_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_9_0_1 [label="Attention_Output_9_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_9_0 [label="Attention_AllReduce_9_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_0_2 [label="QKV_Proj_9_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_9_0_2 [label="Attention_Scores_9_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_9_0_2 [label="Attention_Softmax_9_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_9_0_2 [label="Attention_Dropout_9_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_9_0_2 [label="Attention_Values_9_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_9_0_2 [label="Attention_Weighted_9_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_9_0_2 [label="Attention_Output_9_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_9_0 [label="Attention_AllReduce_9_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_0_3 [label="QKV_Proj_9_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_9_0_3 [label="Attention_Scores_9_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_9_0_3 [label="Attention_Softmax_9_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_9_0_3 [label="Attention_Dropout_9_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_9_0_3 [label="Attention_Values_9_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_9_0_3 [label="Attention_Weighted_9_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_9_0_3 [label="Attention_Output_9_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_9_0 [label="Attention_AllReduce_9_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_1_0 [label="QKV_Proj_9_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_9_1_0 [label="Attention_Scores_9_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_9_1_0 [label="Attention_Softmax_9_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_9_1_0 [label="Attention_Dropout_9_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_9_1_0 [label="Attention_Values_9_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_9_1_0 [label="Attention_Weighted_9_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_9_1_0 [label="Attention_Output_9_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_9_1 [label="Attention_AllReduce_9_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_1_1 [label="QKV_Proj_9_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_9_1_1 [label="Attention_Scores_9_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_9_1_1 [label="Attention_Softmax_9_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_9_1_1 [label="Attention_Dropout_9_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_9_1_1 [label="Attention_Values_9_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_9_1_1 [label="Attention_Weighted_9_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_9_1_1 [label="Attention_Output_9_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_9_1 [label="Attention_AllReduce_9_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_1_2 [label="QKV_Proj_9_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_9_1_2 [label="Attention_Scores_9_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_9_1_2 [label="Attention_Softmax_9_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_9_1_2 [label="Attention_Dropout_9_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_9_1_2 [label="Attention_Values_9_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_9_1_2 [label="Attention_Weighted_9_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_9_1_2 [label="Attention_Output_9_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_9_1 [label="Attention_AllReduce_9_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_1_3 [label="QKV_Proj_9_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_9_1_3 [label="Attention_Scores_9_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_9_1_3 [label="Attention_Softmax_9_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_9_1_3 [label="Attention_Dropout_9_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_9_1_3 [label="Attention_Values_9_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_9_1_3 [label="Attention_Weighted_9_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_9_1_3 [label="Attention_Output_9_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_9_1 [label="Attention_AllReduce_9_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_2_0 [label="QKV_Proj_9_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_9_2_0 [label="Attention_Scores_9_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_9_2_0 [label="Attention_Softmax_9_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_9_2_0 [label="Attention_Dropout_9_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_9_2_0 [label="Attention_Values_9_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_9_2_0 [label="Attention_Weighted_9_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_9_2_0 [label="Attention_Output_9_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_9_2 [label="Attention_AllReduce_9_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_2_1 [label="QKV_Proj_9_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_9_2_1 [label="Attention_Scores_9_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_9_2_1 [label="Attention_Softmax_9_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_9_2_1 [label="Attention_Dropout_9_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_9_2_1 [label="Attention_Values_9_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_9_2_1 [label="Attention_Weighted_9_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_9_2_1 [label="Attention_Output_9_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_9_2 [label="Attention_AllReduce_9_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_2_2 [label="QKV_Proj_9_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_9_2_2 [label="Attention_Scores_9_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_9_2_2 [label="Attention_Softmax_9_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_9_2_2 [label="Attention_Dropout_9_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_9_2_2 [label="Attention_Values_9_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_9_2_2 [label="Attention_Weighted_9_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_9_2_2 [label="Attention_Output_9_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_9_2 [label="Attention_AllReduce_9_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_2_3 [label="QKV_Proj_9_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_9_2_3 [label="Attention_Scores_9_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_9_2_3 [label="Attention_Softmax_9_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_9_2_3 [label="Attention_Dropout_9_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_9_2_3 [label="Attention_Values_9_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_9_2_3 [label="Attention_Weighted_9_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_9_2_3 [label="Attention_Output_9_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_9_2 [label="Attention_AllReduce_9_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_3_0 [label="QKV_Proj_9_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_9_3_0 [label="Attention_Scores_9_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_9_3_0 [label="Attention_Softmax_9_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_9_3_0 [label="Attention_Dropout_9_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_9_3_0 [label="Attention_Values_9_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_9_3_0 [label="Attention_Weighted_9_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_9_3_0 [label="Attention_Output_9_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_9_3 [label="Attention_AllReduce_9_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_3_1 [label="QKV_Proj_9_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_9_3_1 [label="Attention_Scores_9_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_9_3_1 [label="Attention_Softmax_9_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_9_3_1 [label="Attention_Dropout_9_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_9_3_1 [label="Attention_Values_9_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_9_3_1 [label="Attention_Weighted_9_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_9_3_1 [label="Attention_Output_9_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_9_3 [label="Attention_AllReduce_9_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_3_2 [label="QKV_Proj_9_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_9_3_2 [label="Attention_Scores_9_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_9_3_2 [label="Attention_Softmax_9_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_9_3_2 [label="Attention_Dropout_9_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_9_3_2 [label="Attention_Values_9_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_9_3_2 [label="Attention_Weighted_9_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_9_3_2 [label="Attention_Output_9_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_9_3 [label="Attention_AllReduce_9_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_3_3 [label="QKV_Proj_9_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_9_3_3 [label="Attention_Scores_9_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_9_3_3 [label="Attention_Softmax_9_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_9_3_3 [label="Attention_Dropout_9_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_9_3_3 [label="Attention_Values_9_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_9_3_3 [label="Attention_Weighted_9_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_9_3_3 [label="Attention_Output_9_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_9_3 [label="Attention_AllReduce_9_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_4_0 [label="QKV_Proj_9_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_9_4_0 [label="Attention_Scores_9_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_9_4_0 [label="Attention_Softmax_9_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_9_4_0 [label="Attention_Dropout_9_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_9_4_0 [label="Attention_Values_9_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_9_4_0 [label="Attention_Weighted_9_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_9_4_0 [label="Attention_Output_9_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_9_4 [label="Attention_AllReduce_9_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_4_1 [label="QKV_Proj_9_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_9_4_1 [label="Attention_Scores_9_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_9_4_1 [label="Attention_Softmax_9_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_9_4_1 [label="Attention_Dropout_9_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_9_4_1 [label="Attention_Values_9_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_9_4_1 [label="Attention_Weighted_9_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_9_4_1 [label="Attention_Output_9_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_9_4 [label="Attention_AllReduce_9_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_4_2 [label="QKV_Proj_9_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_9_4_2 [label="Attention_Scores_9_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_9_4_2 [label="Attention_Softmax_9_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_9_4_2 [label="Attention_Dropout_9_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_9_4_2 [label="Attention_Values_9_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_9_4_2 [label="Attention_Weighted_9_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_9_4_2 [label="Attention_Output_9_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_9_4 [label="Attention_AllReduce_9_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_4_3 [label="QKV_Proj_9_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_9_4_3 [label="Attention_Scores_9_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_9_4_3 [label="Attention_Softmax_9_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_9_4_3 [label="Attention_Dropout_9_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_9_4_3 [label="Attention_Values_9_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_9_4_3 [label="Attention_Weighted_9_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_9_4_3 [label="Attention_Output_9_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_9_4 [label="Attention_AllReduce_9_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_5_0 [label="QKV_Proj_9_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_9_5_0 [label="Attention_Scores_9_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_9_5_0 [label="Attention_Softmax_9_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_9_5_0 [label="Attention_Dropout_9_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_9_5_0 [label="Attention_Values_9_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_9_5_0 [label="Attention_Weighted_9_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_9_5_0 [label="Attention_Output_9_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_9_5 [label="Attention_AllReduce_9_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_5_1 [label="QKV_Proj_9_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_9_5_1 [label="Attention_Scores_9_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_9_5_1 [label="Attention_Softmax_9_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_9_5_1 [label="Attention_Dropout_9_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_9_5_1 [label="Attention_Values_9_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_9_5_1 [label="Attention_Weighted_9_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_9_5_1 [label="Attention_Output_9_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_9_5 [label="Attention_AllReduce_9_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_5_2 [label="QKV_Proj_9_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_9_5_2 [label="Attention_Scores_9_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_9_5_2 [label="Attention_Softmax_9_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_9_5_2 [label="Attention_Dropout_9_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_9_5_2 [label="Attention_Values_9_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_9_5_2 [label="Attention_Weighted_9_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_9_5_2 [label="Attention_Output_9_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_9_5 [label="Attention_AllReduce_9_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_5_3 [label="QKV_Proj_9_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_9_5_3 [label="Attention_Scores_9_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_9_5_3 [label="Attention_Softmax_9_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_9_5_3 [label="Attention_Dropout_9_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_9_5_3 [label="Attention_Values_9_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_9_5_3 [label="Attention_Weighted_9_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_9_5_3 [label="Attention_Output_9_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_9_5 [label="Attention_AllReduce_9_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_6_0 [label="QKV_Proj_9_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_9_6_0 [label="Attention_Scores_9_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_9_6_0 [label="Attention_Softmax_9_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_9_6_0 [label="Attention_Dropout_9_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_9_6_0 [label="Attention_Values_9_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_9_6_0 [label="Attention_Weighted_9_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_9_6_0 [label="Attention_Output_9_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_9_6 [label="Attention_AllReduce_9_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_6_1 [label="QKV_Proj_9_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_9_6_1 [label="Attention_Scores_9_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_9_6_1 [label="Attention_Softmax_9_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_9_6_1 [label="Attention_Dropout_9_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_9_6_1 [label="Attention_Values_9_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_9_6_1 [label="Attention_Weighted_9_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_9_6_1 [label="Attention_Output_9_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_9_6 [label="Attention_AllReduce_9_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_6_2 [label="QKV_Proj_9_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_9_6_2 [label="Attention_Scores_9_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_9_6_2 [label="Attention_Softmax_9_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_9_6_2 [label="Attention_Dropout_9_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_9_6_2 [label="Attention_Values_9_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_9_6_2 [label="Attention_Weighted_9_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_9_6_2 [label="Attention_Output_9_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_9_6 [label="Attention_AllReduce_9_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_6_3 [label="QKV_Proj_9_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_9_6_3 [label="Attention_Scores_9_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_9_6_3 [label="Attention_Softmax_9_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_9_6_3 [label="Attention_Dropout_9_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_9_6_3 [label="Attention_Values_9_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_9_6_3 [label="Attention_Weighted_9_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_9_6_3 [label="Attention_Output_9_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_9_6 [label="Attention_AllReduce_9_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_7_0 [label="QKV_Proj_9_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_9_7_0 [label="Attention_Scores_9_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_9_7_0 [label="Attention_Softmax_9_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_9_7_0 [label="Attention_Dropout_9_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_9_7_0 [label="Attention_Values_9_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_9_7_0 [label="Attention_Weighted_9_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_9_7_0 [label="Attention_Output_9_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_9_7 [label="Attention_AllReduce_9_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_7_1 [label="QKV_Proj_9_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_9_7_1 [label="Attention_Scores_9_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_9_7_1 [label="Attention_Softmax_9_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_9_7_1 [label="Attention_Dropout_9_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_9_7_1 [label="Attention_Values_9_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_9_7_1 [label="Attention_Weighted_9_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_9_7_1 [label="Attention_Output_9_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_9_7 [label="Attention_AllReduce_9_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_7_2 [label="QKV_Proj_9_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_9_7_2 [label="Attention_Scores_9_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_9_7_2 [label="Attention_Softmax_9_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_9_7_2 [label="Attention_Dropout_9_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_9_7_2 [label="Attention_Values_9_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_9_7_2 [label="Attention_Weighted_9_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_9_7_2 [label="Attention_Output_9_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_9_7 [label="Attention_AllReduce_9_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_7_3 [label="QKV_Proj_9_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_9_7_3 [label="Attention_Scores_9_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_9_7_3 [label="Attention_Softmax_9_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_9_7_3 [label="Attention_Dropout_9_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_9_7_3 [label="Attention_Values_9_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_9_7_3 [label="Attention_Weighted_9_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_9_7_3 [label="Attention_Output_9_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_9_7 [label="Attention_AllReduce_9_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_8_0 [label="QKV_Proj_9_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_9_8_0 [label="Attention_Scores_9_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_9_8_0 [label="Attention_Softmax_9_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_9_8_0 [label="Attention_Dropout_9_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_9_8_0 [label="Attention_Values_9_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_9_8_0 [label="Attention_Weighted_9_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_9_8_0 [label="Attention_Output_9_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_9_8 [label="Attention_AllReduce_9_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_8_1 [label="QKV_Proj_9_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_9_8_1 [label="Attention_Scores_9_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_9_8_1 [label="Attention_Softmax_9_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_9_8_1 [label="Attention_Dropout_9_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_9_8_1 [label="Attention_Values_9_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_9_8_1 [label="Attention_Weighted_9_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_9_8_1 [label="Attention_Output_9_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_9_8 [label="Attention_AllReduce_9_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_8_2 [label="QKV_Proj_9_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_9_8_2 [label="Attention_Scores_9_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_9_8_2 [label="Attention_Softmax_9_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_9_8_2 [label="Attention_Dropout_9_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_9_8_2 [label="Attention_Values_9_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_9_8_2 [label="Attention_Weighted_9_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_9_8_2 [label="Attention_Output_9_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_9_8 [label="Attention_AllReduce_9_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_8_3 [label="QKV_Proj_9_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_9_8_3 [label="Attention_Scores_9_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_9_8_3 [label="Attention_Softmax_9_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_9_8_3 [label="Attention_Dropout_9_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_9_8_3 [label="Attention_Values_9_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_9_8_3 [label="Attention_Weighted_9_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_9_8_3 [label="Attention_Output_9_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_9_8 [label="Attention_AllReduce_9_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_9_0 [label="QKV_Proj_9_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_9_9_0 [label="Attention_Scores_9_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_9_9_0 [label="Attention_Softmax_9_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_9_9_0 [label="Attention_Dropout_9_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_9_9_0 [label="Attention_Values_9_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_9_9_0 [label="Attention_Weighted_9_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_9_9_0 [label="Attention_Output_9_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_9_9 [label="Attention_AllReduce_9_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_9_1 [label="QKV_Proj_9_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_9_9_1 [label="Attention_Scores_9_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_9_9_1 [label="Attention_Softmax_9_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_9_9_1 [label="Attention_Dropout_9_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_9_9_1 [label="Attention_Values_9_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_9_9_1 [label="Attention_Weighted_9_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_9_9_1 [label="Attention_Output_9_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_9_9 [label="Attention_AllReduce_9_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_9_2 [label="QKV_Proj_9_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_9_9_2 [label="Attention_Scores_9_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_9_9_2 [label="Attention_Softmax_9_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_9_9_2 [label="Attention_Dropout_9_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_9_9_2 [label="Attention_Values_9_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_9_9_2 [label="Attention_Weighted_9_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_9_9_2 [label="Attention_Output_9_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_9_9 [label="Attention_AllReduce_9_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_9_3 [label="QKV_Proj_9_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_9_9_3 [label="Attention_Scores_9_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_9_9_3 [label="Attention_Softmax_9_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_9_9_3 [label="Attention_Dropout_9_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_9_9_3 [label="Attention_Values_9_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_9_9_3 [label="Attention_Weighted_9_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_9_9_3 [label="Attention_Output_9_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_9_9 [label="Attention_AllReduce_9_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_10_0 [label="QKV_Proj_9_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_9_10_0 [label="Attention_Scores_9_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_9_10_0 [label="Attention_Softmax_9_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_9_10_0 [label="Attention_Dropout_9_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_9_10_0 [label="Attention_Values_9_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_9_10_0 [label="Attention_Weighted_9_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_9_10_0 [label="Attention_Output_9_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_9_10 [label="Attention_AllReduce_9_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_10_1 [label="QKV_Proj_9_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_9_10_1 [label="Attention_Scores_9_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_9_10_1 [label="Attention_Softmax_9_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_9_10_1 [label="Attention_Dropout_9_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_9_10_1 [label="Attention_Values_9_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_9_10_1 [label="Attention_Weighted_9_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_9_10_1 [label="Attention_Output_9_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_9_10 [label="Attention_AllReduce_9_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_10_2 [label="QKV_Proj_9_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_9_10_2 [label="Attention_Scores_9_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_9_10_2 [label="Attention_Softmax_9_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_9_10_2 [label="Attention_Dropout_9_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_9_10_2 [label="Attention_Values_9_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_9_10_2 [label="Attention_Weighted_9_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_9_10_2 [label="Attention_Output_9_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_9_10 [label="Attention_AllReduce_9_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_10_3 [label="QKV_Proj_9_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_9_10_3 [label="Attention_Scores_9_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_9_10_3 [label="Attention_Softmax_9_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_9_10_3 [label="Attention_Dropout_9_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_9_10_3 [label="Attention_Values_9_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_9_10_3 [label="Attention_Weighted_9_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_9_10_3 [label="Attention_Output_9_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_9_10 [label="Attention_AllReduce_9_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_11_0 [label="QKV_Proj_9_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_9_11_0 [label="Attention_Scores_9_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_9_11_0 [label="Attention_Softmax_9_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_9_11_0 [label="Attention_Dropout_9_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_9_11_0 [label="Attention_Values_9_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_9_11_0 [label="Attention_Weighted_9_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_9_11_0 [label="Attention_Output_9_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_9_11 [label="Attention_AllReduce_9_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_11_1 [label="QKV_Proj_9_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_9_11_1 [label="Attention_Scores_9_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_9_11_1 [label="Attention_Softmax_9_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_9_11_1 [label="Attention_Dropout_9_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_9_11_1 [label="Attention_Values_9_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_9_11_1 [label="Attention_Weighted_9_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_9_11_1 [label="Attention_Output_9_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_9_11 [label="Attention_AllReduce_9_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_11_2 [label="QKV_Proj_9_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_9_11_2 [label="Attention_Scores_9_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_9_11_2 [label="Attention_Softmax_9_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_9_11_2 [label="Attention_Dropout_9_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_9_11_2 [label="Attention_Values_9_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_9_11_2 [label="Attention_Weighted_9_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_9_11_2 [label="Attention_Output_9_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_9_11 [label="Attention_AllReduce_9_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_11_3 [label="QKV_Proj_9_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_9_11_3 [label="Attention_Scores_9_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_9_11_3 [label="Attention_Softmax_9_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_9_11_3 [label="Attention_Dropout_9_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_9_11_3 [label="Attention_Values_9_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_9_11_3 [label="Attention_Weighted_9_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_9_11_3 [label="Attention_Output_9_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_9_11 [label="Attention_AllReduce_9_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_12_0 [label="QKV_Proj_9_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_9_12_0 [label="Attention_Scores_9_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_9_12_0 [label="Attention_Softmax_9_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_9_12_0 [label="Attention_Dropout_9_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_9_12_0 [label="Attention_Values_9_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_9_12_0 [label="Attention_Weighted_9_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_9_12_0 [label="Attention_Output_9_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_9_12 [label="Attention_AllReduce_9_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_12_1 [label="QKV_Proj_9_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_9_12_1 [label="Attention_Scores_9_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_9_12_1 [label="Attention_Softmax_9_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_9_12_1 [label="Attention_Dropout_9_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_9_12_1 [label="Attention_Values_9_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_9_12_1 [label="Attention_Weighted_9_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_9_12_1 [label="Attention_Output_9_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_9_12 [label="Attention_AllReduce_9_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_12_2 [label="QKV_Proj_9_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_9_12_2 [label="Attention_Scores_9_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_9_12_2 [label="Attention_Softmax_9_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_9_12_2 [label="Attention_Dropout_9_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_9_12_2 [label="Attention_Values_9_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_9_12_2 [label="Attention_Weighted_9_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_9_12_2 [label="Attention_Output_9_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_9_12 [label="Attention_AllReduce_9_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_12_3 [label="QKV_Proj_9_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_9_12_3 [label="Attention_Scores_9_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_9_12_3 [label="Attention_Softmax_9_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_9_12_3 [label="Attention_Dropout_9_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_9_12_3 [label="Attention_Values_9_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_9_12_3 [label="Attention_Weighted_9_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_9_12_3 [label="Attention_Output_9_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_9_12 [label="Attention_AllReduce_9_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_13_0 [label="QKV_Proj_9_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_9_13_0 [label="Attention_Scores_9_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_9_13_0 [label="Attention_Softmax_9_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_9_13_0 [label="Attention_Dropout_9_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_9_13_0 [label="Attention_Values_9_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_9_13_0 [label="Attention_Weighted_9_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_9_13_0 [label="Attention_Output_9_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_9_13 [label="Attention_AllReduce_9_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_13_1 [label="QKV_Proj_9_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_9_13_1 [label="Attention_Scores_9_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_9_13_1 [label="Attention_Softmax_9_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_9_13_1 [label="Attention_Dropout_9_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_9_13_1 [label="Attention_Values_9_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_9_13_1 [label="Attention_Weighted_9_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_9_13_1 [label="Attention_Output_9_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_9_13 [label="Attention_AllReduce_9_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_13_2 [label="QKV_Proj_9_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_9_13_2 [label="Attention_Scores_9_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_9_13_2 [label="Attention_Softmax_9_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_9_13_2 [label="Attention_Dropout_9_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_9_13_2 [label="Attention_Values_9_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_9_13_2 [label="Attention_Weighted_9_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_9_13_2 [label="Attention_Output_9_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_9_13 [label="Attention_AllReduce_9_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_13_3 [label="QKV_Proj_9_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_9_13_3 [label="Attention_Scores_9_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_9_13_3 [label="Attention_Softmax_9_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_9_13_3 [label="Attention_Dropout_9_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_9_13_3 [label="Attention_Values_9_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_9_13_3 [label="Attention_Weighted_9_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_9_13_3 [label="Attention_Output_9_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_9_13 [label="Attention_AllReduce_9_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_14_0 [label="QKV_Proj_9_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_9_14_0 [label="Attention_Scores_9_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_9_14_0 [label="Attention_Softmax_9_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_9_14_0 [label="Attention_Dropout_9_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_9_14_0 [label="Attention_Values_9_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_9_14_0 [label="Attention_Weighted_9_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_9_14_0 [label="Attention_Output_9_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_9_14 [label="Attention_AllReduce_9_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_14_1 [label="QKV_Proj_9_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_9_14_1 [label="Attention_Scores_9_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_9_14_1 [label="Attention_Softmax_9_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_9_14_1 [label="Attention_Dropout_9_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_9_14_1 [label="Attention_Values_9_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_9_14_1 [label="Attention_Weighted_9_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_9_14_1 [label="Attention_Output_9_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_9_14 [label="Attention_AllReduce_9_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_14_2 [label="QKV_Proj_9_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_9_14_2 [label="Attention_Scores_9_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_9_14_2 [label="Attention_Softmax_9_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_9_14_2 [label="Attention_Dropout_9_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_9_14_2 [label="Attention_Values_9_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_9_14_2 [label="Attention_Weighted_9_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_9_14_2 [label="Attention_Output_9_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_9_14 [label="Attention_AllReduce_9_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_14_3 [label="QKV_Proj_9_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_9_14_3 [label="Attention_Scores_9_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_9_14_3 [label="Attention_Softmax_9_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_9_14_3 [label="Attention_Dropout_9_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_9_14_3 [label="Attention_Values_9_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_9_14_3 [label="Attention_Weighted_9_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_9_14_3 [label="Attention_Output_9_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_9_14 [label="Attention_AllReduce_9_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_15_0 [label="QKV_Proj_9_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_9_15_0 [label="Attention_Scores_9_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_9_15_0 [label="Attention_Softmax_9_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_9_15_0 [label="Attention_Dropout_9_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_9_15_0 [label="Attention_Values_9_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_9_15_0 [label="Attention_Weighted_9_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_9_15_0 [label="Attention_Output_9_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_9_15 [label="Attention_AllReduce_9_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_15_1 [label="QKV_Proj_9_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_9_15_1 [label="Attention_Scores_9_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_9_15_1 [label="Attention_Softmax_9_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_9_15_1 [label="Attention_Dropout_9_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_9_15_1 [label="Attention_Values_9_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_9_15_1 [label="Attention_Weighted_9_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_9_15_1 [label="Attention_Output_9_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_9_15 [label="Attention_AllReduce_9_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_15_2 [label="QKV_Proj_9_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_9_15_2 [label="Attention_Scores_9_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_9_15_2 [label="Attention_Softmax_9_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_9_15_2 [label="Attention_Dropout_9_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_9_15_2 [label="Attention_Values_9_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_9_15_2 [label="Attention_Weighted_9_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_9_15_2 [label="Attention_Output_9_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_9_15 [label="Attention_AllReduce_9_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_9_15_3 [label="QKV_Proj_9_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_9_15_3 [label="Attention_Scores_9_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_9_15_3 [label="Attention_Softmax_9_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_9_15_3 [label="Attention_Dropout_9_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_9_15_3 [label="Attention_Values_9_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_9_15_3 [label="Attention_Weighted_9_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_9_15_3 [label="Attention_Output_9_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_9_15 [label="Attention_AllReduce_9_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_0_0 [label="MLP_Linear1_9_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_9_0_0 [label="GELU_9_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_9_0_0 [label="MLP_Linear2_9_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_9_0_1 [label="MLP_Linear1_9_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_9_0_1 [label="GELU_9_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_9_0_1 [label="MLP_Linear2_9_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_9_0_2 [label="MLP_Linear1_9_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_9_0_2 [label="GELU_9_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_9_0_2 [label="MLP_Linear2_9_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_9_0_3 [label="MLP_Linear1_9_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_9_0_3 [label="GELU_9_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_9_0_3 [label="MLP_Linear2_9_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_9_0 [label="MLP_AllReduce_9_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_1_0 [label="MLP_Linear1_9_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_9_1_0 [label="GELU_9_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_9_1_0 [label="MLP_Linear2_9_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_9_1_1 [label="MLP_Linear1_9_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_9_1_1 [label="GELU_9_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_9_1_1 [label="MLP_Linear2_9_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_9_1_2 [label="MLP_Linear1_9_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_9_1_2 [label="GELU_9_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_9_1_2 [label="MLP_Linear2_9_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_9_1_3 [label="MLP_Linear1_9_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_9_1_3 [label="GELU_9_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_9_1_3 [label="MLP_Linear2_9_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_9_1 [label="MLP_AllReduce_9_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_2_0 [label="MLP_Linear1_9_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_9_2_0 [label="GELU_9_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_9_2_0 [label="MLP_Linear2_9_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_9_2_1 [label="MLP_Linear1_9_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_9_2_1 [label="GELU_9_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_9_2_1 [label="MLP_Linear2_9_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_9_2_2 [label="MLP_Linear1_9_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_9_2_2 [label="GELU_9_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_9_2_2 [label="MLP_Linear2_9_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_9_2_3 [label="MLP_Linear1_9_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_9_2_3 [label="GELU_9_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_9_2_3 [label="MLP_Linear2_9_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_9_2 [label="MLP_AllReduce_9_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_3_0 [label="MLP_Linear1_9_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_9_3_0 [label="GELU_9_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_9_3_0 [label="MLP_Linear2_9_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_9_3_1 [label="MLP_Linear1_9_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_9_3_1 [label="GELU_9_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_9_3_1 [label="MLP_Linear2_9_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_9_3_2 [label="MLP_Linear1_9_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_9_3_2 [label="GELU_9_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_9_3_2 [label="MLP_Linear2_9_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_9_3_3 [label="MLP_Linear1_9_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_9_3_3 [label="GELU_9_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_9_3_3 [label="MLP_Linear2_9_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_9_3 [label="MLP_AllReduce_9_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_4_0 [label="MLP_Linear1_9_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_9_4_0 [label="GELU_9_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_9_4_0 [label="MLP_Linear2_9_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_9_4_1 [label="MLP_Linear1_9_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_9_4_1 [label="GELU_9_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_9_4_1 [label="MLP_Linear2_9_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_9_4_2 [label="MLP_Linear1_9_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_9_4_2 [label="GELU_9_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_9_4_2 [label="MLP_Linear2_9_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_9_4_3 [label="MLP_Linear1_9_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_9_4_3 [label="GELU_9_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_9_4_3 [label="MLP_Linear2_9_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_9_4 [label="MLP_AllReduce_9_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_5_0 [label="MLP_Linear1_9_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_9_5_0 [label="GELU_9_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_9_5_0 [label="MLP_Linear2_9_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_9_5_1 [label="MLP_Linear1_9_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_9_5_1 [label="GELU_9_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_9_5_1 [label="MLP_Linear2_9_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_9_5_2 [label="MLP_Linear1_9_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_9_5_2 [label="GELU_9_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_9_5_2 [label="MLP_Linear2_9_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_9_5_3 [label="MLP_Linear1_9_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_9_5_3 [label="GELU_9_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_9_5_3 [label="MLP_Linear2_9_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_9_5 [label="MLP_AllReduce_9_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_6_0 [label="MLP_Linear1_9_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_9_6_0 [label="GELU_9_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_9_6_0 [label="MLP_Linear2_9_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_9_6_1 [label="MLP_Linear1_9_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_9_6_1 [label="GELU_9_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_9_6_1 [label="MLP_Linear2_9_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_9_6_2 [label="MLP_Linear1_9_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_9_6_2 [label="GELU_9_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_9_6_2 [label="MLP_Linear2_9_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_9_6_3 [label="MLP_Linear1_9_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_9_6_3 [label="GELU_9_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_9_6_3 [label="MLP_Linear2_9_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_9_6 [label="MLP_AllReduce_9_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_7_0 [label="MLP_Linear1_9_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_9_7_0 [label="GELU_9_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_9_7_0 [label="MLP_Linear2_9_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_9_7_1 [label="MLP_Linear1_9_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_9_7_1 [label="GELU_9_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_9_7_1 [label="MLP_Linear2_9_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_9_7_2 [label="MLP_Linear1_9_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_9_7_2 [label="GELU_9_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_9_7_2 [label="MLP_Linear2_9_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_9_7_3 [label="MLP_Linear1_9_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_9_7_3 [label="GELU_9_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_9_7_3 [label="MLP_Linear2_9_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_9_7 [label="MLP_AllReduce_9_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_8_0 [label="MLP_Linear1_9_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_9_8_0 [label="GELU_9_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_9_8_0 [label="MLP_Linear2_9_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_9_8_1 [label="MLP_Linear1_9_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_9_8_1 [label="GELU_9_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_9_8_1 [label="MLP_Linear2_9_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_9_8_2 [label="MLP_Linear1_9_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_9_8_2 [label="GELU_9_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_9_8_2 [label="MLP_Linear2_9_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_9_8_3 [label="MLP_Linear1_9_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_9_8_3 [label="GELU_9_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_9_8_3 [label="MLP_Linear2_9_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_9_8 [label="MLP_AllReduce_9_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_9_0 [label="MLP_Linear1_9_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_9_9_0 [label="GELU_9_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_9_9_0 [label="MLP_Linear2_9_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_9_9_1 [label="MLP_Linear1_9_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_9_9_1 [label="GELU_9_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_9_9_1 [label="MLP_Linear2_9_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_9_9_2 [label="MLP_Linear1_9_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_9_9_2 [label="GELU_9_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_9_9_2 [label="MLP_Linear2_9_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_9_9_3 [label="MLP_Linear1_9_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_9_9_3 [label="GELU_9_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_9_9_3 [label="MLP_Linear2_9_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_9_9 [label="MLP_AllReduce_9_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_10_0 [label="MLP_Linear1_9_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_9_10_0 [label="GELU_9_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_9_10_0 [label="MLP_Linear2_9_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_9_10_1 [label="MLP_Linear1_9_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_9_10_1 [label="GELU_9_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_9_10_1 [label="MLP_Linear2_9_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_9_10_2 [label="MLP_Linear1_9_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_9_10_2 [label="GELU_9_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_9_10_2 [label="MLP_Linear2_9_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_9_10_3 [label="MLP_Linear1_9_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_9_10_3 [label="GELU_9_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_9_10_3 [label="MLP_Linear2_9_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_9_10 [label="MLP_AllReduce_9_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_11_0 [label="MLP_Linear1_9_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_9_11_0 [label="GELU_9_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_9_11_0 [label="MLP_Linear2_9_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_9_11_1 [label="MLP_Linear1_9_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_9_11_1 [label="GELU_9_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_9_11_1 [label="MLP_Linear2_9_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_9_11_2 [label="MLP_Linear1_9_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_9_11_2 [label="GELU_9_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_9_11_2 [label="MLP_Linear2_9_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_9_11_3 [label="MLP_Linear1_9_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_9_11_3 [label="GELU_9_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_9_11_3 [label="MLP_Linear2_9_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_9_11 [label="MLP_AllReduce_9_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_12_0 [label="MLP_Linear1_9_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_9_12_0 [label="GELU_9_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_9_12_0 [label="MLP_Linear2_9_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_9_12_1 [label="MLP_Linear1_9_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_9_12_1 [label="GELU_9_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_9_12_1 [label="MLP_Linear2_9_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_9_12_2 [label="MLP_Linear1_9_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_9_12_2 [label="GELU_9_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_9_12_2 [label="MLP_Linear2_9_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_9_12_3 [label="MLP_Linear1_9_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_9_12_3 [label="GELU_9_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_9_12_3 [label="MLP_Linear2_9_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_9_12 [label="MLP_AllReduce_9_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_13_0 [label="MLP_Linear1_9_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_9_13_0 [label="GELU_9_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_9_13_0 [label="MLP_Linear2_9_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_9_13_1 [label="MLP_Linear1_9_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_9_13_1 [label="GELU_9_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_9_13_1 [label="MLP_Linear2_9_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_9_13_2 [label="MLP_Linear1_9_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_9_13_2 [label="GELU_9_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_9_13_2 [label="MLP_Linear2_9_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_9_13_3 [label="MLP_Linear1_9_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_9_13_3 [label="GELU_9_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_9_13_3 [label="MLP_Linear2_9_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_9_13 [label="MLP_AllReduce_9_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_14_0 [label="MLP_Linear1_9_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_9_14_0 [label="GELU_9_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_9_14_0 [label="MLP_Linear2_9_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_9_14_1 [label="MLP_Linear1_9_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_9_14_1 [label="GELU_9_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_9_14_1 [label="MLP_Linear2_9_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_9_14_2 [label="MLP_Linear1_9_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_9_14_2 [label="GELU_9_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_9_14_2 [label="MLP_Linear2_9_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_9_14_3 [label="MLP_Linear1_9_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_9_14_3 [label="GELU_9_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_9_14_3 [label="MLP_Linear2_9_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_9_14 [label="MLP_AllReduce_9_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_9_15_0 [label="MLP_Linear1_9_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_9_15_0 [label="GELU_9_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_9_15_0 [label="MLP_Linear2_9_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_9_15_1 [label="MLP_Linear1_9_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_9_15_1 [label="GELU_9_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_9_15_1 [label="MLP_Linear2_9_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_9_15_2 [label="MLP_Linear1_9_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_9_15_2 [label="GELU_9_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_9_15_2 [label="MLP_Linear2_9_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_9_15_3 [label="MLP_Linear1_9_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_9_15_3 [label="GELU_9_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_9_15_3 [label="MLP_Linear2_9_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_9_15 [label="MLP_AllReduce_9_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_9_0 [label="Expert_Route_9_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_1 [label="Expert_Route_9_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_2 [label="Expert_Route_9_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_3 [label="Expert_Route_9_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_4 [label="Expert_Route_9_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_5 [label="Expert_Route_9_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_6 [label="Expert_Route_9_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_7 [label="Expert_Route_9_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_8 [label="Expert_Route_9_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_9 [label="Expert_Route_9_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_10 [label="Expert_Route_9_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_11 [label="Expert_Route_9_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_12 [label="Expert_Route_9_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_13 [label="Expert_Route_9_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_14 [label="Expert_Route_9_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_9_15 [label="Expert_Route_9_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_10_0_0 [label="QKV_Proj_10_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_10_0_0 [label="Attention_Scores_10_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_10_0_0 [label="Attention_Softmax_10_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_10_0_0 [label="Attention_Dropout_10_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_10_0_0 [label="Attention_Values_10_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_10_0_0 [label="Attention_Weighted_10_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_10_0_0 [label="Attention_Output_10_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_10_0 [label="Attention_AllReduce_10_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_0_1 [label="QKV_Proj_10_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_10_0_1 [label="Attention_Scores_10_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_10_0_1 [label="Attention_Softmax_10_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_10_0_1 [label="Attention_Dropout_10_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_10_0_1 [label="Attention_Values_10_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_10_0_1 [label="Attention_Weighted_10_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_10_0_1 [label="Attention_Output_10_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_10_0 [label="Attention_AllReduce_10_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_0_2 [label="QKV_Proj_10_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_10_0_2 [label="Attention_Scores_10_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_10_0_2 [label="Attention_Softmax_10_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_10_0_2 [label="Attention_Dropout_10_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_10_0_2 [label="Attention_Values_10_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_10_0_2 [label="Attention_Weighted_10_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_10_0_2 [label="Attention_Output_10_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_10_0 [label="Attention_AllReduce_10_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_0_3 [label="QKV_Proj_10_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_10_0_3 [label="Attention_Scores_10_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_10_0_3 [label="Attention_Softmax_10_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_10_0_3 [label="Attention_Dropout_10_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_10_0_3 [label="Attention_Values_10_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_10_0_3 [label="Attention_Weighted_10_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_10_0_3 [label="Attention_Output_10_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_10_0 [label="Attention_AllReduce_10_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_1_0 [label="QKV_Proj_10_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_10_1_0 [label="Attention_Scores_10_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_10_1_0 [label="Attention_Softmax_10_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_10_1_0 [label="Attention_Dropout_10_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_10_1_0 [label="Attention_Values_10_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_10_1_0 [label="Attention_Weighted_10_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_10_1_0 [label="Attention_Output_10_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_10_1 [label="Attention_AllReduce_10_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_1_1 [label="QKV_Proj_10_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_10_1_1 [label="Attention_Scores_10_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_10_1_1 [label="Attention_Softmax_10_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_10_1_1 [label="Attention_Dropout_10_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_10_1_1 [label="Attention_Values_10_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_10_1_1 [label="Attention_Weighted_10_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_10_1_1 [label="Attention_Output_10_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_10_1 [label="Attention_AllReduce_10_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_1_2 [label="QKV_Proj_10_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_10_1_2 [label="Attention_Scores_10_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_10_1_2 [label="Attention_Softmax_10_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_10_1_2 [label="Attention_Dropout_10_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_10_1_2 [label="Attention_Values_10_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_10_1_2 [label="Attention_Weighted_10_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_10_1_2 [label="Attention_Output_10_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_10_1 [label="Attention_AllReduce_10_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_1_3 [label="QKV_Proj_10_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_10_1_3 [label="Attention_Scores_10_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_10_1_3 [label="Attention_Softmax_10_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_10_1_3 [label="Attention_Dropout_10_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_10_1_3 [label="Attention_Values_10_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_10_1_3 [label="Attention_Weighted_10_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_10_1_3 [label="Attention_Output_10_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_10_1 [label="Attention_AllReduce_10_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_2_0 [label="QKV_Proj_10_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_10_2_0 [label="Attention_Scores_10_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_10_2_0 [label="Attention_Softmax_10_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_10_2_0 [label="Attention_Dropout_10_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_10_2_0 [label="Attention_Values_10_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_10_2_0 [label="Attention_Weighted_10_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_10_2_0 [label="Attention_Output_10_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_10_2 [label="Attention_AllReduce_10_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_2_1 [label="QKV_Proj_10_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_10_2_1 [label="Attention_Scores_10_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_10_2_1 [label="Attention_Softmax_10_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_10_2_1 [label="Attention_Dropout_10_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_10_2_1 [label="Attention_Values_10_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_10_2_1 [label="Attention_Weighted_10_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_10_2_1 [label="Attention_Output_10_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_10_2 [label="Attention_AllReduce_10_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_2_2 [label="QKV_Proj_10_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_10_2_2 [label="Attention_Scores_10_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_10_2_2 [label="Attention_Softmax_10_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_10_2_2 [label="Attention_Dropout_10_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_10_2_2 [label="Attention_Values_10_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_10_2_2 [label="Attention_Weighted_10_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_10_2_2 [label="Attention_Output_10_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_10_2 [label="Attention_AllReduce_10_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_2_3 [label="QKV_Proj_10_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_10_2_3 [label="Attention_Scores_10_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_10_2_3 [label="Attention_Softmax_10_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_10_2_3 [label="Attention_Dropout_10_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_10_2_3 [label="Attention_Values_10_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_10_2_3 [label="Attention_Weighted_10_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_10_2_3 [label="Attention_Output_10_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_10_2 [label="Attention_AllReduce_10_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_3_0 [label="QKV_Proj_10_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_10_3_0 [label="Attention_Scores_10_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_10_3_0 [label="Attention_Softmax_10_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_10_3_0 [label="Attention_Dropout_10_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_10_3_0 [label="Attention_Values_10_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_10_3_0 [label="Attention_Weighted_10_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_10_3_0 [label="Attention_Output_10_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_10_3 [label="Attention_AllReduce_10_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_3_1 [label="QKV_Proj_10_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_10_3_1 [label="Attention_Scores_10_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_10_3_1 [label="Attention_Softmax_10_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_10_3_1 [label="Attention_Dropout_10_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_10_3_1 [label="Attention_Values_10_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_10_3_1 [label="Attention_Weighted_10_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_10_3_1 [label="Attention_Output_10_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_10_3 [label="Attention_AllReduce_10_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_3_2 [label="QKV_Proj_10_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_10_3_2 [label="Attention_Scores_10_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_10_3_2 [label="Attention_Softmax_10_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_10_3_2 [label="Attention_Dropout_10_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_10_3_2 [label="Attention_Values_10_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_10_3_2 [label="Attention_Weighted_10_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_10_3_2 [label="Attention_Output_10_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_10_3 [label="Attention_AllReduce_10_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_3_3 [label="QKV_Proj_10_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_10_3_3 [label="Attention_Scores_10_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_10_3_3 [label="Attention_Softmax_10_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_10_3_3 [label="Attention_Dropout_10_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_10_3_3 [label="Attention_Values_10_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_10_3_3 [label="Attention_Weighted_10_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_10_3_3 [label="Attention_Output_10_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_10_3 [label="Attention_AllReduce_10_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_4_0 [label="QKV_Proj_10_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_10_4_0 [label="Attention_Scores_10_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_10_4_0 [label="Attention_Softmax_10_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_10_4_0 [label="Attention_Dropout_10_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_10_4_0 [label="Attention_Values_10_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_10_4_0 [label="Attention_Weighted_10_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_10_4_0 [label="Attention_Output_10_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_10_4 [label="Attention_AllReduce_10_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_4_1 [label="QKV_Proj_10_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_10_4_1 [label="Attention_Scores_10_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_10_4_1 [label="Attention_Softmax_10_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_10_4_1 [label="Attention_Dropout_10_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_10_4_1 [label="Attention_Values_10_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_10_4_1 [label="Attention_Weighted_10_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_10_4_1 [label="Attention_Output_10_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_10_4 [label="Attention_AllReduce_10_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_4_2 [label="QKV_Proj_10_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_10_4_2 [label="Attention_Scores_10_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_10_4_2 [label="Attention_Softmax_10_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_10_4_2 [label="Attention_Dropout_10_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_10_4_2 [label="Attention_Values_10_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_10_4_2 [label="Attention_Weighted_10_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_10_4_2 [label="Attention_Output_10_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_10_4 [label="Attention_AllReduce_10_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_4_3 [label="QKV_Proj_10_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_10_4_3 [label="Attention_Scores_10_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_10_4_3 [label="Attention_Softmax_10_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_10_4_3 [label="Attention_Dropout_10_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_10_4_3 [label="Attention_Values_10_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_10_4_3 [label="Attention_Weighted_10_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_10_4_3 [label="Attention_Output_10_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_10_4 [label="Attention_AllReduce_10_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_5_0 [label="QKV_Proj_10_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_10_5_0 [label="Attention_Scores_10_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_10_5_0 [label="Attention_Softmax_10_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_10_5_0 [label="Attention_Dropout_10_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_10_5_0 [label="Attention_Values_10_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_10_5_0 [label="Attention_Weighted_10_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_10_5_0 [label="Attention_Output_10_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_10_5 [label="Attention_AllReduce_10_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_5_1 [label="QKV_Proj_10_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_10_5_1 [label="Attention_Scores_10_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_10_5_1 [label="Attention_Softmax_10_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_10_5_1 [label="Attention_Dropout_10_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_10_5_1 [label="Attention_Values_10_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_10_5_1 [label="Attention_Weighted_10_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_10_5_1 [label="Attention_Output_10_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_10_5 [label="Attention_AllReduce_10_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_5_2 [label="QKV_Proj_10_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_10_5_2 [label="Attention_Scores_10_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_10_5_2 [label="Attention_Softmax_10_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_10_5_2 [label="Attention_Dropout_10_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_10_5_2 [label="Attention_Values_10_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_10_5_2 [label="Attention_Weighted_10_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_10_5_2 [label="Attention_Output_10_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_10_5 [label="Attention_AllReduce_10_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_5_3 [label="QKV_Proj_10_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_10_5_3 [label="Attention_Scores_10_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_10_5_3 [label="Attention_Softmax_10_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_10_5_3 [label="Attention_Dropout_10_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_10_5_3 [label="Attention_Values_10_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_10_5_3 [label="Attention_Weighted_10_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_10_5_3 [label="Attention_Output_10_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_10_5 [label="Attention_AllReduce_10_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_6_0 [label="QKV_Proj_10_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_10_6_0 [label="Attention_Scores_10_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_10_6_0 [label="Attention_Softmax_10_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_10_6_0 [label="Attention_Dropout_10_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_10_6_0 [label="Attention_Values_10_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_10_6_0 [label="Attention_Weighted_10_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_10_6_0 [label="Attention_Output_10_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_10_6 [label="Attention_AllReduce_10_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_6_1 [label="QKV_Proj_10_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_10_6_1 [label="Attention_Scores_10_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_10_6_1 [label="Attention_Softmax_10_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_10_6_1 [label="Attention_Dropout_10_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_10_6_1 [label="Attention_Values_10_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_10_6_1 [label="Attention_Weighted_10_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_10_6_1 [label="Attention_Output_10_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_10_6 [label="Attention_AllReduce_10_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_6_2 [label="QKV_Proj_10_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_10_6_2 [label="Attention_Scores_10_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_10_6_2 [label="Attention_Softmax_10_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_10_6_2 [label="Attention_Dropout_10_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_10_6_2 [label="Attention_Values_10_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_10_6_2 [label="Attention_Weighted_10_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_10_6_2 [label="Attention_Output_10_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_10_6 [label="Attention_AllReduce_10_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_6_3 [label="QKV_Proj_10_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_10_6_3 [label="Attention_Scores_10_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_10_6_3 [label="Attention_Softmax_10_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_10_6_3 [label="Attention_Dropout_10_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_10_6_3 [label="Attention_Values_10_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_10_6_3 [label="Attention_Weighted_10_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_10_6_3 [label="Attention_Output_10_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_10_6 [label="Attention_AllReduce_10_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_7_0 [label="QKV_Proj_10_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_10_7_0 [label="Attention_Scores_10_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_10_7_0 [label="Attention_Softmax_10_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_10_7_0 [label="Attention_Dropout_10_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_10_7_0 [label="Attention_Values_10_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_10_7_0 [label="Attention_Weighted_10_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_10_7_0 [label="Attention_Output_10_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_10_7 [label="Attention_AllReduce_10_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_7_1 [label="QKV_Proj_10_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_10_7_1 [label="Attention_Scores_10_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_10_7_1 [label="Attention_Softmax_10_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_10_7_1 [label="Attention_Dropout_10_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_10_7_1 [label="Attention_Values_10_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_10_7_1 [label="Attention_Weighted_10_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_10_7_1 [label="Attention_Output_10_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_10_7 [label="Attention_AllReduce_10_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_7_2 [label="QKV_Proj_10_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_10_7_2 [label="Attention_Scores_10_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_10_7_2 [label="Attention_Softmax_10_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_10_7_2 [label="Attention_Dropout_10_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_10_7_2 [label="Attention_Values_10_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_10_7_2 [label="Attention_Weighted_10_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_10_7_2 [label="Attention_Output_10_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_10_7 [label="Attention_AllReduce_10_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_7_3 [label="QKV_Proj_10_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_10_7_3 [label="Attention_Scores_10_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_10_7_3 [label="Attention_Softmax_10_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_10_7_3 [label="Attention_Dropout_10_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_10_7_3 [label="Attention_Values_10_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_10_7_3 [label="Attention_Weighted_10_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_10_7_3 [label="Attention_Output_10_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_10_7 [label="Attention_AllReduce_10_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_8_0 [label="QKV_Proj_10_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_10_8_0 [label="Attention_Scores_10_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_10_8_0 [label="Attention_Softmax_10_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_10_8_0 [label="Attention_Dropout_10_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_10_8_0 [label="Attention_Values_10_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_10_8_0 [label="Attention_Weighted_10_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_10_8_0 [label="Attention_Output_10_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_10_8 [label="Attention_AllReduce_10_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_8_1 [label="QKV_Proj_10_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_10_8_1 [label="Attention_Scores_10_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_10_8_1 [label="Attention_Softmax_10_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_10_8_1 [label="Attention_Dropout_10_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_10_8_1 [label="Attention_Values_10_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_10_8_1 [label="Attention_Weighted_10_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_10_8_1 [label="Attention_Output_10_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_10_8 [label="Attention_AllReduce_10_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_8_2 [label="QKV_Proj_10_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_10_8_2 [label="Attention_Scores_10_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_10_8_2 [label="Attention_Softmax_10_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_10_8_2 [label="Attention_Dropout_10_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_10_8_2 [label="Attention_Values_10_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_10_8_2 [label="Attention_Weighted_10_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_10_8_2 [label="Attention_Output_10_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_10_8 [label="Attention_AllReduce_10_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_8_3 [label="QKV_Proj_10_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_10_8_3 [label="Attention_Scores_10_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_10_8_3 [label="Attention_Softmax_10_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_10_8_3 [label="Attention_Dropout_10_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_10_8_3 [label="Attention_Values_10_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_10_8_3 [label="Attention_Weighted_10_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_10_8_3 [label="Attention_Output_10_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_10_8 [label="Attention_AllReduce_10_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_9_0 [label="QKV_Proj_10_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_10_9_0 [label="Attention_Scores_10_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_10_9_0 [label="Attention_Softmax_10_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_10_9_0 [label="Attention_Dropout_10_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_10_9_0 [label="Attention_Values_10_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_10_9_0 [label="Attention_Weighted_10_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_10_9_0 [label="Attention_Output_10_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_10_9 [label="Attention_AllReduce_10_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_9_1 [label="QKV_Proj_10_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_10_9_1 [label="Attention_Scores_10_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_10_9_1 [label="Attention_Softmax_10_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_10_9_1 [label="Attention_Dropout_10_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_10_9_1 [label="Attention_Values_10_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_10_9_1 [label="Attention_Weighted_10_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_10_9_1 [label="Attention_Output_10_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_10_9 [label="Attention_AllReduce_10_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_9_2 [label="QKV_Proj_10_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_10_9_2 [label="Attention_Scores_10_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_10_9_2 [label="Attention_Softmax_10_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_10_9_2 [label="Attention_Dropout_10_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_10_9_2 [label="Attention_Values_10_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_10_9_2 [label="Attention_Weighted_10_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_10_9_2 [label="Attention_Output_10_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_10_9 [label="Attention_AllReduce_10_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_9_3 [label="QKV_Proj_10_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_10_9_3 [label="Attention_Scores_10_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_10_9_3 [label="Attention_Softmax_10_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_10_9_3 [label="Attention_Dropout_10_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_10_9_3 [label="Attention_Values_10_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_10_9_3 [label="Attention_Weighted_10_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_10_9_3 [label="Attention_Output_10_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_10_9 [label="Attention_AllReduce_10_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_10_0 [label="QKV_Proj_10_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_10_10_0 [label="Attention_Scores_10_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_10_10_0 [label="Attention_Softmax_10_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_10_10_0 [label="Attention_Dropout_10_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_10_10_0 [label="Attention_Values_10_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_10_10_0 [label="Attention_Weighted_10_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_10_10_0 [label="Attention_Output_10_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_10_10 [label="Attention_AllReduce_10_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_10_1 [label="QKV_Proj_10_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_10_10_1 [label="Attention_Scores_10_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_10_10_1 [label="Attention_Softmax_10_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_10_10_1 [label="Attention_Dropout_10_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_10_10_1 [label="Attention_Values_10_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_10_10_1 [label="Attention_Weighted_10_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_10_10_1 [label="Attention_Output_10_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_10_10 [label="Attention_AllReduce_10_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_10_2 [label="QKV_Proj_10_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_10_10_2 [label="Attention_Scores_10_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_10_10_2 [label="Attention_Softmax_10_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_10_10_2 [label="Attention_Dropout_10_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_10_10_2 [label="Attention_Values_10_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_10_10_2 [label="Attention_Weighted_10_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_10_10_2 [label="Attention_Output_10_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_10_10 [label="Attention_AllReduce_10_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_10_3 [label="QKV_Proj_10_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_10_10_3 [label="Attention_Scores_10_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_10_10_3 [label="Attention_Softmax_10_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_10_10_3 [label="Attention_Dropout_10_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_10_10_3 [label="Attention_Values_10_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_10_10_3 [label="Attention_Weighted_10_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_10_10_3 [label="Attention_Output_10_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_10_10 [label="Attention_AllReduce_10_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_11_0 [label="QKV_Proj_10_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_10_11_0 [label="Attention_Scores_10_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_10_11_0 [label="Attention_Softmax_10_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_10_11_0 [label="Attention_Dropout_10_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_10_11_0 [label="Attention_Values_10_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_10_11_0 [label="Attention_Weighted_10_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_10_11_0 [label="Attention_Output_10_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_10_11 [label="Attention_AllReduce_10_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_11_1 [label="QKV_Proj_10_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_10_11_1 [label="Attention_Scores_10_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_10_11_1 [label="Attention_Softmax_10_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_10_11_1 [label="Attention_Dropout_10_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_10_11_1 [label="Attention_Values_10_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_10_11_1 [label="Attention_Weighted_10_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_10_11_1 [label="Attention_Output_10_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_10_11 [label="Attention_AllReduce_10_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_11_2 [label="QKV_Proj_10_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_10_11_2 [label="Attention_Scores_10_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_10_11_2 [label="Attention_Softmax_10_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_10_11_2 [label="Attention_Dropout_10_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_10_11_2 [label="Attention_Values_10_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_10_11_2 [label="Attention_Weighted_10_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_10_11_2 [label="Attention_Output_10_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_10_11 [label="Attention_AllReduce_10_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_11_3 [label="QKV_Proj_10_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_10_11_3 [label="Attention_Scores_10_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_10_11_3 [label="Attention_Softmax_10_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_10_11_3 [label="Attention_Dropout_10_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_10_11_3 [label="Attention_Values_10_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_10_11_3 [label="Attention_Weighted_10_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_10_11_3 [label="Attention_Output_10_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_10_11 [label="Attention_AllReduce_10_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_12_0 [label="QKV_Proj_10_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_10_12_0 [label="Attention_Scores_10_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_10_12_0 [label="Attention_Softmax_10_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_10_12_0 [label="Attention_Dropout_10_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_10_12_0 [label="Attention_Values_10_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_10_12_0 [label="Attention_Weighted_10_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_10_12_0 [label="Attention_Output_10_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_10_12 [label="Attention_AllReduce_10_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_12_1 [label="QKV_Proj_10_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_10_12_1 [label="Attention_Scores_10_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_10_12_1 [label="Attention_Softmax_10_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_10_12_1 [label="Attention_Dropout_10_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_10_12_1 [label="Attention_Values_10_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_10_12_1 [label="Attention_Weighted_10_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_10_12_1 [label="Attention_Output_10_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_10_12 [label="Attention_AllReduce_10_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_12_2 [label="QKV_Proj_10_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_10_12_2 [label="Attention_Scores_10_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_10_12_2 [label="Attention_Softmax_10_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_10_12_2 [label="Attention_Dropout_10_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_10_12_2 [label="Attention_Values_10_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_10_12_2 [label="Attention_Weighted_10_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_10_12_2 [label="Attention_Output_10_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_10_12 [label="Attention_AllReduce_10_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_12_3 [label="QKV_Proj_10_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_10_12_3 [label="Attention_Scores_10_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_10_12_3 [label="Attention_Softmax_10_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_10_12_3 [label="Attention_Dropout_10_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_10_12_3 [label="Attention_Values_10_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_10_12_3 [label="Attention_Weighted_10_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_10_12_3 [label="Attention_Output_10_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_10_12 [label="Attention_AllReduce_10_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_13_0 [label="QKV_Proj_10_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_10_13_0 [label="Attention_Scores_10_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_10_13_0 [label="Attention_Softmax_10_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_10_13_0 [label="Attention_Dropout_10_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_10_13_0 [label="Attention_Values_10_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_10_13_0 [label="Attention_Weighted_10_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_10_13_0 [label="Attention_Output_10_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_10_13 [label="Attention_AllReduce_10_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_13_1 [label="QKV_Proj_10_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_10_13_1 [label="Attention_Scores_10_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_10_13_1 [label="Attention_Softmax_10_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_10_13_1 [label="Attention_Dropout_10_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_10_13_1 [label="Attention_Values_10_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_10_13_1 [label="Attention_Weighted_10_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_10_13_1 [label="Attention_Output_10_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_10_13 [label="Attention_AllReduce_10_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_13_2 [label="QKV_Proj_10_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_10_13_2 [label="Attention_Scores_10_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_10_13_2 [label="Attention_Softmax_10_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_10_13_2 [label="Attention_Dropout_10_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_10_13_2 [label="Attention_Values_10_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_10_13_2 [label="Attention_Weighted_10_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_10_13_2 [label="Attention_Output_10_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_10_13 [label="Attention_AllReduce_10_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_13_3 [label="QKV_Proj_10_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_10_13_3 [label="Attention_Scores_10_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_10_13_3 [label="Attention_Softmax_10_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_10_13_3 [label="Attention_Dropout_10_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_10_13_3 [label="Attention_Values_10_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_10_13_3 [label="Attention_Weighted_10_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_10_13_3 [label="Attention_Output_10_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_10_13 [label="Attention_AllReduce_10_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_14_0 [label="QKV_Proj_10_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_10_14_0 [label="Attention_Scores_10_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_10_14_0 [label="Attention_Softmax_10_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_10_14_0 [label="Attention_Dropout_10_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_10_14_0 [label="Attention_Values_10_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_10_14_0 [label="Attention_Weighted_10_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_10_14_0 [label="Attention_Output_10_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_10_14 [label="Attention_AllReduce_10_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_14_1 [label="QKV_Proj_10_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_10_14_1 [label="Attention_Scores_10_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_10_14_1 [label="Attention_Softmax_10_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_10_14_1 [label="Attention_Dropout_10_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_10_14_1 [label="Attention_Values_10_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_10_14_1 [label="Attention_Weighted_10_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_10_14_1 [label="Attention_Output_10_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_10_14 [label="Attention_AllReduce_10_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_14_2 [label="QKV_Proj_10_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_10_14_2 [label="Attention_Scores_10_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_10_14_2 [label="Attention_Softmax_10_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_10_14_2 [label="Attention_Dropout_10_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_10_14_2 [label="Attention_Values_10_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_10_14_2 [label="Attention_Weighted_10_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_10_14_2 [label="Attention_Output_10_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_10_14 [label="Attention_AllReduce_10_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_14_3 [label="QKV_Proj_10_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_10_14_3 [label="Attention_Scores_10_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_10_14_3 [label="Attention_Softmax_10_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_10_14_3 [label="Attention_Dropout_10_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_10_14_3 [label="Attention_Values_10_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_10_14_3 [label="Attention_Weighted_10_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_10_14_3 [label="Attention_Output_10_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_10_14 [label="Attention_AllReduce_10_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_15_0 [label="QKV_Proj_10_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_10_15_0 [label="Attention_Scores_10_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_10_15_0 [label="Attention_Softmax_10_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_10_15_0 [label="Attention_Dropout_10_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_10_15_0 [label="Attention_Values_10_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_10_15_0 [label="Attention_Weighted_10_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_10_15_0 [label="Attention_Output_10_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_10_15 [label="Attention_AllReduce_10_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_15_1 [label="QKV_Proj_10_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_10_15_1 [label="Attention_Scores_10_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_10_15_1 [label="Attention_Softmax_10_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_10_15_1 [label="Attention_Dropout_10_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_10_15_1 [label="Attention_Values_10_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_10_15_1 [label="Attention_Weighted_10_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_10_15_1 [label="Attention_Output_10_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_10_15 [label="Attention_AllReduce_10_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_15_2 [label="QKV_Proj_10_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_10_15_2 [label="Attention_Scores_10_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_10_15_2 [label="Attention_Softmax_10_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_10_15_2 [label="Attention_Dropout_10_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_10_15_2 [label="Attention_Values_10_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_10_15_2 [label="Attention_Weighted_10_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_10_15_2 [label="Attention_Output_10_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_10_15 [label="Attention_AllReduce_10_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_10_15_3 [label="QKV_Proj_10_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_10_15_3 [label="Attention_Scores_10_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_10_15_3 [label="Attention_Softmax_10_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_10_15_3 [label="Attention_Dropout_10_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_10_15_3 [label="Attention_Values_10_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_10_15_3 [label="Attention_Weighted_10_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_10_15_3 [label="Attention_Output_10_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_10_15 [label="Attention_AllReduce_10_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_0_0 [label="MLP_Linear1_10_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_10_0_0 [label="GELU_10_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_10_0_0 [label="MLP_Linear2_10_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_10_0_1 [label="MLP_Linear1_10_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_10_0_1 [label="GELU_10_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_10_0_1 [label="MLP_Linear2_10_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_10_0_2 [label="MLP_Linear1_10_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_10_0_2 [label="GELU_10_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_10_0_2 [label="MLP_Linear2_10_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_10_0_3 [label="MLP_Linear1_10_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_10_0_3 [label="GELU_10_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_10_0_3 [label="MLP_Linear2_10_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_10_0 [label="MLP_AllReduce_10_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_1_0 [label="MLP_Linear1_10_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_10_1_0 [label="GELU_10_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_10_1_0 [label="MLP_Linear2_10_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_10_1_1 [label="MLP_Linear1_10_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_10_1_1 [label="GELU_10_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_10_1_1 [label="MLP_Linear2_10_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_10_1_2 [label="MLP_Linear1_10_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_10_1_2 [label="GELU_10_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_10_1_2 [label="MLP_Linear2_10_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_10_1_3 [label="MLP_Linear1_10_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_10_1_3 [label="GELU_10_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_10_1_3 [label="MLP_Linear2_10_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_10_1 [label="MLP_AllReduce_10_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_2_0 [label="MLP_Linear1_10_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_10_2_0 [label="GELU_10_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_10_2_0 [label="MLP_Linear2_10_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_10_2_1 [label="MLP_Linear1_10_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_10_2_1 [label="GELU_10_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_10_2_1 [label="MLP_Linear2_10_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_10_2_2 [label="MLP_Linear1_10_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_10_2_2 [label="GELU_10_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_10_2_2 [label="MLP_Linear2_10_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_10_2_3 [label="MLP_Linear1_10_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_10_2_3 [label="GELU_10_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_10_2_3 [label="MLP_Linear2_10_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_10_2 [label="MLP_AllReduce_10_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_3_0 [label="MLP_Linear1_10_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_10_3_0 [label="GELU_10_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_10_3_0 [label="MLP_Linear2_10_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_10_3_1 [label="MLP_Linear1_10_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_10_3_1 [label="GELU_10_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_10_3_1 [label="MLP_Linear2_10_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_10_3_2 [label="MLP_Linear1_10_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_10_3_2 [label="GELU_10_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_10_3_2 [label="MLP_Linear2_10_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_10_3_3 [label="MLP_Linear1_10_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_10_3_3 [label="GELU_10_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_10_3_3 [label="MLP_Linear2_10_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_10_3 [label="MLP_AllReduce_10_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_4_0 [label="MLP_Linear1_10_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_10_4_0 [label="GELU_10_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_10_4_0 [label="MLP_Linear2_10_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_10_4_1 [label="MLP_Linear1_10_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_10_4_1 [label="GELU_10_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_10_4_1 [label="MLP_Linear2_10_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_10_4_2 [label="MLP_Linear1_10_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_10_4_2 [label="GELU_10_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_10_4_2 [label="MLP_Linear2_10_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_10_4_3 [label="MLP_Linear1_10_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_10_4_3 [label="GELU_10_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_10_4_3 [label="MLP_Linear2_10_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_10_4 [label="MLP_AllReduce_10_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_5_0 [label="MLP_Linear1_10_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_10_5_0 [label="GELU_10_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_10_5_0 [label="MLP_Linear2_10_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_10_5_1 [label="MLP_Linear1_10_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_10_5_1 [label="GELU_10_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_10_5_1 [label="MLP_Linear2_10_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_10_5_2 [label="MLP_Linear1_10_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_10_5_2 [label="GELU_10_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_10_5_2 [label="MLP_Linear2_10_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_10_5_3 [label="MLP_Linear1_10_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_10_5_3 [label="GELU_10_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_10_5_3 [label="MLP_Linear2_10_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_10_5 [label="MLP_AllReduce_10_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_6_0 [label="MLP_Linear1_10_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_10_6_0 [label="GELU_10_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_10_6_0 [label="MLP_Linear2_10_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_10_6_1 [label="MLP_Linear1_10_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_10_6_1 [label="GELU_10_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_10_6_1 [label="MLP_Linear2_10_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_10_6_2 [label="MLP_Linear1_10_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_10_6_2 [label="GELU_10_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_10_6_2 [label="MLP_Linear2_10_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_10_6_3 [label="MLP_Linear1_10_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_10_6_3 [label="GELU_10_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_10_6_3 [label="MLP_Linear2_10_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_10_6 [label="MLP_AllReduce_10_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_7_0 [label="MLP_Linear1_10_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_10_7_0 [label="GELU_10_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_10_7_0 [label="MLP_Linear2_10_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_10_7_1 [label="MLP_Linear1_10_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_10_7_1 [label="GELU_10_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_10_7_1 [label="MLP_Linear2_10_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_10_7_2 [label="MLP_Linear1_10_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_10_7_2 [label="GELU_10_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_10_7_2 [label="MLP_Linear2_10_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_10_7_3 [label="MLP_Linear1_10_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_10_7_3 [label="GELU_10_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_10_7_3 [label="MLP_Linear2_10_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_10_7 [label="MLP_AllReduce_10_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_8_0 [label="MLP_Linear1_10_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_10_8_0 [label="GELU_10_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_10_8_0 [label="MLP_Linear2_10_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_10_8_1 [label="MLP_Linear1_10_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_10_8_1 [label="GELU_10_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_10_8_1 [label="MLP_Linear2_10_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_10_8_2 [label="MLP_Linear1_10_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_10_8_2 [label="GELU_10_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_10_8_2 [label="MLP_Linear2_10_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_10_8_3 [label="MLP_Linear1_10_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_10_8_3 [label="GELU_10_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_10_8_3 [label="MLP_Linear2_10_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_10_8 [label="MLP_AllReduce_10_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_9_0 [label="MLP_Linear1_10_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_10_9_0 [label="GELU_10_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_10_9_0 [label="MLP_Linear2_10_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_10_9_1 [label="MLP_Linear1_10_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_10_9_1 [label="GELU_10_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_10_9_1 [label="MLP_Linear2_10_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_10_9_2 [label="MLP_Linear1_10_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_10_9_2 [label="GELU_10_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_10_9_2 [label="MLP_Linear2_10_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_10_9_3 [label="MLP_Linear1_10_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_10_9_3 [label="GELU_10_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_10_9_3 [label="MLP_Linear2_10_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_10_9 [label="MLP_AllReduce_10_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_10_0 [label="MLP_Linear1_10_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_10_10_0 [label="GELU_10_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_10_10_0 [label="MLP_Linear2_10_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_10_10_1 [label="MLP_Linear1_10_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_10_10_1 [label="GELU_10_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_10_10_1 [label="MLP_Linear2_10_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_10_10_2 [label="MLP_Linear1_10_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_10_10_2 [label="GELU_10_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_10_10_2 [label="MLP_Linear2_10_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_10_10_3 [label="MLP_Linear1_10_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_10_10_3 [label="GELU_10_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_10_10_3 [label="MLP_Linear2_10_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_10_10 [label="MLP_AllReduce_10_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_11_0 [label="MLP_Linear1_10_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_10_11_0 [label="GELU_10_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_10_11_0 [label="MLP_Linear2_10_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_10_11_1 [label="MLP_Linear1_10_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_10_11_1 [label="GELU_10_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_10_11_1 [label="MLP_Linear2_10_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_10_11_2 [label="MLP_Linear1_10_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_10_11_2 [label="GELU_10_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_10_11_2 [label="MLP_Linear2_10_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_10_11_3 [label="MLP_Linear1_10_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_10_11_3 [label="GELU_10_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_10_11_3 [label="MLP_Linear2_10_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_10_11 [label="MLP_AllReduce_10_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_12_0 [label="MLP_Linear1_10_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_10_12_0 [label="GELU_10_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_10_12_0 [label="MLP_Linear2_10_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_10_12_1 [label="MLP_Linear1_10_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_10_12_1 [label="GELU_10_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_10_12_1 [label="MLP_Linear2_10_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_10_12_2 [label="MLP_Linear1_10_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_10_12_2 [label="GELU_10_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_10_12_2 [label="MLP_Linear2_10_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_10_12_3 [label="MLP_Linear1_10_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_10_12_3 [label="GELU_10_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_10_12_3 [label="MLP_Linear2_10_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_10_12 [label="MLP_AllReduce_10_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_13_0 [label="MLP_Linear1_10_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_10_13_0 [label="GELU_10_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_10_13_0 [label="MLP_Linear2_10_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_10_13_1 [label="MLP_Linear1_10_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_10_13_1 [label="GELU_10_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_10_13_1 [label="MLP_Linear2_10_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_10_13_2 [label="MLP_Linear1_10_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_10_13_2 [label="GELU_10_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_10_13_2 [label="MLP_Linear2_10_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_10_13_3 [label="MLP_Linear1_10_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_10_13_3 [label="GELU_10_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_10_13_3 [label="MLP_Linear2_10_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_10_13 [label="MLP_AllReduce_10_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_14_0 [label="MLP_Linear1_10_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_10_14_0 [label="GELU_10_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_10_14_0 [label="MLP_Linear2_10_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_10_14_1 [label="MLP_Linear1_10_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_10_14_1 [label="GELU_10_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_10_14_1 [label="MLP_Linear2_10_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_10_14_2 [label="MLP_Linear1_10_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_10_14_2 [label="GELU_10_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_10_14_2 [label="MLP_Linear2_10_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_10_14_3 [label="MLP_Linear1_10_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_10_14_3 [label="GELU_10_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_10_14_3 [label="MLP_Linear2_10_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_10_14 [label="MLP_AllReduce_10_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_10_15_0 [label="MLP_Linear1_10_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_10_15_0 [label="GELU_10_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_10_15_0 [label="MLP_Linear2_10_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_10_15_1 [label="MLP_Linear1_10_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_10_15_1 [label="GELU_10_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_10_15_1 [label="MLP_Linear2_10_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_10_15_2 [label="MLP_Linear1_10_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_10_15_2 [label="GELU_10_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_10_15_2 [label="MLP_Linear2_10_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_10_15_3 [label="MLP_Linear1_10_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_10_15_3 [label="GELU_10_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_10_15_3 [label="MLP_Linear2_10_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_10_15 [label="MLP_AllReduce_10_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_10_0 [label="Expert_Route_10_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_1 [label="Expert_Route_10_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_2 [label="Expert_Route_10_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_3 [label="Expert_Route_10_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_4 [label="Expert_Route_10_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_5 [label="Expert_Route_10_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_6 [label="Expert_Route_10_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_7 [label="Expert_Route_10_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_8 [label="Expert_Route_10_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_9 [label="Expert_Route_10_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_10 [label="Expert_Route_10_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_11 [label="Expert_Route_10_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_12 [label="Expert_Route_10_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_13 [label="Expert_Route_10_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_14 [label="Expert_Route_10_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_10_15 [label="Expert_Route_10_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_11_0_0 [label="QKV_Proj_11_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_11_0_0 [label="Attention_Scores_11_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_11_0_0 [label="Attention_Softmax_11_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_11_0_0 [label="Attention_Dropout_11_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_11_0_0 [label="Attention_Values_11_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_11_0_0 [label="Attention_Weighted_11_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_11_0_0 [label="Attention_Output_11_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_11_0 [label="Attention_AllReduce_11_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_0_1 [label="QKV_Proj_11_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_11_0_1 [label="Attention_Scores_11_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_11_0_1 [label="Attention_Softmax_11_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_11_0_1 [label="Attention_Dropout_11_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_11_0_1 [label="Attention_Values_11_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_11_0_1 [label="Attention_Weighted_11_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_11_0_1 [label="Attention_Output_11_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_11_0 [label="Attention_AllReduce_11_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_0_2 [label="QKV_Proj_11_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_11_0_2 [label="Attention_Scores_11_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_11_0_2 [label="Attention_Softmax_11_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_11_0_2 [label="Attention_Dropout_11_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_11_0_2 [label="Attention_Values_11_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_11_0_2 [label="Attention_Weighted_11_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_11_0_2 [label="Attention_Output_11_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_11_0 [label="Attention_AllReduce_11_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_0_3 [label="QKV_Proj_11_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_11_0_3 [label="Attention_Scores_11_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_11_0_3 [label="Attention_Softmax_11_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_11_0_3 [label="Attention_Dropout_11_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_11_0_3 [label="Attention_Values_11_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_11_0_3 [label="Attention_Weighted_11_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_11_0_3 [label="Attention_Output_11_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_11_0 [label="Attention_AllReduce_11_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_1_0 [label="QKV_Proj_11_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_11_1_0 [label="Attention_Scores_11_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_11_1_0 [label="Attention_Softmax_11_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_11_1_0 [label="Attention_Dropout_11_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_11_1_0 [label="Attention_Values_11_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_11_1_0 [label="Attention_Weighted_11_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_11_1_0 [label="Attention_Output_11_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_11_1 [label="Attention_AllReduce_11_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_1_1 [label="QKV_Proj_11_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_11_1_1 [label="Attention_Scores_11_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_11_1_1 [label="Attention_Softmax_11_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_11_1_1 [label="Attention_Dropout_11_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_11_1_1 [label="Attention_Values_11_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_11_1_1 [label="Attention_Weighted_11_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_11_1_1 [label="Attention_Output_11_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_11_1 [label="Attention_AllReduce_11_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_1_2 [label="QKV_Proj_11_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_11_1_2 [label="Attention_Scores_11_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_11_1_2 [label="Attention_Softmax_11_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_11_1_2 [label="Attention_Dropout_11_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_11_1_2 [label="Attention_Values_11_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_11_1_2 [label="Attention_Weighted_11_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_11_1_2 [label="Attention_Output_11_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_11_1 [label="Attention_AllReduce_11_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_1_3 [label="QKV_Proj_11_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_11_1_3 [label="Attention_Scores_11_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_11_1_3 [label="Attention_Softmax_11_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_11_1_3 [label="Attention_Dropout_11_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_11_1_3 [label="Attention_Values_11_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_11_1_3 [label="Attention_Weighted_11_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_11_1_3 [label="Attention_Output_11_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_11_1 [label="Attention_AllReduce_11_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_2_0 [label="QKV_Proj_11_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_11_2_0 [label="Attention_Scores_11_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_11_2_0 [label="Attention_Softmax_11_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_11_2_0 [label="Attention_Dropout_11_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_11_2_0 [label="Attention_Values_11_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_11_2_0 [label="Attention_Weighted_11_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_11_2_0 [label="Attention_Output_11_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_11_2 [label="Attention_AllReduce_11_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_2_1 [label="QKV_Proj_11_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_11_2_1 [label="Attention_Scores_11_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_11_2_1 [label="Attention_Softmax_11_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_11_2_1 [label="Attention_Dropout_11_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_11_2_1 [label="Attention_Values_11_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_11_2_1 [label="Attention_Weighted_11_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_11_2_1 [label="Attention_Output_11_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_11_2 [label="Attention_AllReduce_11_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_2_2 [label="QKV_Proj_11_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_11_2_2 [label="Attention_Scores_11_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_11_2_2 [label="Attention_Softmax_11_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_11_2_2 [label="Attention_Dropout_11_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_11_2_2 [label="Attention_Values_11_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_11_2_2 [label="Attention_Weighted_11_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_11_2_2 [label="Attention_Output_11_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_11_2 [label="Attention_AllReduce_11_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_2_3 [label="QKV_Proj_11_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_11_2_3 [label="Attention_Scores_11_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_11_2_3 [label="Attention_Softmax_11_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_11_2_3 [label="Attention_Dropout_11_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_11_2_3 [label="Attention_Values_11_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_11_2_3 [label="Attention_Weighted_11_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_11_2_3 [label="Attention_Output_11_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_11_2 [label="Attention_AllReduce_11_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_3_0 [label="QKV_Proj_11_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_11_3_0 [label="Attention_Scores_11_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_11_3_0 [label="Attention_Softmax_11_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_11_3_0 [label="Attention_Dropout_11_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_11_3_0 [label="Attention_Values_11_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_11_3_0 [label="Attention_Weighted_11_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_11_3_0 [label="Attention_Output_11_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_11_3 [label="Attention_AllReduce_11_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_3_1 [label="QKV_Proj_11_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_11_3_1 [label="Attention_Scores_11_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_11_3_1 [label="Attention_Softmax_11_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_11_3_1 [label="Attention_Dropout_11_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_11_3_1 [label="Attention_Values_11_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_11_3_1 [label="Attention_Weighted_11_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_11_3_1 [label="Attention_Output_11_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_11_3 [label="Attention_AllReduce_11_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_3_2 [label="QKV_Proj_11_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_11_3_2 [label="Attention_Scores_11_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_11_3_2 [label="Attention_Softmax_11_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_11_3_2 [label="Attention_Dropout_11_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_11_3_2 [label="Attention_Values_11_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_11_3_2 [label="Attention_Weighted_11_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_11_3_2 [label="Attention_Output_11_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_11_3 [label="Attention_AllReduce_11_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_3_3 [label="QKV_Proj_11_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_11_3_3 [label="Attention_Scores_11_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_11_3_3 [label="Attention_Softmax_11_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_11_3_3 [label="Attention_Dropout_11_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_11_3_3 [label="Attention_Values_11_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_11_3_3 [label="Attention_Weighted_11_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_11_3_3 [label="Attention_Output_11_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_11_3 [label="Attention_AllReduce_11_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_4_0 [label="QKV_Proj_11_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_11_4_0 [label="Attention_Scores_11_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_11_4_0 [label="Attention_Softmax_11_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_11_4_0 [label="Attention_Dropout_11_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_11_4_0 [label="Attention_Values_11_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_11_4_0 [label="Attention_Weighted_11_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_11_4_0 [label="Attention_Output_11_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_11_4 [label="Attention_AllReduce_11_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_4_1 [label="QKV_Proj_11_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_11_4_1 [label="Attention_Scores_11_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_11_4_1 [label="Attention_Softmax_11_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_11_4_1 [label="Attention_Dropout_11_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_11_4_1 [label="Attention_Values_11_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_11_4_1 [label="Attention_Weighted_11_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_11_4_1 [label="Attention_Output_11_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_11_4 [label="Attention_AllReduce_11_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_4_2 [label="QKV_Proj_11_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_11_4_2 [label="Attention_Scores_11_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_11_4_2 [label="Attention_Softmax_11_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_11_4_2 [label="Attention_Dropout_11_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_11_4_2 [label="Attention_Values_11_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_11_4_2 [label="Attention_Weighted_11_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_11_4_2 [label="Attention_Output_11_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_11_4 [label="Attention_AllReduce_11_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_4_3 [label="QKV_Proj_11_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_11_4_3 [label="Attention_Scores_11_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_11_4_3 [label="Attention_Softmax_11_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_11_4_3 [label="Attention_Dropout_11_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_11_4_3 [label="Attention_Values_11_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_11_4_3 [label="Attention_Weighted_11_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_11_4_3 [label="Attention_Output_11_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_11_4 [label="Attention_AllReduce_11_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_5_0 [label="QKV_Proj_11_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_11_5_0 [label="Attention_Scores_11_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_11_5_0 [label="Attention_Softmax_11_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_11_5_0 [label="Attention_Dropout_11_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_11_5_0 [label="Attention_Values_11_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_11_5_0 [label="Attention_Weighted_11_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_11_5_0 [label="Attention_Output_11_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_11_5 [label="Attention_AllReduce_11_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_5_1 [label="QKV_Proj_11_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_11_5_1 [label="Attention_Scores_11_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_11_5_1 [label="Attention_Softmax_11_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_11_5_1 [label="Attention_Dropout_11_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_11_5_1 [label="Attention_Values_11_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_11_5_1 [label="Attention_Weighted_11_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_11_5_1 [label="Attention_Output_11_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_11_5 [label="Attention_AllReduce_11_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_5_2 [label="QKV_Proj_11_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_11_5_2 [label="Attention_Scores_11_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_11_5_2 [label="Attention_Softmax_11_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_11_5_2 [label="Attention_Dropout_11_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_11_5_2 [label="Attention_Values_11_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_11_5_2 [label="Attention_Weighted_11_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_11_5_2 [label="Attention_Output_11_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_11_5 [label="Attention_AllReduce_11_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_5_3 [label="QKV_Proj_11_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_11_5_3 [label="Attention_Scores_11_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_11_5_3 [label="Attention_Softmax_11_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_11_5_3 [label="Attention_Dropout_11_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_11_5_3 [label="Attention_Values_11_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_11_5_3 [label="Attention_Weighted_11_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_11_5_3 [label="Attention_Output_11_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_11_5 [label="Attention_AllReduce_11_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_6_0 [label="QKV_Proj_11_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_11_6_0 [label="Attention_Scores_11_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_11_6_0 [label="Attention_Softmax_11_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_11_6_0 [label="Attention_Dropout_11_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_11_6_0 [label="Attention_Values_11_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_11_6_0 [label="Attention_Weighted_11_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_11_6_0 [label="Attention_Output_11_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_11_6 [label="Attention_AllReduce_11_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_6_1 [label="QKV_Proj_11_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_11_6_1 [label="Attention_Scores_11_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_11_6_1 [label="Attention_Softmax_11_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_11_6_1 [label="Attention_Dropout_11_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_11_6_1 [label="Attention_Values_11_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_11_6_1 [label="Attention_Weighted_11_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_11_6_1 [label="Attention_Output_11_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_11_6 [label="Attention_AllReduce_11_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_6_2 [label="QKV_Proj_11_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_11_6_2 [label="Attention_Scores_11_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_11_6_2 [label="Attention_Softmax_11_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_11_6_2 [label="Attention_Dropout_11_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_11_6_2 [label="Attention_Values_11_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_11_6_2 [label="Attention_Weighted_11_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_11_6_2 [label="Attention_Output_11_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_11_6 [label="Attention_AllReduce_11_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_6_3 [label="QKV_Proj_11_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_11_6_3 [label="Attention_Scores_11_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_11_6_3 [label="Attention_Softmax_11_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_11_6_3 [label="Attention_Dropout_11_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_11_6_3 [label="Attention_Values_11_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_11_6_3 [label="Attention_Weighted_11_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_11_6_3 [label="Attention_Output_11_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_11_6 [label="Attention_AllReduce_11_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_7_0 [label="QKV_Proj_11_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_11_7_0 [label="Attention_Scores_11_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_11_7_0 [label="Attention_Softmax_11_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_11_7_0 [label="Attention_Dropout_11_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_11_7_0 [label="Attention_Values_11_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_11_7_0 [label="Attention_Weighted_11_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_11_7_0 [label="Attention_Output_11_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_11_7 [label="Attention_AllReduce_11_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_7_1 [label="QKV_Proj_11_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_11_7_1 [label="Attention_Scores_11_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_11_7_1 [label="Attention_Softmax_11_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_11_7_1 [label="Attention_Dropout_11_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_11_7_1 [label="Attention_Values_11_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_11_7_1 [label="Attention_Weighted_11_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_11_7_1 [label="Attention_Output_11_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_11_7 [label="Attention_AllReduce_11_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_7_2 [label="QKV_Proj_11_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_11_7_2 [label="Attention_Scores_11_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_11_7_2 [label="Attention_Softmax_11_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_11_7_2 [label="Attention_Dropout_11_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_11_7_2 [label="Attention_Values_11_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_11_7_2 [label="Attention_Weighted_11_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_11_7_2 [label="Attention_Output_11_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_11_7 [label="Attention_AllReduce_11_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_7_3 [label="QKV_Proj_11_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_11_7_3 [label="Attention_Scores_11_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_11_7_3 [label="Attention_Softmax_11_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_11_7_3 [label="Attention_Dropout_11_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_11_7_3 [label="Attention_Values_11_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_11_7_3 [label="Attention_Weighted_11_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_11_7_3 [label="Attention_Output_11_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_11_7 [label="Attention_AllReduce_11_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_8_0 [label="QKV_Proj_11_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_11_8_0 [label="Attention_Scores_11_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_11_8_0 [label="Attention_Softmax_11_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_11_8_0 [label="Attention_Dropout_11_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_11_8_0 [label="Attention_Values_11_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_11_8_0 [label="Attention_Weighted_11_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_11_8_0 [label="Attention_Output_11_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_11_8 [label="Attention_AllReduce_11_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_8_1 [label="QKV_Proj_11_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_11_8_1 [label="Attention_Scores_11_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_11_8_1 [label="Attention_Softmax_11_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_11_8_1 [label="Attention_Dropout_11_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_11_8_1 [label="Attention_Values_11_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_11_8_1 [label="Attention_Weighted_11_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_11_8_1 [label="Attention_Output_11_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_11_8 [label="Attention_AllReduce_11_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_8_2 [label="QKV_Proj_11_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_11_8_2 [label="Attention_Scores_11_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_11_8_2 [label="Attention_Softmax_11_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_11_8_2 [label="Attention_Dropout_11_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_11_8_2 [label="Attention_Values_11_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_11_8_2 [label="Attention_Weighted_11_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_11_8_2 [label="Attention_Output_11_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_11_8 [label="Attention_AllReduce_11_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_8_3 [label="QKV_Proj_11_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_11_8_3 [label="Attention_Scores_11_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_11_8_3 [label="Attention_Softmax_11_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_11_8_3 [label="Attention_Dropout_11_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_11_8_3 [label="Attention_Values_11_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_11_8_3 [label="Attention_Weighted_11_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_11_8_3 [label="Attention_Output_11_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_11_8 [label="Attention_AllReduce_11_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_9_0 [label="QKV_Proj_11_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_11_9_0 [label="Attention_Scores_11_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_11_9_0 [label="Attention_Softmax_11_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_11_9_0 [label="Attention_Dropout_11_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_11_9_0 [label="Attention_Values_11_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_11_9_0 [label="Attention_Weighted_11_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_11_9_0 [label="Attention_Output_11_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_11_9 [label="Attention_AllReduce_11_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_9_1 [label="QKV_Proj_11_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_11_9_1 [label="Attention_Scores_11_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_11_9_1 [label="Attention_Softmax_11_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_11_9_1 [label="Attention_Dropout_11_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_11_9_1 [label="Attention_Values_11_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_11_9_1 [label="Attention_Weighted_11_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_11_9_1 [label="Attention_Output_11_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_11_9 [label="Attention_AllReduce_11_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_9_2 [label="QKV_Proj_11_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_11_9_2 [label="Attention_Scores_11_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_11_9_2 [label="Attention_Softmax_11_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_11_9_2 [label="Attention_Dropout_11_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_11_9_2 [label="Attention_Values_11_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_11_9_2 [label="Attention_Weighted_11_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_11_9_2 [label="Attention_Output_11_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_11_9 [label="Attention_AllReduce_11_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_9_3 [label="QKV_Proj_11_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_11_9_3 [label="Attention_Scores_11_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_11_9_3 [label="Attention_Softmax_11_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_11_9_3 [label="Attention_Dropout_11_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_11_9_3 [label="Attention_Values_11_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_11_9_3 [label="Attention_Weighted_11_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_11_9_3 [label="Attention_Output_11_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_11_9 [label="Attention_AllReduce_11_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_10_0 [label="QKV_Proj_11_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_11_10_0 [label="Attention_Scores_11_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_11_10_0 [label="Attention_Softmax_11_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_11_10_0 [label="Attention_Dropout_11_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_11_10_0 [label="Attention_Values_11_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_11_10_0 [label="Attention_Weighted_11_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_11_10_0 [label="Attention_Output_11_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_11_10 [label="Attention_AllReduce_11_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_10_1 [label="QKV_Proj_11_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_11_10_1 [label="Attention_Scores_11_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_11_10_1 [label="Attention_Softmax_11_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_11_10_1 [label="Attention_Dropout_11_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_11_10_1 [label="Attention_Values_11_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_11_10_1 [label="Attention_Weighted_11_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_11_10_1 [label="Attention_Output_11_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_11_10 [label="Attention_AllReduce_11_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_10_2 [label="QKV_Proj_11_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_11_10_2 [label="Attention_Scores_11_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_11_10_2 [label="Attention_Softmax_11_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_11_10_2 [label="Attention_Dropout_11_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_11_10_2 [label="Attention_Values_11_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_11_10_2 [label="Attention_Weighted_11_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_11_10_2 [label="Attention_Output_11_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_11_10 [label="Attention_AllReduce_11_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_10_3 [label="QKV_Proj_11_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_11_10_3 [label="Attention_Scores_11_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_11_10_3 [label="Attention_Softmax_11_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_11_10_3 [label="Attention_Dropout_11_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_11_10_3 [label="Attention_Values_11_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_11_10_3 [label="Attention_Weighted_11_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_11_10_3 [label="Attention_Output_11_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_11_10 [label="Attention_AllReduce_11_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_11_0 [label="QKV_Proj_11_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_11_11_0 [label="Attention_Scores_11_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_11_11_0 [label="Attention_Softmax_11_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_11_11_0 [label="Attention_Dropout_11_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_11_11_0 [label="Attention_Values_11_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_11_11_0 [label="Attention_Weighted_11_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_11_11_0 [label="Attention_Output_11_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_11_11 [label="Attention_AllReduce_11_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_11_1 [label="QKV_Proj_11_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_11_11_1 [label="Attention_Scores_11_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_11_11_1 [label="Attention_Softmax_11_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_11_11_1 [label="Attention_Dropout_11_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_11_11_1 [label="Attention_Values_11_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_11_11_1 [label="Attention_Weighted_11_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_11_11_1 [label="Attention_Output_11_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_11_11 [label="Attention_AllReduce_11_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_11_2 [label="QKV_Proj_11_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_11_11_2 [label="Attention_Scores_11_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_11_11_2 [label="Attention_Softmax_11_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_11_11_2 [label="Attention_Dropout_11_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_11_11_2 [label="Attention_Values_11_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_11_11_2 [label="Attention_Weighted_11_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_11_11_2 [label="Attention_Output_11_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_11_11 [label="Attention_AllReduce_11_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_11_3 [label="QKV_Proj_11_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_11_11_3 [label="Attention_Scores_11_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_11_11_3 [label="Attention_Softmax_11_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_11_11_3 [label="Attention_Dropout_11_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_11_11_3 [label="Attention_Values_11_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_11_11_3 [label="Attention_Weighted_11_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_11_11_3 [label="Attention_Output_11_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_11_11 [label="Attention_AllReduce_11_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_12_0 [label="QKV_Proj_11_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_11_12_0 [label="Attention_Scores_11_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_11_12_0 [label="Attention_Softmax_11_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_11_12_0 [label="Attention_Dropout_11_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_11_12_0 [label="Attention_Values_11_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_11_12_0 [label="Attention_Weighted_11_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_11_12_0 [label="Attention_Output_11_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_11_12 [label="Attention_AllReduce_11_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_12_1 [label="QKV_Proj_11_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_11_12_1 [label="Attention_Scores_11_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_11_12_1 [label="Attention_Softmax_11_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_11_12_1 [label="Attention_Dropout_11_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_11_12_1 [label="Attention_Values_11_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_11_12_1 [label="Attention_Weighted_11_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_11_12_1 [label="Attention_Output_11_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_11_12 [label="Attention_AllReduce_11_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_12_2 [label="QKV_Proj_11_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_11_12_2 [label="Attention_Scores_11_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_11_12_2 [label="Attention_Softmax_11_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_11_12_2 [label="Attention_Dropout_11_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_11_12_2 [label="Attention_Values_11_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_11_12_2 [label="Attention_Weighted_11_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_11_12_2 [label="Attention_Output_11_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_11_12 [label="Attention_AllReduce_11_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_12_3 [label="QKV_Proj_11_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_11_12_3 [label="Attention_Scores_11_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_11_12_3 [label="Attention_Softmax_11_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_11_12_3 [label="Attention_Dropout_11_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_11_12_3 [label="Attention_Values_11_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_11_12_3 [label="Attention_Weighted_11_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_11_12_3 [label="Attention_Output_11_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_11_12 [label="Attention_AllReduce_11_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_13_0 [label="QKV_Proj_11_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_11_13_0 [label="Attention_Scores_11_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_11_13_0 [label="Attention_Softmax_11_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_11_13_0 [label="Attention_Dropout_11_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_11_13_0 [label="Attention_Values_11_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_11_13_0 [label="Attention_Weighted_11_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_11_13_0 [label="Attention_Output_11_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_11_13 [label="Attention_AllReduce_11_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_13_1 [label="QKV_Proj_11_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_11_13_1 [label="Attention_Scores_11_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_11_13_1 [label="Attention_Softmax_11_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_11_13_1 [label="Attention_Dropout_11_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_11_13_1 [label="Attention_Values_11_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_11_13_1 [label="Attention_Weighted_11_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_11_13_1 [label="Attention_Output_11_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_11_13 [label="Attention_AllReduce_11_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_13_2 [label="QKV_Proj_11_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_11_13_2 [label="Attention_Scores_11_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_11_13_2 [label="Attention_Softmax_11_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_11_13_2 [label="Attention_Dropout_11_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_11_13_2 [label="Attention_Values_11_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_11_13_2 [label="Attention_Weighted_11_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_11_13_2 [label="Attention_Output_11_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_11_13 [label="Attention_AllReduce_11_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_13_3 [label="QKV_Proj_11_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_11_13_3 [label="Attention_Scores_11_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_11_13_3 [label="Attention_Softmax_11_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_11_13_3 [label="Attention_Dropout_11_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_11_13_3 [label="Attention_Values_11_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_11_13_3 [label="Attention_Weighted_11_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_11_13_3 [label="Attention_Output_11_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_11_13 [label="Attention_AllReduce_11_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_14_0 [label="QKV_Proj_11_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_11_14_0 [label="Attention_Scores_11_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_11_14_0 [label="Attention_Softmax_11_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_11_14_0 [label="Attention_Dropout_11_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_11_14_0 [label="Attention_Values_11_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_11_14_0 [label="Attention_Weighted_11_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_11_14_0 [label="Attention_Output_11_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_11_14 [label="Attention_AllReduce_11_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_14_1 [label="QKV_Proj_11_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_11_14_1 [label="Attention_Scores_11_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_11_14_1 [label="Attention_Softmax_11_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_11_14_1 [label="Attention_Dropout_11_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_11_14_1 [label="Attention_Values_11_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_11_14_1 [label="Attention_Weighted_11_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_11_14_1 [label="Attention_Output_11_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_11_14 [label="Attention_AllReduce_11_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_14_2 [label="QKV_Proj_11_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_11_14_2 [label="Attention_Scores_11_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_11_14_2 [label="Attention_Softmax_11_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_11_14_2 [label="Attention_Dropout_11_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_11_14_2 [label="Attention_Values_11_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_11_14_2 [label="Attention_Weighted_11_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_11_14_2 [label="Attention_Output_11_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_11_14 [label="Attention_AllReduce_11_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_14_3 [label="QKV_Proj_11_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_11_14_3 [label="Attention_Scores_11_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_11_14_3 [label="Attention_Softmax_11_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_11_14_3 [label="Attention_Dropout_11_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_11_14_3 [label="Attention_Values_11_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_11_14_3 [label="Attention_Weighted_11_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_11_14_3 [label="Attention_Output_11_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_11_14 [label="Attention_AllReduce_11_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_15_0 [label="QKV_Proj_11_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_11_15_0 [label="Attention_Scores_11_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_11_15_0 [label="Attention_Softmax_11_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_11_15_0 [label="Attention_Dropout_11_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_11_15_0 [label="Attention_Values_11_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_11_15_0 [label="Attention_Weighted_11_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_11_15_0 [label="Attention_Output_11_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_11_15 [label="Attention_AllReduce_11_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_15_1 [label="QKV_Proj_11_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_11_15_1 [label="Attention_Scores_11_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_11_15_1 [label="Attention_Softmax_11_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_11_15_1 [label="Attention_Dropout_11_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_11_15_1 [label="Attention_Values_11_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_11_15_1 [label="Attention_Weighted_11_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_11_15_1 [label="Attention_Output_11_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_11_15 [label="Attention_AllReduce_11_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_15_2 [label="QKV_Proj_11_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_11_15_2 [label="Attention_Scores_11_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_11_15_2 [label="Attention_Softmax_11_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_11_15_2 [label="Attention_Dropout_11_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_11_15_2 [label="Attention_Values_11_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_11_15_2 [label="Attention_Weighted_11_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_11_15_2 [label="Attention_Output_11_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_11_15 [label="Attention_AllReduce_11_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_11_15_3 [label="QKV_Proj_11_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_11_15_3 [label="Attention_Scores_11_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_11_15_3 [label="Attention_Softmax_11_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_11_15_3 [label="Attention_Dropout_11_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_11_15_3 [label="Attention_Values_11_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_11_15_3 [label="Attention_Weighted_11_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_11_15_3 [label="Attention_Output_11_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_11_15 [label="Attention_AllReduce_11_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_0_0 [label="MLP_Linear1_11_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_11_0_0 [label="GELU_11_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_11_0_0 [label="MLP_Linear2_11_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_11_0_1 [label="MLP_Linear1_11_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_11_0_1 [label="GELU_11_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_11_0_1 [label="MLP_Linear2_11_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_11_0_2 [label="MLP_Linear1_11_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_11_0_2 [label="GELU_11_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_11_0_2 [label="MLP_Linear2_11_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_11_0_3 [label="MLP_Linear1_11_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_11_0_3 [label="GELU_11_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_11_0_3 [label="MLP_Linear2_11_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_11_0 [label="MLP_AllReduce_11_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_1_0 [label="MLP_Linear1_11_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_11_1_0 [label="GELU_11_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_11_1_0 [label="MLP_Linear2_11_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_11_1_1 [label="MLP_Linear1_11_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_11_1_1 [label="GELU_11_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_11_1_1 [label="MLP_Linear2_11_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_11_1_2 [label="MLP_Linear1_11_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_11_1_2 [label="GELU_11_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_11_1_2 [label="MLP_Linear2_11_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_11_1_3 [label="MLP_Linear1_11_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_11_1_3 [label="GELU_11_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_11_1_3 [label="MLP_Linear2_11_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_11_1 [label="MLP_AllReduce_11_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_2_0 [label="MLP_Linear1_11_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_11_2_0 [label="GELU_11_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_11_2_0 [label="MLP_Linear2_11_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_11_2_1 [label="MLP_Linear1_11_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_11_2_1 [label="GELU_11_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_11_2_1 [label="MLP_Linear2_11_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_11_2_2 [label="MLP_Linear1_11_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_11_2_2 [label="GELU_11_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_11_2_2 [label="MLP_Linear2_11_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_11_2_3 [label="MLP_Linear1_11_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_11_2_3 [label="GELU_11_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_11_2_3 [label="MLP_Linear2_11_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_11_2 [label="MLP_AllReduce_11_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_3_0 [label="MLP_Linear1_11_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_11_3_0 [label="GELU_11_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_11_3_0 [label="MLP_Linear2_11_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_11_3_1 [label="MLP_Linear1_11_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_11_3_1 [label="GELU_11_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_11_3_1 [label="MLP_Linear2_11_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_11_3_2 [label="MLP_Linear1_11_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_11_3_2 [label="GELU_11_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_11_3_2 [label="MLP_Linear2_11_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_11_3_3 [label="MLP_Linear1_11_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_11_3_3 [label="GELU_11_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_11_3_3 [label="MLP_Linear2_11_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_11_3 [label="MLP_AllReduce_11_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_4_0 [label="MLP_Linear1_11_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_11_4_0 [label="GELU_11_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_11_4_0 [label="MLP_Linear2_11_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_11_4_1 [label="MLP_Linear1_11_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_11_4_1 [label="GELU_11_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_11_4_1 [label="MLP_Linear2_11_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_11_4_2 [label="MLP_Linear1_11_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_11_4_2 [label="GELU_11_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_11_4_2 [label="MLP_Linear2_11_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_11_4_3 [label="MLP_Linear1_11_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_11_4_3 [label="GELU_11_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_11_4_3 [label="MLP_Linear2_11_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_11_4 [label="MLP_AllReduce_11_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_5_0 [label="MLP_Linear1_11_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_11_5_0 [label="GELU_11_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_11_5_0 [label="MLP_Linear2_11_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_11_5_1 [label="MLP_Linear1_11_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_11_5_1 [label="GELU_11_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_11_5_1 [label="MLP_Linear2_11_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_11_5_2 [label="MLP_Linear1_11_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_11_5_2 [label="GELU_11_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_11_5_2 [label="MLP_Linear2_11_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_11_5_3 [label="MLP_Linear1_11_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_11_5_3 [label="GELU_11_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_11_5_3 [label="MLP_Linear2_11_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_11_5 [label="MLP_AllReduce_11_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_6_0 [label="MLP_Linear1_11_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_11_6_0 [label="GELU_11_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_11_6_0 [label="MLP_Linear2_11_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_11_6_1 [label="MLP_Linear1_11_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_11_6_1 [label="GELU_11_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_11_6_1 [label="MLP_Linear2_11_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_11_6_2 [label="MLP_Linear1_11_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_11_6_2 [label="GELU_11_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_11_6_2 [label="MLP_Linear2_11_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_11_6_3 [label="MLP_Linear1_11_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_11_6_3 [label="GELU_11_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_11_6_3 [label="MLP_Linear2_11_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_11_6 [label="MLP_AllReduce_11_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_7_0 [label="MLP_Linear1_11_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_11_7_0 [label="GELU_11_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_11_7_0 [label="MLP_Linear2_11_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_11_7_1 [label="MLP_Linear1_11_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_11_7_1 [label="GELU_11_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_11_7_1 [label="MLP_Linear2_11_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_11_7_2 [label="MLP_Linear1_11_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_11_7_2 [label="GELU_11_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_11_7_2 [label="MLP_Linear2_11_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_11_7_3 [label="MLP_Linear1_11_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_11_7_3 [label="GELU_11_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_11_7_3 [label="MLP_Linear2_11_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_11_7 [label="MLP_AllReduce_11_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_8_0 [label="MLP_Linear1_11_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_11_8_0 [label="GELU_11_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_11_8_0 [label="MLP_Linear2_11_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_11_8_1 [label="MLP_Linear1_11_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_11_8_1 [label="GELU_11_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_11_8_1 [label="MLP_Linear2_11_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_11_8_2 [label="MLP_Linear1_11_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_11_8_2 [label="GELU_11_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_11_8_2 [label="MLP_Linear2_11_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_11_8_3 [label="MLP_Linear1_11_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_11_8_3 [label="GELU_11_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_11_8_3 [label="MLP_Linear2_11_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_11_8 [label="MLP_AllReduce_11_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_9_0 [label="MLP_Linear1_11_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_11_9_0 [label="GELU_11_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_11_9_0 [label="MLP_Linear2_11_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_11_9_1 [label="MLP_Linear1_11_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_11_9_1 [label="GELU_11_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_11_9_1 [label="MLP_Linear2_11_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_11_9_2 [label="MLP_Linear1_11_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_11_9_2 [label="GELU_11_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_11_9_2 [label="MLP_Linear2_11_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_11_9_3 [label="MLP_Linear1_11_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_11_9_3 [label="GELU_11_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_11_9_3 [label="MLP_Linear2_11_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_11_9 [label="MLP_AllReduce_11_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_10_0 [label="MLP_Linear1_11_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_11_10_0 [label="GELU_11_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_11_10_0 [label="MLP_Linear2_11_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_11_10_1 [label="MLP_Linear1_11_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_11_10_1 [label="GELU_11_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_11_10_1 [label="MLP_Linear2_11_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_11_10_2 [label="MLP_Linear1_11_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_11_10_2 [label="GELU_11_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_11_10_2 [label="MLP_Linear2_11_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_11_10_3 [label="MLP_Linear1_11_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_11_10_3 [label="GELU_11_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_11_10_3 [label="MLP_Linear2_11_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_11_10 [label="MLP_AllReduce_11_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_11_0 [label="MLP_Linear1_11_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_11_11_0 [label="GELU_11_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_11_11_0 [label="MLP_Linear2_11_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_11_11_1 [label="MLP_Linear1_11_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_11_11_1 [label="GELU_11_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_11_11_1 [label="MLP_Linear2_11_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_11_11_2 [label="MLP_Linear1_11_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_11_11_2 [label="GELU_11_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_11_11_2 [label="MLP_Linear2_11_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_11_11_3 [label="MLP_Linear1_11_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_11_11_3 [label="GELU_11_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_11_11_3 [label="MLP_Linear2_11_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_11_11 [label="MLP_AllReduce_11_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_12_0 [label="MLP_Linear1_11_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_11_12_0 [label="GELU_11_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_11_12_0 [label="MLP_Linear2_11_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_11_12_1 [label="MLP_Linear1_11_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_11_12_1 [label="GELU_11_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_11_12_1 [label="MLP_Linear2_11_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_11_12_2 [label="MLP_Linear1_11_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_11_12_2 [label="GELU_11_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_11_12_2 [label="MLP_Linear2_11_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_11_12_3 [label="MLP_Linear1_11_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_11_12_3 [label="GELU_11_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_11_12_3 [label="MLP_Linear2_11_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_11_12 [label="MLP_AllReduce_11_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_13_0 [label="MLP_Linear1_11_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_11_13_0 [label="GELU_11_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_11_13_0 [label="MLP_Linear2_11_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_11_13_1 [label="MLP_Linear1_11_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_11_13_1 [label="GELU_11_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_11_13_1 [label="MLP_Linear2_11_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_11_13_2 [label="MLP_Linear1_11_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_11_13_2 [label="GELU_11_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_11_13_2 [label="MLP_Linear2_11_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_11_13_3 [label="MLP_Linear1_11_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_11_13_3 [label="GELU_11_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_11_13_3 [label="MLP_Linear2_11_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_11_13 [label="MLP_AllReduce_11_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_14_0 [label="MLP_Linear1_11_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_11_14_0 [label="GELU_11_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_11_14_0 [label="MLP_Linear2_11_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_11_14_1 [label="MLP_Linear1_11_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_11_14_1 [label="GELU_11_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_11_14_1 [label="MLP_Linear2_11_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_11_14_2 [label="MLP_Linear1_11_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_11_14_2 [label="GELU_11_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_11_14_2 [label="MLP_Linear2_11_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_11_14_3 [label="MLP_Linear1_11_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_11_14_3 [label="GELU_11_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_11_14_3 [label="MLP_Linear2_11_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_11_14 [label="MLP_AllReduce_11_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_11_15_0 [label="MLP_Linear1_11_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_11_15_0 [label="GELU_11_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_11_15_0 [label="MLP_Linear2_11_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_11_15_1 [label="MLP_Linear1_11_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_11_15_1 [label="GELU_11_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_11_15_1 [label="MLP_Linear2_11_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_11_15_2 [label="MLP_Linear1_11_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_11_15_2 [label="GELU_11_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_11_15_2 [label="MLP_Linear2_11_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_11_15_3 [label="MLP_Linear1_11_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_11_15_3 [label="GELU_11_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_11_15_3 [label="MLP_Linear2_11_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_11_15 [label="MLP_AllReduce_11_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_11_0 [label="Expert_Route_11_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_1 [label="Expert_Route_11_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_2 [label="Expert_Route_11_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_3 [label="Expert_Route_11_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_4 [label="Expert_Route_11_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_5 [label="Expert_Route_11_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_6 [label="Expert_Route_11_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_7 [label="Expert_Route_11_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_8 [label="Expert_Route_11_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_9 [label="Expert_Route_11_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_10 [label="Expert_Route_11_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_11 [label="Expert_Route_11_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_12 [label="Expert_Route_11_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_13 [label="Expert_Route_11_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_14 [label="Expert_Route_11_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_11_15 [label="Expert_Route_11_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_12_0_0 [label="QKV_Proj_12_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_12_0_0 [label="Attention_Scores_12_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_12_0_0 [label="Attention_Softmax_12_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_12_0_0 [label="Attention_Dropout_12_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_12_0_0 [label="Attention_Values_12_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_12_0_0 [label="Attention_Weighted_12_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_12_0_0 [label="Attention_Output_12_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_12_0 [label="Attention_AllReduce_12_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_0_1 [label="QKV_Proj_12_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_12_0_1 [label="Attention_Scores_12_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_12_0_1 [label="Attention_Softmax_12_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_12_0_1 [label="Attention_Dropout_12_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_12_0_1 [label="Attention_Values_12_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_12_0_1 [label="Attention_Weighted_12_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_12_0_1 [label="Attention_Output_12_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_12_0 [label="Attention_AllReduce_12_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_0_2 [label="QKV_Proj_12_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_12_0_2 [label="Attention_Scores_12_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_12_0_2 [label="Attention_Softmax_12_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_12_0_2 [label="Attention_Dropout_12_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_12_0_2 [label="Attention_Values_12_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_12_0_2 [label="Attention_Weighted_12_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_12_0_2 [label="Attention_Output_12_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_12_0 [label="Attention_AllReduce_12_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_0_3 [label="QKV_Proj_12_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_12_0_3 [label="Attention_Scores_12_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_12_0_3 [label="Attention_Softmax_12_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_12_0_3 [label="Attention_Dropout_12_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_12_0_3 [label="Attention_Values_12_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_12_0_3 [label="Attention_Weighted_12_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_12_0_3 [label="Attention_Output_12_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_12_0 [label="Attention_AllReduce_12_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_1_0 [label="QKV_Proj_12_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_12_1_0 [label="Attention_Scores_12_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_12_1_0 [label="Attention_Softmax_12_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_12_1_0 [label="Attention_Dropout_12_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_12_1_0 [label="Attention_Values_12_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_12_1_0 [label="Attention_Weighted_12_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_12_1_0 [label="Attention_Output_12_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_12_1 [label="Attention_AllReduce_12_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_1_1 [label="QKV_Proj_12_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_12_1_1 [label="Attention_Scores_12_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_12_1_1 [label="Attention_Softmax_12_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_12_1_1 [label="Attention_Dropout_12_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_12_1_1 [label="Attention_Values_12_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_12_1_1 [label="Attention_Weighted_12_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_12_1_1 [label="Attention_Output_12_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_12_1 [label="Attention_AllReduce_12_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_1_2 [label="QKV_Proj_12_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_12_1_2 [label="Attention_Scores_12_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_12_1_2 [label="Attention_Softmax_12_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_12_1_2 [label="Attention_Dropout_12_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_12_1_2 [label="Attention_Values_12_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_12_1_2 [label="Attention_Weighted_12_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_12_1_2 [label="Attention_Output_12_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_12_1 [label="Attention_AllReduce_12_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_1_3 [label="QKV_Proj_12_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_12_1_3 [label="Attention_Scores_12_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_12_1_3 [label="Attention_Softmax_12_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_12_1_3 [label="Attention_Dropout_12_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_12_1_3 [label="Attention_Values_12_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_12_1_3 [label="Attention_Weighted_12_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_12_1_3 [label="Attention_Output_12_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_12_1 [label="Attention_AllReduce_12_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_2_0 [label="QKV_Proj_12_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_12_2_0 [label="Attention_Scores_12_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_12_2_0 [label="Attention_Softmax_12_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_12_2_0 [label="Attention_Dropout_12_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_12_2_0 [label="Attention_Values_12_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_12_2_0 [label="Attention_Weighted_12_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_12_2_0 [label="Attention_Output_12_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_12_2 [label="Attention_AllReduce_12_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_2_1 [label="QKV_Proj_12_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_12_2_1 [label="Attention_Scores_12_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_12_2_1 [label="Attention_Softmax_12_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_12_2_1 [label="Attention_Dropout_12_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_12_2_1 [label="Attention_Values_12_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_12_2_1 [label="Attention_Weighted_12_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_12_2_1 [label="Attention_Output_12_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_12_2 [label="Attention_AllReduce_12_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_2_2 [label="QKV_Proj_12_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_12_2_2 [label="Attention_Scores_12_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_12_2_2 [label="Attention_Softmax_12_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_12_2_2 [label="Attention_Dropout_12_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_12_2_2 [label="Attention_Values_12_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_12_2_2 [label="Attention_Weighted_12_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_12_2_2 [label="Attention_Output_12_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_12_2 [label="Attention_AllReduce_12_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_2_3 [label="QKV_Proj_12_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_12_2_3 [label="Attention_Scores_12_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_12_2_3 [label="Attention_Softmax_12_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_12_2_3 [label="Attention_Dropout_12_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_12_2_3 [label="Attention_Values_12_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_12_2_3 [label="Attention_Weighted_12_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_12_2_3 [label="Attention_Output_12_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_12_2 [label="Attention_AllReduce_12_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_3_0 [label="QKV_Proj_12_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_12_3_0 [label="Attention_Scores_12_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_12_3_0 [label="Attention_Softmax_12_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_12_3_0 [label="Attention_Dropout_12_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_12_3_0 [label="Attention_Values_12_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_12_3_0 [label="Attention_Weighted_12_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_12_3_0 [label="Attention_Output_12_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_12_3 [label="Attention_AllReduce_12_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_3_1 [label="QKV_Proj_12_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_12_3_1 [label="Attention_Scores_12_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_12_3_1 [label="Attention_Softmax_12_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_12_3_1 [label="Attention_Dropout_12_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_12_3_1 [label="Attention_Values_12_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_12_3_1 [label="Attention_Weighted_12_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_12_3_1 [label="Attention_Output_12_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_12_3 [label="Attention_AllReduce_12_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_3_2 [label="QKV_Proj_12_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_12_3_2 [label="Attention_Scores_12_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_12_3_2 [label="Attention_Softmax_12_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_12_3_2 [label="Attention_Dropout_12_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_12_3_2 [label="Attention_Values_12_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_12_3_2 [label="Attention_Weighted_12_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_12_3_2 [label="Attention_Output_12_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_12_3 [label="Attention_AllReduce_12_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_3_3 [label="QKV_Proj_12_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_12_3_3 [label="Attention_Scores_12_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_12_3_3 [label="Attention_Softmax_12_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_12_3_3 [label="Attention_Dropout_12_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_12_3_3 [label="Attention_Values_12_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_12_3_3 [label="Attention_Weighted_12_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_12_3_3 [label="Attention_Output_12_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_12_3 [label="Attention_AllReduce_12_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_4_0 [label="QKV_Proj_12_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_12_4_0 [label="Attention_Scores_12_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_12_4_0 [label="Attention_Softmax_12_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_12_4_0 [label="Attention_Dropout_12_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_12_4_0 [label="Attention_Values_12_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_12_4_0 [label="Attention_Weighted_12_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_12_4_0 [label="Attention_Output_12_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_12_4 [label="Attention_AllReduce_12_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_4_1 [label="QKV_Proj_12_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_12_4_1 [label="Attention_Scores_12_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_12_4_1 [label="Attention_Softmax_12_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_12_4_1 [label="Attention_Dropout_12_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_12_4_1 [label="Attention_Values_12_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_12_4_1 [label="Attention_Weighted_12_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_12_4_1 [label="Attention_Output_12_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_12_4 [label="Attention_AllReduce_12_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_4_2 [label="QKV_Proj_12_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_12_4_2 [label="Attention_Scores_12_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_12_4_2 [label="Attention_Softmax_12_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_12_4_2 [label="Attention_Dropout_12_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_12_4_2 [label="Attention_Values_12_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_12_4_2 [label="Attention_Weighted_12_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_12_4_2 [label="Attention_Output_12_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_12_4 [label="Attention_AllReduce_12_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_4_3 [label="QKV_Proj_12_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_12_4_3 [label="Attention_Scores_12_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_12_4_3 [label="Attention_Softmax_12_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_12_4_3 [label="Attention_Dropout_12_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_12_4_3 [label="Attention_Values_12_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_12_4_3 [label="Attention_Weighted_12_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_12_4_3 [label="Attention_Output_12_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_12_4 [label="Attention_AllReduce_12_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_5_0 [label="QKV_Proj_12_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_12_5_0 [label="Attention_Scores_12_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_12_5_0 [label="Attention_Softmax_12_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_12_5_0 [label="Attention_Dropout_12_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_12_5_0 [label="Attention_Values_12_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_12_5_0 [label="Attention_Weighted_12_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_12_5_0 [label="Attention_Output_12_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_12_5 [label="Attention_AllReduce_12_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_5_1 [label="QKV_Proj_12_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_12_5_1 [label="Attention_Scores_12_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_12_5_1 [label="Attention_Softmax_12_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_12_5_1 [label="Attention_Dropout_12_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_12_5_1 [label="Attention_Values_12_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_12_5_1 [label="Attention_Weighted_12_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_12_5_1 [label="Attention_Output_12_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_12_5 [label="Attention_AllReduce_12_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_5_2 [label="QKV_Proj_12_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_12_5_2 [label="Attention_Scores_12_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_12_5_2 [label="Attention_Softmax_12_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_12_5_2 [label="Attention_Dropout_12_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_12_5_2 [label="Attention_Values_12_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_12_5_2 [label="Attention_Weighted_12_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_12_5_2 [label="Attention_Output_12_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_12_5 [label="Attention_AllReduce_12_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_5_3 [label="QKV_Proj_12_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_12_5_3 [label="Attention_Scores_12_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_12_5_3 [label="Attention_Softmax_12_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_12_5_3 [label="Attention_Dropout_12_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_12_5_3 [label="Attention_Values_12_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_12_5_3 [label="Attention_Weighted_12_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_12_5_3 [label="Attention_Output_12_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_12_5 [label="Attention_AllReduce_12_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_6_0 [label="QKV_Proj_12_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_12_6_0 [label="Attention_Scores_12_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_12_6_0 [label="Attention_Softmax_12_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_12_6_0 [label="Attention_Dropout_12_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_12_6_0 [label="Attention_Values_12_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_12_6_0 [label="Attention_Weighted_12_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_12_6_0 [label="Attention_Output_12_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_12_6 [label="Attention_AllReduce_12_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_6_1 [label="QKV_Proj_12_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_12_6_1 [label="Attention_Scores_12_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_12_6_1 [label="Attention_Softmax_12_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_12_6_1 [label="Attention_Dropout_12_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_12_6_1 [label="Attention_Values_12_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_12_6_1 [label="Attention_Weighted_12_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_12_6_1 [label="Attention_Output_12_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_12_6 [label="Attention_AllReduce_12_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_6_2 [label="QKV_Proj_12_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_12_6_2 [label="Attention_Scores_12_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_12_6_2 [label="Attention_Softmax_12_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_12_6_2 [label="Attention_Dropout_12_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_12_6_2 [label="Attention_Values_12_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_12_6_2 [label="Attention_Weighted_12_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_12_6_2 [label="Attention_Output_12_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_12_6 [label="Attention_AllReduce_12_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_6_3 [label="QKV_Proj_12_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_12_6_3 [label="Attention_Scores_12_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_12_6_3 [label="Attention_Softmax_12_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_12_6_3 [label="Attention_Dropout_12_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_12_6_3 [label="Attention_Values_12_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_12_6_3 [label="Attention_Weighted_12_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_12_6_3 [label="Attention_Output_12_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_12_6 [label="Attention_AllReduce_12_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_7_0 [label="QKV_Proj_12_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_12_7_0 [label="Attention_Scores_12_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_12_7_0 [label="Attention_Softmax_12_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_12_7_0 [label="Attention_Dropout_12_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_12_7_0 [label="Attention_Values_12_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_12_7_0 [label="Attention_Weighted_12_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_12_7_0 [label="Attention_Output_12_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_12_7 [label="Attention_AllReduce_12_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_7_1 [label="QKV_Proj_12_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_12_7_1 [label="Attention_Scores_12_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_12_7_1 [label="Attention_Softmax_12_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_12_7_1 [label="Attention_Dropout_12_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_12_7_1 [label="Attention_Values_12_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_12_7_1 [label="Attention_Weighted_12_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_12_7_1 [label="Attention_Output_12_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_12_7 [label="Attention_AllReduce_12_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_7_2 [label="QKV_Proj_12_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_12_7_2 [label="Attention_Scores_12_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_12_7_2 [label="Attention_Softmax_12_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_12_7_2 [label="Attention_Dropout_12_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_12_7_2 [label="Attention_Values_12_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_12_7_2 [label="Attention_Weighted_12_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_12_7_2 [label="Attention_Output_12_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_12_7 [label="Attention_AllReduce_12_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_7_3 [label="QKV_Proj_12_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_12_7_3 [label="Attention_Scores_12_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_12_7_3 [label="Attention_Softmax_12_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_12_7_3 [label="Attention_Dropout_12_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_12_7_3 [label="Attention_Values_12_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_12_7_3 [label="Attention_Weighted_12_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_12_7_3 [label="Attention_Output_12_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_12_7 [label="Attention_AllReduce_12_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_8_0 [label="QKV_Proj_12_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_12_8_0 [label="Attention_Scores_12_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_12_8_0 [label="Attention_Softmax_12_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_12_8_0 [label="Attention_Dropout_12_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_12_8_0 [label="Attention_Values_12_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_12_8_0 [label="Attention_Weighted_12_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_12_8_0 [label="Attention_Output_12_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_12_8 [label="Attention_AllReduce_12_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_8_1 [label="QKV_Proj_12_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_12_8_1 [label="Attention_Scores_12_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_12_8_1 [label="Attention_Softmax_12_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_12_8_1 [label="Attention_Dropout_12_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_12_8_1 [label="Attention_Values_12_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_12_8_1 [label="Attention_Weighted_12_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_12_8_1 [label="Attention_Output_12_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_12_8 [label="Attention_AllReduce_12_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_8_2 [label="QKV_Proj_12_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_12_8_2 [label="Attention_Scores_12_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_12_8_2 [label="Attention_Softmax_12_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_12_8_2 [label="Attention_Dropout_12_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_12_8_2 [label="Attention_Values_12_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_12_8_2 [label="Attention_Weighted_12_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_12_8_2 [label="Attention_Output_12_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_12_8 [label="Attention_AllReduce_12_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_8_3 [label="QKV_Proj_12_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_12_8_3 [label="Attention_Scores_12_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_12_8_3 [label="Attention_Softmax_12_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_12_8_3 [label="Attention_Dropout_12_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_12_8_3 [label="Attention_Values_12_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_12_8_3 [label="Attention_Weighted_12_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_12_8_3 [label="Attention_Output_12_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_12_8 [label="Attention_AllReduce_12_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_9_0 [label="QKV_Proj_12_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_12_9_0 [label="Attention_Scores_12_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_12_9_0 [label="Attention_Softmax_12_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_12_9_0 [label="Attention_Dropout_12_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_12_9_0 [label="Attention_Values_12_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_12_9_0 [label="Attention_Weighted_12_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_12_9_0 [label="Attention_Output_12_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_12_9 [label="Attention_AllReduce_12_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_9_1 [label="QKV_Proj_12_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_12_9_1 [label="Attention_Scores_12_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_12_9_1 [label="Attention_Softmax_12_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_12_9_1 [label="Attention_Dropout_12_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_12_9_1 [label="Attention_Values_12_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_12_9_1 [label="Attention_Weighted_12_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_12_9_1 [label="Attention_Output_12_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_12_9 [label="Attention_AllReduce_12_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_9_2 [label="QKV_Proj_12_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_12_9_2 [label="Attention_Scores_12_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_12_9_2 [label="Attention_Softmax_12_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_12_9_2 [label="Attention_Dropout_12_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_12_9_2 [label="Attention_Values_12_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_12_9_2 [label="Attention_Weighted_12_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_12_9_2 [label="Attention_Output_12_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_12_9 [label="Attention_AllReduce_12_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_9_3 [label="QKV_Proj_12_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_12_9_3 [label="Attention_Scores_12_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_12_9_3 [label="Attention_Softmax_12_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_12_9_3 [label="Attention_Dropout_12_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_12_9_3 [label="Attention_Values_12_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_12_9_3 [label="Attention_Weighted_12_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_12_9_3 [label="Attention_Output_12_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_12_9 [label="Attention_AllReduce_12_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_10_0 [label="QKV_Proj_12_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_12_10_0 [label="Attention_Scores_12_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_12_10_0 [label="Attention_Softmax_12_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_12_10_0 [label="Attention_Dropout_12_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_12_10_0 [label="Attention_Values_12_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_12_10_0 [label="Attention_Weighted_12_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_12_10_0 [label="Attention_Output_12_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_12_10 [label="Attention_AllReduce_12_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_10_1 [label="QKV_Proj_12_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_12_10_1 [label="Attention_Scores_12_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_12_10_1 [label="Attention_Softmax_12_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_12_10_1 [label="Attention_Dropout_12_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_12_10_1 [label="Attention_Values_12_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_12_10_1 [label="Attention_Weighted_12_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_12_10_1 [label="Attention_Output_12_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_12_10 [label="Attention_AllReduce_12_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_10_2 [label="QKV_Proj_12_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_12_10_2 [label="Attention_Scores_12_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_12_10_2 [label="Attention_Softmax_12_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_12_10_2 [label="Attention_Dropout_12_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_12_10_2 [label="Attention_Values_12_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_12_10_2 [label="Attention_Weighted_12_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_12_10_2 [label="Attention_Output_12_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_12_10 [label="Attention_AllReduce_12_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_10_3 [label="QKV_Proj_12_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_12_10_3 [label="Attention_Scores_12_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_12_10_3 [label="Attention_Softmax_12_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_12_10_3 [label="Attention_Dropout_12_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_12_10_3 [label="Attention_Values_12_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_12_10_3 [label="Attention_Weighted_12_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_12_10_3 [label="Attention_Output_12_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_12_10 [label="Attention_AllReduce_12_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_11_0 [label="QKV_Proj_12_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_12_11_0 [label="Attention_Scores_12_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_12_11_0 [label="Attention_Softmax_12_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_12_11_0 [label="Attention_Dropout_12_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_12_11_0 [label="Attention_Values_12_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_12_11_0 [label="Attention_Weighted_12_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_12_11_0 [label="Attention_Output_12_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_12_11 [label="Attention_AllReduce_12_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_11_1 [label="QKV_Proj_12_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_12_11_1 [label="Attention_Scores_12_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_12_11_1 [label="Attention_Softmax_12_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_12_11_1 [label="Attention_Dropout_12_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_12_11_1 [label="Attention_Values_12_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_12_11_1 [label="Attention_Weighted_12_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_12_11_1 [label="Attention_Output_12_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_12_11 [label="Attention_AllReduce_12_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_11_2 [label="QKV_Proj_12_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_12_11_2 [label="Attention_Scores_12_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_12_11_2 [label="Attention_Softmax_12_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_12_11_2 [label="Attention_Dropout_12_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_12_11_2 [label="Attention_Values_12_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_12_11_2 [label="Attention_Weighted_12_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_12_11_2 [label="Attention_Output_12_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_12_11 [label="Attention_AllReduce_12_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_11_3 [label="QKV_Proj_12_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_12_11_3 [label="Attention_Scores_12_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_12_11_3 [label="Attention_Softmax_12_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_12_11_3 [label="Attention_Dropout_12_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_12_11_3 [label="Attention_Values_12_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_12_11_3 [label="Attention_Weighted_12_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_12_11_3 [label="Attention_Output_12_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_12_11 [label="Attention_AllReduce_12_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_12_0 [label="QKV_Proj_12_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_12_12_0 [label="Attention_Scores_12_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_12_12_0 [label="Attention_Softmax_12_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_12_12_0 [label="Attention_Dropout_12_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_12_12_0 [label="Attention_Values_12_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_12_12_0 [label="Attention_Weighted_12_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_12_12_0 [label="Attention_Output_12_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_12_12 [label="Attention_AllReduce_12_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_12_1 [label="QKV_Proj_12_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_12_12_1 [label="Attention_Scores_12_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_12_12_1 [label="Attention_Softmax_12_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_12_12_1 [label="Attention_Dropout_12_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_12_12_1 [label="Attention_Values_12_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_12_12_1 [label="Attention_Weighted_12_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_12_12_1 [label="Attention_Output_12_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_12_12 [label="Attention_AllReduce_12_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_12_2 [label="QKV_Proj_12_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_12_12_2 [label="Attention_Scores_12_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_12_12_2 [label="Attention_Softmax_12_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_12_12_2 [label="Attention_Dropout_12_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_12_12_2 [label="Attention_Values_12_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_12_12_2 [label="Attention_Weighted_12_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_12_12_2 [label="Attention_Output_12_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_12_12 [label="Attention_AllReduce_12_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_12_3 [label="QKV_Proj_12_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_12_12_3 [label="Attention_Scores_12_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_12_12_3 [label="Attention_Softmax_12_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_12_12_3 [label="Attention_Dropout_12_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_12_12_3 [label="Attention_Values_12_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_12_12_3 [label="Attention_Weighted_12_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_12_12_3 [label="Attention_Output_12_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_12_12 [label="Attention_AllReduce_12_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_13_0 [label="QKV_Proj_12_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_12_13_0 [label="Attention_Scores_12_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_12_13_0 [label="Attention_Softmax_12_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_12_13_0 [label="Attention_Dropout_12_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_12_13_0 [label="Attention_Values_12_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_12_13_0 [label="Attention_Weighted_12_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_12_13_0 [label="Attention_Output_12_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_12_13 [label="Attention_AllReduce_12_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_13_1 [label="QKV_Proj_12_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_12_13_1 [label="Attention_Scores_12_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_12_13_1 [label="Attention_Softmax_12_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_12_13_1 [label="Attention_Dropout_12_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_12_13_1 [label="Attention_Values_12_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_12_13_1 [label="Attention_Weighted_12_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_12_13_1 [label="Attention_Output_12_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_12_13 [label="Attention_AllReduce_12_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_13_2 [label="QKV_Proj_12_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_12_13_2 [label="Attention_Scores_12_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_12_13_2 [label="Attention_Softmax_12_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_12_13_2 [label="Attention_Dropout_12_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_12_13_2 [label="Attention_Values_12_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_12_13_2 [label="Attention_Weighted_12_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_12_13_2 [label="Attention_Output_12_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_12_13 [label="Attention_AllReduce_12_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_13_3 [label="QKV_Proj_12_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_12_13_3 [label="Attention_Scores_12_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_12_13_3 [label="Attention_Softmax_12_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_12_13_3 [label="Attention_Dropout_12_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_12_13_3 [label="Attention_Values_12_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_12_13_3 [label="Attention_Weighted_12_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_12_13_3 [label="Attention_Output_12_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_12_13 [label="Attention_AllReduce_12_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_14_0 [label="QKV_Proj_12_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_12_14_0 [label="Attention_Scores_12_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_12_14_0 [label="Attention_Softmax_12_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_12_14_0 [label="Attention_Dropout_12_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_12_14_0 [label="Attention_Values_12_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_12_14_0 [label="Attention_Weighted_12_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_12_14_0 [label="Attention_Output_12_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_12_14 [label="Attention_AllReduce_12_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_14_1 [label="QKV_Proj_12_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_12_14_1 [label="Attention_Scores_12_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_12_14_1 [label="Attention_Softmax_12_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_12_14_1 [label="Attention_Dropout_12_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_12_14_1 [label="Attention_Values_12_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_12_14_1 [label="Attention_Weighted_12_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_12_14_1 [label="Attention_Output_12_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_12_14 [label="Attention_AllReduce_12_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_14_2 [label="QKV_Proj_12_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_12_14_2 [label="Attention_Scores_12_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_12_14_2 [label="Attention_Softmax_12_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_12_14_2 [label="Attention_Dropout_12_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_12_14_2 [label="Attention_Values_12_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_12_14_2 [label="Attention_Weighted_12_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_12_14_2 [label="Attention_Output_12_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_12_14 [label="Attention_AllReduce_12_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_14_3 [label="QKV_Proj_12_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_12_14_3 [label="Attention_Scores_12_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_12_14_3 [label="Attention_Softmax_12_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_12_14_3 [label="Attention_Dropout_12_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_12_14_3 [label="Attention_Values_12_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_12_14_3 [label="Attention_Weighted_12_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_12_14_3 [label="Attention_Output_12_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_12_14 [label="Attention_AllReduce_12_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_15_0 [label="QKV_Proj_12_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_12_15_0 [label="Attention_Scores_12_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_12_15_0 [label="Attention_Softmax_12_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_12_15_0 [label="Attention_Dropout_12_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_12_15_0 [label="Attention_Values_12_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_12_15_0 [label="Attention_Weighted_12_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_12_15_0 [label="Attention_Output_12_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_12_15 [label="Attention_AllReduce_12_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_15_1 [label="QKV_Proj_12_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_12_15_1 [label="Attention_Scores_12_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_12_15_1 [label="Attention_Softmax_12_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_12_15_1 [label="Attention_Dropout_12_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_12_15_1 [label="Attention_Values_12_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_12_15_1 [label="Attention_Weighted_12_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_12_15_1 [label="Attention_Output_12_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_12_15 [label="Attention_AllReduce_12_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_15_2 [label="QKV_Proj_12_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_12_15_2 [label="Attention_Scores_12_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_12_15_2 [label="Attention_Softmax_12_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_12_15_2 [label="Attention_Dropout_12_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_12_15_2 [label="Attention_Values_12_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_12_15_2 [label="Attention_Weighted_12_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_12_15_2 [label="Attention_Output_12_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_12_15 [label="Attention_AllReduce_12_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_12_15_3 [label="QKV_Proj_12_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_12_15_3 [label="Attention_Scores_12_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_12_15_3 [label="Attention_Softmax_12_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_12_15_3 [label="Attention_Dropout_12_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_12_15_3 [label="Attention_Values_12_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_12_15_3 [label="Attention_Weighted_12_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_12_15_3 [label="Attention_Output_12_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_12_15 [label="Attention_AllReduce_12_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_0_0 [label="MLP_Linear1_12_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_12_0_0 [label="GELU_12_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_12_0_0 [label="MLP_Linear2_12_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_12_0_1 [label="MLP_Linear1_12_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_12_0_1 [label="GELU_12_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_12_0_1 [label="MLP_Linear2_12_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_12_0_2 [label="MLP_Linear1_12_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_12_0_2 [label="GELU_12_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_12_0_2 [label="MLP_Linear2_12_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_12_0_3 [label="MLP_Linear1_12_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_12_0_3 [label="GELU_12_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_12_0_3 [label="MLP_Linear2_12_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_12_0 [label="MLP_AllReduce_12_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_1_0 [label="MLP_Linear1_12_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_12_1_0 [label="GELU_12_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_12_1_0 [label="MLP_Linear2_12_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_12_1_1 [label="MLP_Linear1_12_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_12_1_1 [label="GELU_12_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_12_1_1 [label="MLP_Linear2_12_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_12_1_2 [label="MLP_Linear1_12_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_12_1_2 [label="GELU_12_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_12_1_2 [label="MLP_Linear2_12_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_12_1_3 [label="MLP_Linear1_12_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_12_1_3 [label="GELU_12_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_12_1_3 [label="MLP_Linear2_12_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_12_1 [label="MLP_AllReduce_12_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_2_0 [label="MLP_Linear1_12_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_12_2_0 [label="GELU_12_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_12_2_0 [label="MLP_Linear2_12_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_12_2_1 [label="MLP_Linear1_12_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_12_2_1 [label="GELU_12_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_12_2_1 [label="MLP_Linear2_12_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_12_2_2 [label="MLP_Linear1_12_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_12_2_2 [label="GELU_12_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_12_2_2 [label="MLP_Linear2_12_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_12_2_3 [label="MLP_Linear1_12_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_12_2_3 [label="GELU_12_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_12_2_3 [label="MLP_Linear2_12_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_12_2 [label="MLP_AllReduce_12_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_3_0 [label="MLP_Linear1_12_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_12_3_0 [label="GELU_12_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_12_3_0 [label="MLP_Linear2_12_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_12_3_1 [label="MLP_Linear1_12_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_12_3_1 [label="GELU_12_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_12_3_1 [label="MLP_Linear2_12_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_12_3_2 [label="MLP_Linear1_12_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_12_3_2 [label="GELU_12_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_12_3_2 [label="MLP_Linear2_12_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_12_3_3 [label="MLP_Linear1_12_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_12_3_3 [label="GELU_12_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_12_3_3 [label="MLP_Linear2_12_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_12_3 [label="MLP_AllReduce_12_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_4_0 [label="MLP_Linear1_12_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_12_4_0 [label="GELU_12_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_12_4_0 [label="MLP_Linear2_12_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_12_4_1 [label="MLP_Linear1_12_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_12_4_1 [label="GELU_12_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_12_4_1 [label="MLP_Linear2_12_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_12_4_2 [label="MLP_Linear1_12_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_12_4_2 [label="GELU_12_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_12_4_2 [label="MLP_Linear2_12_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_12_4_3 [label="MLP_Linear1_12_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_12_4_3 [label="GELU_12_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_12_4_3 [label="MLP_Linear2_12_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_12_4 [label="MLP_AllReduce_12_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_5_0 [label="MLP_Linear1_12_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_12_5_0 [label="GELU_12_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_12_5_0 [label="MLP_Linear2_12_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_12_5_1 [label="MLP_Linear1_12_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_12_5_1 [label="GELU_12_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_12_5_1 [label="MLP_Linear2_12_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_12_5_2 [label="MLP_Linear1_12_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_12_5_2 [label="GELU_12_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_12_5_2 [label="MLP_Linear2_12_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_12_5_3 [label="MLP_Linear1_12_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_12_5_3 [label="GELU_12_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_12_5_3 [label="MLP_Linear2_12_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_12_5 [label="MLP_AllReduce_12_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_6_0 [label="MLP_Linear1_12_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_12_6_0 [label="GELU_12_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_12_6_0 [label="MLP_Linear2_12_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_12_6_1 [label="MLP_Linear1_12_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_12_6_1 [label="GELU_12_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_12_6_1 [label="MLP_Linear2_12_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_12_6_2 [label="MLP_Linear1_12_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_12_6_2 [label="GELU_12_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_12_6_2 [label="MLP_Linear2_12_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_12_6_3 [label="MLP_Linear1_12_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_12_6_3 [label="GELU_12_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_12_6_3 [label="MLP_Linear2_12_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_12_6 [label="MLP_AllReduce_12_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_7_0 [label="MLP_Linear1_12_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_12_7_0 [label="GELU_12_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_12_7_0 [label="MLP_Linear2_12_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_12_7_1 [label="MLP_Linear1_12_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_12_7_1 [label="GELU_12_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_12_7_1 [label="MLP_Linear2_12_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_12_7_2 [label="MLP_Linear1_12_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_12_7_2 [label="GELU_12_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_12_7_2 [label="MLP_Linear2_12_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_12_7_3 [label="MLP_Linear1_12_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_12_7_3 [label="GELU_12_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_12_7_3 [label="MLP_Linear2_12_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_12_7 [label="MLP_AllReduce_12_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_8_0 [label="MLP_Linear1_12_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_12_8_0 [label="GELU_12_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_12_8_0 [label="MLP_Linear2_12_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_12_8_1 [label="MLP_Linear1_12_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_12_8_1 [label="GELU_12_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_12_8_1 [label="MLP_Linear2_12_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_12_8_2 [label="MLP_Linear1_12_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_12_8_2 [label="GELU_12_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_12_8_2 [label="MLP_Linear2_12_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_12_8_3 [label="MLP_Linear1_12_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_12_8_3 [label="GELU_12_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_12_8_3 [label="MLP_Linear2_12_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_12_8 [label="MLP_AllReduce_12_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_9_0 [label="MLP_Linear1_12_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_12_9_0 [label="GELU_12_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_12_9_0 [label="MLP_Linear2_12_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_12_9_1 [label="MLP_Linear1_12_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_12_9_1 [label="GELU_12_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_12_9_1 [label="MLP_Linear2_12_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_12_9_2 [label="MLP_Linear1_12_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_12_9_2 [label="GELU_12_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_12_9_2 [label="MLP_Linear2_12_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_12_9_3 [label="MLP_Linear1_12_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_12_9_3 [label="GELU_12_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_12_9_3 [label="MLP_Linear2_12_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_12_9 [label="MLP_AllReduce_12_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_10_0 [label="MLP_Linear1_12_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_12_10_0 [label="GELU_12_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_12_10_0 [label="MLP_Linear2_12_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_12_10_1 [label="MLP_Linear1_12_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_12_10_1 [label="GELU_12_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_12_10_1 [label="MLP_Linear2_12_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_12_10_2 [label="MLP_Linear1_12_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_12_10_2 [label="GELU_12_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_12_10_2 [label="MLP_Linear2_12_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_12_10_3 [label="MLP_Linear1_12_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_12_10_3 [label="GELU_12_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_12_10_3 [label="MLP_Linear2_12_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_12_10 [label="MLP_AllReduce_12_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_11_0 [label="MLP_Linear1_12_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_12_11_0 [label="GELU_12_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_12_11_0 [label="MLP_Linear2_12_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_12_11_1 [label="MLP_Linear1_12_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_12_11_1 [label="GELU_12_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_12_11_1 [label="MLP_Linear2_12_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_12_11_2 [label="MLP_Linear1_12_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_12_11_2 [label="GELU_12_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_12_11_2 [label="MLP_Linear2_12_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_12_11_3 [label="MLP_Linear1_12_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_12_11_3 [label="GELU_12_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_12_11_3 [label="MLP_Linear2_12_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_12_11 [label="MLP_AllReduce_12_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_12_0 [label="MLP_Linear1_12_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_12_12_0 [label="GELU_12_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_12_12_0 [label="MLP_Linear2_12_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_12_12_1 [label="MLP_Linear1_12_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_12_12_1 [label="GELU_12_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_12_12_1 [label="MLP_Linear2_12_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_12_12_2 [label="MLP_Linear1_12_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_12_12_2 [label="GELU_12_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_12_12_2 [label="MLP_Linear2_12_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_12_12_3 [label="MLP_Linear1_12_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_12_12_3 [label="GELU_12_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_12_12_3 [label="MLP_Linear2_12_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_12_12 [label="MLP_AllReduce_12_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_13_0 [label="MLP_Linear1_12_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_12_13_0 [label="GELU_12_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_12_13_0 [label="MLP_Linear2_12_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_12_13_1 [label="MLP_Linear1_12_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_12_13_1 [label="GELU_12_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_12_13_1 [label="MLP_Linear2_12_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_12_13_2 [label="MLP_Linear1_12_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_12_13_2 [label="GELU_12_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_12_13_2 [label="MLP_Linear2_12_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_12_13_3 [label="MLP_Linear1_12_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_12_13_3 [label="GELU_12_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_12_13_3 [label="MLP_Linear2_12_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_12_13 [label="MLP_AllReduce_12_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_14_0 [label="MLP_Linear1_12_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_12_14_0 [label="GELU_12_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_12_14_0 [label="MLP_Linear2_12_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_12_14_1 [label="MLP_Linear1_12_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_12_14_1 [label="GELU_12_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_12_14_1 [label="MLP_Linear2_12_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_12_14_2 [label="MLP_Linear1_12_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_12_14_2 [label="GELU_12_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_12_14_2 [label="MLP_Linear2_12_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_12_14_3 [label="MLP_Linear1_12_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_12_14_3 [label="GELU_12_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_12_14_3 [label="MLP_Linear2_12_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_12_14 [label="MLP_AllReduce_12_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_12_15_0 [label="MLP_Linear1_12_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_12_15_0 [label="GELU_12_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_12_15_0 [label="MLP_Linear2_12_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_12_15_1 [label="MLP_Linear1_12_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_12_15_1 [label="GELU_12_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_12_15_1 [label="MLP_Linear2_12_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_12_15_2 [label="MLP_Linear1_12_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_12_15_2 [label="GELU_12_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_12_15_2 [label="MLP_Linear2_12_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_12_15_3 [label="MLP_Linear1_12_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_12_15_3 [label="GELU_12_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_12_15_3 [label="MLP_Linear2_12_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_12_15 [label="MLP_AllReduce_12_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_12_0 [label="Expert_Route_12_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_1 [label="Expert_Route_12_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_2 [label="Expert_Route_12_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_3 [label="Expert_Route_12_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_4 [label="Expert_Route_12_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_5 [label="Expert_Route_12_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_6 [label="Expert_Route_12_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_7 [label="Expert_Route_12_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_8 [label="Expert_Route_12_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_9 [label="Expert_Route_12_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_10 [label="Expert_Route_12_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_11 [label="Expert_Route_12_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_12 [label="Expert_Route_12_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_13 [label="Expert_Route_12_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_14 [label="Expert_Route_12_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_12_15 [label="Expert_Route_12_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_13_0_0 [label="QKV_Proj_13_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_13_0_0 [label="Attention_Scores_13_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_13_0_0 [label="Attention_Softmax_13_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_13_0_0 [label="Attention_Dropout_13_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_13_0_0 [label="Attention_Values_13_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_13_0_0 [label="Attention_Weighted_13_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_13_0_0 [label="Attention_Output_13_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_13_0 [label="Attention_AllReduce_13_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_0_1 [label="QKV_Proj_13_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_13_0_1 [label="Attention_Scores_13_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_13_0_1 [label="Attention_Softmax_13_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_13_0_1 [label="Attention_Dropout_13_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_13_0_1 [label="Attention_Values_13_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_13_0_1 [label="Attention_Weighted_13_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_13_0_1 [label="Attention_Output_13_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_13_0 [label="Attention_AllReduce_13_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_0_2 [label="QKV_Proj_13_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_13_0_2 [label="Attention_Scores_13_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_13_0_2 [label="Attention_Softmax_13_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_13_0_2 [label="Attention_Dropout_13_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_13_0_2 [label="Attention_Values_13_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_13_0_2 [label="Attention_Weighted_13_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_13_0_2 [label="Attention_Output_13_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_13_0 [label="Attention_AllReduce_13_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_0_3 [label="QKV_Proj_13_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_13_0_3 [label="Attention_Scores_13_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_13_0_3 [label="Attention_Softmax_13_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_13_0_3 [label="Attention_Dropout_13_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_13_0_3 [label="Attention_Values_13_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_13_0_3 [label="Attention_Weighted_13_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_13_0_3 [label="Attention_Output_13_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_13_0 [label="Attention_AllReduce_13_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_1_0 [label="QKV_Proj_13_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_13_1_0 [label="Attention_Scores_13_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_13_1_0 [label="Attention_Softmax_13_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_13_1_0 [label="Attention_Dropout_13_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_13_1_0 [label="Attention_Values_13_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_13_1_0 [label="Attention_Weighted_13_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_13_1_0 [label="Attention_Output_13_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_13_1 [label="Attention_AllReduce_13_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_1_1 [label="QKV_Proj_13_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_13_1_1 [label="Attention_Scores_13_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_13_1_1 [label="Attention_Softmax_13_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_13_1_1 [label="Attention_Dropout_13_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_13_1_1 [label="Attention_Values_13_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_13_1_1 [label="Attention_Weighted_13_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_13_1_1 [label="Attention_Output_13_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_13_1 [label="Attention_AllReduce_13_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_1_2 [label="QKV_Proj_13_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_13_1_2 [label="Attention_Scores_13_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_13_1_2 [label="Attention_Softmax_13_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_13_1_2 [label="Attention_Dropout_13_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_13_1_2 [label="Attention_Values_13_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_13_1_2 [label="Attention_Weighted_13_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_13_1_2 [label="Attention_Output_13_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_13_1 [label="Attention_AllReduce_13_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_1_3 [label="QKV_Proj_13_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_13_1_3 [label="Attention_Scores_13_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_13_1_3 [label="Attention_Softmax_13_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_13_1_3 [label="Attention_Dropout_13_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_13_1_3 [label="Attention_Values_13_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_13_1_3 [label="Attention_Weighted_13_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_13_1_3 [label="Attention_Output_13_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_13_1 [label="Attention_AllReduce_13_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_2_0 [label="QKV_Proj_13_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_13_2_0 [label="Attention_Scores_13_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_13_2_0 [label="Attention_Softmax_13_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_13_2_0 [label="Attention_Dropout_13_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_13_2_0 [label="Attention_Values_13_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_13_2_0 [label="Attention_Weighted_13_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_13_2_0 [label="Attention_Output_13_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_13_2 [label="Attention_AllReduce_13_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_2_1 [label="QKV_Proj_13_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_13_2_1 [label="Attention_Scores_13_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_13_2_1 [label="Attention_Softmax_13_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_13_2_1 [label="Attention_Dropout_13_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_13_2_1 [label="Attention_Values_13_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_13_2_1 [label="Attention_Weighted_13_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_13_2_1 [label="Attention_Output_13_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_13_2 [label="Attention_AllReduce_13_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_2_2 [label="QKV_Proj_13_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_13_2_2 [label="Attention_Scores_13_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_13_2_2 [label="Attention_Softmax_13_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_13_2_2 [label="Attention_Dropout_13_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_13_2_2 [label="Attention_Values_13_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_13_2_2 [label="Attention_Weighted_13_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_13_2_2 [label="Attention_Output_13_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_13_2 [label="Attention_AllReduce_13_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_2_3 [label="QKV_Proj_13_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_13_2_3 [label="Attention_Scores_13_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_13_2_3 [label="Attention_Softmax_13_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_13_2_3 [label="Attention_Dropout_13_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_13_2_3 [label="Attention_Values_13_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_13_2_3 [label="Attention_Weighted_13_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_13_2_3 [label="Attention_Output_13_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_13_2 [label="Attention_AllReduce_13_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_3_0 [label="QKV_Proj_13_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_13_3_0 [label="Attention_Scores_13_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_13_3_0 [label="Attention_Softmax_13_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_13_3_0 [label="Attention_Dropout_13_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_13_3_0 [label="Attention_Values_13_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_13_3_0 [label="Attention_Weighted_13_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_13_3_0 [label="Attention_Output_13_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_13_3 [label="Attention_AllReduce_13_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_3_1 [label="QKV_Proj_13_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_13_3_1 [label="Attention_Scores_13_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_13_3_1 [label="Attention_Softmax_13_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_13_3_1 [label="Attention_Dropout_13_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_13_3_1 [label="Attention_Values_13_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_13_3_1 [label="Attention_Weighted_13_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_13_3_1 [label="Attention_Output_13_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_13_3 [label="Attention_AllReduce_13_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_3_2 [label="QKV_Proj_13_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_13_3_2 [label="Attention_Scores_13_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_13_3_2 [label="Attention_Softmax_13_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_13_3_2 [label="Attention_Dropout_13_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_13_3_2 [label="Attention_Values_13_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_13_3_2 [label="Attention_Weighted_13_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_13_3_2 [label="Attention_Output_13_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_13_3 [label="Attention_AllReduce_13_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_3_3 [label="QKV_Proj_13_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_13_3_3 [label="Attention_Scores_13_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_13_3_3 [label="Attention_Softmax_13_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_13_3_3 [label="Attention_Dropout_13_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_13_3_3 [label="Attention_Values_13_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_13_3_3 [label="Attention_Weighted_13_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_13_3_3 [label="Attention_Output_13_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_13_3 [label="Attention_AllReduce_13_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_4_0 [label="QKV_Proj_13_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_13_4_0 [label="Attention_Scores_13_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_13_4_0 [label="Attention_Softmax_13_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_13_4_0 [label="Attention_Dropout_13_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_13_4_0 [label="Attention_Values_13_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_13_4_0 [label="Attention_Weighted_13_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_13_4_0 [label="Attention_Output_13_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_13_4 [label="Attention_AllReduce_13_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_4_1 [label="QKV_Proj_13_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_13_4_1 [label="Attention_Scores_13_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_13_4_1 [label="Attention_Softmax_13_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_13_4_1 [label="Attention_Dropout_13_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_13_4_1 [label="Attention_Values_13_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_13_4_1 [label="Attention_Weighted_13_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_13_4_1 [label="Attention_Output_13_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_13_4 [label="Attention_AllReduce_13_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_4_2 [label="QKV_Proj_13_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_13_4_2 [label="Attention_Scores_13_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_13_4_2 [label="Attention_Softmax_13_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_13_4_2 [label="Attention_Dropout_13_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_13_4_2 [label="Attention_Values_13_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_13_4_2 [label="Attention_Weighted_13_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_13_4_2 [label="Attention_Output_13_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_13_4 [label="Attention_AllReduce_13_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_4_3 [label="QKV_Proj_13_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_13_4_3 [label="Attention_Scores_13_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_13_4_3 [label="Attention_Softmax_13_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_13_4_3 [label="Attention_Dropout_13_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_13_4_3 [label="Attention_Values_13_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_13_4_3 [label="Attention_Weighted_13_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_13_4_3 [label="Attention_Output_13_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_13_4 [label="Attention_AllReduce_13_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_5_0 [label="QKV_Proj_13_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_13_5_0 [label="Attention_Scores_13_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_13_5_0 [label="Attention_Softmax_13_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_13_5_0 [label="Attention_Dropout_13_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_13_5_0 [label="Attention_Values_13_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_13_5_0 [label="Attention_Weighted_13_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_13_5_0 [label="Attention_Output_13_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_13_5 [label="Attention_AllReduce_13_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_5_1 [label="QKV_Proj_13_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_13_5_1 [label="Attention_Scores_13_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_13_5_1 [label="Attention_Softmax_13_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_13_5_1 [label="Attention_Dropout_13_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_13_5_1 [label="Attention_Values_13_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_13_5_1 [label="Attention_Weighted_13_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_13_5_1 [label="Attention_Output_13_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_13_5 [label="Attention_AllReduce_13_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_5_2 [label="QKV_Proj_13_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_13_5_2 [label="Attention_Scores_13_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_13_5_2 [label="Attention_Softmax_13_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_13_5_2 [label="Attention_Dropout_13_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_13_5_2 [label="Attention_Values_13_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_13_5_2 [label="Attention_Weighted_13_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_13_5_2 [label="Attention_Output_13_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_13_5 [label="Attention_AllReduce_13_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_5_3 [label="QKV_Proj_13_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_13_5_3 [label="Attention_Scores_13_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_13_5_3 [label="Attention_Softmax_13_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_13_5_3 [label="Attention_Dropout_13_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_13_5_3 [label="Attention_Values_13_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_13_5_3 [label="Attention_Weighted_13_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_13_5_3 [label="Attention_Output_13_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_13_5 [label="Attention_AllReduce_13_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_6_0 [label="QKV_Proj_13_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_13_6_0 [label="Attention_Scores_13_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_13_6_0 [label="Attention_Softmax_13_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_13_6_0 [label="Attention_Dropout_13_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_13_6_0 [label="Attention_Values_13_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_13_6_0 [label="Attention_Weighted_13_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_13_6_0 [label="Attention_Output_13_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_13_6 [label="Attention_AllReduce_13_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_6_1 [label="QKV_Proj_13_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_13_6_1 [label="Attention_Scores_13_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_13_6_1 [label="Attention_Softmax_13_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_13_6_1 [label="Attention_Dropout_13_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_13_6_1 [label="Attention_Values_13_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_13_6_1 [label="Attention_Weighted_13_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_13_6_1 [label="Attention_Output_13_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_13_6 [label="Attention_AllReduce_13_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_6_2 [label="QKV_Proj_13_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_13_6_2 [label="Attention_Scores_13_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_13_6_2 [label="Attention_Softmax_13_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_13_6_2 [label="Attention_Dropout_13_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_13_6_2 [label="Attention_Values_13_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_13_6_2 [label="Attention_Weighted_13_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_13_6_2 [label="Attention_Output_13_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_13_6 [label="Attention_AllReduce_13_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_6_3 [label="QKV_Proj_13_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_13_6_3 [label="Attention_Scores_13_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_13_6_3 [label="Attention_Softmax_13_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_13_6_3 [label="Attention_Dropout_13_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_13_6_3 [label="Attention_Values_13_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_13_6_3 [label="Attention_Weighted_13_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_13_6_3 [label="Attention_Output_13_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_13_6 [label="Attention_AllReduce_13_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_7_0 [label="QKV_Proj_13_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_13_7_0 [label="Attention_Scores_13_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_13_7_0 [label="Attention_Softmax_13_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_13_7_0 [label="Attention_Dropout_13_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_13_7_0 [label="Attention_Values_13_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_13_7_0 [label="Attention_Weighted_13_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_13_7_0 [label="Attention_Output_13_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_13_7 [label="Attention_AllReduce_13_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_7_1 [label="QKV_Proj_13_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_13_7_1 [label="Attention_Scores_13_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_13_7_1 [label="Attention_Softmax_13_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_13_7_1 [label="Attention_Dropout_13_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_13_7_1 [label="Attention_Values_13_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_13_7_1 [label="Attention_Weighted_13_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_13_7_1 [label="Attention_Output_13_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_13_7 [label="Attention_AllReduce_13_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_7_2 [label="QKV_Proj_13_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_13_7_2 [label="Attention_Scores_13_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_13_7_2 [label="Attention_Softmax_13_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_13_7_2 [label="Attention_Dropout_13_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_13_7_2 [label="Attention_Values_13_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_13_7_2 [label="Attention_Weighted_13_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_13_7_2 [label="Attention_Output_13_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_13_7 [label="Attention_AllReduce_13_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_7_3 [label="QKV_Proj_13_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_13_7_3 [label="Attention_Scores_13_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_13_7_3 [label="Attention_Softmax_13_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_13_7_3 [label="Attention_Dropout_13_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_13_7_3 [label="Attention_Values_13_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_13_7_3 [label="Attention_Weighted_13_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_13_7_3 [label="Attention_Output_13_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_13_7 [label="Attention_AllReduce_13_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_8_0 [label="QKV_Proj_13_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_13_8_0 [label="Attention_Scores_13_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_13_8_0 [label="Attention_Softmax_13_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_13_8_0 [label="Attention_Dropout_13_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_13_8_0 [label="Attention_Values_13_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_13_8_0 [label="Attention_Weighted_13_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_13_8_0 [label="Attention_Output_13_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_13_8 [label="Attention_AllReduce_13_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_8_1 [label="QKV_Proj_13_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_13_8_1 [label="Attention_Scores_13_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_13_8_1 [label="Attention_Softmax_13_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_13_8_1 [label="Attention_Dropout_13_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_13_8_1 [label="Attention_Values_13_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_13_8_1 [label="Attention_Weighted_13_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_13_8_1 [label="Attention_Output_13_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_13_8 [label="Attention_AllReduce_13_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_8_2 [label="QKV_Proj_13_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_13_8_2 [label="Attention_Scores_13_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_13_8_2 [label="Attention_Softmax_13_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_13_8_2 [label="Attention_Dropout_13_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_13_8_2 [label="Attention_Values_13_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_13_8_2 [label="Attention_Weighted_13_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_13_8_2 [label="Attention_Output_13_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_13_8 [label="Attention_AllReduce_13_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_8_3 [label="QKV_Proj_13_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_13_8_3 [label="Attention_Scores_13_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_13_8_3 [label="Attention_Softmax_13_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_13_8_3 [label="Attention_Dropout_13_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_13_8_3 [label="Attention_Values_13_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_13_8_3 [label="Attention_Weighted_13_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_13_8_3 [label="Attention_Output_13_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_13_8 [label="Attention_AllReduce_13_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_9_0 [label="QKV_Proj_13_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_13_9_0 [label="Attention_Scores_13_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_13_9_0 [label="Attention_Softmax_13_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_13_9_0 [label="Attention_Dropout_13_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_13_9_0 [label="Attention_Values_13_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_13_9_0 [label="Attention_Weighted_13_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_13_9_0 [label="Attention_Output_13_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_13_9 [label="Attention_AllReduce_13_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_9_1 [label="QKV_Proj_13_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_13_9_1 [label="Attention_Scores_13_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_13_9_1 [label="Attention_Softmax_13_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_13_9_1 [label="Attention_Dropout_13_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_13_9_1 [label="Attention_Values_13_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_13_9_1 [label="Attention_Weighted_13_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_13_9_1 [label="Attention_Output_13_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_13_9 [label="Attention_AllReduce_13_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_9_2 [label="QKV_Proj_13_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_13_9_2 [label="Attention_Scores_13_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_13_9_2 [label="Attention_Softmax_13_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_13_9_2 [label="Attention_Dropout_13_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_13_9_2 [label="Attention_Values_13_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_13_9_2 [label="Attention_Weighted_13_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_13_9_2 [label="Attention_Output_13_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_13_9 [label="Attention_AllReduce_13_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_9_3 [label="QKV_Proj_13_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_13_9_3 [label="Attention_Scores_13_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_13_9_3 [label="Attention_Softmax_13_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_13_9_3 [label="Attention_Dropout_13_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_13_9_3 [label="Attention_Values_13_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_13_9_3 [label="Attention_Weighted_13_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_13_9_3 [label="Attention_Output_13_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_13_9 [label="Attention_AllReduce_13_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_10_0 [label="QKV_Proj_13_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_13_10_0 [label="Attention_Scores_13_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_13_10_0 [label="Attention_Softmax_13_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_13_10_0 [label="Attention_Dropout_13_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_13_10_0 [label="Attention_Values_13_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_13_10_0 [label="Attention_Weighted_13_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_13_10_0 [label="Attention_Output_13_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_13_10 [label="Attention_AllReduce_13_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_10_1 [label="QKV_Proj_13_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_13_10_1 [label="Attention_Scores_13_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_13_10_1 [label="Attention_Softmax_13_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_13_10_1 [label="Attention_Dropout_13_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_13_10_1 [label="Attention_Values_13_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_13_10_1 [label="Attention_Weighted_13_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_13_10_1 [label="Attention_Output_13_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_13_10 [label="Attention_AllReduce_13_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_10_2 [label="QKV_Proj_13_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_13_10_2 [label="Attention_Scores_13_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_13_10_2 [label="Attention_Softmax_13_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_13_10_2 [label="Attention_Dropout_13_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_13_10_2 [label="Attention_Values_13_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_13_10_2 [label="Attention_Weighted_13_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_13_10_2 [label="Attention_Output_13_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_13_10 [label="Attention_AllReduce_13_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_10_3 [label="QKV_Proj_13_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_13_10_3 [label="Attention_Scores_13_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_13_10_3 [label="Attention_Softmax_13_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_13_10_3 [label="Attention_Dropout_13_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_13_10_3 [label="Attention_Values_13_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_13_10_3 [label="Attention_Weighted_13_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_13_10_3 [label="Attention_Output_13_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_13_10 [label="Attention_AllReduce_13_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_11_0 [label="QKV_Proj_13_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_13_11_0 [label="Attention_Scores_13_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_13_11_0 [label="Attention_Softmax_13_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_13_11_0 [label="Attention_Dropout_13_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_13_11_0 [label="Attention_Values_13_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_13_11_0 [label="Attention_Weighted_13_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_13_11_0 [label="Attention_Output_13_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_13_11 [label="Attention_AllReduce_13_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_11_1 [label="QKV_Proj_13_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_13_11_1 [label="Attention_Scores_13_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_13_11_1 [label="Attention_Softmax_13_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_13_11_1 [label="Attention_Dropout_13_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_13_11_1 [label="Attention_Values_13_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_13_11_1 [label="Attention_Weighted_13_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_13_11_1 [label="Attention_Output_13_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_13_11 [label="Attention_AllReduce_13_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_11_2 [label="QKV_Proj_13_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_13_11_2 [label="Attention_Scores_13_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_13_11_2 [label="Attention_Softmax_13_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_13_11_2 [label="Attention_Dropout_13_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_13_11_2 [label="Attention_Values_13_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_13_11_2 [label="Attention_Weighted_13_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_13_11_2 [label="Attention_Output_13_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_13_11 [label="Attention_AllReduce_13_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_11_3 [label="QKV_Proj_13_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_13_11_3 [label="Attention_Scores_13_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_13_11_3 [label="Attention_Softmax_13_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_13_11_3 [label="Attention_Dropout_13_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_13_11_3 [label="Attention_Values_13_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_13_11_3 [label="Attention_Weighted_13_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_13_11_3 [label="Attention_Output_13_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_13_11 [label="Attention_AllReduce_13_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_12_0 [label="QKV_Proj_13_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_13_12_0 [label="Attention_Scores_13_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_13_12_0 [label="Attention_Softmax_13_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_13_12_0 [label="Attention_Dropout_13_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_13_12_0 [label="Attention_Values_13_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_13_12_0 [label="Attention_Weighted_13_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_13_12_0 [label="Attention_Output_13_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_13_12 [label="Attention_AllReduce_13_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_12_1 [label="QKV_Proj_13_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_13_12_1 [label="Attention_Scores_13_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_13_12_1 [label="Attention_Softmax_13_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_13_12_1 [label="Attention_Dropout_13_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_13_12_1 [label="Attention_Values_13_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_13_12_1 [label="Attention_Weighted_13_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_13_12_1 [label="Attention_Output_13_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_13_12 [label="Attention_AllReduce_13_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_12_2 [label="QKV_Proj_13_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_13_12_2 [label="Attention_Scores_13_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_13_12_2 [label="Attention_Softmax_13_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_13_12_2 [label="Attention_Dropout_13_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_13_12_2 [label="Attention_Values_13_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_13_12_2 [label="Attention_Weighted_13_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_13_12_2 [label="Attention_Output_13_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_13_12 [label="Attention_AllReduce_13_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_12_3 [label="QKV_Proj_13_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_13_12_3 [label="Attention_Scores_13_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_13_12_3 [label="Attention_Softmax_13_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_13_12_3 [label="Attention_Dropout_13_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_13_12_3 [label="Attention_Values_13_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_13_12_3 [label="Attention_Weighted_13_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_13_12_3 [label="Attention_Output_13_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_13_12 [label="Attention_AllReduce_13_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_13_0 [label="QKV_Proj_13_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_13_13_0 [label="Attention_Scores_13_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_13_13_0 [label="Attention_Softmax_13_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_13_13_0 [label="Attention_Dropout_13_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_13_13_0 [label="Attention_Values_13_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_13_13_0 [label="Attention_Weighted_13_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_13_13_0 [label="Attention_Output_13_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_13_13 [label="Attention_AllReduce_13_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_13_1 [label="QKV_Proj_13_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_13_13_1 [label="Attention_Scores_13_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_13_13_1 [label="Attention_Softmax_13_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_13_13_1 [label="Attention_Dropout_13_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_13_13_1 [label="Attention_Values_13_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_13_13_1 [label="Attention_Weighted_13_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_13_13_1 [label="Attention_Output_13_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_13_13 [label="Attention_AllReduce_13_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_13_2 [label="QKV_Proj_13_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_13_13_2 [label="Attention_Scores_13_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_13_13_2 [label="Attention_Softmax_13_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_13_13_2 [label="Attention_Dropout_13_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_13_13_2 [label="Attention_Values_13_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_13_13_2 [label="Attention_Weighted_13_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_13_13_2 [label="Attention_Output_13_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_13_13 [label="Attention_AllReduce_13_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_13_3 [label="QKV_Proj_13_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_13_13_3 [label="Attention_Scores_13_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_13_13_3 [label="Attention_Softmax_13_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_13_13_3 [label="Attention_Dropout_13_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_13_13_3 [label="Attention_Values_13_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_13_13_3 [label="Attention_Weighted_13_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_13_13_3 [label="Attention_Output_13_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_13_13 [label="Attention_AllReduce_13_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_14_0 [label="QKV_Proj_13_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_13_14_0 [label="Attention_Scores_13_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_13_14_0 [label="Attention_Softmax_13_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_13_14_0 [label="Attention_Dropout_13_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_13_14_0 [label="Attention_Values_13_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_13_14_0 [label="Attention_Weighted_13_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_13_14_0 [label="Attention_Output_13_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_13_14 [label="Attention_AllReduce_13_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_14_1 [label="QKV_Proj_13_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_13_14_1 [label="Attention_Scores_13_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_13_14_1 [label="Attention_Softmax_13_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_13_14_1 [label="Attention_Dropout_13_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_13_14_1 [label="Attention_Values_13_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_13_14_1 [label="Attention_Weighted_13_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_13_14_1 [label="Attention_Output_13_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_13_14 [label="Attention_AllReduce_13_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_14_2 [label="QKV_Proj_13_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_13_14_2 [label="Attention_Scores_13_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_13_14_2 [label="Attention_Softmax_13_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_13_14_2 [label="Attention_Dropout_13_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_13_14_2 [label="Attention_Values_13_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_13_14_2 [label="Attention_Weighted_13_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_13_14_2 [label="Attention_Output_13_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_13_14 [label="Attention_AllReduce_13_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_14_3 [label="QKV_Proj_13_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_13_14_3 [label="Attention_Scores_13_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_13_14_3 [label="Attention_Softmax_13_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_13_14_3 [label="Attention_Dropout_13_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_13_14_3 [label="Attention_Values_13_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_13_14_3 [label="Attention_Weighted_13_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_13_14_3 [label="Attention_Output_13_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_13_14 [label="Attention_AllReduce_13_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_15_0 [label="QKV_Proj_13_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_13_15_0 [label="Attention_Scores_13_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_13_15_0 [label="Attention_Softmax_13_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_13_15_0 [label="Attention_Dropout_13_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_13_15_0 [label="Attention_Values_13_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_13_15_0 [label="Attention_Weighted_13_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_13_15_0 [label="Attention_Output_13_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_13_15 [label="Attention_AllReduce_13_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_15_1 [label="QKV_Proj_13_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_13_15_1 [label="Attention_Scores_13_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_13_15_1 [label="Attention_Softmax_13_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_13_15_1 [label="Attention_Dropout_13_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_13_15_1 [label="Attention_Values_13_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_13_15_1 [label="Attention_Weighted_13_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_13_15_1 [label="Attention_Output_13_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_13_15 [label="Attention_AllReduce_13_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_15_2 [label="QKV_Proj_13_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_13_15_2 [label="Attention_Scores_13_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_13_15_2 [label="Attention_Softmax_13_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_13_15_2 [label="Attention_Dropout_13_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_13_15_2 [label="Attention_Values_13_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_13_15_2 [label="Attention_Weighted_13_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_13_15_2 [label="Attention_Output_13_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_13_15 [label="Attention_AllReduce_13_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_13_15_3 [label="QKV_Proj_13_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_13_15_3 [label="Attention_Scores_13_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_13_15_3 [label="Attention_Softmax_13_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_13_15_3 [label="Attention_Dropout_13_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_13_15_3 [label="Attention_Values_13_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_13_15_3 [label="Attention_Weighted_13_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_13_15_3 [label="Attention_Output_13_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_13_15 [label="Attention_AllReduce_13_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_0_0 [label="MLP_Linear1_13_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_13_0_0 [label="GELU_13_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_13_0_0 [label="MLP_Linear2_13_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_13_0_1 [label="MLP_Linear1_13_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_13_0_1 [label="GELU_13_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_13_0_1 [label="MLP_Linear2_13_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_13_0_2 [label="MLP_Linear1_13_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_13_0_2 [label="GELU_13_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_13_0_2 [label="MLP_Linear2_13_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_13_0_3 [label="MLP_Linear1_13_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_13_0_3 [label="GELU_13_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_13_0_3 [label="MLP_Linear2_13_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_13_0 [label="MLP_AllReduce_13_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_1_0 [label="MLP_Linear1_13_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_13_1_0 [label="GELU_13_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_13_1_0 [label="MLP_Linear2_13_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_13_1_1 [label="MLP_Linear1_13_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_13_1_1 [label="GELU_13_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_13_1_1 [label="MLP_Linear2_13_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_13_1_2 [label="MLP_Linear1_13_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_13_1_2 [label="GELU_13_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_13_1_2 [label="MLP_Linear2_13_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_13_1_3 [label="MLP_Linear1_13_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_13_1_3 [label="GELU_13_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_13_1_3 [label="MLP_Linear2_13_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_13_1 [label="MLP_AllReduce_13_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_2_0 [label="MLP_Linear1_13_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_13_2_0 [label="GELU_13_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_13_2_0 [label="MLP_Linear2_13_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_13_2_1 [label="MLP_Linear1_13_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_13_2_1 [label="GELU_13_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_13_2_1 [label="MLP_Linear2_13_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_13_2_2 [label="MLP_Linear1_13_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_13_2_2 [label="GELU_13_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_13_2_2 [label="MLP_Linear2_13_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_13_2_3 [label="MLP_Linear1_13_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_13_2_3 [label="GELU_13_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_13_2_3 [label="MLP_Linear2_13_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_13_2 [label="MLP_AllReduce_13_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_3_0 [label="MLP_Linear1_13_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_13_3_0 [label="GELU_13_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_13_3_0 [label="MLP_Linear2_13_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_13_3_1 [label="MLP_Linear1_13_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_13_3_1 [label="GELU_13_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_13_3_1 [label="MLP_Linear2_13_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_13_3_2 [label="MLP_Linear1_13_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_13_3_2 [label="GELU_13_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_13_3_2 [label="MLP_Linear2_13_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_13_3_3 [label="MLP_Linear1_13_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_13_3_3 [label="GELU_13_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_13_3_3 [label="MLP_Linear2_13_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_13_3 [label="MLP_AllReduce_13_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_4_0 [label="MLP_Linear1_13_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_13_4_0 [label="GELU_13_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_13_4_0 [label="MLP_Linear2_13_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_13_4_1 [label="MLP_Linear1_13_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_13_4_1 [label="GELU_13_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_13_4_1 [label="MLP_Linear2_13_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_13_4_2 [label="MLP_Linear1_13_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_13_4_2 [label="GELU_13_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_13_4_2 [label="MLP_Linear2_13_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_13_4_3 [label="MLP_Linear1_13_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_13_4_3 [label="GELU_13_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_13_4_3 [label="MLP_Linear2_13_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_13_4 [label="MLP_AllReduce_13_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_5_0 [label="MLP_Linear1_13_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_13_5_0 [label="GELU_13_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_13_5_0 [label="MLP_Linear2_13_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_13_5_1 [label="MLP_Linear1_13_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_13_5_1 [label="GELU_13_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_13_5_1 [label="MLP_Linear2_13_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_13_5_2 [label="MLP_Linear1_13_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_13_5_2 [label="GELU_13_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_13_5_2 [label="MLP_Linear2_13_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_13_5_3 [label="MLP_Linear1_13_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_13_5_3 [label="GELU_13_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_13_5_3 [label="MLP_Linear2_13_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_13_5 [label="MLP_AllReduce_13_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_6_0 [label="MLP_Linear1_13_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_13_6_0 [label="GELU_13_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_13_6_0 [label="MLP_Linear2_13_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_13_6_1 [label="MLP_Linear1_13_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_13_6_1 [label="GELU_13_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_13_6_1 [label="MLP_Linear2_13_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_13_6_2 [label="MLP_Linear1_13_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_13_6_2 [label="GELU_13_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_13_6_2 [label="MLP_Linear2_13_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_13_6_3 [label="MLP_Linear1_13_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_13_6_3 [label="GELU_13_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_13_6_3 [label="MLP_Linear2_13_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_13_6 [label="MLP_AllReduce_13_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_7_0 [label="MLP_Linear1_13_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_13_7_0 [label="GELU_13_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_13_7_0 [label="MLP_Linear2_13_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_13_7_1 [label="MLP_Linear1_13_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_13_7_1 [label="GELU_13_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_13_7_1 [label="MLP_Linear2_13_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_13_7_2 [label="MLP_Linear1_13_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_13_7_2 [label="GELU_13_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_13_7_2 [label="MLP_Linear2_13_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_13_7_3 [label="MLP_Linear1_13_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_13_7_3 [label="GELU_13_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_13_7_3 [label="MLP_Linear2_13_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_13_7 [label="MLP_AllReduce_13_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_8_0 [label="MLP_Linear1_13_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_13_8_0 [label="GELU_13_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_13_8_0 [label="MLP_Linear2_13_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_13_8_1 [label="MLP_Linear1_13_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_13_8_1 [label="GELU_13_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_13_8_1 [label="MLP_Linear2_13_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_13_8_2 [label="MLP_Linear1_13_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_13_8_2 [label="GELU_13_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_13_8_2 [label="MLP_Linear2_13_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_13_8_3 [label="MLP_Linear1_13_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_13_8_3 [label="GELU_13_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_13_8_3 [label="MLP_Linear2_13_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_13_8 [label="MLP_AllReduce_13_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_9_0 [label="MLP_Linear1_13_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_13_9_0 [label="GELU_13_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_13_9_0 [label="MLP_Linear2_13_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_13_9_1 [label="MLP_Linear1_13_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_13_9_1 [label="GELU_13_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_13_9_1 [label="MLP_Linear2_13_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_13_9_2 [label="MLP_Linear1_13_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_13_9_2 [label="GELU_13_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_13_9_2 [label="MLP_Linear2_13_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_13_9_3 [label="MLP_Linear1_13_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_13_9_3 [label="GELU_13_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_13_9_3 [label="MLP_Linear2_13_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_13_9 [label="MLP_AllReduce_13_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_10_0 [label="MLP_Linear1_13_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_13_10_0 [label="GELU_13_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_13_10_0 [label="MLP_Linear2_13_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_13_10_1 [label="MLP_Linear1_13_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_13_10_1 [label="GELU_13_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_13_10_1 [label="MLP_Linear2_13_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_13_10_2 [label="MLP_Linear1_13_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_13_10_2 [label="GELU_13_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_13_10_2 [label="MLP_Linear2_13_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_13_10_3 [label="MLP_Linear1_13_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_13_10_3 [label="GELU_13_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_13_10_3 [label="MLP_Linear2_13_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_13_10 [label="MLP_AllReduce_13_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_11_0 [label="MLP_Linear1_13_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_13_11_0 [label="GELU_13_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_13_11_0 [label="MLP_Linear2_13_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_13_11_1 [label="MLP_Linear1_13_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_13_11_1 [label="GELU_13_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_13_11_1 [label="MLP_Linear2_13_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_13_11_2 [label="MLP_Linear1_13_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_13_11_2 [label="GELU_13_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_13_11_2 [label="MLP_Linear2_13_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_13_11_3 [label="MLP_Linear1_13_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_13_11_3 [label="GELU_13_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_13_11_3 [label="MLP_Linear2_13_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_13_11 [label="MLP_AllReduce_13_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_12_0 [label="MLP_Linear1_13_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_13_12_0 [label="GELU_13_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_13_12_0 [label="MLP_Linear2_13_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_13_12_1 [label="MLP_Linear1_13_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_13_12_1 [label="GELU_13_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_13_12_1 [label="MLP_Linear2_13_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_13_12_2 [label="MLP_Linear1_13_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_13_12_2 [label="GELU_13_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_13_12_2 [label="MLP_Linear2_13_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_13_12_3 [label="MLP_Linear1_13_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_13_12_3 [label="GELU_13_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_13_12_3 [label="MLP_Linear2_13_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_13_12 [label="MLP_AllReduce_13_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_13_0 [label="MLP_Linear1_13_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_13_13_0 [label="GELU_13_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_13_13_0 [label="MLP_Linear2_13_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_13_13_1 [label="MLP_Linear1_13_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_13_13_1 [label="GELU_13_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_13_13_1 [label="MLP_Linear2_13_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_13_13_2 [label="MLP_Linear1_13_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_13_13_2 [label="GELU_13_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_13_13_2 [label="MLP_Linear2_13_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_13_13_3 [label="MLP_Linear1_13_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_13_13_3 [label="GELU_13_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_13_13_3 [label="MLP_Linear2_13_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_13_13 [label="MLP_AllReduce_13_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_14_0 [label="MLP_Linear1_13_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_13_14_0 [label="GELU_13_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_13_14_0 [label="MLP_Linear2_13_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_13_14_1 [label="MLP_Linear1_13_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_13_14_1 [label="GELU_13_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_13_14_1 [label="MLP_Linear2_13_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_13_14_2 [label="MLP_Linear1_13_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_13_14_2 [label="GELU_13_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_13_14_2 [label="MLP_Linear2_13_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_13_14_3 [label="MLP_Linear1_13_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_13_14_3 [label="GELU_13_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_13_14_3 [label="MLP_Linear2_13_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_13_14 [label="MLP_AllReduce_13_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_13_15_0 [label="MLP_Linear1_13_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_13_15_0 [label="GELU_13_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_13_15_0 [label="MLP_Linear2_13_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_13_15_1 [label="MLP_Linear1_13_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_13_15_1 [label="GELU_13_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_13_15_1 [label="MLP_Linear2_13_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_13_15_2 [label="MLP_Linear1_13_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_13_15_2 [label="GELU_13_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_13_15_2 [label="MLP_Linear2_13_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_13_15_3 [label="MLP_Linear1_13_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_13_15_3 [label="GELU_13_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_13_15_3 [label="MLP_Linear2_13_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_13_15 [label="MLP_AllReduce_13_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_13_0 [label="Expert_Route_13_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_1 [label="Expert_Route_13_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_2 [label="Expert_Route_13_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_3 [label="Expert_Route_13_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_4 [label="Expert_Route_13_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_5 [label="Expert_Route_13_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_6 [label="Expert_Route_13_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_7 [label="Expert_Route_13_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_8 [label="Expert_Route_13_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_9 [label="Expert_Route_13_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_10 [label="Expert_Route_13_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_11 [label="Expert_Route_13_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_12 [label="Expert_Route_13_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_13 [label="Expert_Route_13_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_14 [label="Expert_Route_13_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_13_15 [label="Expert_Route_13_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_14_0_0 [label="QKV_Proj_14_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_14_0_0 [label="Attention_Scores_14_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_14_0_0 [label="Attention_Softmax_14_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_14_0_0 [label="Attention_Dropout_14_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_14_0_0 [label="Attention_Values_14_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_14_0_0 [label="Attention_Weighted_14_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_14_0_0 [label="Attention_Output_14_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_14_0 [label="Attention_AllReduce_14_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_0_1 [label="QKV_Proj_14_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_14_0_1 [label="Attention_Scores_14_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_14_0_1 [label="Attention_Softmax_14_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_14_0_1 [label="Attention_Dropout_14_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_14_0_1 [label="Attention_Values_14_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_14_0_1 [label="Attention_Weighted_14_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_14_0_1 [label="Attention_Output_14_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_14_0 [label="Attention_AllReduce_14_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_0_2 [label="QKV_Proj_14_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_14_0_2 [label="Attention_Scores_14_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_14_0_2 [label="Attention_Softmax_14_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_14_0_2 [label="Attention_Dropout_14_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_14_0_2 [label="Attention_Values_14_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_14_0_2 [label="Attention_Weighted_14_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_14_0_2 [label="Attention_Output_14_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_14_0 [label="Attention_AllReduce_14_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_0_3 [label="QKV_Proj_14_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_14_0_3 [label="Attention_Scores_14_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_14_0_3 [label="Attention_Softmax_14_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_14_0_3 [label="Attention_Dropout_14_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_14_0_3 [label="Attention_Values_14_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_14_0_3 [label="Attention_Weighted_14_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_14_0_3 [label="Attention_Output_14_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_14_0 [label="Attention_AllReduce_14_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_1_0 [label="QKV_Proj_14_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_14_1_0 [label="Attention_Scores_14_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_14_1_0 [label="Attention_Softmax_14_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_14_1_0 [label="Attention_Dropout_14_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_14_1_0 [label="Attention_Values_14_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_14_1_0 [label="Attention_Weighted_14_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_14_1_0 [label="Attention_Output_14_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_14_1 [label="Attention_AllReduce_14_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_1_1 [label="QKV_Proj_14_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_14_1_1 [label="Attention_Scores_14_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_14_1_1 [label="Attention_Softmax_14_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_14_1_1 [label="Attention_Dropout_14_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_14_1_1 [label="Attention_Values_14_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_14_1_1 [label="Attention_Weighted_14_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_14_1_1 [label="Attention_Output_14_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_14_1 [label="Attention_AllReduce_14_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_1_2 [label="QKV_Proj_14_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_14_1_2 [label="Attention_Scores_14_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_14_1_2 [label="Attention_Softmax_14_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_14_1_2 [label="Attention_Dropout_14_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_14_1_2 [label="Attention_Values_14_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_14_1_2 [label="Attention_Weighted_14_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_14_1_2 [label="Attention_Output_14_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_14_1 [label="Attention_AllReduce_14_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_1_3 [label="QKV_Proj_14_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_14_1_3 [label="Attention_Scores_14_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_14_1_3 [label="Attention_Softmax_14_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_14_1_3 [label="Attention_Dropout_14_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_14_1_3 [label="Attention_Values_14_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_14_1_3 [label="Attention_Weighted_14_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_14_1_3 [label="Attention_Output_14_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_14_1 [label="Attention_AllReduce_14_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_2_0 [label="QKV_Proj_14_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_14_2_0 [label="Attention_Scores_14_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_14_2_0 [label="Attention_Softmax_14_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_14_2_0 [label="Attention_Dropout_14_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_14_2_0 [label="Attention_Values_14_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_14_2_0 [label="Attention_Weighted_14_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_14_2_0 [label="Attention_Output_14_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_14_2 [label="Attention_AllReduce_14_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_2_1 [label="QKV_Proj_14_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_14_2_1 [label="Attention_Scores_14_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_14_2_1 [label="Attention_Softmax_14_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_14_2_1 [label="Attention_Dropout_14_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_14_2_1 [label="Attention_Values_14_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_14_2_1 [label="Attention_Weighted_14_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_14_2_1 [label="Attention_Output_14_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_14_2 [label="Attention_AllReduce_14_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_2_2 [label="QKV_Proj_14_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_14_2_2 [label="Attention_Scores_14_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_14_2_2 [label="Attention_Softmax_14_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_14_2_2 [label="Attention_Dropout_14_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_14_2_2 [label="Attention_Values_14_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_14_2_2 [label="Attention_Weighted_14_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_14_2_2 [label="Attention_Output_14_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_14_2 [label="Attention_AllReduce_14_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_2_3 [label="QKV_Proj_14_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_14_2_3 [label="Attention_Scores_14_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_14_2_3 [label="Attention_Softmax_14_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_14_2_3 [label="Attention_Dropout_14_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_14_2_3 [label="Attention_Values_14_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_14_2_3 [label="Attention_Weighted_14_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_14_2_3 [label="Attention_Output_14_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_14_2 [label="Attention_AllReduce_14_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_3_0 [label="QKV_Proj_14_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_14_3_0 [label="Attention_Scores_14_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_14_3_0 [label="Attention_Softmax_14_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_14_3_0 [label="Attention_Dropout_14_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_14_3_0 [label="Attention_Values_14_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_14_3_0 [label="Attention_Weighted_14_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_14_3_0 [label="Attention_Output_14_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_14_3 [label="Attention_AllReduce_14_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_3_1 [label="QKV_Proj_14_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_14_3_1 [label="Attention_Scores_14_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_14_3_1 [label="Attention_Softmax_14_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_14_3_1 [label="Attention_Dropout_14_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_14_3_1 [label="Attention_Values_14_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_14_3_1 [label="Attention_Weighted_14_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_14_3_1 [label="Attention_Output_14_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_14_3 [label="Attention_AllReduce_14_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_3_2 [label="QKV_Proj_14_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_14_3_2 [label="Attention_Scores_14_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_14_3_2 [label="Attention_Softmax_14_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_14_3_2 [label="Attention_Dropout_14_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_14_3_2 [label="Attention_Values_14_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_14_3_2 [label="Attention_Weighted_14_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_14_3_2 [label="Attention_Output_14_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_14_3 [label="Attention_AllReduce_14_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_3_3 [label="QKV_Proj_14_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_14_3_3 [label="Attention_Scores_14_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_14_3_3 [label="Attention_Softmax_14_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_14_3_3 [label="Attention_Dropout_14_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_14_3_3 [label="Attention_Values_14_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_14_3_3 [label="Attention_Weighted_14_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_14_3_3 [label="Attention_Output_14_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_14_3 [label="Attention_AllReduce_14_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_4_0 [label="QKV_Proj_14_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_14_4_0 [label="Attention_Scores_14_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_14_4_0 [label="Attention_Softmax_14_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_14_4_0 [label="Attention_Dropout_14_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_14_4_0 [label="Attention_Values_14_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_14_4_0 [label="Attention_Weighted_14_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_14_4_0 [label="Attention_Output_14_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_14_4 [label="Attention_AllReduce_14_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_4_1 [label="QKV_Proj_14_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_14_4_1 [label="Attention_Scores_14_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_14_4_1 [label="Attention_Softmax_14_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_14_4_1 [label="Attention_Dropout_14_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_14_4_1 [label="Attention_Values_14_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_14_4_1 [label="Attention_Weighted_14_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_14_4_1 [label="Attention_Output_14_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_14_4 [label="Attention_AllReduce_14_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_4_2 [label="QKV_Proj_14_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_14_4_2 [label="Attention_Scores_14_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_14_4_2 [label="Attention_Softmax_14_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_14_4_2 [label="Attention_Dropout_14_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_14_4_2 [label="Attention_Values_14_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_14_4_2 [label="Attention_Weighted_14_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_14_4_2 [label="Attention_Output_14_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_14_4 [label="Attention_AllReduce_14_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_4_3 [label="QKV_Proj_14_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_14_4_3 [label="Attention_Scores_14_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_14_4_3 [label="Attention_Softmax_14_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_14_4_3 [label="Attention_Dropout_14_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_14_4_3 [label="Attention_Values_14_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_14_4_3 [label="Attention_Weighted_14_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_14_4_3 [label="Attention_Output_14_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_14_4 [label="Attention_AllReduce_14_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_5_0 [label="QKV_Proj_14_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_14_5_0 [label="Attention_Scores_14_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_14_5_0 [label="Attention_Softmax_14_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_14_5_0 [label="Attention_Dropout_14_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_14_5_0 [label="Attention_Values_14_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_14_5_0 [label="Attention_Weighted_14_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_14_5_0 [label="Attention_Output_14_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_14_5 [label="Attention_AllReduce_14_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_5_1 [label="QKV_Proj_14_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_14_5_1 [label="Attention_Scores_14_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_14_5_1 [label="Attention_Softmax_14_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_14_5_1 [label="Attention_Dropout_14_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_14_5_1 [label="Attention_Values_14_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_14_5_1 [label="Attention_Weighted_14_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_14_5_1 [label="Attention_Output_14_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_14_5 [label="Attention_AllReduce_14_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_5_2 [label="QKV_Proj_14_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_14_5_2 [label="Attention_Scores_14_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_14_5_2 [label="Attention_Softmax_14_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_14_5_2 [label="Attention_Dropout_14_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_14_5_2 [label="Attention_Values_14_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_14_5_2 [label="Attention_Weighted_14_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_14_5_2 [label="Attention_Output_14_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_14_5 [label="Attention_AllReduce_14_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_5_3 [label="QKV_Proj_14_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_14_5_3 [label="Attention_Scores_14_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_14_5_3 [label="Attention_Softmax_14_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_14_5_3 [label="Attention_Dropout_14_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_14_5_3 [label="Attention_Values_14_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_14_5_3 [label="Attention_Weighted_14_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_14_5_3 [label="Attention_Output_14_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_14_5 [label="Attention_AllReduce_14_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_6_0 [label="QKV_Proj_14_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_14_6_0 [label="Attention_Scores_14_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_14_6_0 [label="Attention_Softmax_14_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_14_6_0 [label="Attention_Dropout_14_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_14_6_0 [label="Attention_Values_14_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_14_6_0 [label="Attention_Weighted_14_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_14_6_0 [label="Attention_Output_14_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_14_6 [label="Attention_AllReduce_14_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_6_1 [label="QKV_Proj_14_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_14_6_1 [label="Attention_Scores_14_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_14_6_1 [label="Attention_Softmax_14_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_14_6_1 [label="Attention_Dropout_14_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_14_6_1 [label="Attention_Values_14_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_14_6_1 [label="Attention_Weighted_14_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_14_6_1 [label="Attention_Output_14_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_14_6 [label="Attention_AllReduce_14_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_6_2 [label="QKV_Proj_14_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_14_6_2 [label="Attention_Scores_14_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_14_6_2 [label="Attention_Softmax_14_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_14_6_2 [label="Attention_Dropout_14_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_14_6_2 [label="Attention_Values_14_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_14_6_2 [label="Attention_Weighted_14_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_14_6_2 [label="Attention_Output_14_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_14_6 [label="Attention_AllReduce_14_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_6_3 [label="QKV_Proj_14_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_14_6_3 [label="Attention_Scores_14_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_14_6_3 [label="Attention_Softmax_14_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_14_6_3 [label="Attention_Dropout_14_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_14_6_3 [label="Attention_Values_14_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_14_6_3 [label="Attention_Weighted_14_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_14_6_3 [label="Attention_Output_14_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_14_6 [label="Attention_AllReduce_14_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_7_0 [label="QKV_Proj_14_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_14_7_0 [label="Attention_Scores_14_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_14_7_0 [label="Attention_Softmax_14_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_14_7_0 [label="Attention_Dropout_14_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_14_7_0 [label="Attention_Values_14_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_14_7_0 [label="Attention_Weighted_14_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_14_7_0 [label="Attention_Output_14_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_14_7 [label="Attention_AllReduce_14_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_7_1 [label="QKV_Proj_14_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_14_7_1 [label="Attention_Scores_14_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_14_7_1 [label="Attention_Softmax_14_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_14_7_1 [label="Attention_Dropout_14_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_14_7_1 [label="Attention_Values_14_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_14_7_1 [label="Attention_Weighted_14_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_14_7_1 [label="Attention_Output_14_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_14_7 [label="Attention_AllReduce_14_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_7_2 [label="QKV_Proj_14_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_14_7_2 [label="Attention_Scores_14_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_14_7_2 [label="Attention_Softmax_14_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_14_7_2 [label="Attention_Dropout_14_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_14_7_2 [label="Attention_Values_14_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_14_7_2 [label="Attention_Weighted_14_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_14_7_2 [label="Attention_Output_14_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_14_7 [label="Attention_AllReduce_14_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_7_3 [label="QKV_Proj_14_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_14_7_3 [label="Attention_Scores_14_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_14_7_3 [label="Attention_Softmax_14_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_14_7_3 [label="Attention_Dropout_14_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_14_7_3 [label="Attention_Values_14_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_14_7_3 [label="Attention_Weighted_14_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_14_7_3 [label="Attention_Output_14_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_14_7 [label="Attention_AllReduce_14_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_8_0 [label="QKV_Proj_14_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_14_8_0 [label="Attention_Scores_14_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_14_8_0 [label="Attention_Softmax_14_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_14_8_0 [label="Attention_Dropout_14_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_14_8_0 [label="Attention_Values_14_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_14_8_0 [label="Attention_Weighted_14_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_14_8_0 [label="Attention_Output_14_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_14_8 [label="Attention_AllReduce_14_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_8_1 [label="QKV_Proj_14_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_14_8_1 [label="Attention_Scores_14_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_14_8_1 [label="Attention_Softmax_14_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_14_8_1 [label="Attention_Dropout_14_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_14_8_1 [label="Attention_Values_14_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_14_8_1 [label="Attention_Weighted_14_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_14_8_1 [label="Attention_Output_14_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_14_8 [label="Attention_AllReduce_14_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_8_2 [label="QKV_Proj_14_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_14_8_2 [label="Attention_Scores_14_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_14_8_2 [label="Attention_Softmax_14_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_14_8_2 [label="Attention_Dropout_14_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_14_8_2 [label="Attention_Values_14_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_14_8_2 [label="Attention_Weighted_14_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_14_8_2 [label="Attention_Output_14_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_14_8 [label="Attention_AllReduce_14_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_8_3 [label="QKV_Proj_14_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_14_8_3 [label="Attention_Scores_14_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_14_8_3 [label="Attention_Softmax_14_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_14_8_3 [label="Attention_Dropout_14_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_14_8_3 [label="Attention_Values_14_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_14_8_3 [label="Attention_Weighted_14_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_14_8_3 [label="Attention_Output_14_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_14_8 [label="Attention_AllReduce_14_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_9_0 [label="QKV_Proj_14_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_14_9_0 [label="Attention_Scores_14_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_14_9_0 [label="Attention_Softmax_14_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_14_9_0 [label="Attention_Dropout_14_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_14_9_0 [label="Attention_Values_14_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_14_9_0 [label="Attention_Weighted_14_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_14_9_0 [label="Attention_Output_14_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_14_9 [label="Attention_AllReduce_14_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_9_1 [label="QKV_Proj_14_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_14_9_1 [label="Attention_Scores_14_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_14_9_1 [label="Attention_Softmax_14_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_14_9_1 [label="Attention_Dropout_14_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_14_9_1 [label="Attention_Values_14_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_14_9_1 [label="Attention_Weighted_14_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_14_9_1 [label="Attention_Output_14_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_14_9 [label="Attention_AllReduce_14_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_9_2 [label="QKV_Proj_14_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_14_9_2 [label="Attention_Scores_14_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_14_9_2 [label="Attention_Softmax_14_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_14_9_2 [label="Attention_Dropout_14_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_14_9_2 [label="Attention_Values_14_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_14_9_2 [label="Attention_Weighted_14_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_14_9_2 [label="Attention_Output_14_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_14_9 [label="Attention_AllReduce_14_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_9_3 [label="QKV_Proj_14_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_14_9_3 [label="Attention_Scores_14_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_14_9_3 [label="Attention_Softmax_14_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_14_9_3 [label="Attention_Dropout_14_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_14_9_3 [label="Attention_Values_14_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_14_9_3 [label="Attention_Weighted_14_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_14_9_3 [label="Attention_Output_14_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_14_9 [label="Attention_AllReduce_14_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_10_0 [label="QKV_Proj_14_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_14_10_0 [label="Attention_Scores_14_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_14_10_0 [label="Attention_Softmax_14_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_14_10_0 [label="Attention_Dropout_14_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_14_10_0 [label="Attention_Values_14_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_14_10_0 [label="Attention_Weighted_14_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_14_10_0 [label="Attention_Output_14_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_14_10 [label="Attention_AllReduce_14_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_10_1 [label="QKV_Proj_14_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_14_10_1 [label="Attention_Scores_14_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_14_10_1 [label="Attention_Softmax_14_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_14_10_1 [label="Attention_Dropout_14_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_14_10_1 [label="Attention_Values_14_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_14_10_1 [label="Attention_Weighted_14_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_14_10_1 [label="Attention_Output_14_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_14_10 [label="Attention_AllReduce_14_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_10_2 [label="QKV_Proj_14_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_14_10_2 [label="Attention_Scores_14_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_14_10_2 [label="Attention_Softmax_14_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_14_10_2 [label="Attention_Dropout_14_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_14_10_2 [label="Attention_Values_14_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_14_10_2 [label="Attention_Weighted_14_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_14_10_2 [label="Attention_Output_14_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_14_10 [label="Attention_AllReduce_14_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_10_3 [label="QKV_Proj_14_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_14_10_3 [label="Attention_Scores_14_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_14_10_3 [label="Attention_Softmax_14_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_14_10_3 [label="Attention_Dropout_14_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_14_10_3 [label="Attention_Values_14_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_14_10_3 [label="Attention_Weighted_14_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_14_10_3 [label="Attention_Output_14_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_14_10 [label="Attention_AllReduce_14_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_11_0 [label="QKV_Proj_14_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_14_11_0 [label="Attention_Scores_14_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_14_11_0 [label="Attention_Softmax_14_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_14_11_0 [label="Attention_Dropout_14_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_14_11_0 [label="Attention_Values_14_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_14_11_0 [label="Attention_Weighted_14_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_14_11_0 [label="Attention_Output_14_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_14_11 [label="Attention_AllReduce_14_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_11_1 [label="QKV_Proj_14_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_14_11_1 [label="Attention_Scores_14_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_14_11_1 [label="Attention_Softmax_14_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_14_11_1 [label="Attention_Dropout_14_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_14_11_1 [label="Attention_Values_14_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_14_11_1 [label="Attention_Weighted_14_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_14_11_1 [label="Attention_Output_14_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_14_11 [label="Attention_AllReduce_14_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_11_2 [label="QKV_Proj_14_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_14_11_2 [label="Attention_Scores_14_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_14_11_2 [label="Attention_Softmax_14_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_14_11_2 [label="Attention_Dropout_14_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_14_11_2 [label="Attention_Values_14_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_14_11_2 [label="Attention_Weighted_14_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_14_11_2 [label="Attention_Output_14_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_14_11 [label="Attention_AllReduce_14_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_11_3 [label="QKV_Proj_14_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_14_11_3 [label="Attention_Scores_14_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_14_11_3 [label="Attention_Softmax_14_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_14_11_3 [label="Attention_Dropout_14_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_14_11_3 [label="Attention_Values_14_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_14_11_3 [label="Attention_Weighted_14_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_14_11_3 [label="Attention_Output_14_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_14_11 [label="Attention_AllReduce_14_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_12_0 [label="QKV_Proj_14_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_14_12_0 [label="Attention_Scores_14_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_14_12_0 [label="Attention_Softmax_14_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_14_12_0 [label="Attention_Dropout_14_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_14_12_0 [label="Attention_Values_14_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_14_12_0 [label="Attention_Weighted_14_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_14_12_0 [label="Attention_Output_14_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_14_12 [label="Attention_AllReduce_14_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_12_1 [label="QKV_Proj_14_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_14_12_1 [label="Attention_Scores_14_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_14_12_1 [label="Attention_Softmax_14_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_14_12_1 [label="Attention_Dropout_14_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_14_12_1 [label="Attention_Values_14_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_14_12_1 [label="Attention_Weighted_14_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_14_12_1 [label="Attention_Output_14_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_14_12 [label="Attention_AllReduce_14_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_12_2 [label="QKV_Proj_14_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_14_12_2 [label="Attention_Scores_14_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_14_12_2 [label="Attention_Softmax_14_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_14_12_2 [label="Attention_Dropout_14_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_14_12_2 [label="Attention_Values_14_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_14_12_2 [label="Attention_Weighted_14_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_14_12_2 [label="Attention_Output_14_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_14_12 [label="Attention_AllReduce_14_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_12_3 [label="QKV_Proj_14_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_14_12_3 [label="Attention_Scores_14_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_14_12_3 [label="Attention_Softmax_14_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_14_12_3 [label="Attention_Dropout_14_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_14_12_3 [label="Attention_Values_14_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_14_12_3 [label="Attention_Weighted_14_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_14_12_3 [label="Attention_Output_14_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_14_12 [label="Attention_AllReduce_14_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_13_0 [label="QKV_Proj_14_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_14_13_0 [label="Attention_Scores_14_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_14_13_0 [label="Attention_Softmax_14_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_14_13_0 [label="Attention_Dropout_14_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_14_13_0 [label="Attention_Values_14_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_14_13_0 [label="Attention_Weighted_14_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_14_13_0 [label="Attention_Output_14_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_14_13 [label="Attention_AllReduce_14_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_13_1 [label="QKV_Proj_14_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_14_13_1 [label="Attention_Scores_14_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_14_13_1 [label="Attention_Softmax_14_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_14_13_1 [label="Attention_Dropout_14_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_14_13_1 [label="Attention_Values_14_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_14_13_1 [label="Attention_Weighted_14_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_14_13_1 [label="Attention_Output_14_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_14_13 [label="Attention_AllReduce_14_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_13_2 [label="QKV_Proj_14_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_14_13_2 [label="Attention_Scores_14_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_14_13_2 [label="Attention_Softmax_14_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_14_13_2 [label="Attention_Dropout_14_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_14_13_2 [label="Attention_Values_14_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_14_13_2 [label="Attention_Weighted_14_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_14_13_2 [label="Attention_Output_14_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_14_13 [label="Attention_AllReduce_14_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_13_3 [label="QKV_Proj_14_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_14_13_3 [label="Attention_Scores_14_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_14_13_3 [label="Attention_Softmax_14_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_14_13_3 [label="Attention_Dropout_14_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_14_13_3 [label="Attention_Values_14_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_14_13_3 [label="Attention_Weighted_14_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_14_13_3 [label="Attention_Output_14_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_14_13 [label="Attention_AllReduce_14_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_14_0 [label="QKV_Proj_14_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_14_14_0 [label="Attention_Scores_14_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_14_14_0 [label="Attention_Softmax_14_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_14_14_0 [label="Attention_Dropout_14_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_14_14_0 [label="Attention_Values_14_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_14_14_0 [label="Attention_Weighted_14_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_14_14_0 [label="Attention_Output_14_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_14_14 [label="Attention_AllReduce_14_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_14_1 [label="QKV_Proj_14_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_14_14_1 [label="Attention_Scores_14_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_14_14_1 [label="Attention_Softmax_14_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_14_14_1 [label="Attention_Dropout_14_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_14_14_1 [label="Attention_Values_14_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_14_14_1 [label="Attention_Weighted_14_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_14_14_1 [label="Attention_Output_14_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_14_14 [label="Attention_AllReduce_14_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_14_2 [label="QKV_Proj_14_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_14_14_2 [label="Attention_Scores_14_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_14_14_2 [label="Attention_Softmax_14_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_14_14_2 [label="Attention_Dropout_14_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_14_14_2 [label="Attention_Values_14_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_14_14_2 [label="Attention_Weighted_14_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_14_14_2 [label="Attention_Output_14_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_14_14 [label="Attention_AllReduce_14_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_14_3 [label="QKV_Proj_14_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_14_14_3 [label="Attention_Scores_14_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_14_14_3 [label="Attention_Softmax_14_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_14_14_3 [label="Attention_Dropout_14_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_14_14_3 [label="Attention_Values_14_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_14_14_3 [label="Attention_Weighted_14_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_14_14_3 [label="Attention_Output_14_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_14_14 [label="Attention_AllReduce_14_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_15_0 [label="QKV_Proj_14_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_14_15_0 [label="Attention_Scores_14_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_14_15_0 [label="Attention_Softmax_14_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_14_15_0 [label="Attention_Dropout_14_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_14_15_0 [label="Attention_Values_14_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_14_15_0 [label="Attention_Weighted_14_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_14_15_0 [label="Attention_Output_14_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_14_15 [label="Attention_AllReduce_14_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_15_1 [label="QKV_Proj_14_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_14_15_1 [label="Attention_Scores_14_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_14_15_1 [label="Attention_Softmax_14_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_14_15_1 [label="Attention_Dropout_14_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_14_15_1 [label="Attention_Values_14_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_14_15_1 [label="Attention_Weighted_14_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_14_15_1 [label="Attention_Output_14_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_14_15 [label="Attention_AllReduce_14_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_15_2 [label="QKV_Proj_14_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_14_15_2 [label="Attention_Scores_14_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_14_15_2 [label="Attention_Softmax_14_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_14_15_2 [label="Attention_Dropout_14_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_14_15_2 [label="Attention_Values_14_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_14_15_2 [label="Attention_Weighted_14_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_14_15_2 [label="Attention_Output_14_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_14_15 [label="Attention_AllReduce_14_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_14_15_3 [label="QKV_Proj_14_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_14_15_3 [label="Attention_Scores_14_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_14_15_3 [label="Attention_Softmax_14_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_14_15_3 [label="Attention_Dropout_14_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_14_15_3 [label="Attention_Values_14_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_14_15_3 [label="Attention_Weighted_14_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_14_15_3 [label="Attention_Output_14_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_14_15 [label="Attention_AllReduce_14_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_0_0 [label="MLP_Linear1_14_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_14_0_0 [label="GELU_14_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_14_0_0 [label="MLP_Linear2_14_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_14_0_1 [label="MLP_Linear1_14_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_14_0_1 [label="GELU_14_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_14_0_1 [label="MLP_Linear2_14_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_14_0_2 [label="MLP_Linear1_14_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_14_0_2 [label="GELU_14_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_14_0_2 [label="MLP_Linear2_14_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_14_0_3 [label="MLP_Linear1_14_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_14_0_3 [label="GELU_14_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_14_0_3 [label="MLP_Linear2_14_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_14_0 [label="MLP_AllReduce_14_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_1_0 [label="MLP_Linear1_14_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_14_1_0 [label="GELU_14_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_14_1_0 [label="MLP_Linear2_14_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_14_1_1 [label="MLP_Linear1_14_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_14_1_1 [label="GELU_14_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_14_1_1 [label="MLP_Linear2_14_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_14_1_2 [label="MLP_Linear1_14_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_14_1_2 [label="GELU_14_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_14_1_2 [label="MLP_Linear2_14_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_14_1_3 [label="MLP_Linear1_14_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_14_1_3 [label="GELU_14_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_14_1_3 [label="MLP_Linear2_14_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_14_1 [label="MLP_AllReduce_14_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_2_0 [label="MLP_Linear1_14_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_14_2_0 [label="GELU_14_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_14_2_0 [label="MLP_Linear2_14_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_14_2_1 [label="MLP_Linear1_14_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_14_2_1 [label="GELU_14_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_14_2_1 [label="MLP_Linear2_14_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_14_2_2 [label="MLP_Linear1_14_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_14_2_2 [label="GELU_14_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_14_2_2 [label="MLP_Linear2_14_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_14_2_3 [label="MLP_Linear1_14_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_14_2_3 [label="GELU_14_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_14_2_3 [label="MLP_Linear2_14_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_14_2 [label="MLP_AllReduce_14_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_3_0 [label="MLP_Linear1_14_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_14_3_0 [label="GELU_14_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_14_3_0 [label="MLP_Linear2_14_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_14_3_1 [label="MLP_Linear1_14_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_14_3_1 [label="GELU_14_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_14_3_1 [label="MLP_Linear2_14_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_14_3_2 [label="MLP_Linear1_14_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_14_3_2 [label="GELU_14_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_14_3_2 [label="MLP_Linear2_14_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_14_3_3 [label="MLP_Linear1_14_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_14_3_3 [label="GELU_14_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_14_3_3 [label="MLP_Linear2_14_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_14_3 [label="MLP_AllReduce_14_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_4_0 [label="MLP_Linear1_14_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_14_4_0 [label="GELU_14_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_14_4_0 [label="MLP_Linear2_14_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_14_4_1 [label="MLP_Linear1_14_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_14_4_1 [label="GELU_14_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_14_4_1 [label="MLP_Linear2_14_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_14_4_2 [label="MLP_Linear1_14_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_14_4_2 [label="GELU_14_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_14_4_2 [label="MLP_Linear2_14_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_14_4_3 [label="MLP_Linear1_14_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_14_4_3 [label="GELU_14_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_14_4_3 [label="MLP_Linear2_14_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_14_4 [label="MLP_AllReduce_14_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_5_0 [label="MLP_Linear1_14_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_14_5_0 [label="GELU_14_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_14_5_0 [label="MLP_Linear2_14_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_14_5_1 [label="MLP_Linear1_14_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_14_5_1 [label="GELU_14_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_14_5_1 [label="MLP_Linear2_14_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_14_5_2 [label="MLP_Linear1_14_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_14_5_2 [label="GELU_14_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_14_5_2 [label="MLP_Linear2_14_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_14_5_3 [label="MLP_Linear1_14_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_14_5_3 [label="GELU_14_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_14_5_3 [label="MLP_Linear2_14_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_14_5 [label="MLP_AllReduce_14_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_6_0 [label="MLP_Linear1_14_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_14_6_0 [label="GELU_14_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_14_6_0 [label="MLP_Linear2_14_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_14_6_1 [label="MLP_Linear1_14_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_14_6_1 [label="GELU_14_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_14_6_1 [label="MLP_Linear2_14_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_14_6_2 [label="MLP_Linear1_14_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_14_6_2 [label="GELU_14_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_14_6_2 [label="MLP_Linear2_14_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_14_6_3 [label="MLP_Linear1_14_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_14_6_3 [label="GELU_14_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_14_6_3 [label="MLP_Linear2_14_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_14_6 [label="MLP_AllReduce_14_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_7_0 [label="MLP_Linear1_14_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_14_7_0 [label="GELU_14_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_14_7_0 [label="MLP_Linear2_14_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_14_7_1 [label="MLP_Linear1_14_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_14_7_1 [label="GELU_14_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_14_7_1 [label="MLP_Linear2_14_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_14_7_2 [label="MLP_Linear1_14_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_14_7_2 [label="GELU_14_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_14_7_2 [label="MLP_Linear2_14_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_14_7_3 [label="MLP_Linear1_14_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_14_7_3 [label="GELU_14_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_14_7_3 [label="MLP_Linear2_14_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_14_7 [label="MLP_AllReduce_14_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_8_0 [label="MLP_Linear1_14_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_14_8_0 [label="GELU_14_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_14_8_0 [label="MLP_Linear2_14_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_14_8_1 [label="MLP_Linear1_14_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_14_8_1 [label="GELU_14_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_14_8_1 [label="MLP_Linear2_14_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_14_8_2 [label="MLP_Linear1_14_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_14_8_2 [label="GELU_14_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_14_8_2 [label="MLP_Linear2_14_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_14_8_3 [label="MLP_Linear1_14_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_14_8_3 [label="GELU_14_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_14_8_3 [label="MLP_Linear2_14_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_14_8 [label="MLP_AllReduce_14_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_9_0 [label="MLP_Linear1_14_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_14_9_0 [label="GELU_14_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_14_9_0 [label="MLP_Linear2_14_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_14_9_1 [label="MLP_Linear1_14_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_14_9_1 [label="GELU_14_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_14_9_1 [label="MLP_Linear2_14_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_14_9_2 [label="MLP_Linear1_14_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_14_9_2 [label="GELU_14_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_14_9_2 [label="MLP_Linear2_14_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_14_9_3 [label="MLP_Linear1_14_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_14_9_3 [label="GELU_14_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_14_9_3 [label="MLP_Linear2_14_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_14_9 [label="MLP_AllReduce_14_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_10_0 [label="MLP_Linear1_14_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_14_10_0 [label="GELU_14_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_14_10_0 [label="MLP_Linear2_14_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_14_10_1 [label="MLP_Linear1_14_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_14_10_1 [label="GELU_14_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_14_10_1 [label="MLP_Linear2_14_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_14_10_2 [label="MLP_Linear1_14_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_14_10_2 [label="GELU_14_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_14_10_2 [label="MLP_Linear2_14_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_14_10_3 [label="MLP_Linear1_14_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_14_10_3 [label="GELU_14_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_14_10_3 [label="MLP_Linear2_14_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_14_10 [label="MLP_AllReduce_14_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_11_0 [label="MLP_Linear1_14_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_14_11_0 [label="GELU_14_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_14_11_0 [label="MLP_Linear2_14_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_14_11_1 [label="MLP_Linear1_14_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_14_11_1 [label="GELU_14_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_14_11_1 [label="MLP_Linear2_14_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_14_11_2 [label="MLP_Linear1_14_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_14_11_2 [label="GELU_14_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_14_11_2 [label="MLP_Linear2_14_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_14_11_3 [label="MLP_Linear1_14_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_14_11_3 [label="GELU_14_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_14_11_3 [label="MLP_Linear2_14_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_14_11 [label="MLP_AllReduce_14_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_12_0 [label="MLP_Linear1_14_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_14_12_0 [label="GELU_14_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_14_12_0 [label="MLP_Linear2_14_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_14_12_1 [label="MLP_Linear1_14_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_14_12_1 [label="GELU_14_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_14_12_1 [label="MLP_Linear2_14_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_14_12_2 [label="MLP_Linear1_14_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_14_12_2 [label="GELU_14_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_14_12_2 [label="MLP_Linear2_14_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_14_12_3 [label="MLP_Linear1_14_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_14_12_3 [label="GELU_14_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_14_12_3 [label="MLP_Linear2_14_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_14_12 [label="MLP_AllReduce_14_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_13_0 [label="MLP_Linear1_14_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_14_13_0 [label="GELU_14_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_14_13_0 [label="MLP_Linear2_14_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_14_13_1 [label="MLP_Linear1_14_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_14_13_1 [label="GELU_14_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_14_13_1 [label="MLP_Linear2_14_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_14_13_2 [label="MLP_Linear1_14_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_14_13_2 [label="GELU_14_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_14_13_2 [label="MLP_Linear2_14_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_14_13_3 [label="MLP_Linear1_14_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_14_13_3 [label="GELU_14_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_14_13_3 [label="MLP_Linear2_14_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_14_13 [label="MLP_AllReduce_14_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_14_0 [label="MLP_Linear1_14_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_14_14_0 [label="GELU_14_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_14_14_0 [label="MLP_Linear2_14_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_14_14_1 [label="MLP_Linear1_14_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_14_14_1 [label="GELU_14_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_14_14_1 [label="MLP_Linear2_14_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_14_14_2 [label="MLP_Linear1_14_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_14_14_2 [label="GELU_14_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_14_14_2 [label="MLP_Linear2_14_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_14_14_3 [label="MLP_Linear1_14_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_14_14_3 [label="GELU_14_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_14_14_3 [label="MLP_Linear2_14_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_14_14 [label="MLP_AllReduce_14_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_14_15_0 [label="MLP_Linear1_14_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_14_15_0 [label="GELU_14_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_14_15_0 [label="MLP_Linear2_14_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_14_15_1 [label="MLP_Linear1_14_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_14_15_1 [label="GELU_14_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_14_15_1 [label="MLP_Linear2_14_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_14_15_2 [label="MLP_Linear1_14_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_14_15_2 [label="GELU_14_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_14_15_2 [label="MLP_Linear2_14_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_14_15_3 [label="MLP_Linear1_14_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_14_15_3 [label="GELU_14_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_14_15_3 [label="MLP_Linear2_14_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_14_15 [label="MLP_AllReduce_14_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_14_0 [label="Expert_Route_14_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_1 [label="Expert_Route_14_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_2 [label="Expert_Route_14_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_3 [label="Expert_Route_14_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_4 [label="Expert_Route_14_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_5 [label="Expert_Route_14_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_6 [label="Expert_Route_14_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_7 [label="Expert_Route_14_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_8 [label="Expert_Route_14_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_9 [label="Expert_Route_14_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_10 [label="Expert_Route_14_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_11 [label="Expert_Route_14_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_12 [label="Expert_Route_14_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_13 [label="Expert_Route_14_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_14 [label="Expert_Route_14_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_14_15 [label="Expert_Route_14_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	qkv_15_0_0 [label="QKV_Proj_15_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_15_0_0 [label="Attention_Scores_15_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_15_0_0 [label="Attention_Softmax_15_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_15_0_0 [label="Attention_Dropout_15_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_15_0_0 [label="Attention_Values_15_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_15_0_0 [label="Attention_Weighted_15_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_15_0_0 [label="Attention_Output_15_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_15_0 [label="Attention_AllReduce_15_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_0_1 [label="QKV_Proj_15_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_15_0_1 [label="Attention_Scores_15_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_15_0_1 [label="Attention_Softmax_15_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_15_0_1 [label="Attention_Dropout_15_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_15_0_1 [label="Attention_Values_15_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_15_0_1 [label="Attention_Weighted_15_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_15_0_1 [label="Attention_Output_15_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_15_0 [label="Attention_AllReduce_15_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_0_2 [label="QKV_Proj_15_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_15_0_2 [label="Attention_Scores_15_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_15_0_2 [label="Attention_Softmax_15_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_15_0_2 [label="Attention_Dropout_15_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_15_0_2 [label="Attention_Values_15_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_15_0_2 [label="Attention_Weighted_15_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_15_0_2 [label="Attention_Output_15_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_15_0 [label="Attention_AllReduce_15_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_0_3 [label="QKV_Proj_15_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_scores_15_0_3 [label="Attention_Scores_15_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_softmax_15_0_3 [label="Attention_Softmax_15_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_dropout_15_0_3 [label="Attention_Dropout_15_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcoral]
	attn_values_15_0_3 [label="Attention_Values_15_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_weighted_15_0_3 [label="Attention_Weighted_15_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcoral]
	attn_out_15_0_3 [label="Attention_Output_15_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	attn_allreduce_15_0 [label="Attention_AllReduce_15_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_1_0 [label="QKV_Proj_15_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_15_1_0 [label="Attention_Scores_15_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_15_1_0 [label="Attention_Softmax_15_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_15_1_0 [label="Attention_Dropout_15_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_15_1_0 [label="Attention_Values_15_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_15_1_0 [label="Attention_Weighted_15_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_15_1_0 [label="Attention_Output_15_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_15_1 [label="Attention_AllReduce_15_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_1_1 [label="QKV_Proj_15_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_15_1_1 [label="Attention_Scores_15_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_15_1_1 [label="Attention_Softmax_15_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_15_1_1 [label="Attention_Dropout_15_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_15_1_1 [label="Attention_Values_15_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_15_1_1 [label="Attention_Weighted_15_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_15_1_1 [label="Attention_Output_15_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_15_1 [label="Attention_AllReduce_15_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_1_2 [label="QKV_Proj_15_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_15_1_2 [label="Attention_Scores_15_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_15_1_2 [label="Attention_Softmax_15_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_15_1_2 [label="Attention_Dropout_15_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_15_1_2 [label="Attention_Values_15_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_15_1_2 [label="Attention_Weighted_15_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_15_1_2 [label="Attention_Output_15_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_15_1 [label="Attention_AllReduce_15_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_1_3 [label="QKV_Proj_15_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_scores_15_1_3 [label="Attention_Scores_15_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_softmax_15_1_3 [label="Attention_Softmax_15_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_dropout_15_1_3 [label="Attention_Dropout_15_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgreen]
	attn_values_15_1_3 [label="Attention_Values_15_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_weighted_15_1_3 [label="Attention_Weighted_15_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgreen]
	attn_out_15_1_3 [label="Attention_Output_15_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	attn_allreduce_15_1 [label="Attention_AllReduce_15_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_2_0 [label="QKV_Proj_15_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_15_2_0 [label="Attention_Scores_15_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_15_2_0 [label="Attention_Softmax_15_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_15_2_0 [label="Attention_Dropout_15_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_15_2_0 [label="Attention_Values_15_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_15_2_0 [label="Attention_Weighted_15_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_15_2_0 [label="Attention_Output_15_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_15_2 [label="Attention_AllReduce_15_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_2_1 [label="QKV_Proj_15_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_15_2_1 [label="Attention_Scores_15_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_15_2_1 [label="Attention_Softmax_15_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_15_2_1 [label="Attention_Dropout_15_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_15_2_1 [label="Attention_Values_15_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_15_2_1 [label="Attention_Weighted_15_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_15_2_1 [label="Attention_Output_15_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_15_2 [label="Attention_AllReduce_15_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_2_2 [label="QKV_Proj_15_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_15_2_2 [label="Attention_Scores_15_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_15_2_2 [label="Attention_Softmax_15_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_15_2_2 [label="Attention_Dropout_15_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_15_2_2 [label="Attention_Values_15_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_15_2_2 [label="Attention_Weighted_15_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_15_2_2 [label="Attention_Output_15_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_15_2 [label="Attention_AllReduce_15_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_2_3 [label="QKV_Proj_15_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_scores_15_2_3 [label="Attention_Scores_15_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_softmax_15_2_3 [label="Attention_Softmax_15_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_dropout_15_2_3 [label="Attention_Dropout_15_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightblue]
	attn_values_15_2_3 [label="Attention_Values_15_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_weighted_15_2_3 [label="Attention_Weighted_15_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightblue]
	attn_out_15_2_3 [label="Attention_Output_15_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	attn_allreduce_15_2 [label="Attention_AllReduce_15_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_3_0 [label="QKV_Proj_15_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_15_3_0 [label="Attention_Scores_15_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_15_3_0 [label="Attention_Softmax_15_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_15_3_0 [label="Attention_Dropout_15_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_15_3_0 [label="Attention_Values_15_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_15_3_0 [label="Attention_Weighted_15_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_15_3_0 [label="Attention_Output_15_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_15_3 [label="Attention_AllReduce_15_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_3_1 [label="QKV_Proj_15_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_15_3_1 [label="Attention_Scores_15_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_15_3_1 [label="Attention_Softmax_15_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_15_3_1 [label="Attention_Dropout_15_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_15_3_1 [label="Attention_Values_15_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_15_3_1 [label="Attention_Weighted_15_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_15_3_1 [label="Attention_Output_15_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_15_3 [label="Attention_AllReduce_15_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_3_2 [label="QKV_Proj_15_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_15_3_2 [label="Attention_Scores_15_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_15_3_2 [label="Attention_Softmax_15_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_15_3_2 [label="Attention_Dropout_15_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_15_3_2 [label="Attention_Values_15_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_15_3_2 [label="Attention_Weighted_15_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_15_3_2 [label="Attention_Output_15_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_15_3 [label="Attention_AllReduce_15_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_3_3 [label="QKV_Proj_15_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_scores_15_3_3 [label="Attention_Scores_15_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_softmax_15_3_3 [label="Attention_Softmax_15_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_dropout_15_3_3 [label="Attention_Dropout_15_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightyellow]
	attn_values_15_3_3 [label="Attention_Values_15_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_weighted_15_3_3 [label="Attention_Weighted_15_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightyellow]
	attn_out_15_3_3 [label="Attention_Output_15_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	attn_allreduce_15_3 [label="Attention_AllReduce_15_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_4_0 [label="QKV_Proj_15_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_15_4_0 [label="Attention_Scores_15_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_15_4_0 [label="Attention_Softmax_15_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_15_4_0 [label="Attention_Dropout_15_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_15_4_0 [label="Attention_Values_15_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_15_4_0 [label="Attention_Weighted_15_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_15_4_0 [label="Attention_Output_15_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_15_4 [label="Attention_AllReduce_15_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_4_1 [label="QKV_Proj_15_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_15_4_1 [label="Attention_Scores_15_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_15_4_1 [label="Attention_Softmax_15_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_15_4_1 [label="Attention_Dropout_15_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_15_4_1 [label="Attention_Values_15_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_15_4_1 [label="Attention_Weighted_15_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_15_4_1 [label="Attention_Output_15_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_15_4 [label="Attention_AllReduce_15_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_4_2 [label="QKV_Proj_15_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_15_4_2 [label="Attention_Scores_15_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_15_4_2 [label="Attention_Softmax_15_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_15_4_2 [label="Attention_Dropout_15_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_15_4_2 [label="Attention_Values_15_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_15_4_2 [label="Attention_Weighted_15_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_15_4_2 [label="Attention_Output_15_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_15_4 [label="Attention_AllReduce_15_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_4_3 [label="QKV_Proj_15_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_scores_15_4_3 [label="Attention_Scores_15_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_softmax_15_4_3 [label="Attention_Softmax_15_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_dropout_15_4_3 [label="Attention_Dropout_15_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpink]
	attn_values_15_4_3 [label="Attention_Values_15_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_weighted_15_4_3 [label="Attention_Weighted_15_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpink]
	attn_out_15_4_3 [label="Attention_Output_15_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	attn_allreduce_15_4 [label="Attention_AllReduce_15_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_5_0 [label="QKV_Proj_15_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_15_5_0 [label="Attention_Scores_15_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_15_5_0 [label="Attention_Softmax_15_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_15_5_0 [label="Attention_Dropout_15_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_15_5_0 [label="Attention_Values_15_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_15_5_0 [label="Attention_Weighted_15_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_15_5_0 [label="Attention_Output_15_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_15_5 [label="Attention_AllReduce_15_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_5_1 [label="QKV_Proj_15_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_15_5_1 [label="Attention_Scores_15_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_15_5_1 [label="Attention_Softmax_15_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_15_5_1 [label="Attention_Dropout_15_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_15_5_1 [label="Attention_Values_15_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_15_5_1 [label="Attention_Weighted_15_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_15_5_1 [label="Attention_Output_15_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_15_5 [label="Attention_AllReduce_15_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_5_2 [label="QKV_Proj_15_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_15_5_2 [label="Attention_Scores_15_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_15_5_2 [label="Attention_Softmax_15_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_15_5_2 [label="Attention_Dropout_15_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_15_5_2 [label="Attention_Values_15_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_15_5_2 [label="Attention_Weighted_15_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_15_5_2 [label="Attention_Output_15_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_15_5 [label="Attention_AllReduce_15_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_5_3 [label="QKV_Proj_15_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_scores_15_5_3 [label="Attention_Scores_15_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_softmax_15_5_3 [label="Attention_Softmax_15_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_dropout_15_5_3 [label="Attention_Dropout_15_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgray]
	attn_values_15_5_3 [label="Attention_Values_15_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_weighted_15_5_3 [label="Attention_Weighted_15_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgray]
	attn_out_15_5_3 [label="Attention_Output_15_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	attn_allreduce_15_5 [label="Attention_AllReduce_15_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_6_0 [label="QKV_Proj_15_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_15_6_0 [label="Attention_Scores_15_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_15_6_0 [label="Attention_Softmax_15_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_15_6_0 [label="Attention_Dropout_15_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_15_6_0 [label="Attention_Values_15_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_15_6_0 [label="Attention_Weighted_15_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_15_6_0 [label="Attention_Output_15_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_15_6 [label="Attention_AllReduce_15_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_6_1 [label="QKV_Proj_15_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_15_6_1 [label="Attention_Scores_15_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_15_6_1 [label="Attention_Softmax_15_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_15_6_1 [label="Attention_Dropout_15_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_15_6_1 [label="Attention_Values_15_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_15_6_1 [label="Attention_Weighted_15_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_15_6_1 [label="Attention_Output_15_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_15_6 [label="Attention_AllReduce_15_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_6_2 [label="QKV_Proj_15_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_15_6_2 [label="Attention_Scores_15_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_15_6_2 [label="Attention_Softmax_15_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_15_6_2 [label="Attention_Dropout_15_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_15_6_2 [label="Attention_Values_15_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_15_6_2 [label="Attention_Weighted_15_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_15_6_2 [label="Attention_Output_15_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_15_6 [label="Attention_AllReduce_15_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_6_3 [label="QKV_Proj_15_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_scores_15_6_3 [label="Attention_Scores_15_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_softmax_15_6_3 [label="Attention_Softmax_15_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_dropout_15_6_3 [label="Attention_Dropout_15_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsalmon]
	attn_values_15_6_3 [label="Attention_Values_15_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_weighted_15_6_3 [label="Attention_Weighted_15_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsalmon]
	attn_out_15_6_3 [label="Attention_Output_15_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	attn_allreduce_15_6 [label="Attention_AllReduce_15_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_7_0 [label="QKV_Proj_15_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_15_7_0 [label="Attention_Scores_15_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_15_7_0 [label="Attention_Softmax_15_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_15_7_0 [label="Attention_Dropout_15_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_15_7_0 [label="Attention_Values_15_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_15_7_0 [label="Attention_Weighted_15_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_15_7_0 [label="Attention_Output_15_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_15_7 [label="Attention_AllReduce_15_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_7_1 [label="QKV_Proj_15_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_15_7_1 [label="Attention_Scores_15_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_15_7_1 [label="Attention_Softmax_15_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_15_7_1 [label="Attention_Dropout_15_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_15_7_1 [label="Attention_Values_15_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_15_7_1 [label="Attention_Weighted_15_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_15_7_1 [label="Attention_Output_15_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_15_7 [label="Attention_AllReduce_15_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_7_2 [label="QKV_Proj_15_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_15_7_2 [label="Attention_Scores_15_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_15_7_2 [label="Attention_Softmax_15_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_15_7_2 [label="Attention_Dropout_15_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_15_7_2 [label="Attention_Values_15_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_15_7_2 [label="Attention_Weighted_15_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_15_7_2 [label="Attention_Output_15_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_15_7 [label="Attention_AllReduce_15_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_7_3 [label="QKV_Proj_15_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_scores_15_7_3 [label="Attention_Scores_15_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_softmax_15_7_3 [label="Attention_Softmax_15_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_dropout_15_7_3 [label="Attention_Dropout_15_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightseagreen]
	attn_values_15_7_3 [label="Attention_Values_15_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_weighted_15_7_3 [label="Attention_Weighted_15_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightseagreen]
	attn_out_15_7_3 [label="Attention_Output_15_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	attn_allreduce_15_7 [label="Attention_AllReduce_15_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_8_0 [label="QKV_Proj_15_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_15_8_0 [label="Attention_Scores_15_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_15_8_0 [label="Attention_Softmax_15_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_15_8_0 [label="Attention_Dropout_15_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_15_8_0 [label="Attention_Values_15_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_15_8_0 [label="Attention_Weighted_15_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_15_8_0 [label="Attention_Output_15_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_15_8 [label="Attention_AllReduce_15_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_8_1 [label="QKV_Proj_15_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_15_8_1 [label="Attention_Scores_15_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_15_8_1 [label="Attention_Softmax_15_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_15_8_1 [label="Attention_Dropout_15_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_15_8_1 [label="Attention_Values_15_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_15_8_1 [label="Attention_Weighted_15_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_15_8_1 [label="Attention_Output_15_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_15_8 [label="Attention_AllReduce_15_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_8_2 [label="QKV_Proj_15_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_15_8_2 [label="Attention_Scores_15_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_15_8_2 [label="Attention_Softmax_15_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_15_8_2 [label="Attention_Dropout_15_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_15_8_2 [label="Attention_Values_15_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_15_8_2 [label="Attention_Weighted_15_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_15_8_2 [label="Attention_Output_15_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_15_8 [label="Attention_AllReduce_15_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_8_3 [label="QKV_Proj_15_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_scores_15_8_3 [label="Attention_Scores_15_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_softmax_15_8_3 [label="Attention_Softmax_15_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_dropout_15_8_3 [label="Attention_Dropout_15_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightskyblue]
	attn_values_15_8_3 [label="Attention_Values_15_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_weighted_15_8_3 [label="Attention_Weighted_15_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightskyblue]
	attn_out_15_8_3 [label="Attention_Output_15_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	attn_allreduce_15_8 [label="Attention_AllReduce_15_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_9_0 [label="QKV_Proj_15_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_15_9_0 [label="Attention_Scores_15_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_15_9_0 [label="Attention_Softmax_15_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_15_9_0 [label="Attention_Dropout_15_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_15_9_0 [label="Attention_Values_15_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_15_9_0 [label="Attention_Weighted_15_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_15_9_0 [label="Attention_Output_15_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_15_9 [label="Attention_AllReduce_15_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_9_1 [label="QKV_Proj_15_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_15_9_1 [label="Attention_Scores_15_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_15_9_1 [label="Attention_Softmax_15_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_15_9_1 [label="Attention_Dropout_15_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_15_9_1 [label="Attention_Values_15_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_15_9_1 [label="Attention_Weighted_15_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_15_9_1 [label="Attention_Output_15_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_15_9 [label="Attention_AllReduce_15_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_9_2 [label="QKV_Proj_15_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_15_9_2 [label="Attention_Scores_15_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_15_9_2 [label="Attention_Softmax_15_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_15_9_2 [label="Attention_Dropout_15_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_15_9_2 [label="Attention_Values_15_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_15_9_2 [label="Attention_Weighted_15_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_15_9_2 [label="Attention_Output_15_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_15_9 [label="Attention_AllReduce_15_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_9_3 [label="QKV_Proj_15_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_scores_15_9_3 [label="Attention_Scores_15_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_softmax_15_9_3 [label="Attention_Softmax_15_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_dropout_15_9_3 [label="Attention_Dropout_15_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightsteelblue]
	attn_values_15_9_3 [label="Attention_Values_15_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_weighted_15_9_3 [label="Attention_Weighted_15_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightsteelblue]
	attn_out_15_9_3 [label="Attention_Output_15_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	attn_allreduce_15_9 [label="Attention_AllReduce_15_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_10_0 [label="QKV_Proj_15_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_15_10_0 [label="Attention_Scores_15_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_15_10_0 [label="Attention_Softmax_15_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_15_10_0 [label="Attention_Dropout_15_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_15_10_0 [label="Attention_Values_15_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_15_10_0 [label="Attention_Weighted_15_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_15_10_0 [label="Attention_Output_15_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_15_10 [label="Attention_AllReduce_15_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_10_1 [label="QKV_Proj_15_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_15_10_1 [label="Attention_Scores_15_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_15_10_1 [label="Attention_Softmax_15_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_15_10_1 [label="Attention_Dropout_15_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_15_10_1 [label="Attention_Values_15_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_15_10_1 [label="Attention_Weighted_15_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_15_10_1 [label="Attention_Output_15_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_15_10 [label="Attention_AllReduce_15_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_10_2 [label="QKV_Proj_15_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_15_10_2 [label="Attention_Scores_15_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_15_10_2 [label="Attention_Softmax_15_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_15_10_2 [label="Attention_Dropout_15_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_15_10_2 [label="Attention_Values_15_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_15_10_2 [label="Attention_Weighted_15_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_15_10_2 [label="Attention_Output_15_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_15_10 [label="Attention_AllReduce_15_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_10_3 [label="QKV_Proj_15_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_scores_15_10_3 [label="Attention_Scores_15_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_softmax_15_10_3 [label="Attention_Softmax_15_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_dropout_15_10_3 [label="Attention_Dropout_15_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightcyan]
	attn_values_15_10_3 [label="Attention_Values_15_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_weighted_15_10_3 [label="Attention_Weighted_15_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightcyan]
	attn_out_15_10_3 [label="Attention_Output_15_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	attn_allreduce_15_10 [label="Attention_AllReduce_15_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_11_0 [label="QKV_Proj_15_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_15_11_0 [label="Attention_Scores_15_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_15_11_0 [label="Attention_Softmax_15_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_15_11_0 [label="Attention_Dropout_15_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_15_11_0 [label="Attention_Values_15_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_15_11_0 [label="Attention_Weighted_15_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_15_11_0 [label="Attention_Output_15_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_15_11 [label="Attention_AllReduce_15_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_11_1 [label="QKV_Proj_15_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_15_11_1 [label="Attention_Scores_15_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_15_11_1 [label="Attention_Softmax_15_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_15_11_1 [label="Attention_Dropout_15_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_15_11_1 [label="Attention_Values_15_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_15_11_1 [label="Attention_Weighted_15_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_15_11_1 [label="Attention_Output_15_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_15_11 [label="Attention_AllReduce_15_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_11_2 [label="QKV_Proj_15_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_15_11_2 [label="Attention_Scores_15_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_15_11_2 [label="Attention_Softmax_15_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_15_11_2 [label="Attention_Dropout_15_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_15_11_2 [label="Attention_Values_15_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_15_11_2 [label="Attention_Weighted_15_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_15_11_2 [label="Attention_Output_15_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_15_11 [label="Attention_AllReduce_15_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_11_3 [label="QKV_Proj_15_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_scores_15_11_3 [label="Attention_Scores_15_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_softmax_15_11_3 [label="Attention_Softmax_15_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_dropout_15_11_3 [label="Attention_Dropout_15_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightgoldenrodyellow]
	attn_values_15_11_3 [label="Attention_Values_15_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_weighted_15_11_3 [label="Attention_Weighted_15_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightgoldenrodyellow]
	attn_out_15_11_3 [label="Attention_Output_15_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	attn_allreduce_15_11 [label="Attention_AllReduce_15_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_12_0 [label="QKV_Proj_15_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_15_12_0 [label="Attention_Scores_15_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_15_12_0 [label="Attention_Softmax_15_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_15_12_0 [label="Attention_Dropout_15_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_15_12_0 [label="Attention_Values_15_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_15_12_0 [label="Attention_Weighted_15_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_15_12_0 [label="Attention_Output_15_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_15_12 [label="Attention_AllReduce_15_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_12_1 [label="QKV_Proj_15_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_15_12_1 [label="Attention_Scores_15_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_15_12_1 [label="Attention_Softmax_15_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_15_12_1 [label="Attention_Dropout_15_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_15_12_1 [label="Attention_Values_15_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_15_12_1 [label="Attention_Weighted_15_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_15_12_1 [label="Attention_Output_15_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_15_12 [label="Attention_AllReduce_15_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_12_2 [label="QKV_Proj_15_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_15_12_2 [label="Attention_Scores_15_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_15_12_2 [label="Attention_Softmax_15_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_15_12_2 [label="Attention_Dropout_15_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_15_12_2 [label="Attention_Values_15_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_15_12_2 [label="Attention_Weighted_15_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_15_12_2 [label="Attention_Output_15_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_15_12 [label="Attention_AllReduce_15_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_12_3 [label="QKV_Proj_15_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_scores_15_12_3 [label="Attention_Scores_15_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_softmax_15_12_3 [label="Attention_Softmax_15_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_dropout_15_12_3 [label="Attention_Dropout_15_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightmagenta]
	attn_values_15_12_3 [label="Attention_Values_15_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_weighted_15_12_3 [label="Attention_Weighted_15_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightmagenta]
	attn_out_15_12_3 [label="Attention_Output_15_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	attn_allreduce_15_12 [label="Attention_AllReduce_15_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_13_0 [label="QKV_Proj_15_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_15_13_0 [label="Attention_Scores_15_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_15_13_0 [label="Attention_Softmax_15_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_15_13_0 [label="Attention_Dropout_15_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_15_13_0 [label="Attention_Values_15_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_15_13_0 [label="Attention_Weighted_15_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_15_13_0 [label="Attention_Output_15_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_15_13 [label="Attention_AllReduce_15_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_13_1 [label="QKV_Proj_15_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_15_13_1 [label="Attention_Scores_15_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_15_13_1 [label="Attention_Softmax_15_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_15_13_1 [label="Attention_Dropout_15_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_15_13_1 [label="Attention_Values_15_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_15_13_1 [label="Attention_Weighted_15_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_15_13_1 [label="Attention_Output_15_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_15_13 [label="Attention_AllReduce_15_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_13_2 [label="QKV_Proj_15_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_15_13_2 [label="Attention_Scores_15_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_15_13_2 [label="Attention_Softmax_15_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_15_13_2 [label="Attention_Dropout_15_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_15_13_2 [label="Attention_Values_15_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_15_13_2 [label="Attention_Weighted_15_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_15_13_2 [label="Attention_Output_15_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_15_13 [label="Attention_AllReduce_15_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_13_3 [label="QKV_Proj_15_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_scores_15_13_3 [label="Attention_Scores_15_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_softmax_15_13_3 [label="Attention_Softmax_15_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_dropout_15_13_3 [label="Attention_Dropout_15_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightorange]
	attn_values_15_13_3 [label="Attention_Values_15_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_weighted_15_13_3 [label="Attention_Weighted_15_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightorange]
	attn_out_15_13_3 [label="Attention_Output_15_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	attn_allreduce_15_13 [label="Attention_AllReduce_15_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_14_0 [label="QKV_Proj_15_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_15_14_0 [label="Attention_Scores_15_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_15_14_0 [label="Attention_Softmax_15_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_15_14_0 [label="Attention_Dropout_15_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_15_14_0 [label="Attention_Values_15_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_15_14_0 [label="Attention_Weighted_15_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_15_14_0 [label="Attention_Output_15_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_15_14 [label="Attention_AllReduce_15_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_14_1 [label="QKV_Proj_15_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_15_14_1 [label="Attention_Scores_15_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_15_14_1 [label="Attention_Softmax_15_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_15_14_1 [label="Attention_Dropout_15_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_15_14_1 [label="Attention_Values_15_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_15_14_1 [label="Attention_Weighted_15_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_15_14_1 [label="Attention_Output_15_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_15_14 [label="Attention_AllReduce_15_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_14_2 [label="QKV_Proj_15_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_15_14_2 [label="Attention_Scores_15_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_15_14_2 [label="Attention_Softmax_15_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_15_14_2 [label="Attention_Dropout_15_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_15_14_2 [label="Attention_Values_15_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_15_14_2 [label="Attention_Weighted_15_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_15_14_2 [label="Attention_Output_15_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_15_14 [label="Attention_AllReduce_15_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_14_3 [label="QKV_Proj_15_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_scores_15_14_3 [label="Attention_Scores_15_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_softmax_15_14_3 [label="Attention_Softmax_15_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_dropout_15_14_3 [label="Attention_Dropout_15_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightpurple]
	attn_values_15_14_3 [label="Attention_Values_15_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_weighted_15_14_3 [label="Attention_Weighted_15_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightpurple]
	attn_out_15_14_3 [label="Attention_Output_15_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	attn_allreduce_15_14 [label="Attention_AllReduce_15_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_15_0 [label="QKV_Proj_15_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_15_15_0 [label="Attention_Scores_15_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_15_15_0 [label="Attention_Softmax_15_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_15_15_0 [label="Attention_Dropout_15_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_15_15_0 [label="Attention_Values_15_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_15_15_0 [label="Attention_Weighted_15_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_15_15_0 [label="Attention_Output_15_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_15_15 [label="Attention_AllReduce_15_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_15_1 [label="QKV_Proj_15_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_15_15_1 [label="Attention_Scores_15_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_15_15_1 [label="Attention_Softmax_15_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_15_15_1 [label="Attention_Dropout_15_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_15_15_1 [label="Attention_Values_15_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_15_15_1 [label="Attention_Weighted_15_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_15_15_1 [label="Attention_Output_15_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_15_15 [label="Attention_AllReduce_15_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_15_2 [label="QKV_Proj_15_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_15_15_2 [label="Attention_Scores_15_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_15_15_2 [label="Attention_Softmax_15_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_15_15_2 [label="Attention_Dropout_15_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_15_15_2 [label="Attention_Values_15_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_15_15_2 [label="Attention_Weighted_15_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_15_15_2 [label="Attention_Output_15_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_15_15 [label="Attention_AllReduce_15_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	qkv_15_15_3 [label="QKV_Proj_15_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_scores_15_15_3 [label="Attention_Scores_15_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_softmax_15_15_3 [label="Attention_Softmax_15_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_dropout_15_15_3 [label="Attention_Dropout_15_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8]\nOutput: [batch_size=128, seq_len=128, seq_len=128, heads=8]" fillcolor=lightteal]
	attn_values_15_15_3 [label="Attention_Values_15_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_weighted_15_15_3 [label="Attention_Weighted_15_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, seq_len=128, heads=8], [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, heads=8, d_k=64]" fillcolor=lightteal]
	attn_out_15_15_3 [label="Attention_Output_15_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	attn_allreduce_15_15 [label="Attention_AllReduce_15_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_0_0 [label="MLP_Linear1_15_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_15_0_0 [label="GELU_15_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_15_0_0 [label="MLP_Linear2_15_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_15_0_1 [label="MLP_Linear1_15_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_15_0_1 [label="GELU_15_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_15_0_1 [label="MLP_Linear2_15_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_15_0_2 [label="MLP_Linear1_15_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_15_0_2 [label="GELU_15_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_15_0_2 [label="MLP_Linear2_15_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp1_15_0_3 [label="MLP_Linear1_15_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	gelu_15_0_3 [label="GELU_15_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcoral]
	mlp2_15_0_3 [label="MLP_Linear2_15_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	mlp_allreduce_15_0 [label="MLP_AllReduce_15_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_1_0 [label="MLP_Linear1_15_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_15_1_0 [label="GELU_15_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_15_1_0 [label="MLP_Linear2_15_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_15_1_1 [label="MLP_Linear1_15_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_15_1_1 [label="GELU_15_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_15_1_1 [label="MLP_Linear2_15_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_15_1_2 [label="MLP_Linear1_15_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_15_1_2 [label="GELU_15_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_15_1_2 [label="MLP_Linear2_15_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp1_15_1_3 [label="MLP_Linear1_15_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	gelu_15_1_3 [label="GELU_15_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgreen]
	mlp2_15_1_3 [label="MLP_Linear2_15_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	mlp_allreduce_15_1 [label="MLP_AllReduce_15_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_2_0 [label="MLP_Linear1_15_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_15_2_0 [label="GELU_15_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_15_2_0 [label="MLP_Linear2_15_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_15_2_1 [label="MLP_Linear1_15_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_15_2_1 [label="GELU_15_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_15_2_1 [label="MLP_Linear2_15_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_15_2_2 [label="MLP_Linear1_15_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_15_2_2 [label="GELU_15_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_15_2_2 [label="MLP_Linear2_15_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp1_15_2_3 [label="MLP_Linear1_15_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	gelu_15_2_3 [label="GELU_15_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightblue]
	mlp2_15_2_3 [label="MLP_Linear2_15_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	mlp_allreduce_15_2 [label="MLP_AllReduce_15_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_3_0 [label="MLP_Linear1_15_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_15_3_0 [label="GELU_15_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_15_3_0 [label="MLP_Linear2_15_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_15_3_1 [label="MLP_Linear1_15_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_15_3_1 [label="GELU_15_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_15_3_1 [label="MLP_Linear2_15_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_15_3_2 [label="MLP_Linear1_15_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_15_3_2 [label="GELU_15_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_15_3_2 [label="MLP_Linear2_15_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp1_15_3_3 [label="MLP_Linear1_15_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	gelu_15_3_3 [label="GELU_15_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightyellow]
	mlp2_15_3_3 [label="MLP_Linear2_15_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	mlp_allreduce_15_3 [label="MLP_AllReduce_15_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_4_0 [label="MLP_Linear1_15_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_15_4_0 [label="GELU_15_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_15_4_0 [label="MLP_Linear2_15_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_15_4_1 [label="MLP_Linear1_15_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_15_4_1 [label="GELU_15_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_15_4_1 [label="MLP_Linear2_15_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_15_4_2 [label="MLP_Linear1_15_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_15_4_2 [label="GELU_15_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_15_4_2 [label="MLP_Linear2_15_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp1_15_4_3 [label="MLP_Linear1_15_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	gelu_15_4_3 [label="GELU_15_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpink]
	mlp2_15_4_3 [label="MLP_Linear2_15_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	mlp_allreduce_15_4 [label="MLP_AllReduce_15_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_5_0 [label="MLP_Linear1_15_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_15_5_0 [label="GELU_15_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_15_5_0 [label="MLP_Linear2_15_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_15_5_1 [label="MLP_Linear1_15_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_15_5_1 [label="GELU_15_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_15_5_1 [label="MLP_Linear2_15_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_15_5_2 [label="MLP_Linear1_15_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_15_5_2 [label="GELU_15_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_15_5_2 [label="MLP_Linear2_15_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp1_15_5_3 [label="MLP_Linear1_15_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	gelu_15_5_3 [label="GELU_15_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgray]
	mlp2_15_5_3 [label="MLP_Linear2_15_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	mlp_allreduce_15_5 [label="MLP_AllReduce_15_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_6_0 [label="MLP_Linear1_15_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_15_6_0 [label="GELU_15_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_15_6_0 [label="MLP_Linear2_15_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_15_6_1 [label="MLP_Linear1_15_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_15_6_1 [label="GELU_15_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_15_6_1 [label="MLP_Linear2_15_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_15_6_2 [label="MLP_Linear1_15_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_15_6_2 [label="GELU_15_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_15_6_2 [label="MLP_Linear2_15_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp1_15_6_3 [label="MLP_Linear1_15_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	gelu_15_6_3 [label="GELU_15_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsalmon]
	mlp2_15_6_3 [label="MLP_Linear2_15_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	mlp_allreduce_15_6 [label="MLP_AllReduce_15_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_7_0 [label="MLP_Linear1_15_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_15_7_0 [label="GELU_15_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_15_7_0 [label="MLP_Linear2_15_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_15_7_1 [label="MLP_Linear1_15_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_15_7_1 [label="GELU_15_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_15_7_1 [label="MLP_Linear2_15_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_15_7_2 [label="MLP_Linear1_15_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_15_7_2 [label="GELU_15_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_15_7_2 [label="MLP_Linear2_15_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp1_15_7_3 [label="MLP_Linear1_15_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	gelu_15_7_3 [label="GELU_15_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightseagreen]
	mlp2_15_7_3 [label="MLP_Linear2_15_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	mlp_allreduce_15_7 [label="MLP_AllReduce_15_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_8_0 [label="MLP_Linear1_15_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_15_8_0 [label="GELU_15_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_15_8_0 [label="MLP_Linear2_15_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_15_8_1 [label="MLP_Linear1_15_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_15_8_1 [label="GELU_15_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_15_8_1 [label="MLP_Linear2_15_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_15_8_2 [label="MLP_Linear1_15_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_15_8_2 [label="GELU_15_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_15_8_2 [label="MLP_Linear2_15_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp1_15_8_3 [label="MLP_Linear1_15_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	gelu_15_8_3 [label="GELU_15_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightskyblue]
	mlp2_15_8_3 [label="MLP_Linear2_15_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	mlp_allreduce_15_8 [label="MLP_AllReduce_15_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_9_0 [label="MLP_Linear1_15_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_15_9_0 [label="GELU_15_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_15_9_0 [label="MLP_Linear2_15_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_15_9_1 [label="MLP_Linear1_15_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_15_9_1 [label="GELU_15_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_15_9_1 [label="MLP_Linear2_15_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_15_9_2 [label="MLP_Linear1_15_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_15_9_2 [label="GELU_15_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_15_9_2 [label="MLP_Linear2_15_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp1_15_9_3 [label="MLP_Linear1_15_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	gelu_15_9_3 [label="GELU_15_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightsteelblue]
	mlp2_15_9_3 [label="MLP_Linear2_15_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	mlp_allreduce_15_9 [label="MLP_AllReduce_15_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_10_0 [label="MLP_Linear1_15_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_15_10_0 [label="GELU_15_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_15_10_0 [label="MLP_Linear2_15_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_15_10_1 [label="MLP_Linear1_15_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_15_10_1 [label="GELU_15_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_15_10_1 [label="MLP_Linear2_15_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_15_10_2 [label="MLP_Linear1_15_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_15_10_2 [label="GELU_15_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_15_10_2 [label="MLP_Linear2_15_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp1_15_10_3 [label="MLP_Linear1_15_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	gelu_15_10_3 [label="GELU_15_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightcyan]
	mlp2_15_10_3 [label="MLP_Linear2_15_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	mlp_allreduce_15_10 [label="MLP_AllReduce_15_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_11_0 [label="MLP_Linear1_15_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_15_11_0 [label="GELU_15_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_15_11_0 [label="MLP_Linear2_15_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_15_11_1 [label="MLP_Linear1_15_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_15_11_1 [label="GELU_15_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_15_11_1 [label="MLP_Linear2_15_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_15_11_2 [label="MLP_Linear1_15_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_15_11_2 [label="GELU_15_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_15_11_2 [label="MLP_Linear2_15_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp1_15_11_3 [label="MLP_Linear1_15_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	gelu_15_11_3 [label="GELU_15_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightgoldenrodyellow]
	mlp2_15_11_3 [label="MLP_Linear2_15_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	mlp_allreduce_15_11 [label="MLP_AllReduce_15_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_12_0 [label="MLP_Linear1_15_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_15_12_0 [label="GELU_15_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_15_12_0 [label="MLP_Linear2_15_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_15_12_1 [label="MLP_Linear1_15_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_15_12_1 [label="GELU_15_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_15_12_1 [label="MLP_Linear2_15_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_15_12_2 [label="MLP_Linear1_15_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_15_12_2 [label="GELU_15_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_15_12_2 [label="MLP_Linear2_15_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp1_15_12_3 [label="MLP_Linear1_15_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	gelu_15_12_3 [label="GELU_15_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightmagenta]
	mlp2_15_12_3 [label="MLP_Linear2_15_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	mlp_allreduce_15_12 [label="MLP_AllReduce_15_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_13_0 [label="MLP_Linear1_15_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_15_13_0 [label="GELU_15_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_15_13_0 [label="MLP_Linear2_15_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_15_13_1 [label="MLP_Linear1_15_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_15_13_1 [label="GELU_15_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_15_13_1 [label="MLP_Linear2_15_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_15_13_2 [label="MLP_Linear1_15_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_15_13_2 [label="GELU_15_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_15_13_2 [label="MLP_Linear2_15_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp1_15_13_3 [label="MLP_Linear1_15_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	gelu_15_13_3 [label="GELU_15_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightorange]
	mlp2_15_13_3 [label="MLP_Linear2_15_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	mlp_allreduce_15_13 [label="MLP_AllReduce_15_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_14_0 [label="MLP_Linear1_15_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_15_14_0 [label="GELU_15_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_15_14_0 [label="MLP_Linear2_15_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_15_14_1 [label="MLP_Linear1_15_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_15_14_1 [label="GELU_15_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_15_14_1 [label="MLP_Linear2_15_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_15_14_2 [label="MLP_Linear1_15_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_15_14_2 [label="GELU_15_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_15_14_2 [label="MLP_Linear2_15_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp1_15_14_3 [label="MLP_Linear1_15_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	gelu_15_14_3 [label="GELU_15_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightpurple]
	mlp2_15_14_3 [label="MLP_Linear2_15_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	mlp_allreduce_15_14 [label="MLP_AllReduce_15_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	mlp1_15_15_0 [label="MLP_Linear1_15_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_15_15_0 [label="GELU_15_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_15_15_0 [label="MLP_Linear2_15_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_15_15_1 [label="MLP_Linear1_15_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_15_15_1 [label="GELU_15_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_15_15_1 [label="MLP_Linear2_15_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_15_15_2 [label="MLP_Linear1_15_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_15_15_2 [label="GELU_15_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_15_15_2 [label="MLP_Linear2_15_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp1_15_15_3 [label="MLP_Linear1_15_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	gelu_15_15_3 [label="GELU_15_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, ffn=512]" fillcolor=lightteal]
	mlp2_15_15_3 [label="MLP_Linear2_15_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, ffn=512]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	mlp_allreduce_15_15 [label="MLP_AllReduce_15_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=red penwidth=2 shape=ellipse style=filled]
	expert_route_15_0 [label="Expert_Route_15_0\nEP:0\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_1 [label="Expert_Route_15_1\nEP:1\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_2 [label="Expert_Route_15_2\nEP:2\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_3 [label="Expert_Route_15_3\nEP:3\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_4 [label="Expert_Route_15_4\nEP:4\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_5 [label="Expert_Route_15_5\nEP:5\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_6 [label="Expert_Route_15_6\nEP:6\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_7 [label="Expert_Route_15_7\nEP:7\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_8 [label="Expert_Route_15_8\nEP:8\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_9 [label="Expert_Route_15_9\nEP:9\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_10 [label="Expert_Route_15_10\nEP:10\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_11 [label="Expert_Route_15_11\nEP:11\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_12 [label="Expert_Route_15_12\nEP:12\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_13 [label="Expert_Route_15_13\nEP:13\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_14 [label="Expert_Route_15_14\nEP:14\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	expert_route_15_15 [label="Expert_Route_15_15\nEP:15\nInput: [batch_size=128, seq_len=128, hidden=512]\nOutput: [batch_size=128, seq_len=128, hidden=512]" fillcolor=orange penwidth=2 shape=ellipse style=filled]
	final_norm_0_0 [label="Final_Norm_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	final_norm_0_1 [label="Final_Norm_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	final_norm_0_2 [label="Final_Norm_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	final_norm_0_3 [label="Final_Norm_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcoral]
	final_norm_1_0 [label="Final_Norm_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	final_norm_1_1 [label="Final_Norm_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	final_norm_1_2 [label="Final_Norm_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	final_norm_1_3 [label="Final_Norm_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgreen]
	final_norm_2_0 [label="Final_Norm_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	final_norm_2_1 [label="Final_Norm_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	final_norm_2_2 [label="Final_Norm_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	final_norm_2_3 [label="Final_Norm_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightblue]
	final_norm_3_0 [label="Final_Norm_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	final_norm_3_1 [label="Final_Norm_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	final_norm_3_2 [label="Final_Norm_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	final_norm_3_3 [label="Final_Norm_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightyellow]
	final_norm_4_0 [label="Final_Norm_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	final_norm_4_1 [label="Final_Norm_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	final_norm_4_2 [label="Final_Norm_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	final_norm_4_3 [label="Final_Norm_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpink]
	final_norm_5_0 [label="Final_Norm_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	final_norm_5_1 [label="Final_Norm_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	final_norm_5_2 [label="Final_Norm_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	final_norm_5_3 [label="Final_Norm_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgray]
	final_norm_6_0 [label="Final_Norm_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	final_norm_6_1 [label="Final_Norm_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	final_norm_6_2 [label="Final_Norm_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	final_norm_6_3 [label="Final_Norm_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsalmon]
	final_norm_7_0 [label="Final_Norm_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	final_norm_7_1 [label="Final_Norm_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	final_norm_7_2 [label="Final_Norm_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	final_norm_7_3 [label="Final_Norm_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightseagreen]
	final_norm_8_0 [label="Final_Norm_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	final_norm_8_1 [label="Final_Norm_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	final_norm_8_2 [label="Final_Norm_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	final_norm_8_3 [label="Final_Norm_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightskyblue]
	final_norm_9_0 [label="Final_Norm_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	final_norm_9_1 [label="Final_Norm_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	final_norm_9_2 [label="Final_Norm_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	final_norm_9_3 [label="Final_Norm_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightsteelblue]
	final_norm_10_0 [label="Final_Norm_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	final_norm_10_1 [label="Final_Norm_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	final_norm_10_2 [label="Final_Norm_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	final_norm_10_3 [label="Final_Norm_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightcyan]
	final_norm_11_0 [label="Final_Norm_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	final_norm_11_1 [label="Final_Norm_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	final_norm_11_2 [label="Final_Norm_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	final_norm_11_3 [label="Final_Norm_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightgoldenrodyellow]
	final_norm_12_0 [label="Final_Norm_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	final_norm_12_1 [label="Final_Norm_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	final_norm_12_2 [label="Final_Norm_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	final_norm_12_3 [label="Final_Norm_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightmagenta]
	final_norm_13_0 [label="Final_Norm_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	final_norm_13_1 [label="Final_Norm_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	final_norm_13_2 [label="Final_Norm_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	final_norm_13_3 [label="Final_Norm_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightorange]
	final_norm_14_0 [label="Final_Norm_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	final_norm_14_1 [label="Final_Norm_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	final_norm_14_2 [label="Final_Norm_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	final_norm_14_3 [label="Final_Norm_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightpurple]
	final_norm_15_0 [label="Final_Norm_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	final_norm_15_1 [label="Final_Norm_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	final_norm_15_2 [label="Final_Norm_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	final_norm_15_3 [label="Final_Norm_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, hidden=128]" fillcolor=lightteal]
	output_proj_0_0 [label="Output_Proj_0_0\nGPU:0\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightcoral]
	output_proj_0_1 [label="Output_Proj_0_1\nGPU:1\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightcoral]
	output_proj_0_2 [label="Output_Proj_0_2\nGPU:2\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightcoral]
	output_proj_0_3 [label="Output_Proj_0_3\nGPU:3\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightcoral]
	output_proj_1_0 [label="Output_Proj_1_0\nGPU:4\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightgreen]
	output_proj_1_1 [label="Output_Proj_1_1\nGPU:5\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightgreen]
	output_proj_1_2 [label="Output_Proj_1_2\nGPU:6\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightgreen]
	output_proj_1_3 [label="Output_Proj_1_3\nGPU:7\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightgreen]
	output_proj_2_0 [label="Output_Proj_2_0\nGPU:8\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightblue]
	output_proj_2_1 [label="Output_Proj_2_1\nGPU:9\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightblue]
	output_proj_2_2 [label="Output_Proj_2_2\nGPU:10\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightblue]
	output_proj_2_3 [label="Output_Proj_2_3\nGPU:11\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightblue]
	output_proj_3_0 [label="Output_Proj_3_0\nGPU:12\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightyellow]
	output_proj_3_1 [label="Output_Proj_3_1\nGPU:13\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightyellow]
	output_proj_3_2 [label="Output_Proj_3_2\nGPU:14\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightyellow]
	output_proj_3_3 [label="Output_Proj_3_3\nGPU:15\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightyellow]
	output_proj_4_0 [label="Output_Proj_4_0\nGPU:16\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightpink]
	output_proj_4_1 [label="Output_Proj_4_1\nGPU:17\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightpink]
	output_proj_4_2 [label="Output_Proj_4_2\nGPU:18\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightpink]
	output_proj_4_3 [label="Output_Proj_4_3\nGPU:19\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightpink]
	output_proj_5_0 [label="Output_Proj_5_0\nGPU:20\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightgray]
	output_proj_5_1 [label="Output_Proj_5_1\nGPU:21\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightgray]
	output_proj_5_2 [label="Output_Proj_5_2\nGPU:22\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightgray]
	output_proj_5_3 [label="Output_Proj_5_3\nGPU:23\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightgray]
	output_proj_6_0 [label="Output_Proj_6_0\nGPU:24\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightsalmon]
	output_proj_6_1 [label="Output_Proj_6_1\nGPU:25\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightsalmon]
	output_proj_6_2 [label="Output_Proj_6_2\nGPU:26\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightsalmon]
	output_proj_6_3 [label="Output_Proj_6_3\nGPU:27\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightsalmon]
	output_proj_7_0 [label="Output_Proj_7_0\nGPU:28\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightseagreen]
	output_proj_7_1 [label="Output_Proj_7_1\nGPU:29\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightseagreen]
	output_proj_7_2 [label="Output_Proj_7_2\nGPU:30\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightseagreen]
	output_proj_7_3 [label="Output_Proj_7_3\nGPU:31\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightseagreen]
	output_proj_8_0 [label="Output_Proj_8_0\nGPU:32\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightskyblue]
	output_proj_8_1 [label="Output_Proj_8_1\nGPU:33\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightskyblue]
	output_proj_8_2 [label="Output_Proj_8_2\nGPU:34\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightskyblue]
	output_proj_8_3 [label="Output_Proj_8_3\nGPU:35\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightskyblue]
	output_proj_9_0 [label="Output_Proj_9_0\nGPU:36\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightsteelblue]
	output_proj_9_1 [label="Output_Proj_9_1\nGPU:37\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightsteelblue]
	output_proj_9_2 [label="Output_Proj_9_2\nGPU:38\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightsteelblue]
	output_proj_9_3 [label="Output_Proj_9_3\nGPU:39\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightsteelblue]
	output_proj_10_0 [label="Output_Proj_10_0\nGPU:40\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightcyan]
	output_proj_10_1 [label="Output_Proj_10_1\nGPU:41\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightcyan]
	output_proj_10_2 [label="Output_Proj_10_2\nGPU:42\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightcyan]
	output_proj_10_3 [label="Output_Proj_10_3\nGPU:43\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightcyan]
	output_proj_11_0 [label="Output_Proj_11_0\nGPU:44\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightgoldenrodyellow]
	output_proj_11_1 [label="Output_Proj_11_1\nGPU:45\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightgoldenrodyellow]
	output_proj_11_2 [label="Output_Proj_11_2\nGPU:46\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightgoldenrodyellow]
	output_proj_11_3 [label="Output_Proj_11_3\nGPU:47\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightgoldenrodyellow]
	output_proj_12_0 [label="Output_Proj_12_0\nGPU:48\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightmagenta]
	output_proj_12_1 [label="Output_Proj_12_1\nGPU:49\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightmagenta]
	output_proj_12_2 [label="Output_Proj_12_2\nGPU:50\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightmagenta]
	output_proj_12_3 [label="Output_Proj_12_3\nGPU:51\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightmagenta]
	output_proj_13_0 [label="Output_Proj_13_0\nGPU:52\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightorange]
	output_proj_13_1 [label="Output_Proj_13_1\nGPU:53\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightorange]
	output_proj_13_2 [label="Output_Proj_13_2\nGPU:54\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightorange]
	output_proj_13_3 [label="Output_Proj_13_3\nGPU:55\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightorange]
	output_proj_14_0 [label="Output_Proj_14_0\nGPU:56\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightpurple]
	output_proj_14_1 [label="Output_Proj_14_1\nGPU:57\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightpurple]
	output_proj_14_2 [label="Output_Proj_14_2\nGPU:58\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightpurple]
	output_proj_14_3 [label="Output_Proj_14_3\nGPU:59\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightpurple]
	output_proj_15_0 [label="Output_Proj_15_0\nGPU:60\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightteal]
	output_proj_15_1 [label="Output_Proj_15_1\nGPU:61\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightteal]
	output_proj_15_2 [label="Output_Proj_15_2\nGPU:62\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightteal]
	output_proj_15_3 [label="Output_Proj_15_3\nGPU:63\nInput: [batch_size=128, seq_len=128, hidden=128]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=lightteal]
	final_allreduce [label="Final_AllReduce\nAll GPUs\nInput: [batch_size=128, seq_len=128, vocab=32000]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=red penwidth=3 shape=ellipse style=filled]
	output [label="Output\nInput: [batch_size=128, seq_len=128, vocab=32000]\nOutput: [batch_size=128, seq_len=128, vocab=32000]" fillcolor=white penwidth=2 shape=ellipse style=filled]
	final_allreduce -> output
}
