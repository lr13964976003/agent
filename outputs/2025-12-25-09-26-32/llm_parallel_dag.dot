// LLM Parallel Inference DAG - EP-16 × TP-4 × PP-1
digraph {
	nodesep=0.8 rankdir=TB ranksep=1.5 size="50,30"
	node [fontsize=10 height=0.6 width=2.0]
	node [fillcolor=lightblue shape=ellipse style=filled]
	node [fillcolor=lightgreen shape=box style=filled]
	node [fillcolor=lightyellow shape=parallelogram style=filled]
	input [label="Input\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightcoral shape=ellipse]
	embedding_0 [label="Embedding_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_1 [label="Embedding_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_2 [label="Embedding_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_3 [label="Embedding_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_4 [label="Embedding_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_5 [label="Embedding_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_6 [label="Embedding_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_7 [label="Embedding_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_8 [label="Embedding_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_9 [label="Embedding_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_10 [label="Embedding_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_11 [label="Embedding_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_12 [label="Embedding_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_13 [label="Embedding_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_14 [label="Embedding_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_15 [label="Embedding_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_16 [label="Embedding_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_17 [label="Embedding_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_18 [label="Embedding_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_19 [label="Embedding_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_20 [label="Embedding_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_21 [label="Embedding_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_22 [label="Embedding_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_23 [label="Embedding_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_24 [label="Embedding_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_25 [label="Embedding_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_26 [label="Embedding_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_27 [label="Embedding_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_28 [label="Embedding_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_29 [label="Embedding_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_30 [label="Embedding_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_31 [label="Embedding_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_32 [label="Embedding_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_33 [label="Embedding_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_34 [label="Embedding_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_35 [label="Embedding_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_36 [label="Embedding_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_37 [label="Embedding_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_38 [label="Embedding_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_39 [label="Embedding_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_40 [label="Embedding_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_41 [label="Embedding_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_42 [label="Embedding_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_43 [label="Embedding_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_44 [label="Embedding_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_45 [label="Embedding_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_46 [label="Embedding_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_47 [label="Embedding_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_48 [label="Embedding_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_49 [label="Embedding_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_50 [label="Embedding_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_51 [label="Embedding_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_52 [label="Embedding_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_53 [label="Embedding_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_54 [label="Embedding_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_55 [label="Embedding_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_56 [label="Embedding_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_57 [label="Embedding_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_58 [label="Embedding_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_59 [label="Embedding_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_60 [label="Embedding_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_61 [label="Embedding_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_62 [label="Embedding_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	embedding_63 [label="Embedding_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	input -> embedding_0
	input -> embedding_1
	input -> embedding_2
	input -> embedding_3
	input -> embedding_4
	input -> embedding_5
	input -> embedding_6
	input -> embedding_7
	input -> embedding_8
	input -> embedding_9
	input -> embedding_10
	input -> embedding_11
	input -> embedding_12
	input -> embedding_13
	input -> embedding_14
	input -> embedding_15
	input -> embedding_16
	input -> embedding_17
	input -> embedding_18
	input -> embedding_19
	input -> embedding_20
	input -> embedding_21
	input -> embedding_22
	input -> embedding_23
	input -> embedding_24
	input -> embedding_25
	input -> embedding_26
	input -> embedding_27
	input -> embedding_28
	input -> embedding_29
	input -> embedding_30
	input -> embedding_31
	input -> embedding_32
	input -> embedding_33
	input -> embedding_34
	input -> embedding_35
	input -> embedding_36
	input -> embedding_37
	input -> embedding_38
	input -> embedding_39
	input -> embedding_40
	input -> embedding_41
	input -> embedding_42
	input -> embedding_43
	input -> embedding_44
	input -> embedding_45
	input -> embedding_46
	input -> embedding_47
	input -> embedding_48
	input -> embedding_49
	input -> embedding_50
	input -> embedding_51
	input -> embedding_52
	input -> embedding_53
	input -> embedding_54
	input -> embedding_55
	input -> embedding_56
	input -> embedding_57
	input -> embedding_58
	input -> embedding_59
	input -> embedding_60
	input -> embedding_61
	input -> embedding_62
	input -> embedding_63
	layer_0_input [label="Layer0_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	embedding_0 -> layer_0_input
	embedding_4 -> layer_0_input
	embedding_8 -> layer_0_input
	embedding_12 -> layer_0_input
	embedding_16 -> layer_0_input
	embedding_20 -> layer_0_input
	embedding_24 -> layer_0_input
	embedding_28 -> layer_0_input
	embedding_32 -> layer_0_input
	embedding_36 -> layer_0_input
	embedding_40 -> layer_0_input
	embedding_44 -> layer_0_input
	embedding_48 -> layer_0_input
	embedding_52 -> layer_0_input
	embedding_56 -> layer_0_input
	embedding_60 -> layer_0_input
	layer_0_qkv_0 [label="Layer0_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_1 [label="Layer0_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_2 [label="Layer0_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_3 [label="Layer0_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_0 [label="Layer0_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_1 [label="Layer0_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_2 [label="Layer0_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_3 [label="Layer0_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_0 [label="Layer0_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_1 [label="Layer0_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_2 [label="Layer0_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_3 [label="Layer0_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_0 [label="Layer0_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_1 [label="Layer0_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_2 [label="Layer0_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_3 [label="Layer0_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_0 [label="Layer0_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_0
	layer_0_qkv_0 -> layer_0_attn_scores_0
	layer_0_attn_scores_0 -> layer_0_attn_softmax_0
	layer_0_attn_softmax_0 -> layer_0_attn_out_0
	layer_0_attn_out_0 -> layer_0_attn_allreduce_0
	layer_0_input -> layer_0_qkv_1
	layer_0_qkv_1 -> layer_0_attn_scores_1
	layer_0_attn_scores_1 -> layer_0_attn_softmax_1
	layer_0_attn_softmax_1 -> layer_0_attn_out_1
	layer_0_attn_out_1 -> layer_0_attn_allreduce_0
	layer_0_input -> layer_0_qkv_2
	layer_0_qkv_2 -> layer_0_attn_scores_2
	layer_0_attn_scores_2 -> layer_0_attn_softmax_2
	layer_0_attn_softmax_2 -> layer_0_attn_out_2
	layer_0_attn_out_2 -> layer_0_attn_allreduce_0
	layer_0_input -> layer_0_qkv_3
	layer_0_qkv_3 -> layer_0_attn_scores_3
	layer_0_attn_scores_3 -> layer_0_attn_softmax_3
	layer_0_attn_softmax_3 -> layer_0_attn_out_3
	layer_0_attn_out_3 -> layer_0_attn_allreduce_0
	layer_0_qkv_4 [label="Layer0_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_5 [label="Layer0_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_6 [label="Layer0_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_7 [label="Layer0_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_4 [label="Layer0_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_5 [label="Layer0_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_6 [label="Layer0_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_7 [label="Layer0_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_4 [label="Layer0_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_5 [label="Layer0_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_6 [label="Layer0_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_7 [label="Layer0_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_4 [label="Layer0_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_5 [label="Layer0_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_6 [label="Layer0_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_7 [label="Layer0_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_1 [label="Layer0_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_4
	layer_0_qkv_4 -> layer_0_attn_scores_4
	layer_0_attn_scores_4 -> layer_0_attn_softmax_4
	layer_0_attn_softmax_4 -> layer_0_attn_out_4
	layer_0_attn_out_4 -> layer_0_attn_allreduce_1
	layer_0_input -> layer_0_qkv_5
	layer_0_qkv_5 -> layer_0_attn_scores_5
	layer_0_attn_scores_5 -> layer_0_attn_softmax_5
	layer_0_attn_softmax_5 -> layer_0_attn_out_5
	layer_0_attn_out_5 -> layer_0_attn_allreduce_1
	layer_0_input -> layer_0_qkv_6
	layer_0_qkv_6 -> layer_0_attn_scores_6
	layer_0_attn_scores_6 -> layer_0_attn_softmax_6
	layer_0_attn_softmax_6 -> layer_0_attn_out_6
	layer_0_attn_out_6 -> layer_0_attn_allreduce_1
	layer_0_input -> layer_0_qkv_7
	layer_0_qkv_7 -> layer_0_attn_scores_7
	layer_0_attn_scores_7 -> layer_0_attn_softmax_7
	layer_0_attn_softmax_7 -> layer_0_attn_out_7
	layer_0_attn_out_7 -> layer_0_attn_allreduce_1
	layer_0_qkv_8 [label="Layer0_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_9 [label="Layer0_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_10 [label="Layer0_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_11 [label="Layer0_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_8 [label="Layer0_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_9 [label="Layer0_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_10 [label="Layer0_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_11 [label="Layer0_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_8 [label="Layer0_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_9 [label="Layer0_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_10 [label="Layer0_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_11 [label="Layer0_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_8 [label="Layer0_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_9 [label="Layer0_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_10 [label="Layer0_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_11 [label="Layer0_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_2 [label="Layer0_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_8
	layer_0_qkv_8 -> layer_0_attn_scores_8
	layer_0_attn_scores_8 -> layer_0_attn_softmax_8
	layer_0_attn_softmax_8 -> layer_0_attn_out_8
	layer_0_attn_out_8 -> layer_0_attn_allreduce_2
	layer_0_input -> layer_0_qkv_9
	layer_0_qkv_9 -> layer_0_attn_scores_9
	layer_0_attn_scores_9 -> layer_0_attn_softmax_9
	layer_0_attn_softmax_9 -> layer_0_attn_out_9
	layer_0_attn_out_9 -> layer_0_attn_allreduce_2
	layer_0_input -> layer_0_qkv_10
	layer_0_qkv_10 -> layer_0_attn_scores_10
	layer_0_attn_scores_10 -> layer_0_attn_softmax_10
	layer_0_attn_softmax_10 -> layer_0_attn_out_10
	layer_0_attn_out_10 -> layer_0_attn_allreduce_2
	layer_0_input -> layer_0_qkv_11
	layer_0_qkv_11 -> layer_0_attn_scores_11
	layer_0_attn_scores_11 -> layer_0_attn_softmax_11
	layer_0_attn_softmax_11 -> layer_0_attn_out_11
	layer_0_attn_out_11 -> layer_0_attn_allreduce_2
	layer_0_qkv_12 [label="Layer0_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_13 [label="Layer0_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_14 [label="Layer0_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_15 [label="Layer0_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_12 [label="Layer0_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_13 [label="Layer0_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_14 [label="Layer0_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_15 [label="Layer0_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_12 [label="Layer0_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_13 [label="Layer0_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_14 [label="Layer0_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_15 [label="Layer0_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_12 [label="Layer0_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_13 [label="Layer0_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_14 [label="Layer0_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_15 [label="Layer0_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_3 [label="Layer0_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_12
	layer_0_qkv_12 -> layer_0_attn_scores_12
	layer_0_attn_scores_12 -> layer_0_attn_softmax_12
	layer_0_attn_softmax_12 -> layer_0_attn_out_12
	layer_0_attn_out_12 -> layer_0_attn_allreduce_3
	layer_0_input -> layer_0_qkv_13
	layer_0_qkv_13 -> layer_0_attn_scores_13
	layer_0_attn_scores_13 -> layer_0_attn_softmax_13
	layer_0_attn_softmax_13 -> layer_0_attn_out_13
	layer_0_attn_out_13 -> layer_0_attn_allreduce_3
	layer_0_input -> layer_0_qkv_14
	layer_0_qkv_14 -> layer_0_attn_scores_14
	layer_0_attn_scores_14 -> layer_0_attn_softmax_14
	layer_0_attn_softmax_14 -> layer_0_attn_out_14
	layer_0_attn_out_14 -> layer_0_attn_allreduce_3
	layer_0_input -> layer_0_qkv_15
	layer_0_qkv_15 -> layer_0_attn_scores_15
	layer_0_attn_scores_15 -> layer_0_attn_softmax_15
	layer_0_attn_softmax_15 -> layer_0_attn_out_15
	layer_0_attn_out_15 -> layer_0_attn_allreduce_3
	layer_0_qkv_16 [label="Layer0_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_17 [label="Layer0_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_18 [label="Layer0_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_19 [label="Layer0_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_16 [label="Layer0_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_17 [label="Layer0_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_18 [label="Layer0_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_19 [label="Layer0_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_16 [label="Layer0_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_17 [label="Layer0_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_18 [label="Layer0_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_19 [label="Layer0_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_16 [label="Layer0_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_17 [label="Layer0_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_18 [label="Layer0_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_19 [label="Layer0_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_4 [label="Layer0_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_16
	layer_0_qkv_16 -> layer_0_attn_scores_16
	layer_0_attn_scores_16 -> layer_0_attn_softmax_16
	layer_0_attn_softmax_16 -> layer_0_attn_out_16
	layer_0_attn_out_16 -> layer_0_attn_allreduce_4
	layer_0_input -> layer_0_qkv_17
	layer_0_qkv_17 -> layer_0_attn_scores_17
	layer_0_attn_scores_17 -> layer_0_attn_softmax_17
	layer_0_attn_softmax_17 -> layer_0_attn_out_17
	layer_0_attn_out_17 -> layer_0_attn_allreduce_4
	layer_0_input -> layer_0_qkv_18
	layer_0_qkv_18 -> layer_0_attn_scores_18
	layer_0_attn_scores_18 -> layer_0_attn_softmax_18
	layer_0_attn_softmax_18 -> layer_0_attn_out_18
	layer_0_attn_out_18 -> layer_0_attn_allreduce_4
	layer_0_input -> layer_0_qkv_19
	layer_0_qkv_19 -> layer_0_attn_scores_19
	layer_0_attn_scores_19 -> layer_0_attn_softmax_19
	layer_0_attn_softmax_19 -> layer_0_attn_out_19
	layer_0_attn_out_19 -> layer_0_attn_allreduce_4
	layer_0_qkv_20 [label="Layer0_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_21 [label="Layer0_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_22 [label="Layer0_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_23 [label="Layer0_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_20 [label="Layer0_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_21 [label="Layer0_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_22 [label="Layer0_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_23 [label="Layer0_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_20 [label="Layer0_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_21 [label="Layer0_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_22 [label="Layer0_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_23 [label="Layer0_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_20 [label="Layer0_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_21 [label="Layer0_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_22 [label="Layer0_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_23 [label="Layer0_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_5 [label="Layer0_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_20
	layer_0_qkv_20 -> layer_0_attn_scores_20
	layer_0_attn_scores_20 -> layer_0_attn_softmax_20
	layer_0_attn_softmax_20 -> layer_0_attn_out_20
	layer_0_attn_out_20 -> layer_0_attn_allreduce_5
	layer_0_input -> layer_0_qkv_21
	layer_0_qkv_21 -> layer_0_attn_scores_21
	layer_0_attn_scores_21 -> layer_0_attn_softmax_21
	layer_0_attn_softmax_21 -> layer_0_attn_out_21
	layer_0_attn_out_21 -> layer_0_attn_allreduce_5
	layer_0_input -> layer_0_qkv_22
	layer_0_qkv_22 -> layer_0_attn_scores_22
	layer_0_attn_scores_22 -> layer_0_attn_softmax_22
	layer_0_attn_softmax_22 -> layer_0_attn_out_22
	layer_0_attn_out_22 -> layer_0_attn_allreduce_5
	layer_0_input -> layer_0_qkv_23
	layer_0_qkv_23 -> layer_0_attn_scores_23
	layer_0_attn_scores_23 -> layer_0_attn_softmax_23
	layer_0_attn_softmax_23 -> layer_0_attn_out_23
	layer_0_attn_out_23 -> layer_0_attn_allreduce_5
	layer_0_qkv_24 [label="Layer0_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_25 [label="Layer0_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_26 [label="Layer0_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_27 [label="Layer0_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_24 [label="Layer0_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_25 [label="Layer0_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_26 [label="Layer0_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_27 [label="Layer0_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_24 [label="Layer0_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_25 [label="Layer0_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_26 [label="Layer0_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_27 [label="Layer0_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_24 [label="Layer0_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_25 [label="Layer0_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_26 [label="Layer0_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_27 [label="Layer0_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_6 [label="Layer0_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_24
	layer_0_qkv_24 -> layer_0_attn_scores_24
	layer_0_attn_scores_24 -> layer_0_attn_softmax_24
	layer_0_attn_softmax_24 -> layer_0_attn_out_24
	layer_0_attn_out_24 -> layer_0_attn_allreduce_6
	layer_0_input -> layer_0_qkv_25
	layer_0_qkv_25 -> layer_0_attn_scores_25
	layer_0_attn_scores_25 -> layer_0_attn_softmax_25
	layer_0_attn_softmax_25 -> layer_0_attn_out_25
	layer_0_attn_out_25 -> layer_0_attn_allreduce_6
	layer_0_input -> layer_0_qkv_26
	layer_0_qkv_26 -> layer_0_attn_scores_26
	layer_0_attn_scores_26 -> layer_0_attn_softmax_26
	layer_0_attn_softmax_26 -> layer_0_attn_out_26
	layer_0_attn_out_26 -> layer_0_attn_allreduce_6
	layer_0_input -> layer_0_qkv_27
	layer_0_qkv_27 -> layer_0_attn_scores_27
	layer_0_attn_scores_27 -> layer_0_attn_softmax_27
	layer_0_attn_softmax_27 -> layer_0_attn_out_27
	layer_0_attn_out_27 -> layer_0_attn_allreduce_6
	layer_0_qkv_28 [label="Layer0_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_29 [label="Layer0_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_30 [label="Layer0_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_31 [label="Layer0_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_28 [label="Layer0_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_29 [label="Layer0_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_30 [label="Layer0_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_31 [label="Layer0_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_28 [label="Layer0_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_29 [label="Layer0_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_30 [label="Layer0_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_31 [label="Layer0_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_28 [label="Layer0_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_29 [label="Layer0_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_30 [label="Layer0_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_31 [label="Layer0_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_7 [label="Layer0_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_28
	layer_0_qkv_28 -> layer_0_attn_scores_28
	layer_0_attn_scores_28 -> layer_0_attn_softmax_28
	layer_0_attn_softmax_28 -> layer_0_attn_out_28
	layer_0_attn_out_28 -> layer_0_attn_allreduce_7
	layer_0_input -> layer_0_qkv_29
	layer_0_qkv_29 -> layer_0_attn_scores_29
	layer_0_attn_scores_29 -> layer_0_attn_softmax_29
	layer_0_attn_softmax_29 -> layer_0_attn_out_29
	layer_0_attn_out_29 -> layer_0_attn_allreduce_7
	layer_0_input -> layer_0_qkv_30
	layer_0_qkv_30 -> layer_0_attn_scores_30
	layer_0_attn_scores_30 -> layer_0_attn_softmax_30
	layer_0_attn_softmax_30 -> layer_0_attn_out_30
	layer_0_attn_out_30 -> layer_0_attn_allreduce_7
	layer_0_input -> layer_0_qkv_31
	layer_0_qkv_31 -> layer_0_attn_scores_31
	layer_0_attn_scores_31 -> layer_0_attn_softmax_31
	layer_0_attn_softmax_31 -> layer_0_attn_out_31
	layer_0_attn_out_31 -> layer_0_attn_allreduce_7
	layer_0_qkv_32 [label="Layer0_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_33 [label="Layer0_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_34 [label="Layer0_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_35 [label="Layer0_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_32 [label="Layer0_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_33 [label="Layer0_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_34 [label="Layer0_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_35 [label="Layer0_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_32 [label="Layer0_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_33 [label="Layer0_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_34 [label="Layer0_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_35 [label="Layer0_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_32 [label="Layer0_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_33 [label="Layer0_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_34 [label="Layer0_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_35 [label="Layer0_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_8 [label="Layer0_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_32
	layer_0_qkv_32 -> layer_0_attn_scores_32
	layer_0_attn_scores_32 -> layer_0_attn_softmax_32
	layer_0_attn_softmax_32 -> layer_0_attn_out_32
	layer_0_attn_out_32 -> layer_0_attn_allreduce_8
	layer_0_input -> layer_0_qkv_33
	layer_0_qkv_33 -> layer_0_attn_scores_33
	layer_0_attn_scores_33 -> layer_0_attn_softmax_33
	layer_0_attn_softmax_33 -> layer_0_attn_out_33
	layer_0_attn_out_33 -> layer_0_attn_allreduce_8
	layer_0_input -> layer_0_qkv_34
	layer_0_qkv_34 -> layer_0_attn_scores_34
	layer_0_attn_scores_34 -> layer_0_attn_softmax_34
	layer_0_attn_softmax_34 -> layer_0_attn_out_34
	layer_0_attn_out_34 -> layer_0_attn_allreduce_8
	layer_0_input -> layer_0_qkv_35
	layer_0_qkv_35 -> layer_0_attn_scores_35
	layer_0_attn_scores_35 -> layer_0_attn_softmax_35
	layer_0_attn_softmax_35 -> layer_0_attn_out_35
	layer_0_attn_out_35 -> layer_0_attn_allreduce_8
	layer_0_qkv_36 [label="Layer0_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_37 [label="Layer0_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_38 [label="Layer0_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_39 [label="Layer0_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_36 [label="Layer0_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_37 [label="Layer0_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_38 [label="Layer0_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_39 [label="Layer0_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_36 [label="Layer0_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_37 [label="Layer0_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_38 [label="Layer0_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_39 [label="Layer0_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_36 [label="Layer0_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_37 [label="Layer0_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_38 [label="Layer0_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_39 [label="Layer0_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_9 [label="Layer0_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_36
	layer_0_qkv_36 -> layer_0_attn_scores_36
	layer_0_attn_scores_36 -> layer_0_attn_softmax_36
	layer_0_attn_softmax_36 -> layer_0_attn_out_36
	layer_0_attn_out_36 -> layer_0_attn_allreduce_9
	layer_0_input -> layer_0_qkv_37
	layer_0_qkv_37 -> layer_0_attn_scores_37
	layer_0_attn_scores_37 -> layer_0_attn_softmax_37
	layer_0_attn_softmax_37 -> layer_0_attn_out_37
	layer_0_attn_out_37 -> layer_0_attn_allreduce_9
	layer_0_input -> layer_0_qkv_38
	layer_0_qkv_38 -> layer_0_attn_scores_38
	layer_0_attn_scores_38 -> layer_0_attn_softmax_38
	layer_0_attn_softmax_38 -> layer_0_attn_out_38
	layer_0_attn_out_38 -> layer_0_attn_allreduce_9
	layer_0_input -> layer_0_qkv_39
	layer_0_qkv_39 -> layer_0_attn_scores_39
	layer_0_attn_scores_39 -> layer_0_attn_softmax_39
	layer_0_attn_softmax_39 -> layer_0_attn_out_39
	layer_0_attn_out_39 -> layer_0_attn_allreduce_9
	layer_0_qkv_40 [label="Layer0_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_41 [label="Layer0_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_42 [label="Layer0_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_43 [label="Layer0_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_40 [label="Layer0_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_41 [label="Layer0_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_42 [label="Layer0_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_43 [label="Layer0_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_40 [label="Layer0_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_41 [label="Layer0_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_42 [label="Layer0_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_43 [label="Layer0_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_40 [label="Layer0_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_41 [label="Layer0_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_42 [label="Layer0_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_43 [label="Layer0_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_10 [label="Layer0_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_40
	layer_0_qkv_40 -> layer_0_attn_scores_40
	layer_0_attn_scores_40 -> layer_0_attn_softmax_40
	layer_0_attn_softmax_40 -> layer_0_attn_out_40
	layer_0_attn_out_40 -> layer_0_attn_allreduce_10
	layer_0_input -> layer_0_qkv_41
	layer_0_qkv_41 -> layer_0_attn_scores_41
	layer_0_attn_scores_41 -> layer_0_attn_softmax_41
	layer_0_attn_softmax_41 -> layer_0_attn_out_41
	layer_0_attn_out_41 -> layer_0_attn_allreduce_10
	layer_0_input -> layer_0_qkv_42
	layer_0_qkv_42 -> layer_0_attn_scores_42
	layer_0_attn_scores_42 -> layer_0_attn_softmax_42
	layer_0_attn_softmax_42 -> layer_0_attn_out_42
	layer_0_attn_out_42 -> layer_0_attn_allreduce_10
	layer_0_input -> layer_0_qkv_43
	layer_0_qkv_43 -> layer_0_attn_scores_43
	layer_0_attn_scores_43 -> layer_0_attn_softmax_43
	layer_0_attn_softmax_43 -> layer_0_attn_out_43
	layer_0_attn_out_43 -> layer_0_attn_allreduce_10
	layer_0_qkv_44 [label="Layer0_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_45 [label="Layer0_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_46 [label="Layer0_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_47 [label="Layer0_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_44 [label="Layer0_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_45 [label="Layer0_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_46 [label="Layer0_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_47 [label="Layer0_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_44 [label="Layer0_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_45 [label="Layer0_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_46 [label="Layer0_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_47 [label="Layer0_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_44 [label="Layer0_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_45 [label="Layer0_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_46 [label="Layer0_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_47 [label="Layer0_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_11 [label="Layer0_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_44
	layer_0_qkv_44 -> layer_0_attn_scores_44
	layer_0_attn_scores_44 -> layer_0_attn_softmax_44
	layer_0_attn_softmax_44 -> layer_0_attn_out_44
	layer_0_attn_out_44 -> layer_0_attn_allreduce_11
	layer_0_input -> layer_0_qkv_45
	layer_0_qkv_45 -> layer_0_attn_scores_45
	layer_0_attn_scores_45 -> layer_0_attn_softmax_45
	layer_0_attn_softmax_45 -> layer_0_attn_out_45
	layer_0_attn_out_45 -> layer_0_attn_allreduce_11
	layer_0_input -> layer_0_qkv_46
	layer_0_qkv_46 -> layer_0_attn_scores_46
	layer_0_attn_scores_46 -> layer_0_attn_softmax_46
	layer_0_attn_softmax_46 -> layer_0_attn_out_46
	layer_0_attn_out_46 -> layer_0_attn_allreduce_11
	layer_0_input -> layer_0_qkv_47
	layer_0_qkv_47 -> layer_0_attn_scores_47
	layer_0_attn_scores_47 -> layer_0_attn_softmax_47
	layer_0_attn_softmax_47 -> layer_0_attn_out_47
	layer_0_attn_out_47 -> layer_0_attn_allreduce_11
	layer_0_qkv_48 [label="Layer0_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_49 [label="Layer0_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_50 [label="Layer0_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_51 [label="Layer0_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_48 [label="Layer0_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_49 [label="Layer0_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_50 [label="Layer0_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_51 [label="Layer0_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_48 [label="Layer0_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_49 [label="Layer0_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_50 [label="Layer0_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_51 [label="Layer0_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_48 [label="Layer0_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_49 [label="Layer0_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_50 [label="Layer0_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_51 [label="Layer0_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_12 [label="Layer0_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_48
	layer_0_qkv_48 -> layer_0_attn_scores_48
	layer_0_attn_scores_48 -> layer_0_attn_softmax_48
	layer_0_attn_softmax_48 -> layer_0_attn_out_48
	layer_0_attn_out_48 -> layer_0_attn_allreduce_12
	layer_0_input -> layer_0_qkv_49
	layer_0_qkv_49 -> layer_0_attn_scores_49
	layer_0_attn_scores_49 -> layer_0_attn_softmax_49
	layer_0_attn_softmax_49 -> layer_0_attn_out_49
	layer_0_attn_out_49 -> layer_0_attn_allreduce_12
	layer_0_input -> layer_0_qkv_50
	layer_0_qkv_50 -> layer_0_attn_scores_50
	layer_0_attn_scores_50 -> layer_0_attn_softmax_50
	layer_0_attn_softmax_50 -> layer_0_attn_out_50
	layer_0_attn_out_50 -> layer_0_attn_allreduce_12
	layer_0_input -> layer_0_qkv_51
	layer_0_qkv_51 -> layer_0_attn_scores_51
	layer_0_attn_scores_51 -> layer_0_attn_softmax_51
	layer_0_attn_softmax_51 -> layer_0_attn_out_51
	layer_0_attn_out_51 -> layer_0_attn_allreduce_12
	layer_0_qkv_52 [label="Layer0_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_53 [label="Layer0_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_54 [label="Layer0_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_55 [label="Layer0_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_52 [label="Layer0_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_53 [label="Layer0_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_54 [label="Layer0_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_55 [label="Layer0_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_52 [label="Layer0_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_53 [label="Layer0_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_54 [label="Layer0_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_55 [label="Layer0_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_52 [label="Layer0_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_53 [label="Layer0_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_54 [label="Layer0_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_55 [label="Layer0_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_13 [label="Layer0_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_52
	layer_0_qkv_52 -> layer_0_attn_scores_52
	layer_0_attn_scores_52 -> layer_0_attn_softmax_52
	layer_0_attn_softmax_52 -> layer_0_attn_out_52
	layer_0_attn_out_52 -> layer_0_attn_allreduce_13
	layer_0_input -> layer_0_qkv_53
	layer_0_qkv_53 -> layer_0_attn_scores_53
	layer_0_attn_scores_53 -> layer_0_attn_softmax_53
	layer_0_attn_softmax_53 -> layer_0_attn_out_53
	layer_0_attn_out_53 -> layer_0_attn_allreduce_13
	layer_0_input -> layer_0_qkv_54
	layer_0_qkv_54 -> layer_0_attn_scores_54
	layer_0_attn_scores_54 -> layer_0_attn_softmax_54
	layer_0_attn_softmax_54 -> layer_0_attn_out_54
	layer_0_attn_out_54 -> layer_0_attn_allreduce_13
	layer_0_input -> layer_0_qkv_55
	layer_0_qkv_55 -> layer_0_attn_scores_55
	layer_0_attn_scores_55 -> layer_0_attn_softmax_55
	layer_0_attn_softmax_55 -> layer_0_attn_out_55
	layer_0_attn_out_55 -> layer_0_attn_allreduce_13
	layer_0_qkv_56 [label="Layer0_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_57 [label="Layer0_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_58 [label="Layer0_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_59 [label="Layer0_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_56 [label="Layer0_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_57 [label="Layer0_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_58 [label="Layer0_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_59 [label="Layer0_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_56 [label="Layer0_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_57 [label="Layer0_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_58 [label="Layer0_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_59 [label="Layer0_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_56 [label="Layer0_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_57 [label="Layer0_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_58 [label="Layer0_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_59 [label="Layer0_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_14 [label="Layer0_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_56
	layer_0_qkv_56 -> layer_0_attn_scores_56
	layer_0_attn_scores_56 -> layer_0_attn_softmax_56
	layer_0_attn_softmax_56 -> layer_0_attn_out_56
	layer_0_attn_out_56 -> layer_0_attn_allreduce_14
	layer_0_input -> layer_0_qkv_57
	layer_0_qkv_57 -> layer_0_attn_scores_57
	layer_0_attn_scores_57 -> layer_0_attn_softmax_57
	layer_0_attn_softmax_57 -> layer_0_attn_out_57
	layer_0_attn_out_57 -> layer_0_attn_allreduce_14
	layer_0_input -> layer_0_qkv_58
	layer_0_qkv_58 -> layer_0_attn_scores_58
	layer_0_attn_scores_58 -> layer_0_attn_softmax_58
	layer_0_attn_softmax_58 -> layer_0_attn_out_58
	layer_0_attn_out_58 -> layer_0_attn_allreduce_14
	layer_0_input -> layer_0_qkv_59
	layer_0_qkv_59 -> layer_0_attn_scores_59
	layer_0_attn_scores_59 -> layer_0_attn_softmax_59
	layer_0_attn_softmax_59 -> layer_0_attn_out_59
	layer_0_attn_out_59 -> layer_0_attn_allreduce_14
	layer_0_qkv_60 [label="Layer0_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_61 [label="Layer0_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_62 [label="Layer0_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_qkv_63 [label="Layer0_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_60 [label="Layer0_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_61 [label="Layer0_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_62 [label="Layer0_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_scores_63 [label="Layer0_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_60 [label="Layer0_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_61 [label="Layer0_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_62 [label="Layer0_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_softmax_63 [label="Layer0_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_60 [label="Layer0_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_61 [label="Layer0_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_62 [label="Layer0_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_out_63 [label="Layer0_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_15 [label="Layer0_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_input -> layer_0_qkv_60
	layer_0_qkv_60 -> layer_0_attn_scores_60
	layer_0_attn_scores_60 -> layer_0_attn_softmax_60
	layer_0_attn_softmax_60 -> layer_0_attn_out_60
	layer_0_attn_out_60 -> layer_0_attn_allreduce_15
	layer_0_input -> layer_0_qkv_61
	layer_0_qkv_61 -> layer_0_attn_scores_61
	layer_0_attn_scores_61 -> layer_0_attn_softmax_61
	layer_0_attn_softmax_61 -> layer_0_attn_out_61
	layer_0_attn_out_61 -> layer_0_attn_allreduce_15
	layer_0_input -> layer_0_qkv_62
	layer_0_qkv_62 -> layer_0_attn_scores_62
	layer_0_attn_scores_62 -> layer_0_attn_softmax_62
	layer_0_attn_softmax_62 -> layer_0_attn_out_62
	layer_0_attn_out_62 -> layer_0_attn_allreduce_15
	layer_0_input -> layer_0_qkv_63
	layer_0_qkv_63 -> layer_0_attn_scores_63
	layer_0_attn_scores_63 -> layer_0_attn_softmax_63
	layer_0_attn_softmax_63 -> layer_0_attn_out_63
	layer_0_attn_out_63 -> layer_0_attn_allreduce_15
	layer_0_gate_0 [label="Layer0_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_4 [label="Layer0_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_8 [label="Layer0_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_12 [label="Layer0_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_16 [label="Layer0_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_20 [label="Layer0_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_24 [label="Layer0_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_28 [label="Layer0_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_32 [label="Layer0_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_36 [label="Layer0_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_40 [label="Layer0_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_44 [label="Layer0_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_48 [label="Layer0_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_52 [label="Layer0_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_56 [label="Layer0_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_gate_60 [label="Layer0_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_0_alltoall [label="Layer0_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_0 [label="Layer0_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_1 [label="Layer0_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_2 [label="Layer0_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_3 [label="Layer0_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_0 [label="Layer0_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_1 [label="Layer0_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_2 [label="Layer0_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_3 [label="Layer0_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_0 [label="Layer0_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_1 [label="Layer0_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_2 [label="Layer0_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_3 [label="Layer0_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_0 [label="Layer0_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_4 [label="Layer0_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_5 [label="Layer0_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_6 [label="Layer0_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_7 [label="Layer0_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_4 [label="Layer0_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_5 [label="Layer0_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_6 [label="Layer0_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_7 [label="Layer0_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_4 [label="Layer0_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_5 [label="Layer0_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_6 [label="Layer0_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_7 [label="Layer0_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_1 [label="Layer0_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_8 [label="Layer0_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_9 [label="Layer0_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_10 [label="Layer0_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_11 [label="Layer0_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_8 [label="Layer0_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_9 [label="Layer0_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_10 [label="Layer0_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_11 [label="Layer0_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_8 [label="Layer0_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_9 [label="Layer0_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_10 [label="Layer0_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_11 [label="Layer0_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_2 [label="Layer0_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_12 [label="Layer0_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_13 [label="Layer0_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_14 [label="Layer0_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_15 [label="Layer0_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_12 [label="Layer0_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_13 [label="Layer0_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_14 [label="Layer0_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_15 [label="Layer0_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_12 [label="Layer0_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_13 [label="Layer0_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_14 [label="Layer0_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_15 [label="Layer0_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_3 [label="Layer0_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_16 [label="Layer0_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_17 [label="Layer0_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_18 [label="Layer0_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_19 [label="Layer0_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_16 [label="Layer0_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_17 [label="Layer0_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_18 [label="Layer0_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_19 [label="Layer0_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_16 [label="Layer0_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_17 [label="Layer0_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_18 [label="Layer0_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_19 [label="Layer0_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_4 [label="Layer0_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_20 [label="Layer0_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_21 [label="Layer0_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_22 [label="Layer0_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_23 [label="Layer0_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_20 [label="Layer0_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_21 [label="Layer0_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_22 [label="Layer0_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_23 [label="Layer0_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_20 [label="Layer0_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_21 [label="Layer0_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_22 [label="Layer0_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_23 [label="Layer0_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_5 [label="Layer0_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_24 [label="Layer0_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_25 [label="Layer0_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_26 [label="Layer0_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_27 [label="Layer0_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_24 [label="Layer0_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_25 [label="Layer0_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_26 [label="Layer0_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_27 [label="Layer0_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_24 [label="Layer0_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_25 [label="Layer0_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_26 [label="Layer0_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_27 [label="Layer0_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_6 [label="Layer0_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_28 [label="Layer0_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_29 [label="Layer0_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_30 [label="Layer0_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_31 [label="Layer0_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_28 [label="Layer0_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_29 [label="Layer0_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_30 [label="Layer0_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_31 [label="Layer0_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_28 [label="Layer0_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_29 [label="Layer0_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_30 [label="Layer0_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_31 [label="Layer0_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_7 [label="Layer0_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_32 [label="Layer0_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_33 [label="Layer0_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_34 [label="Layer0_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_35 [label="Layer0_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_32 [label="Layer0_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_33 [label="Layer0_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_34 [label="Layer0_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_35 [label="Layer0_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_32 [label="Layer0_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_33 [label="Layer0_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_34 [label="Layer0_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_35 [label="Layer0_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_8 [label="Layer0_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_36 [label="Layer0_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_37 [label="Layer0_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_38 [label="Layer0_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_39 [label="Layer0_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_36 [label="Layer0_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_37 [label="Layer0_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_38 [label="Layer0_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_39 [label="Layer0_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_36 [label="Layer0_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_37 [label="Layer0_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_38 [label="Layer0_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_39 [label="Layer0_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_9 [label="Layer0_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_40 [label="Layer0_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_41 [label="Layer0_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_42 [label="Layer0_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_43 [label="Layer0_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_40 [label="Layer0_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_41 [label="Layer0_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_42 [label="Layer0_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_43 [label="Layer0_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_40 [label="Layer0_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_41 [label="Layer0_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_42 [label="Layer0_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_43 [label="Layer0_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_10 [label="Layer0_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_44 [label="Layer0_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_45 [label="Layer0_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_46 [label="Layer0_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_47 [label="Layer0_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_44 [label="Layer0_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_45 [label="Layer0_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_46 [label="Layer0_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_47 [label="Layer0_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_44 [label="Layer0_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_45 [label="Layer0_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_46 [label="Layer0_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_47 [label="Layer0_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_11 [label="Layer0_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_48 [label="Layer0_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_49 [label="Layer0_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_50 [label="Layer0_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_51 [label="Layer0_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_48 [label="Layer0_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_49 [label="Layer0_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_50 [label="Layer0_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_51 [label="Layer0_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_48 [label="Layer0_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_49 [label="Layer0_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_50 [label="Layer0_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_51 [label="Layer0_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_12 [label="Layer0_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_52 [label="Layer0_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_53 [label="Layer0_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_54 [label="Layer0_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_55 [label="Layer0_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_52 [label="Layer0_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_53 [label="Layer0_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_54 [label="Layer0_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_55 [label="Layer0_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_52 [label="Layer0_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_53 [label="Layer0_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_54 [label="Layer0_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_55 [label="Layer0_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_13 [label="Layer0_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_56 [label="Layer0_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_57 [label="Layer0_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_58 [label="Layer0_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_59 [label="Layer0_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_56 [label="Layer0_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_57 [label="Layer0_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_58 [label="Layer0_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_59 [label="Layer0_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_56 [label="Layer0_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_57 [label="Layer0_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_58 [label="Layer0_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_59 [label="Layer0_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_14 [label="Layer0_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert1_60 [label="Layer0_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_61 [label="Layer0_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_62 [label="Layer0_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert1_63 [label="Layer0_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_60 [label="Layer0_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_61 [label="Layer0_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_62 [label="Layer0_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert_act_63 [label="Layer0_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_0_expert2_60 [label="Layer0_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_61 [label="Layer0_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_62 [label="Layer0_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert2_63 [label="Layer0_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_0_expert_allreduce_15 [label="Layer0_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_0_expert_agg [label="Layer0_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_0_norm [label="Layer0_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_0_attn_allreduce_0 -> layer_0_gate_0 [style=dashed]
	layer_0_gate_0 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_0
	layer_0_expert1_0 -> layer_0_expert_act_0
	layer_0_expert_act_0 -> layer_0_expert2_0
	layer_0_expert2_0 -> layer_0_expert_allreduce_0
	layer_0_alltoall -> layer_0_expert1_1
	layer_0_expert1_1 -> layer_0_expert_act_1
	layer_0_expert_act_1 -> layer_0_expert2_1
	layer_0_expert2_1 -> layer_0_expert_allreduce_0
	layer_0_alltoall -> layer_0_expert1_2
	layer_0_expert1_2 -> layer_0_expert_act_2
	layer_0_expert_act_2 -> layer_0_expert2_2
	layer_0_expert2_2 -> layer_0_expert_allreduce_0
	layer_0_alltoall -> layer_0_expert1_3
	layer_0_expert1_3 -> layer_0_expert_act_3
	layer_0_expert_act_3 -> layer_0_expert2_3
	layer_0_expert2_3 -> layer_0_expert_allreduce_0
	layer_0_expert_allreduce_0 -> layer_0_expert_agg
	layer_0_attn_allreduce_1 -> layer_0_gate_4 [style=dashed]
	layer_0_gate_4 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_4
	layer_0_expert1_4 -> layer_0_expert_act_4
	layer_0_expert_act_4 -> layer_0_expert2_4
	layer_0_expert2_4 -> layer_0_expert_allreduce_1
	layer_0_alltoall -> layer_0_expert1_5
	layer_0_expert1_5 -> layer_0_expert_act_5
	layer_0_expert_act_5 -> layer_0_expert2_5
	layer_0_expert2_5 -> layer_0_expert_allreduce_1
	layer_0_alltoall -> layer_0_expert1_6
	layer_0_expert1_6 -> layer_0_expert_act_6
	layer_0_expert_act_6 -> layer_0_expert2_6
	layer_0_expert2_6 -> layer_0_expert_allreduce_1
	layer_0_alltoall -> layer_0_expert1_7
	layer_0_expert1_7 -> layer_0_expert_act_7
	layer_0_expert_act_7 -> layer_0_expert2_7
	layer_0_expert2_7 -> layer_0_expert_allreduce_1
	layer_0_expert_allreduce_1 -> layer_0_expert_agg
	layer_0_attn_allreduce_2 -> layer_0_gate_8 [style=dashed]
	layer_0_gate_8 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_8
	layer_0_expert1_8 -> layer_0_expert_act_8
	layer_0_expert_act_8 -> layer_0_expert2_8
	layer_0_expert2_8 -> layer_0_expert_allreduce_2
	layer_0_alltoall -> layer_0_expert1_9
	layer_0_expert1_9 -> layer_0_expert_act_9
	layer_0_expert_act_9 -> layer_0_expert2_9
	layer_0_expert2_9 -> layer_0_expert_allreduce_2
	layer_0_alltoall -> layer_0_expert1_10
	layer_0_expert1_10 -> layer_0_expert_act_10
	layer_0_expert_act_10 -> layer_0_expert2_10
	layer_0_expert2_10 -> layer_0_expert_allreduce_2
	layer_0_alltoall -> layer_0_expert1_11
	layer_0_expert1_11 -> layer_0_expert_act_11
	layer_0_expert_act_11 -> layer_0_expert2_11
	layer_0_expert2_11 -> layer_0_expert_allreduce_2
	layer_0_expert_allreduce_2 -> layer_0_expert_agg
	layer_0_attn_allreduce_3 -> layer_0_gate_12 [style=dashed]
	layer_0_gate_12 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_12
	layer_0_expert1_12 -> layer_0_expert_act_12
	layer_0_expert_act_12 -> layer_0_expert2_12
	layer_0_expert2_12 -> layer_0_expert_allreduce_3
	layer_0_alltoall -> layer_0_expert1_13
	layer_0_expert1_13 -> layer_0_expert_act_13
	layer_0_expert_act_13 -> layer_0_expert2_13
	layer_0_expert2_13 -> layer_0_expert_allreduce_3
	layer_0_alltoall -> layer_0_expert1_14
	layer_0_expert1_14 -> layer_0_expert_act_14
	layer_0_expert_act_14 -> layer_0_expert2_14
	layer_0_expert2_14 -> layer_0_expert_allreduce_3
	layer_0_alltoall -> layer_0_expert1_15
	layer_0_expert1_15 -> layer_0_expert_act_15
	layer_0_expert_act_15 -> layer_0_expert2_15
	layer_0_expert2_15 -> layer_0_expert_allreduce_3
	layer_0_expert_allreduce_3 -> layer_0_expert_agg
	layer_0_attn_allreduce_4 -> layer_0_gate_16 [style=dashed]
	layer_0_gate_16 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_16
	layer_0_expert1_16 -> layer_0_expert_act_16
	layer_0_expert_act_16 -> layer_0_expert2_16
	layer_0_expert2_16 -> layer_0_expert_allreduce_4
	layer_0_alltoall -> layer_0_expert1_17
	layer_0_expert1_17 -> layer_0_expert_act_17
	layer_0_expert_act_17 -> layer_0_expert2_17
	layer_0_expert2_17 -> layer_0_expert_allreduce_4
	layer_0_alltoall -> layer_0_expert1_18
	layer_0_expert1_18 -> layer_0_expert_act_18
	layer_0_expert_act_18 -> layer_0_expert2_18
	layer_0_expert2_18 -> layer_0_expert_allreduce_4
	layer_0_alltoall -> layer_0_expert1_19
	layer_0_expert1_19 -> layer_0_expert_act_19
	layer_0_expert_act_19 -> layer_0_expert2_19
	layer_0_expert2_19 -> layer_0_expert_allreduce_4
	layer_0_expert_allreduce_4 -> layer_0_expert_agg
	layer_0_attn_allreduce_5 -> layer_0_gate_20 [style=dashed]
	layer_0_gate_20 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_20
	layer_0_expert1_20 -> layer_0_expert_act_20
	layer_0_expert_act_20 -> layer_0_expert2_20
	layer_0_expert2_20 -> layer_0_expert_allreduce_5
	layer_0_alltoall -> layer_0_expert1_21
	layer_0_expert1_21 -> layer_0_expert_act_21
	layer_0_expert_act_21 -> layer_0_expert2_21
	layer_0_expert2_21 -> layer_0_expert_allreduce_5
	layer_0_alltoall -> layer_0_expert1_22
	layer_0_expert1_22 -> layer_0_expert_act_22
	layer_0_expert_act_22 -> layer_0_expert2_22
	layer_0_expert2_22 -> layer_0_expert_allreduce_5
	layer_0_alltoall -> layer_0_expert1_23
	layer_0_expert1_23 -> layer_0_expert_act_23
	layer_0_expert_act_23 -> layer_0_expert2_23
	layer_0_expert2_23 -> layer_0_expert_allreduce_5
	layer_0_expert_allreduce_5 -> layer_0_expert_agg
	layer_0_attn_allreduce_6 -> layer_0_gate_24 [style=dashed]
	layer_0_gate_24 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_24
	layer_0_expert1_24 -> layer_0_expert_act_24
	layer_0_expert_act_24 -> layer_0_expert2_24
	layer_0_expert2_24 -> layer_0_expert_allreduce_6
	layer_0_alltoall -> layer_0_expert1_25
	layer_0_expert1_25 -> layer_0_expert_act_25
	layer_0_expert_act_25 -> layer_0_expert2_25
	layer_0_expert2_25 -> layer_0_expert_allreduce_6
	layer_0_alltoall -> layer_0_expert1_26
	layer_0_expert1_26 -> layer_0_expert_act_26
	layer_0_expert_act_26 -> layer_0_expert2_26
	layer_0_expert2_26 -> layer_0_expert_allreduce_6
	layer_0_alltoall -> layer_0_expert1_27
	layer_0_expert1_27 -> layer_0_expert_act_27
	layer_0_expert_act_27 -> layer_0_expert2_27
	layer_0_expert2_27 -> layer_0_expert_allreduce_6
	layer_0_expert_allreduce_6 -> layer_0_expert_agg
	layer_0_attn_allreduce_7 -> layer_0_gate_28 [style=dashed]
	layer_0_gate_28 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_28
	layer_0_expert1_28 -> layer_0_expert_act_28
	layer_0_expert_act_28 -> layer_0_expert2_28
	layer_0_expert2_28 -> layer_0_expert_allreduce_7
	layer_0_alltoall -> layer_0_expert1_29
	layer_0_expert1_29 -> layer_0_expert_act_29
	layer_0_expert_act_29 -> layer_0_expert2_29
	layer_0_expert2_29 -> layer_0_expert_allreduce_7
	layer_0_alltoall -> layer_0_expert1_30
	layer_0_expert1_30 -> layer_0_expert_act_30
	layer_0_expert_act_30 -> layer_0_expert2_30
	layer_0_expert2_30 -> layer_0_expert_allreduce_7
	layer_0_alltoall -> layer_0_expert1_31
	layer_0_expert1_31 -> layer_0_expert_act_31
	layer_0_expert_act_31 -> layer_0_expert2_31
	layer_0_expert2_31 -> layer_0_expert_allreduce_7
	layer_0_expert_allreduce_7 -> layer_0_expert_agg
	layer_0_attn_allreduce_8 -> layer_0_gate_32 [style=dashed]
	layer_0_gate_32 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_32
	layer_0_expert1_32 -> layer_0_expert_act_32
	layer_0_expert_act_32 -> layer_0_expert2_32
	layer_0_expert2_32 -> layer_0_expert_allreduce_8
	layer_0_alltoall -> layer_0_expert1_33
	layer_0_expert1_33 -> layer_0_expert_act_33
	layer_0_expert_act_33 -> layer_0_expert2_33
	layer_0_expert2_33 -> layer_0_expert_allreduce_8
	layer_0_alltoall -> layer_0_expert1_34
	layer_0_expert1_34 -> layer_0_expert_act_34
	layer_0_expert_act_34 -> layer_0_expert2_34
	layer_0_expert2_34 -> layer_0_expert_allreduce_8
	layer_0_alltoall -> layer_0_expert1_35
	layer_0_expert1_35 -> layer_0_expert_act_35
	layer_0_expert_act_35 -> layer_0_expert2_35
	layer_0_expert2_35 -> layer_0_expert_allreduce_8
	layer_0_expert_allreduce_8 -> layer_0_expert_agg
	layer_0_attn_allreduce_9 -> layer_0_gate_36 [style=dashed]
	layer_0_gate_36 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_36
	layer_0_expert1_36 -> layer_0_expert_act_36
	layer_0_expert_act_36 -> layer_0_expert2_36
	layer_0_expert2_36 -> layer_0_expert_allreduce_9
	layer_0_alltoall -> layer_0_expert1_37
	layer_0_expert1_37 -> layer_0_expert_act_37
	layer_0_expert_act_37 -> layer_0_expert2_37
	layer_0_expert2_37 -> layer_0_expert_allreduce_9
	layer_0_alltoall -> layer_0_expert1_38
	layer_0_expert1_38 -> layer_0_expert_act_38
	layer_0_expert_act_38 -> layer_0_expert2_38
	layer_0_expert2_38 -> layer_0_expert_allreduce_9
	layer_0_alltoall -> layer_0_expert1_39
	layer_0_expert1_39 -> layer_0_expert_act_39
	layer_0_expert_act_39 -> layer_0_expert2_39
	layer_0_expert2_39 -> layer_0_expert_allreduce_9
	layer_0_expert_allreduce_9 -> layer_0_expert_agg
	layer_0_attn_allreduce_10 -> layer_0_gate_40 [style=dashed]
	layer_0_gate_40 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_40
	layer_0_expert1_40 -> layer_0_expert_act_40
	layer_0_expert_act_40 -> layer_0_expert2_40
	layer_0_expert2_40 -> layer_0_expert_allreduce_10
	layer_0_alltoall -> layer_0_expert1_41
	layer_0_expert1_41 -> layer_0_expert_act_41
	layer_0_expert_act_41 -> layer_0_expert2_41
	layer_0_expert2_41 -> layer_0_expert_allreduce_10
	layer_0_alltoall -> layer_0_expert1_42
	layer_0_expert1_42 -> layer_0_expert_act_42
	layer_0_expert_act_42 -> layer_0_expert2_42
	layer_0_expert2_42 -> layer_0_expert_allreduce_10
	layer_0_alltoall -> layer_0_expert1_43
	layer_0_expert1_43 -> layer_0_expert_act_43
	layer_0_expert_act_43 -> layer_0_expert2_43
	layer_0_expert2_43 -> layer_0_expert_allreduce_10
	layer_0_expert_allreduce_10 -> layer_0_expert_agg
	layer_0_attn_allreduce_11 -> layer_0_gate_44 [style=dashed]
	layer_0_gate_44 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_44
	layer_0_expert1_44 -> layer_0_expert_act_44
	layer_0_expert_act_44 -> layer_0_expert2_44
	layer_0_expert2_44 -> layer_0_expert_allreduce_11
	layer_0_alltoall -> layer_0_expert1_45
	layer_0_expert1_45 -> layer_0_expert_act_45
	layer_0_expert_act_45 -> layer_0_expert2_45
	layer_0_expert2_45 -> layer_0_expert_allreduce_11
	layer_0_alltoall -> layer_0_expert1_46
	layer_0_expert1_46 -> layer_0_expert_act_46
	layer_0_expert_act_46 -> layer_0_expert2_46
	layer_0_expert2_46 -> layer_0_expert_allreduce_11
	layer_0_alltoall -> layer_0_expert1_47
	layer_0_expert1_47 -> layer_0_expert_act_47
	layer_0_expert_act_47 -> layer_0_expert2_47
	layer_0_expert2_47 -> layer_0_expert_allreduce_11
	layer_0_expert_allreduce_11 -> layer_0_expert_agg
	layer_0_attn_allreduce_12 -> layer_0_gate_48 [style=dashed]
	layer_0_gate_48 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_48
	layer_0_expert1_48 -> layer_0_expert_act_48
	layer_0_expert_act_48 -> layer_0_expert2_48
	layer_0_expert2_48 -> layer_0_expert_allreduce_12
	layer_0_alltoall -> layer_0_expert1_49
	layer_0_expert1_49 -> layer_0_expert_act_49
	layer_0_expert_act_49 -> layer_0_expert2_49
	layer_0_expert2_49 -> layer_0_expert_allreduce_12
	layer_0_alltoall -> layer_0_expert1_50
	layer_0_expert1_50 -> layer_0_expert_act_50
	layer_0_expert_act_50 -> layer_0_expert2_50
	layer_0_expert2_50 -> layer_0_expert_allreduce_12
	layer_0_alltoall -> layer_0_expert1_51
	layer_0_expert1_51 -> layer_0_expert_act_51
	layer_0_expert_act_51 -> layer_0_expert2_51
	layer_0_expert2_51 -> layer_0_expert_allreduce_12
	layer_0_expert_allreduce_12 -> layer_0_expert_agg
	layer_0_attn_allreduce_13 -> layer_0_gate_52 [style=dashed]
	layer_0_gate_52 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_52
	layer_0_expert1_52 -> layer_0_expert_act_52
	layer_0_expert_act_52 -> layer_0_expert2_52
	layer_0_expert2_52 -> layer_0_expert_allreduce_13
	layer_0_alltoall -> layer_0_expert1_53
	layer_0_expert1_53 -> layer_0_expert_act_53
	layer_0_expert_act_53 -> layer_0_expert2_53
	layer_0_expert2_53 -> layer_0_expert_allreduce_13
	layer_0_alltoall -> layer_0_expert1_54
	layer_0_expert1_54 -> layer_0_expert_act_54
	layer_0_expert_act_54 -> layer_0_expert2_54
	layer_0_expert2_54 -> layer_0_expert_allreduce_13
	layer_0_alltoall -> layer_0_expert1_55
	layer_0_expert1_55 -> layer_0_expert_act_55
	layer_0_expert_act_55 -> layer_0_expert2_55
	layer_0_expert2_55 -> layer_0_expert_allreduce_13
	layer_0_expert_allreduce_13 -> layer_0_expert_agg
	layer_0_attn_allreduce_14 -> layer_0_gate_56 [style=dashed]
	layer_0_gate_56 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_56
	layer_0_expert1_56 -> layer_0_expert_act_56
	layer_0_expert_act_56 -> layer_0_expert2_56
	layer_0_expert2_56 -> layer_0_expert_allreduce_14
	layer_0_alltoall -> layer_0_expert1_57
	layer_0_expert1_57 -> layer_0_expert_act_57
	layer_0_expert_act_57 -> layer_0_expert2_57
	layer_0_expert2_57 -> layer_0_expert_allreduce_14
	layer_0_alltoall -> layer_0_expert1_58
	layer_0_expert1_58 -> layer_0_expert_act_58
	layer_0_expert_act_58 -> layer_0_expert2_58
	layer_0_expert2_58 -> layer_0_expert_allreduce_14
	layer_0_alltoall -> layer_0_expert1_59
	layer_0_expert1_59 -> layer_0_expert_act_59
	layer_0_expert_act_59 -> layer_0_expert2_59
	layer_0_expert2_59 -> layer_0_expert_allreduce_14
	layer_0_expert_allreduce_14 -> layer_0_expert_agg
	layer_0_attn_allreduce_15 -> layer_0_gate_60 [style=dashed]
	layer_0_gate_60 -> layer_0_alltoall [style=dashed]
	layer_0_alltoall -> layer_0_expert1_60
	layer_0_expert1_60 -> layer_0_expert_act_60
	layer_0_expert_act_60 -> layer_0_expert2_60
	layer_0_expert2_60 -> layer_0_expert_allreduce_15
	layer_0_alltoall -> layer_0_expert1_61
	layer_0_expert1_61 -> layer_0_expert_act_61
	layer_0_expert_act_61 -> layer_0_expert2_61
	layer_0_expert2_61 -> layer_0_expert_allreduce_15
	layer_0_alltoall -> layer_0_expert1_62
	layer_0_expert1_62 -> layer_0_expert_act_62
	layer_0_expert_act_62 -> layer_0_expert2_62
	layer_0_expert2_62 -> layer_0_expert_allreduce_15
	layer_0_alltoall -> layer_0_expert1_63
	layer_0_expert1_63 -> layer_0_expert_act_63
	layer_0_expert_act_63 -> layer_0_expert2_63
	layer_0_expert2_63 -> layer_0_expert_allreduce_15
	layer_0_expert_allreduce_15 -> layer_0_expert_agg
	layer_0_expert_agg -> layer_0_norm
	layer_0_norm -> layer_1_input
	layer_1_input [label="Layer1_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_1_qkv_0 [label="Layer1_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_1 [label="Layer1_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_2 [label="Layer1_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_3 [label="Layer1_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_0 [label="Layer1_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_1 [label="Layer1_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_2 [label="Layer1_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_3 [label="Layer1_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_0 [label="Layer1_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_1 [label="Layer1_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_2 [label="Layer1_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_3 [label="Layer1_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_0 [label="Layer1_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_1 [label="Layer1_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_2 [label="Layer1_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_3 [label="Layer1_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_0 [label="Layer1_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_0
	layer_1_qkv_0 -> layer_1_attn_scores_0
	layer_1_attn_scores_0 -> layer_1_attn_softmax_0
	layer_1_attn_softmax_0 -> layer_1_attn_out_0
	layer_1_attn_out_0 -> layer_1_attn_allreduce_0
	layer_1_input -> layer_1_qkv_1
	layer_1_qkv_1 -> layer_1_attn_scores_1
	layer_1_attn_scores_1 -> layer_1_attn_softmax_1
	layer_1_attn_softmax_1 -> layer_1_attn_out_1
	layer_1_attn_out_1 -> layer_1_attn_allreduce_0
	layer_1_input -> layer_1_qkv_2
	layer_1_qkv_2 -> layer_1_attn_scores_2
	layer_1_attn_scores_2 -> layer_1_attn_softmax_2
	layer_1_attn_softmax_2 -> layer_1_attn_out_2
	layer_1_attn_out_2 -> layer_1_attn_allreduce_0
	layer_1_input -> layer_1_qkv_3
	layer_1_qkv_3 -> layer_1_attn_scores_3
	layer_1_attn_scores_3 -> layer_1_attn_softmax_3
	layer_1_attn_softmax_3 -> layer_1_attn_out_3
	layer_1_attn_out_3 -> layer_1_attn_allreduce_0
	layer_1_qkv_4 [label="Layer1_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_5 [label="Layer1_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_6 [label="Layer1_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_7 [label="Layer1_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_4 [label="Layer1_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_5 [label="Layer1_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_6 [label="Layer1_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_7 [label="Layer1_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_4 [label="Layer1_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_5 [label="Layer1_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_6 [label="Layer1_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_7 [label="Layer1_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_4 [label="Layer1_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_5 [label="Layer1_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_6 [label="Layer1_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_7 [label="Layer1_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_1 [label="Layer1_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_4
	layer_1_qkv_4 -> layer_1_attn_scores_4
	layer_1_attn_scores_4 -> layer_1_attn_softmax_4
	layer_1_attn_softmax_4 -> layer_1_attn_out_4
	layer_1_attn_out_4 -> layer_1_attn_allreduce_1
	layer_1_input -> layer_1_qkv_5
	layer_1_qkv_5 -> layer_1_attn_scores_5
	layer_1_attn_scores_5 -> layer_1_attn_softmax_5
	layer_1_attn_softmax_5 -> layer_1_attn_out_5
	layer_1_attn_out_5 -> layer_1_attn_allreduce_1
	layer_1_input -> layer_1_qkv_6
	layer_1_qkv_6 -> layer_1_attn_scores_6
	layer_1_attn_scores_6 -> layer_1_attn_softmax_6
	layer_1_attn_softmax_6 -> layer_1_attn_out_6
	layer_1_attn_out_6 -> layer_1_attn_allreduce_1
	layer_1_input -> layer_1_qkv_7
	layer_1_qkv_7 -> layer_1_attn_scores_7
	layer_1_attn_scores_7 -> layer_1_attn_softmax_7
	layer_1_attn_softmax_7 -> layer_1_attn_out_7
	layer_1_attn_out_7 -> layer_1_attn_allreduce_1
	layer_1_qkv_8 [label="Layer1_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_9 [label="Layer1_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_10 [label="Layer1_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_11 [label="Layer1_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_8 [label="Layer1_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_9 [label="Layer1_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_10 [label="Layer1_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_11 [label="Layer1_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_8 [label="Layer1_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_9 [label="Layer1_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_10 [label="Layer1_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_11 [label="Layer1_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_8 [label="Layer1_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_9 [label="Layer1_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_10 [label="Layer1_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_11 [label="Layer1_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_2 [label="Layer1_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_8
	layer_1_qkv_8 -> layer_1_attn_scores_8
	layer_1_attn_scores_8 -> layer_1_attn_softmax_8
	layer_1_attn_softmax_8 -> layer_1_attn_out_8
	layer_1_attn_out_8 -> layer_1_attn_allreduce_2
	layer_1_input -> layer_1_qkv_9
	layer_1_qkv_9 -> layer_1_attn_scores_9
	layer_1_attn_scores_9 -> layer_1_attn_softmax_9
	layer_1_attn_softmax_9 -> layer_1_attn_out_9
	layer_1_attn_out_9 -> layer_1_attn_allreduce_2
	layer_1_input -> layer_1_qkv_10
	layer_1_qkv_10 -> layer_1_attn_scores_10
	layer_1_attn_scores_10 -> layer_1_attn_softmax_10
	layer_1_attn_softmax_10 -> layer_1_attn_out_10
	layer_1_attn_out_10 -> layer_1_attn_allreduce_2
	layer_1_input -> layer_1_qkv_11
	layer_1_qkv_11 -> layer_1_attn_scores_11
	layer_1_attn_scores_11 -> layer_1_attn_softmax_11
	layer_1_attn_softmax_11 -> layer_1_attn_out_11
	layer_1_attn_out_11 -> layer_1_attn_allreduce_2
	layer_1_qkv_12 [label="Layer1_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_13 [label="Layer1_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_14 [label="Layer1_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_15 [label="Layer1_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_12 [label="Layer1_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_13 [label="Layer1_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_14 [label="Layer1_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_15 [label="Layer1_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_12 [label="Layer1_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_13 [label="Layer1_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_14 [label="Layer1_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_15 [label="Layer1_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_12 [label="Layer1_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_13 [label="Layer1_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_14 [label="Layer1_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_15 [label="Layer1_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_3 [label="Layer1_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_12
	layer_1_qkv_12 -> layer_1_attn_scores_12
	layer_1_attn_scores_12 -> layer_1_attn_softmax_12
	layer_1_attn_softmax_12 -> layer_1_attn_out_12
	layer_1_attn_out_12 -> layer_1_attn_allreduce_3
	layer_1_input -> layer_1_qkv_13
	layer_1_qkv_13 -> layer_1_attn_scores_13
	layer_1_attn_scores_13 -> layer_1_attn_softmax_13
	layer_1_attn_softmax_13 -> layer_1_attn_out_13
	layer_1_attn_out_13 -> layer_1_attn_allreduce_3
	layer_1_input -> layer_1_qkv_14
	layer_1_qkv_14 -> layer_1_attn_scores_14
	layer_1_attn_scores_14 -> layer_1_attn_softmax_14
	layer_1_attn_softmax_14 -> layer_1_attn_out_14
	layer_1_attn_out_14 -> layer_1_attn_allreduce_3
	layer_1_input -> layer_1_qkv_15
	layer_1_qkv_15 -> layer_1_attn_scores_15
	layer_1_attn_scores_15 -> layer_1_attn_softmax_15
	layer_1_attn_softmax_15 -> layer_1_attn_out_15
	layer_1_attn_out_15 -> layer_1_attn_allreduce_3
	layer_1_qkv_16 [label="Layer1_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_17 [label="Layer1_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_18 [label="Layer1_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_19 [label="Layer1_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_16 [label="Layer1_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_17 [label="Layer1_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_18 [label="Layer1_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_19 [label="Layer1_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_16 [label="Layer1_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_17 [label="Layer1_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_18 [label="Layer1_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_19 [label="Layer1_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_16 [label="Layer1_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_17 [label="Layer1_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_18 [label="Layer1_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_19 [label="Layer1_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_4 [label="Layer1_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_16
	layer_1_qkv_16 -> layer_1_attn_scores_16
	layer_1_attn_scores_16 -> layer_1_attn_softmax_16
	layer_1_attn_softmax_16 -> layer_1_attn_out_16
	layer_1_attn_out_16 -> layer_1_attn_allreduce_4
	layer_1_input -> layer_1_qkv_17
	layer_1_qkv_17 -> layer_1_attn_scores_17
	layer_1_attn_scores_17 -> layer_1_attn_softmax_17
	layer_1_attn_softmax_17 -> layer_1_attn_out_17
	layer_1_attn_out_17 -> layer_1_attn_allreduce_4
	layer_1_input -> layer_1_qkv_18
	layer_1_qkv_18 -> layer_1_attn_scores_18
	layer_1_attn_scores_18 -> layer_1_attn_softmax_18
	layer_1_attn_softmax_18 -> layer_1_attn_out_18
	layer_1_attn_out_18 -> layer_1_attn_allreduce_4
	layer_1_input -> layer_1_qkv_19
	layer_1_qkv_19 -> layer_1_attn_scores_19
	layer_1_attn_scores_19 -> layer_1_attn_softmax_19
	layer_1_attn_softmax_19 -> layer_1_attn_out_19
	layer_1_attn_out_19 -> layer_1_attn_allreduce_4
	layer_1_qkv_20 [label="Layer1_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_21 [label="Layer1_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_22 [label="Layer1_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_23 [label="Layer1_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_20 [label="Layer1_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_21 [label="Layer1_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_22 [label="Layer1_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_23 [label="Layer1_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_20 [label="Layer1_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_21 [label="Layer1_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_22 [label="Layer1_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_23 [label="Layer1_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_20 [label="Layer1_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_21 [label="Layer1_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_22 [label="Layer1_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_23 [label="Layer1_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_5 [label="Layer1_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_20
	layer_1_qkv_20 -> layer_1_attn_scores_20
	layer_1_attn_scores_20 -> layer_1_attn_softmax_20
	layer_1_attn_softmax_20 -> layer_1_attn_out_20
	layer_1_attn_out_20 -> layer_1_attn_allreduce_5
	layer_1_input -> layer_1_qkv_21
	layer_1_qkv_21 -> layer_1_attn_scores_21
	layer_1_attn_scores_21 -> layer_1_attn_softmax_21
	layer_1_attn_softmax_21 -> layer_1_attn_out_21
	layer_1_attn_out_21 -> layer_1_attn_allreduce_5
	layer_1_input -> layer_1_qkv_22
	layer_1_qkv_22 -> layer_1_attn_scores_22
	layer_1_attn_scores_22 -> layer_1_attn_softmax_22
	layer_1_attn_softmax_22 -> layer_1_attn_out_22
	layer_1_attn_out_22 -> layer_1_attn_allreduce_5
	layer_1_input -> layer_1_qkv_23
	layer_1_qkv_23 -> layer_1_attn_scores_23
	layer_1_attn_scores_23 -> layer_1_attn_softmax_23
	layer_1_attn_softmax_23 -> layer_1_attn_out_23
	layer_1_attn_out_23 -> layer_1_attn_allreduce_5
	layer_1_qkv_24 [label="Layer1_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_25 [label="Layer1_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_26 [label="Layer1_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_27 [label="Layer1_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_24 [label="Layer1_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_25 [label="Layer1_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_26 [label="Layer1_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_27 [label="Layer1_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_24 [label="Layer1_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_25 [label="Layer1_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_26 [label="Layer1_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_27 [label="Layer1_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_24 [label="Layer1_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_25 [label="Layer1_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_26 [label="Layer1_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_27 [label="Layer1_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_6 [label="Layer1_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_24
	layer_1_qkv_24 -> layer_1_attn_scores_24
	layer_1_attn_scores_24 -> layer_1_attn_softmax_24
	layer_1_attn_softmax_24 -> layer_1_attn_out_24
	layer_1_attn_out_24 -> layer_1_attn_allreduce_6
	layer_1_input -> layer_1_qkv_25
	layer_1_qkv_25 -> layer_1_attn_scores_25
	layer_1_attn_scores_25 -> layer_1_attn_softmax_25
	layer_1_attn_softmax_25 -> layer_1_attn_out_25
	layer_1_attn_out_25 -> layer_1_attn_allreduce_6
	layer_1_input -> layer_1_qkv_26
	layer_1_qkv_26 -> layer_1_attn_scores_26
	layer_1_attn_scores_26 -> layer_1_attn_softmax_26
	layer_1_attn_softmax_26 -> layer_1_attn_out_26
	layer_1_attn_out_26 -> layer_1_attn_allreduce_6
	layer_1_input -> layer_1_qkv_27
	layer_1_qkv_27 -> layer_1_attn_scores_27
	layer_1_attn_scores_27 -> layer_1_attn_softmax_27
	layer_1_attn_softmax_27 -> layer_1_attn_out_27
	layer_1_attn_out_27 -> layer_1_attn_allreduce_6
	layer_1_qkv_28 [label="Layer1_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_29 [label="Layer1_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_30 [label="Layer1_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_31 [label="Layer1_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_28 [label="Layer1_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_29 [label="Layer1_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_30 [label="Layer1_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_31 [label="Layer1_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_28 [label="Layer1_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_29 [label="Layer1_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_30 [label="Layer1_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_31 [label="Layer1_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_28 [label="Layer1_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_29 [label="Layer1_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_30 [label="Layer1_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_31 [label="Layer1_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_7 [label="Layer1_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_28
	layer_1_qkv_28 -> layer_1_attn_scores_28
	layer_1_attn_scores_28 -> layer_1_attn_softmax_28
	layer_1_attn_softmax_28 -> layer_1_attn_out_28
	layer_1_attn_out_28 -> layer_1_attn_allreduce_7
	layer_1_input -> layer_1_qkv_29
	layer_1_qkv_29 -> layer_1_attn_scores_29
	layer_1_attn_scores_29 -> layer_1_attn_softmax_29
	layer_1_attn_softmax_29 -> layer_1_attn_out_29
	layer_1_attn_out_29 -> layer_1_attn_allreduce_7
	layer_1_input -> layer_1_qkv_30
	layer_1_qkv_30 -> layer_1_attn_scores_30
	layer_1_attn_scores_30 -> layer_1_attn_softmax_30
	layer_1_attn_softmax_30 -> layer_1_attn_out_30
	layer_1_attn_out_30 -> layer_1_attn_allreduce_7
	layer_1_input -> layer_1_qkv_31
	layer_1_qkv_31 -> layer_1_attn_scores_31
	layer_1_attn_scores_31 -> layer_1_attn_softmax_31
	layer_1_attn_softmax_31 -> layer_1_attn_out_31
	layer_1_attn_out_31 -> layer_1_attn_allreduce_7
	layer_1_qkv_32 [label="Layer1_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_33 [label="Layer1_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_34 [label="Layer1_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_35 [label="Layer1_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_32 [label="Layer1_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_33 [label="Layer1_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_34 [label="Layer1_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_35 [label="Layer1_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_32 [label="Layer1_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_33 [label="Layer1_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_34 [label="Layer1_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_35 [label="Layer1_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_32 [label="Layer1_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_33 [label="Layer1_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_34 [label="Layer1_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_35 [label="Layer1_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_8 [label="Layer1_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_32
	layer_1_qkv_32 -> layer_1_attn_scores_32
	layer_1_attn_scores_32 -> layer_1_attn_softmax_32
	layer_1_attn_softmax_32 -> layer_1_attn_out_32
	layer_1_attn_out_32 -> layer_1_attn_allreduce_8
	layer_1_input -> layer_1_qkv_33
	layer_1_qkv_33 -> layer_1_attn_scores_33
	layer_1_attn_scores_33 -> layer_1_attn_softmax_33
	layer_1_attn_softmax_33 -> layer_1_attn_out_33
	layer_1_attn_out_33 -> layer_1_attn_allreduce_8
	layer_1_input -> layer_1_qkv_34
	layer_1_qkv_34 -> layer_1_attn_scores_34
	layer_1_attn_scores_34 -> layer_1_attn_softmax_34
	layer_1_attn_softmax_34 -> layer_1_attn_out_34
	layer_1_attn_out_34 -> layer_1_attn_allreduce_8
	layer_1_input -> layer_1_qkv_35
	layer_1_qkv_35 -> layer_1_attn_scores_35
	layer_1_attn_scores_35 -> layer_1_attn_softmax_35
	layer_1_attn_softmax_35 -> layer_1_attn_out_35
	layer_1_attn_out_35 -> layer_1_attn_allreduce_8
	layer_1_qkv_36 [label="Layer1_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_37 [label="Layer1_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_38 [label="Layer1_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_39 [label="Layer1_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_36 [label="Layer1_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_37 [label="Layer1_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_38 [label="Layer1_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_39 [label="Layer1_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_36 [label="Layer1_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_37 [label="Layer1_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_38 [label="Layer1_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_39 [label="Layer1_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_36 [label="Layer1_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_37 [label="Layer1_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_38 [label="Layer1_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_39 [label="Layer1_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_9 [label="Layer1_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_36
	layer_1_qkv_36 -> layer_1_attn_scores_36
	layer_1_attn_scores_36 -> layer_1_attn_softmax_36
	layer_1_attn_softmax_36 -> layer_1_attn_out_36
	layer_1_attn_out_36 -> layer_1_attn_allreduce_9
	layer_1_input -> layer_1_qkv_37
	layer_1_qkv_37 -> layer_1_attn_scores_37
	layer_1_attn_scores_37 -> layer_1_attn_softmax_37
	layer_1_attn_softmax_37 -> layer_1_attn_out_37
	layer_1_attn_out_37 -> layer_1_attn_allreduce_9
	layer_1_input -> layer_1_qkv_38
	layer_1_qkv_38 -> layer_1_attn_scores_38
	layer_1_attn_scores_38 -> layer_1_attn_softmax_38
	layer_1_attn_softmax_38 -> layer_1_attn_out_38
	layer_1_attn_out_38 -> layer_1_attn_allreduce_9
	layer_1_input -> layer_1_qkv_39
	layer_1_qkv_39 -> layer_1_attn_scores_39
	layer_1_attn_scores_39 -> layer_1_attn_softmax_39
	layer_1_attn_softmax_39 -> layer_1_attn_out_39
	layer_1_attn_out_39 -> layer_1_attn_allreduce_9
	layer_1_qkv_40 [label="Layer1_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_41 [label="Layer1_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_42 [label="Layer1_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_43 [label="Layer1_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_40 [label="Layer1_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_41 [label="Layer1_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_42 [label="Layer1_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_43 [label="Layer1_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_40 [label="Layer1_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_41 [label="Layer1_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_42 [label="Layer1_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_43 [label="Layer1_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_40 [label="Layer1_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_41 [label="Layer1_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_42 [label="Layer1_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_43 [label="Layer1_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_10 [label="Layer1_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_40
	layer_1_qkv_40 -> layer_1_attn_scores_40
	layer_1_attn_scores_40 -> layer_1_attn_softmax_40
	layer_1_attn_softmax_40 -> layer_1_attn_out_40
	layer_1_attn_out_40 -> layer_1_attn_allreduce_10
	layer_1_input -> layer_1_qkv_41
	layer_1_qkv_41 -> layer_1_attn_scores_41
	layer_1_attn_scores_41 -> layer_1_attn_softmax_41
	layer_1_attn_softmax_41 -> layer_1_attn_out_41
	layer_1_attn_out_41 -> layer_1_attn_allreduce_10
	layer_1_input -> layer_1_qkv_42
	layer_1_qkv_42 -> layer_1_attn_scores_42
	layer_1_attn_scores_42 -> layer_1_attn_softmax_42
	layer_1_attn_softmax_42 -> layer_1_attn_out_42
	layer_1_attn_out_42 -> layer_1_attn_allreduce_10
	layer_1_input -> layer_1_qkv_43
	layer_1_qkv_43 -> layer_1_attn_scores_43
	layer_1_attn_scores_43 -> layer_1_attn_softmax_43
	layer_1_attn_softmax_43 -> layer_1_attn_out_43
	layer_1_attn_out_43 -> layer_1_attn_allreduce_10
	layer_1_qkv_44 [label="Layer1_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_45 [label="Layer1_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_46 [label="Layer1_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_47 [label="Layer1_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_44 [label="Layer1_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_45 [label="Layer1_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_46 [label="Layer1_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_47 [label="Layer1_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_44 [label="Layer1_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_45 [label="Layer1_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_46 [label="Layer1_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_47 [label="Layer1_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_44 [label="Layer1_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_45 [label="Layer1_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_46 [label="Layer1_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_47 [label="Layer1_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_11 [label="Layer1_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_44
	layer_1_qkv_44 -> layer_1_attn_scores_44
	layer_1_attn_scores_44 -> layer_1_attn_softmax_44
	layer_1_attn_softmax_44 -> layer_1_attn_out_44
	layer_1_attn_out_44 -> layer_1_attn_allreduce_11
	layer_1_input -> layer_1_qkv_45
	layer_1_qkv_45 -> layer_1_attn_scores_45
	layer_1_attn_scores_45 -> layer_1_attn_softmax_45
	layer_1_attn_softmax_45 -> layer_1_attn_out_45
	layer_1_attn_out_45 -> layer_1_attn_allreduce_11
	layer_1_input -> layer_1_qkv_46
	layer_1_qkv_46 -> layer_1_attn_scores_46
	layer_1_attn_scores_46 -> layer_1_attn_softmax_46
	layer_1_attn_softmax_46 -> layer_1_attn_out_46
	layer_1_attn_out_46 -> layer_1_attn_allreduce_11
	layer_1_input -> layer_1_qkv_47
	layer_1_qkv_47 -> layer_1_attn_scores_47
	layer_1_attn_scores_47 -> layer_1_attn_softmax_47
	layer_1_attn_softmax_47 -> layer_1_attn_out_47
	layer_1_attn_out_47 -> layer_1_attn_allreduce_11
	layer_1_qkv_48 [label="Layer1_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_49 [label="Layer1_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_50 [label="Layer1_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_51 [label="Layer1_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_48 [label="Layer1_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_49 [label="Layer1_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_50 [label="Layer1_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_51 [label="Layer1_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_48 [label="Layer1_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_49 [label="Layer1_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_50 [label="Layer1_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_51 [label="Layer1_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_48 [label="Layer1_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_49 [label="Layer1_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_50 [label="Layer1_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_51 [label="Layer1_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_12 [label="Layer1_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_48
	layer_1_qkv_48 -> layer_1_attn_scores_48
	layer_1_attn_scores_48 -> layer_1_attn_softmax_48
	layer_1_attn_softmax_48 -> layer_1_attn_out_48
	layer_1_attn_out_48 -> layer_1_attn_allreduce_12
	layer_1_input -> layer_1_qkv_49
	layer_1_qkv_49 -> layer_1_attn_scores_49
	layer_1_attn_scores_49 -> layer_1_attn_softmax_49
	layer_1_attn_softmax_49 -> layer_1_attn_out_49
	layer_1_attn_out_49 -> layer_1_attn_allreduce_12
	layer_1_input -> layer_1_qkv_50
	layer_1_qkv_50 -> layer_1_attn_scores_50
	layer_1_attn_scores_50 -> layer_1_attn_softmax_50
	layer_1_attn_softmax_50 -> layer_1_attn_out_50
	layer_1_attn_out_50 -> layer_1_attn_allreduce_12
	layer_1_input -> layer_1_qkv_51
	layer_1_qkv_51 -> layer_1_attn_scores_51
	layer_1_attn_scores_51 -> layer_1_attn_softmax_51
	layer_1_attn_softmax_51 -> layer_1_attn_out_51
	layer_1_attn_out_51 -> layer_1_attn_allreduce_12
	layer_1_qkv_52 [label="Layer1_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_53 [label="Layer1_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_54 [label="Layer1_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_55 [label="Layer1_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_52 [label="Layer1_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_53 [label="Layer1_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_54 [label="Layer1_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_55 [label="Layer1_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_52 [label="Layer1_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_53 [label="Layer1_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_54 [label="Layer1_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_55 [label="Layer1_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_52 [label="Layer1_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_53 [label="Layer1_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_54 [label="Layer1_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_55 [label="Layer1_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_13 [label="Layer1_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_52
	layer_1_qkv_52 -> layer_1_attn_scores_52
	layer_1_attn_scores_52 -> layer_1_attn_softmax_52
	layer_1_attn_softmax_52 -> layer_1_attn_out_52
	layer_1_attn_out_52 -> layer_1_attn_allreduce_13
	layer_1_input -> layer_1_qkv_53
	layer_1_qkv_53 -> layer_1_attn_scores_53
	layer_1_attn_scores_53 -> layer_1_attn_softmax_53
	layer_1_attn_softmax_53 -> layer_1_attn_out_53
	layer_1_attn_out_53 -> layer_1_attn_allreduce_13
	layer_1_input -> layer_1_qkv_54
	layer_1_qkv_54 -> layer_1_attn_scores_54
	layer_1_attn_scores_54 -> layer_1_attn_softmax_54
	layer_1_attn_softmax_54 -> layer_1_attn_out_54
	layer_1_attn_out_54 -> layer_1_attn_allreduce_13
	layer_1_input -> layer_1_qkv_55
	layer_1_qkv_55 -> layer_1_attn_scores_55
	layer_1_attn_scores_55 -> layer_1_attn_softmax_55
	layer_1_attn_softmax_55 -> layer_1_attn_out_55
	layer_1_attn_out_55 -> layer_1_attn_allreduce_13
	layer_1_qkv_56 [label="Layer1_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_57 [label="Layer1_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_58 [label="Layer1_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_59 [label="Layer1_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_56 [label="Layer1_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_57 [label="Layer1_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_58 [label="Layer1_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_59 [label="Layer1_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_56 [label="Layer1_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_57 [label="Layer1_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_58 [label="Layer1_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_59 [label="Layer1_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_56 [label="Layer1_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_57 [label="Layer1_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_58 [label="Layer1_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_59 [label="Layer1_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_14 [label="Layer1_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_56
	layer_1_qkv_56 -> layer_1_attn_scores_56
	layer_1_attn_scores_56 -> layer_1_attn_softmax_56
	layer_1_attn_softmax_56 -> layer_1_attn_out_56
	layer_1_attn_out_56 -> layer_1_attn_allreduce_14
	layer_1_input -> layer_1_qkv_57
	layer_1_qkv_57 -> layer_1_attn_scores_57
	layer_1_attn_scores_57 -> layer_1_attn_softmax_57
	layer_1_attn_softmax_57 -> layer_1_attn_out_57
	layer_1_attn_out_57 -> layer_1_attn_allreduce_14
	layer_1_input -> layer_1_qkv_58
	layer_1_qkv_58 -> layer_1_attn_scores_58
	layer_1_attn_scores_58 -> layer_1_attn_softmax_58
	layer_1_attn_softmax_58 -> layer_1_attn_out_58
	layer_1_attn_out_58 -> layer_1_attn_allreduce_14
	layer_1_input -> layer_1_qkv_59
	layer_1_qkv_59 -> layer_1_attn_scores_59
	layer_1_attn_scores_59 -> layer_1_attn_softmax_59
	layer_1_attn_softmax_59 -> layer_1_attn_out_59
	layer_1_attn_out_59 -> layer_1_attn_allreduce_14
	layer_1_qkv_60 [label="Layer1_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_61 [label="Layer1_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_62 [label="Layer1_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_qkv_63 [label="Layer1_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_60 [label="Layer1_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_61 [label="Layer1_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_62 [label="Layer1_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_scores_63 [label="Layer1_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_60 [label="Layer1_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_61 [label="Layer1_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_62 [label="Layer1_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_softmax_63 [label="Layer1_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_60 [label="Layer1_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_61 [label="Layer1_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_62 [label="Layer1_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_out_63 [label="Layer1_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_15 [label="Layer1_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_input -> layer_1_qkv_60
	layer_1_qkv_60 -> layer_1_attn_scores_60
	layer_1_attn_scores_60 -> layer_1_attn_softmax_60
	layer_1_attn_softmax_60 -> layer_1_attn_out_60
	layer_1_attn_out_60 -> layer_1_attn_allreduce_15
	layer_1_input -> layer_1_qkv_61
	layer_1_qkv_61 -> layer_1_attn_scores_61
	layer_1_attn_scores_61 -> layer_1_attn_softmax_61
	layer_1_attn_softmax_61 -> layer_1_attn_out_61
	layer_1_attn_out_61 -> layer_1_attn_allreduce_15
	layer_1_input -> layer_1_qkv_62
	layer_1_qkv_62 -> layer_1_attn_scores_62
	layer_1_attn_scores_62 -> layer_1_attn_softmax_62
	layer_1_attn_softmax_62 -> layer_1_attn_out_62
	layer_1_attn_out_62 -> layer_1_attn_allreduce_15
	layer_1_input -> layer_1_qkv_63
	layer_1_qkv_63 -> layer_1_attn_scores_63
	layer_1_attn_scores_63 -> layer_1_attn_softmax_63
	layer_1_attn_softmax_63 -> layer_1_attn_out_63
	layer_1_attn_out_63 -> layer_1_attn_allreduce_15
	layer_1_gate_0 [label="Layer1_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_4 [label="Layer1_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_8 [label="Layer1_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_12 [label="Layer1_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_16 [label="Layer1_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_20 [label="Layer1_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_24 [label="Layer1_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_28 [label="Layer1_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_32 [label="Layer1_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_36 [label="Layer1_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_40 [label="Layer1_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_44 [label="Layer1_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_48 [label="Layer1_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_52 [label="Layer1_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_56 [label="Layer1_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_gate_60 [label="Layer1_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_1_alltoall [label="Layer1_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_0 [label="Layer1_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_1 [label="Layer1_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_2 [label="Layer1_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_3 [label="Layer1_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_0 [label="Layer1_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_1 [label="Layer1_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_2 [label="Layer1_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_3 [label="Layer1_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_0 [label="Layer1_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_1 [label="Layer1_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_2 [label="Layer1_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_3 [label="Layer1_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_0 [label="Layer1_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_4 [label="Layer1_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_5 [label="Layer1_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_6 [label="Layer1_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_7 [label="Layer1_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_4 [label="Layer1_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_5 [label="Layer1_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_6 [label="Layer1_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_7 [label="Layer1_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_4 [label="Layer1_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_5 [label="Layer1_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_6 [label="Layer1_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_7 [label="Layer1_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_1 [label="Layer1_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_8 [label="Layer1_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_9 [label="Layer1_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_10 [label="Layer1_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_11 [label="Layer1_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_8 [label="Layer1_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_9 [label="Layer1_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_10 [label="Layer1_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_11 [label="Layer1_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_8 [label="Layer1_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_9 [label="Layer1_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_10 [label="Layer1_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_11 [label="Layer1_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_2 [label="Layer1_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_12 [label="Layer1_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_13 [label="Layer1_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_14 [label="Layer1_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_15 [label="Layer1_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_12 [label="Layer1_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_13 [label="Layer1_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_14 [label="Layer1_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_15 [label="Layer1_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_12 [label="Layer1_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_13 [label="Layer1_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_14 [label="Layer1_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_15 [label="Layer1_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_3 [label="Layer1_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_16 [label="Layer1_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_17 [label="Layer1_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_18 [label="Layer1_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_19 [label="Layer1_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_16 [label="Layer1_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_17 [label="Layer1_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_18 [label="Layer1_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_19 [label="Layer1_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_16 [label="Layer1_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_17 [label="Layer1_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_18 [label="Layer1_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_19 [label="Layer1_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_4 [label="Layer1_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_20 [label="Layer1_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_21 [label="Layer1_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_22 [label="Layer1_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_23 [label="Layer1_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_20 [label="Layer1_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_21 [label="Layer1_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_22 [label="Layer1_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_23 [label="Layer1_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_20 [label="Layer1_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_21 [label="Layer1_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_22 [label="Layer1_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_23 [label="Layer1_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_5 [label="Layer1_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_24 [label="Layer1_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_25 [label="Layer1_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_26 [label="Layer1_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_27 [label="Layer1_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_24 [label="Layer1_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_25 [label="Layer1_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_26 [label="Layer1_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_27 [label="Layer1_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_24 [label="Layer1_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_25 [label="Layer1_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_26 [label="Layer1_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_27 [label="Layer1_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_6 [label="Layer1_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_28 [label="Layer1_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_29 [label="Layer1_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_30 [label="Layer1_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_31 [label="Layer1_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_28 [label="Layer1_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_29 [label="Layer1_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_30 [label="Layer1_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_31 [label="Layer1_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_28 [label="Layer1_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_29 [label="Layer1_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_30 [label="Layer1_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_31 [label="Layer1_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_7 [label="Layer1_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_32 [label="Layer1_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_33 [label="Layer1_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_34 [label="Layer1_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_35 [label="Layer1_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_32 [label="Layer1_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_33 [label="Layer1_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_34 [label="Layer1_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_35 [label="Layer1_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_32 [label="Layer1_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_33 [label="Layer1_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_34 [label="Layer1_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_35 [label="Layer1_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_8 [label="Layer1_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_36 [label="Layer1_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_37 [label="Layer1_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_38 [label="Layer1_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_39 [label="Layer1_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_36 [label="Layer1_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_37 [label="Layer1_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_38 [label="Layer1_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_39 [label="Layer1_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_36 [label="Layer1_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_37 [label="Layer1_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_38 [label="Layer1_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_39 [label="Layer1_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_9 [label="Layer1_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_40 [label="Layer1_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_41 [label="Layer1_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_42 [label="Layer1_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_43 [label="Layer1_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_40 [label="Layer1_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_41 [label="Layer1_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_42 [label="Layer1_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_43 [label="Layer1_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_40 [label="Layer1_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_41 [label="Layer1_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_42 [label="Layer1_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_43 [label="Layer1_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_10 [label="Layer1_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_44 [label="Layer1_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_45 [label="Layer1_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_46 [label="Layer1_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_47 [label="Layer1_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_44 [label="Layer1_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_45 [label="Layer1_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_46 [label="Layer1_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_47 [label="Layer1_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_44 [label="Layer1_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_45 [label="Layer1_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_46 [label="Layer1_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_47 [label="Layer1_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_11 [label="Layer1_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_48 [label="Layer1_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_49 [label="Layer1_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_50 [label="Layer1_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_51 [label="Layer1_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_48 [label="Layer1_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_49 [label="Layer1_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_50 [label="Layer1_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_51 [label="Layer1_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_48 [label="Layer1_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_49 [label="Layer1_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_50 [label="Layer1_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_51 [label="Layer1_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_12 [label="Layer1_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_52 [label="Layer1_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_53 [label="Layer1_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_54 [label="Layer1_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_55 [label="Layer1_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_52 [label="Layer1_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_53 [label="Layer1_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_54 [label="Layer1_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_55 [label="Layer1_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_52 [label="Layer1_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_53 [label="Layer1_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_54 [label="Layer1_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_55 [label="Layer1_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_13 [label="Layer1_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_56 [label="Layer1_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_57 [label="Layer1_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_58 [label="Layer1_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_59 [label="Layer1_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_56 [label="Layer1_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_57 [label="Layer1_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_58 [label="Layer1_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_59 [label="Layer1_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_56 [label="Layer1_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_57 [label="Layer1_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_58 [label="Layer1_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_59 [label="Layer1_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_14 [label="Layer1_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert1_60 [label="Layer1_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_61 [label="Layer1_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_62 [label="Layer1_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert1_63 [label="Layer1_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_60 [label="Layer1_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_61 [label="Layer1_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_62 [label="Layer1_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert_act_63 [label="Layer1_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_1_expert2_60 [label="Layer1_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_61 [label="Layer1_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_62 [label="Layer1_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert2_63 [label="Layer1_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_1_expert_allreduce_15 [label="Layer1_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_1_expert_agg [label="Layer1_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_1_norm [label="Layer1_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_1_attn_allreduce_0 -> layer_1_gate_0 [style=dashed]
	layer_1_gate_0 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_0
	layer_1_expert1_0 -> layer_1_expert_act_0
	layer_1_expert_act_0 -> layer_1_expert2_0
	layer_1_expert2_0 -> layer_1_expert_allreduce_0
	layer_1_alltoall -> layer_1_expert1_1
	layer_1_expert1_1 -> layer_1_expert_act_1
	layer_1_expert_act_1 -> layer_1_expert2_1
	layer_1_expert2_1 -> layer_1_expert_allreduce_0
	layer_1_alltoall -> layer_1_expert1_2
	layer_1_expert1_2 -> layer_1_expert_act_2
	layer_1_expert_act_2 -> layer_1_expert2_2
	layer_1_expert2_2 -> layer_1_expert_allreduce_0
	layer_1_alltoall -> layer_1_expert1_3
	layer_1_expert1_3 -> layer_1_expert_act_3
	layer_1_expert_act_3 -> layer_1_expert2_3
	layer_1_expert2_3 -> layer_1_expert_allreduce_0
	layer_1_expert_allreduce_0 -> layer_1_expert_agg
	layer_1_attn_allreduce_1 -> layer_1_gate_4 [style=dashed]
	layer_1_gate_4 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_4
	layer_1_expert1_4 -> layer_1_expert_act_4
	layer_1_expert_act_4 -> layer_1_expert2_4
	layer_1_expert2_4 -> layer_1_expert_allreduce_1
	layer_1_alltoall -> layer_1_expert1_5
	layer_1_expert1_5 -> layer_1_expert_act_5
	layer_1_expert_act_5 -> layer_1_expert2_5
	layer_1_expert2_5 -> layer_1_expert_allreduce_1
	layer_1_alltoall -> layer_1_expert1_6
	layer_1_expert1_6 -> layer_1_expert_act_6
	layer_1_expert_act_6 -> layer_1_expert2_6
	layer_1_expert2_6 -> layer_1_expert_allreduce_1
	layer_1_alltoall -> layer_1_expert1_7
	layer_1_expert1_7 -> layer_1_expert_act_7
	layer_1_expert_act_7 -> layer_1_expert2_7
	layer_1_expert2_7 -> layer_1_expert_allreduce_1
	layer_1_expert_allreduce_1 -> layer_1_expert_agg
	layer_1_attn_allreduce_2 -> layer_1_gate_8 [style=dashed]
	layer_1_gate_8 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_8
	layer_1_expert1_8 -> layer_1_expert_act_8
	layer_1_expert_act_8 -> layer_1_expert2_8
	layer_1_expert2_8 -> layer_1_expert_allreduce_2
	layer_1_alltoall -> layer_1_expert1_9
	layer_1_expert1_9 -> layer_1_expert_act_9
	layer_1_expert_act_9 -> layer_1_expert2_9
	layer_1_expert2_9 -> layer_1_expert_allreduce_2
	layer_1_alltoall -> layer_1_expert1_10
	layer_1_expert1_10 -> layer_1_expert_act_10
	layer_1_expert_act_10 -> layer_1_expert2_10
	layer_1_expert2_10 -> layer_1_expert_allreduce_2
	layer_1_alltoall -> layer_1_expert1_11
	layer_1_expert1_11 -> layer_1_expert_act_11
	layer_1_expert_act_11 -> layer_1_expert2_11
	layer_1_expert2_11 -> layer_1_expert_allreduce_2
	layer_1_expert_allreduce_2 -> layer_1_expert_agg
	layer_1_attn_allreduce_3 -> layer_1_gate_12 [style=dashed]
	layer_1_gate_12 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_12
	layer_1_expert1_12 -> layer_1_expert_act_12
	layer_1_expert_act_12 -> layer_1_expert2_12
	layer_1_expert2_12 -> layer_1_expert_allreduce_3
	layer_1_alltoall -> layer_1_expert1_13
	layer_1_expert1_13 -> layer_1_expert_act_13
	layer_1_expert_act_13 -> layer_1_expert2_13
	layer_1_expert2_13 -> layer_1_expert_allreduce_3
	layer_1_alltoall -> layer_1_expert1_14
	layer_1_expert1_14 -> layer_1_expert_act_14
	layer_1_expert_act_14 -> layer_1_expert2_14
	layer_1_expert2_14 -> layer_1_expert_allreduce_3
	layer_1_alltoall -> layer_1_expert1_15
	layer_1_expert1_15 -> layer_1_expert_act_15
	layer_1_expert_act_15 -> layer_1_expert2_15
	layer_1_expert2_15 -> layer_1_expert_allreduce_3
	layer_1_expert_allreduce_3 -> layer_1_expert_agg
	layer_1_attn_allreduce_4 -> layer_1_gate_16 [style=dashed]
	layer_1_gate_16 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_16
	layer_1_expert1_16 -> layer_1_expert_act_16
	layer_1_expert_act_16 -> layer_1_expert2_16
	layer_1_expert2_16 -> layer_1_expert_allreduce_4
	layer_1_alltoall -> layer_1_expert1_17
	layer_1_expert1_17 -> layer_1_expert_act_17
	layer_1_expert_act_17 -> layer_1_expert2_17
	layer_1_expert2_17 -> layer_1_expert_allreduce_4
	layer_1_alltoall -> layer_1_expert1_18
	layer_1_expert1_18 -> layer_1_expert_act_18
	layer_1_expert_act_18 -> layer_1_expert2_18
	layer_1_expert2_18 -> layer_1_expert_allreduce_4
	layer_1_alltoall -> layer_1_expert1_19
	layer_1_expert1_19 -> layer_1_expert_act_19
	layer_1_expert_act_19 -> layer_1_expert2_19
	layer_1_expert2_19 -> layer_1_expert_allreduce_4
	layer_1_expert_allreduce_4 -> layer_1_expert_agg
	layer_1_attn_allreduce_5 -> layer_1_gate_20 [style=dashed]
	layer_1_gate_20 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_20
	layer_1_expert1_20 -> layer_1_expert_act_20
	layer_1_expert_act_20 -> layer_1_expert2_20
	layer_1_expert2_20 -> layer_1_expert_allreduce_5
	layer_1_alltoall -> layer_1_expert1_21
	layer_1_expert1_21 -> layer_1_expert_act_21
	layer_1_expert_act_21 -> layer_1_expert2_21
	layer_1_expert2_21 -> layer_1_expert_allreduce_5
	layer_1_alltoall -> layer_1_expert1_22
	layer_1_expert1_22 -> layer_1_expert_act_22
	layer_1_expert_act_22 -> layer_1_expert2_22
	layer_1_expert2_22 -> layer_1_expert_allreduce_5
	layer_1_alltoall -> layer_1_expert1_23
	layer_1_expert1_23 -> layer_1_expert_act_23
	layer_1_expert_act_23 -> layer_1_expert2_23
	layer_1_expert2_23 -> layer_1_expert_allreduce_5
	layer_1_expert_allreduce_5 -> layer_1_expert_agg
	layer_1_attn_allreduce_6 -> layer_1_gate_24 [style=dashed]
	layer_1_gate_24 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_24
	layer_1_expert1_24 -> layer_1_expert_act_24
	layer_1_expert_act_24 -> layer_1_expert2_24
	layer_1_expert2_24 -> layer_1_expert_allreduce_6
	layer_1_alltoall -> layer_1_expert1_25
	layer_1_expert1_25 -> layer_1_expert_act_25
	layer_1_expert_act_25 -> layer_1_expert2_25
	layer_1_expert2_25 -> layer_1_expert_allreduce_6
	layer_1_alltoall -> layer_1_expert1_26
	layer_1_expert1_26 -> layer_1_expert_act_26
	layer_1_expert_act_26 -> layer_1_expert2_26
	layer_1_expert2_26 -> layer_1_expert_allreduce_6
	layer_1_alltoall -> layer_1_expert1_27
	layer_1_expert1_27 -> layer_1_expert_act_27
	layer_1_expert_act_27 -> layer_1_expert2_27
	layer_1_expert2_27 -> layer_1_expert_allreduce_6
	layer_1_expert_allreduce_6 -> layer_1_expert_agg
	layer_1_attn_allreduce_7 -> layer_1_gate_28 [style=dashed]
	layer_1_gate_28 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_28
	layer_1_expert1_28 -> layer_1_expert_act_28
	layer_1_expert_act_28 -> layer_1_expert2_28
	layer_1_expert2_28 -> layer_1_expert_allreduce_7
	layer_1_alltoall -> layer_1_expert1_29
	layer_1_expert1_29 -> layer_1_expert_act_29
	layer_1_expert_act_29 -> layer_1_expert2_29
	layer_1_expert2_29 -> layer_1_expert_allreduce_7
	layer_1_alltoall -> layer_1_expert1_30
	layer_1_expert1_30 -> layer_1_expert_act_30
	layer_1_expert_act_30 -> layer_1_expert2_30
	layer_1_expert2_30 -> layer_1_expert_allreduce_7
	layer_1_alltoall -> layer_1_expert1_31
	layer_1_expert1_31 -> layer_1_expert_act_31
	layer_1_expert_act_31 -> layer_1_expert2_31
	layer_1_expert2_31 -> layer_1_expert_allreduce_7
	layer_1_expert_allreduce_7 -> layer_1_expert_agg
	layer_1_attn_allreduce_8 -> layer_1_gate_32 [style=dashed]
	layer_1_gate_32 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_32
	layer_1_expert1_32 -> layer_1_expert_act_32
	layer_1_expert_act_32 -> layer_1_expert2_32
	layer_1_expert2_32 -> layer_1_expert_allreduce_8
	layer_1_alltoall -> layer_1_expert1_33
	layer_1_expert1_33 -> layer_1_expert_act_33
	layer_1_expert_act_33 -> layer_1_expert2_33
	layer_1_expert2_33 -> layer_1_expert_allreduce_8
	layer_1_alltoall -> layer_1_expert1_34
	layer_1_expert1_34 -> layer_1_expert_act_34
	layer_1_expert_act_34 -> layer_1_expert2_34
	layer_1_expert2_34 -> layer_1_expert_allreduce_8
	layer_1_alltoall -> layer_1_expert1_35
	layer_1_expert1_35 -> layer_1_expert_act_35
	layer_1_expert_act_35 -> layer_1_expert2_35
	layer_1_expert2_35 -> layer_1_expert_allreduce_8
	layer_1_expert_allreduce_8 -> layer_1_expert_agg
	layer_1_attn_allreduce_9 -> layer_1_gate_36 [style=dashed]
	layer_1_gate_36 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_36
	layer_1_expert1_36 -> layer_1_expert_act_36
	layer_1_expert_act_36 -> layer_1_expert2_36
	layer_1_expert2_36 -> layer_1_expert_allreduce_9
	layer_1_alltoall -> layer_1_expert1_37
	layer_1_expert1_37 -> layer_1_expert_act_37
	layer_1_expert_act_37 -> layer_1_expert2_37
	layer_1_expert2_37 -> layer_1_expert_allreduce_9
	layer_1_alltoall -> layer_1_expert1_38
	layer_1_expert1_38 -> layer_1_expert_act_38
	layer_1_expert_act_38 -> layer_1_expert2_38
	layer_1_expert2_38 -> layer_1_expert_allreduce_9
	layer_1_alltoall -> layer_1_expert1_39
	layer_1_expert1_39 -> layer_1_expert_act_39
	layer_1_expert_act_39 -> layer_1_expert2_39
	layer_1_expert2_39 -> layer_1_expert_allreduce_9
	layer_1_expert_allreduce_9 -> layer_1_expert_agg
	layer_1_attn_allreduce_10 -> layer_1_gate_40 [style=dashed]
	layer_1_gate_40 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_40
	layer_1_expert1_40 -> layer_1_expert_act_40
	layer_1_expert_act_40 -> layer_1_expert2_40
	layer_1_expert2_40 -> layer_1_expert_allreduce_10
	layer_1_alltoall -> layer_1_expert1_41
	layer_1_expert1_41 -> layer_1_expert_act_41
	layer_1_expert_act_41 -> layer_1_expert2_41
	layer_1_expert2_41 -> layer_1_expert_allreduce_10
	layer_1_alltoall -> layer_1_expert1_42
	layer_1_expert1_42 -> layer_1_expert_act_42
	layer_1_expert_act_42 -> layer_1_expert2_42
	layer_1_expert2_42 -> layer_1_expert_allreduce_10
	layer_1_alltoall -> layer_1_expert1_43
	layer_1_expert1_43 -> layer_1_expert_act_43
	layer_1_expert_act_43 -> layer_1_expert2_43
	layer_1_expert2_43 -> layer_1_expert_allreduce_10
	layer_1_expert_allreduce_10 -> layer_1_expert_agg
	layer_1_attn_allreduce_11 -> layer_1_gate_44 [style=dashed]
	layer_1_gate_44 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_44
	layer_1_expert1_44 -> layer_1_expert_act_44
	layer_1_expert_act_44 -> layer_1_expert2_44
	layer_1_expert2_44 -> layer_1_expert_allreduce_11
	layer_1_alltoall -> layer_1_expert1_45
	layer_1_expert1_45 -> layer_1_expert_act_45
	layer_1_expert_act_45 -> layer_1_expert2_45
	layer_1_expert2_45 -> layer_1_expert_allreduce_11
	layer_1_alltoall -> layer_1_expert1_46
	layer_1_expert1_46 -> layer_1_expert_act_46
	layer_1_expert_act_46 -> layer_1_expert2_46
	layer_1_expert2_46 -> layer_1_expert_allreduce_11
	layer_1_alltoall -> layer_1_expert1_47
	layer_1_expert1_47 -> layer_1_expert_act_47
	layer_1_expert_act_47 -> layer_1_expert2_47
	layer_1_expert2_47 -> layer_1_expert_allreduce_11
	layer_1_expert_allreduce_11 -> layer_1_expert_agg
	layer_1_attn_allreduce_12 -> layer_1_gate_48 [style=dashed]
	layer_1_gate_48 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_48
	layer_1_expert1_48 -> layer_1_expert_act_48
	layer_1_expert_act_48 -> layer_1_expert2_48
	layer_1_expert2_48 -> layer_1_expert_allreduce_12
	layer_1_alltoall -> layer_1_expert1_49
	layer_1_expert1_49 -> layer_1_expert_act_49
	layer_1_expert_act_49 -> layer_1_expert2_49
	layer_1_expert2_49 -> layer_1_expert_allreduce_12
	layer_1_alltoall -> layer_1_expert1_50
	layer_1_expert1_50 -> layer_1_expert_act_50
	layer_1_expert_act_50 -> layer_1_expert2_50
	layer_1_expert2_50 -> layer_1_expert_allreduce_12
	layer_1_alltoall -> layer_1_expert1_51
	layer_1_expert1_51 -> layer_1_expert_act_51
	layer_1_expert_act_51 -> layer_1_expert2_51
	layer_1_expert2_51 -> layer_1_expert_allreduce_12
	layer_1_expert_allreduce_12 -> layer_1_expert_agg
	layer_1_attn_allreduce_13 -> layer_1_gate_52 [style=dashed]
	layer_1_gate_52 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_52
	layer_1_expert1_52 -> layer_1_expert_act_52
	layer_1_expert_act_52 -> layer_1_expert2_52
	layer_1_expert2_52 -> layer_1_expert_allreduce_13
	layer_1_alltoall -> layer_1_expert1_53
	layer_1_expert1_53 -> layer_1_expert_act_53
	layer_1_expert_act_53 -> layer_1_expert2_53
	layer_1_expert2_53 -> layer_1_expert_allreduce_13
	layer_1_alltoall -> layer_1_expert1_54
	layer_1_expert1_54 -> layer_1_expert_act_54
	layer_1_expert_act_54 -> layer_1_expert2_54
	layer_1_expert2_54 -> layer_1_expert_allreduce_13
	layer_1_alltoall -> layer_1_expert1_55
	layer_1_expert1_55 -> layer_1_expert_act_55
	layer_1_expert_act_55 -> layer_1_expert2_55
	layer_1_expert2_55 -> layer_1_expert_allreduce_13
	layer_1_expert_allreduce_13 -> layer_1_expert_agg
	layer_1_attn_allreduce_14 -> layer_1_gate_56 [style=dashed]
	layer_1_gate_56 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_56
	layer_1_expert1_56 -> layer_1_expert_act_56
	layer_1_expert_act_56 -> layer_1_expert2_56
	layer_1_expert2_56 -> layer_1_expert_allreduce_14
	layer_1_alltoall -> layer_1_expert1_57
	layer_1_expert1_57 -> layer_1_expert_act_57
	layer_1_expert_act_57 -> layer_1_expert2_57
	layer_1_expert2_57 -> layer_1_expert_allreduce_14
	layer_1_alltoall -> layer_1_expert1_58
	layer_1_expert1_58 -> layer_1_expert_act_58
	layer_1_expert_act_58 -> layer_1_expert2_58
	layer_1_expert2_58 -> layer_1_expert_allreduce_14
	layer_1_alltoall -> layer_1_expert1_59
	layer_1_expert1_59 -> layer_1_expert_act_59
	layer_1_expert_act_59 -> layer_1_expert2_59
	layer_1_expert2_59 -> layer_1_expert_allreduce_14
	layer_1_expert_allreduce_14 -> layer_1_expert_agg
	layer_1_attn_allreduce_15 -> layer_1_gate_60 [style=dashed]
	layer_1_gate_60 -> layer_1_alltoall [style=dashed]
	layer_1_alltoall -> layer_1_expert1_60
	layer_1_expert1_60 -> layer_1_expert_act_60
	layer_1_expert_act_60 -> layer_1_expert2_60
	layer_1_expert2_60 -> layer_1_expert_allreduce_15
	layer_1_alltoall -> layer_1_expert1_61
	layer_1_expert1_61 -> layer_1_expert_act_61
	layer_1_expert_act_61 -> layer_1_expert2_61
	layer_1_expert2_61 -> layer_1_expert_allreduce_15
	layer_1_alltoall -> layer_1_expert1_62
	layer_1_expert1_62 -> layer_1_expert_act_62
	layer_1_expert_act_62 -> layer_1_expert2_62
	layer_1_expert2_62 -> layer_1_expert_allreduce_15
	layer_1_alltoall -> layer_1_expert1_63
	layer_1_expert1_63 -> layer_1_expert_act_63
	layer_1_expert_act_63 -> layer_1_expert2_63
	layer_1_expert2_63 -> layer_1_expert_allreduce_15
	layer_1_expert_allreduce_15 -> layer_1_expert_agg
	layer_1_expert_agg -> layer_1_norm
	layer_1_norm -> layer_2_input
	layer_2_input [label="Layer2_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_2_qkv_0 [label="Layer2_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_1 [label="Layer2_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_2 [label="Layer2_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_3 [label="Layer2_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_0 [label="Layer2_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_1 [label="Layer2_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_2 [label="Layer2_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_3 [label="Layer2_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_0 [label="Layer2_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_1 [label="Layer2_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_2 [label="Layer2_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_3 [label="Layer2_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_0 [label="Layer2_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_1 [label="Layer2_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_2 [label="Layer2_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_3 [label="Layer2_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_0 [label="Layer2_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_0
	layer_2_qkv_0 -> layer_2_attn_scores_0
	layer_2_attn_scores_0 -> layer_2_attn_softmax_0
	layer_2_attn_softmax_0 -> layer_2_attn_out_0
	layer_2_attn_out_0 -> layer_2_attn_allreduce_0
	layer_2_input -> layer_2_qkv_1
	layer_2_qkv_1 -> layer_2_attn_scores_1
	layer_2_attn_scores_1 -> layer_2_attn_softmax_1
	layer_2_attn_softmax_1 -> layer_2_attn_out_1
	layer_2_attn_out_1 -> layer_2_attn_allreduce_0
	layer_2_input -> layer_2_qkv_2
	layer_2_qkv_2 -> layer_2_attn_scores_2
	layer_2_attn_scores_2 -> layer_2_attn_softmax_2
	layer_2_attn_softmax_2 -> layer_2_attn_out_2
	layer_2_attn_out_2 -> layer_2_attn_allreduce_0
	layer_2_input -> layer_2_qkv_3
	layer_2_qkv_3 -> layer_2_attn_scores_3
	layer_2_attn_scores_3 -> layer_2_attn_softmax_3
	layer_2_attn_softmax_3 -> layer_2_attn_out_3
	layer_2_attn_out_3 -> layer_2_attn_allreduce_0
	layer_2_qkv_4 [label="Layer2_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_5 [label="Layer2_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_6 [label="Layer2_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_7 [label="Layer2_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_4 [label="Layer2_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_5 [label="Layer2_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_6 [label="Layer2_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_7 [label="Layer2_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_4 [label="Layer2_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_5 [label="Layer2_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_6 [label="Layer2_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_7 [label="Layer2_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_4 [label="Layer2_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_5 [label="Layer2_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_6 [label="Layer2_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_7 [label="Layer2_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_1 [label="Layer2_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_4
	layer_2_qkv_4 -> layer_2_attn_scores_4
	layer_2_attn_scores_4 -> layer_2_attn_softmax_4
	layer_2_attn_softmax_4 -> layer_2_attn_out_4
	layer_2_attn_out_4 -> layer_2_attn_allreduce_1
	layer_2_input -> layer_2_qkv_5
	layer_2_qkv_5 -> layer_2_attn_scores_5
	layer_2_attn_scores_5 -> layer_2_attn_softmax_5
	layer_2_attn_softmax_5 -> layer_2_attn_out_5
	layer_2_attn_out_5 -> layer_2_attn_allreduce_1
	layer_2_input -> layer_2_qkv_6
	layer_2_qkv_6 -> layer_2_attn_scores_6
	layer_2_attn_scores_6 -> layer_2_attn_softmax_6
	layer_2_attn_softmax_6 -> layer_2_attn_out_6
	layer_2_attn_out_6 -> layer_2_attn_allreduce_1
	layer_2_input -> layer_2_qkv_7
	layer_2_qkv_7 -> layer_2_attn_scores_7
	layer_2_attn_scores_7 -> layer_2_attn_softmax_7
	layer_2_attn_softmax_7 -> layer_2_attn_out_7
	layer_2_attn_out_7 -> layer_2_attn_allreduce_1
	layer_2_qkv_8 [label="Layer2_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_9 [label="Layer2_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_10 [label="Layer2_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_11 [label="Layer2_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_8 [label="Layer2_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_9 [label="Layer2_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_10 [label="Layer2_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_11 [label="Layer2_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_8 [label="Layer2_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_9 [label="Layer2_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_10 [label="Layer2_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_11 [label="Layer2_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_8 [label="Layer2_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_9 [label="Layer2_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_10 [label="Layer2_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_11 [label="Layer2_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_2 [label="Layer2_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_8
	layer_2_qkv_8 -> layer_2_attn_scores_8
	layer_2_attn_scores_8 -> layer_2_attn_softmax_8
	layer_2_attn_softmax_8 -> layer_2_attn_out_8
	layer_2_attn_out_8 -> layer_2_attn_allreduce_2
	layer_2_input -> layer_2_qkv_9
	layer_2_qkv_9 -> layer_2_attn_scores_9
	layer_2_attn_scores_9 -> layer_2_attn_softmax_9
	layer_2_attn_softmax_9 -> layer_2_attn_out_9
	layer_2_attn_out_9 -> layer_2_attn_allreduce_2
	layer_2_input -> layer_2_qkv_10
	layer_2_qkv_10 -> layer_2_attn_scores_10
	layer_2_attn_scores_10 -> layer_2_attn_softmax_10
	layer_2_attn_softmax_10 -> layer_2_attn_out_10
	layer_2_attn_out_10 -> layer_2_attn_allreduce_2
	layer_2_input -> layer_2_qkv_11
	layer_2_qkv_11 -> layer_2_attn_scores_11
	layer_2_attn_scores_11 -> layer_2_attn_softmax_11
	layer_2_attn_softmax_11 -> layer_2_attn_out_11
	layer_2_attn_out_11 -> layer_2_attn_allreduce_2
	layer_2_qkv_12 [label="Layer2_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_13 [label="Layer2_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_14 [label="Layer2_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_15 [label="Layer2_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_12 [label="Layer2_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_13 [label="Layer2_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_14 [label="Layer2_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_15 [label="Layer2_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_12 [label="Layer2_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_13 [label="Layer2_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_14 [label="Layer2_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_15 [label="Layer2_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_12 [label="Layer2_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_13 [label="Layer2_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_14 [label="Layer2_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_15 [label="Layer2_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_3 [label="Layer2_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_12
	layer_2_qkv_12 -> layer_2_attn_scores_12
	layer_2_attn_scores_12 -> layer_2_attn_softmax_12
	layer_2_attn_softmax_12 -> layer_2_attn_out_12
	layer_2_attn_out_12 -> layer_2_attn_allreduce_3
	layer_2_input -> layer_2_qkv_13
	layer_2_qkv_13 -> layer_2_attn_scores_13
	layer_2_attn_scores_13 -> layer_2_attn_softmax_13
	layer_2_attn_softmax_13 -> layer_2_attn_out_13
	layer_2_attn_out_13 -> layer_2_attn_allreduce_3
	layer_2_input -> layer_2_qkv_14
	layer_2_qkv_14 -> layer_2_attn_scores_14
	layer_2_attn_scores_14 -> layer_2_attn_softmax_14
	layer_2_attn_softmax_14 -> layer_2_attn_out_14
	layer_2_attn_out_14 -> layer_2_attn_allreduce_3
	layer_2_input -> layer_2_qkv_15
	layer_2_qkv_15 -> layer_2_attn_scores_15
	layer_2_attn_scores_15 -> layer_2_attn_softmax_15
	layer_2_attn_softmax_15 -> layer_2_attn_out_15
	layer_2_attn_out_15 -> layer_2_attn_allreduce_3
	layer_2_qkv_16 [label="Layer2_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_17 [label="Layer2_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_18 [label="Layer2_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_19 [label="Layer2_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_16 [label="Layer2_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_17 [label="Layer2_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_18 [label="Layer2_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_19 [label="Layer2_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_16 [label="Layer2_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_17 [label="Layer2_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_18 [label="Layer2_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_19 [label="Layer2_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_16 [label="Layer2_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_17 [label="Layer2_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_18 [label="Layer2_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_19 [label="Layer2_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_4 [label="Layer2_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_16
	layer_2_qkv_16 -> layer_2_attn_scores_16
	layer_2_attn_scores_16 -> layer_2_attn_softmax_16
	layer_2_attn_softmax_16 -> layer_2_attn_out_16
	layer_2_attn_out_16 -> layer_2_attn_allreduce_4
	layer_2_input -> layer_2_qkv_17
	layer_2_qkv_17 -> layer_2_attn_scores_17
	layer_2_attn_scores_17 -> layer_2_attn_softmax_17
	layer_2_attn_softmax_17 -> layer_2_attn_out_17
	layer_2_attn_out_17 -> layer_2_attn_allreduce_4
	layer_2_input -> layer_2_qkv_18
	layer_2_qkv_18 -> layer_2_attn_scores_18
	layer_2_attn_scores_18 -> layer_2_attn_softmax_18
	layer_2_attn_softmax_18 -> layer_2_attn_out_18
	layer_2_attn_out_18 -> layer_2_attn_allreduce_4
	layer_2_input -> layer_2_qkv_19
	layer_2_qkv_19 -> layer_2_attn_scores_19
	layer_2_attn_scores_19 -> layer_2_attn_softmax_19
	layer_2_attn_softmax_19 -> layer_2_attn_out_19
	layer_2_attn_out_19 -> layer_2_attn_allreduce_4
	layer_2_qkv_20 [label="Layer2_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_21 [label="Layer2_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_22 [label="Layer2_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_23 [label="Layer2_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_20 [label="Layer2_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_21 [label="Layer2_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_22 [label="Layer2_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_23 [label="Layer2_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_20 [label="Layer2_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_21 [label="Layer2_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_22 [label="Layer2_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_23 [label="Layer2_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_20 [label="Layer2_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_21 [label="Layer2_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_22 [label="Layer2_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_23 [label="Layer2_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_5 [label="Layer2_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_20
	layer_2_qkv_20 -> layer_2_attn_scores_20
	layer_2_attn_scores_20 -> layer_2_attn_softmax_20
	layer_2_attn_softmax_20 -> layer_2_attn_out_20
	layer_2_attn_out_20 -> layer_2_attn_allreduce_5
	layer_2_input -> layer_2_qkv_21
	layer_2_qkv_21 -> layer_2_attn_scores_21
	layer_2_attn_scores_21 -> layer_2_attn_softmax_21
	layer_2_attn_softmax_21 -> layer_2_attn_out_21
	layer_2_attn_out_21 -> layer_2_attn_allreduce_5
	layer_2_input -> layer_2_qkv_22
	layer_2_qkv_22 -> layer_2_attn_scores_22
	layer_2_attn_scores_22 -> layer_2_attn_softmax_22
	layer_2_attn_softmax_22 -> layer_2_attn_out_22
	layer_2_attn_out_22 -> layer_2_attn_allreduce_5
	layer_2_input -> layer_2_qkv_23
	layer_2_qkv_23 -> layer_2_attn_scores_23
	layer_2_attn_scores_23 -> layer_2_attn_softmax_23
	layer_2_attn_softmax_23 -> layer_2_attn_out_23
	layer_2_attn_out_23 -> layer_2_attn_allreduce_5
	layer_2_qkv_24 [label="Layer2_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_25 [label="Layer2_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_26 [label="Layer2_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_27 [label="Layer2_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_24 [label="Layer2_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_25 [label="Layer2_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_26 [label="Layer2_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_27 [label="Layer2_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_24 [label="Layer2_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_25 [label="Layer2_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_26 [label="Layer2_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_27 [label="Layer2_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_24 [label="Layer2_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_25 [label="Layer2_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_26 [label="Layer2_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_27 [label="Layer2_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_6 [label="Layer2_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_24
	layer_2_qkv_24 -> layer_2_attn_scores_24
	layer_2_attn_scores_24 -> layer_2_attn_softmax_24
	layer_2_attn_softmax_24 -> layer_2_attn_out_24
	layer_2_attn_out_24 -> layer_2_attn_allreduce_6
	layer_2_input -> layer_2_qkv_25
	layer_2_qkv_25 -> layer_2_attn_scores_25
	layer_2_attn_scores_25 -> layer_2_attn_softmax_25
	layer_2_attn_softmax_25 -> layer_2_attn_out_25
	layer_2_attn_out_25 -> layer_2_attn_allreduce_6
	layer_2_input -> layer_2_qkv_26
	layer_2_qkv_26 -> layer_2_attn_scores_26
	layer_2_attn_scores_26 -> layer_2_attn_softmax_26
	layer_2_attn_softmax_26 -> layer_2_attn_out_26
	layer_2_attn_out_26 -> layer_2_attn_allreduce_6
	layer_2_input -> layer_2_qkv_27
	layer_2_qkv_27 -> layer_2_attn_scores_27
	layer_2_attn_scores_27 -> layer_2_attn_softmax_27
	layer_2_attn_softmax_27 -> layer_2_attn_out_27
	layer_2_attn_out_27 -> layer_2_attn_allreduce_6
	layer_2_qkv_28 [label="Layer2_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_29 [label="Layer2_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_30 [label="Layer2_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_31 [label="Layer2_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_28 [label="Layer2_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_29 [label="Layer2_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_30 [label="Layer2_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_31 [label="Layer2_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_28 [label="Layer2_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_29 [label="Layer2_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_30 [label="Layer2_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_31 [label="Layer2_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_28 [label="Layer2_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_29 [label="Layer2_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_30 [label="Layer2_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_31 [label="Layer2_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_7 [label="Layer2_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_28
	layer_2_qkv_28 -> layer_2_attn_scores_28
	layer_2_attn_scores_28 -> layer_2_attn_softmax_28
	layer_2_attn_softmax_28 -> layer_2_attn_out_28
	layer_2_attn_out_28 -> layer_2_attn_allreduce_7
	layer_2_input -> layer_2_qkv_29
	layer_2_qkv_29 -> layer_2_attn_scores_29
	layer_2_attn_scores_29 -> layer_2_attn_softmax_29
	layer_2_attn_softmax_29 -> layer_2_attn_out_29
	layer_2_attn_out_29 -> layer_2_attn_allreduce_7
	layer_2_input -> layer_2_qkv_30
	layer_2_qkv_30 -> layer_2_attn_scores_30
	layer_2_attn_scores_30 -> layer_2_attn_softmax_30
	layer_2_attn_softmax_30 -> layer_2_attn_out_30
	layer_2_attn_out_30 -> layer_2_attn_allreduce_7
	layer_2_input -> layer_2_qkv_31
	layer_2_qkv_31 -> layer_2_attn_scores_31
	layer_2_attn_scores_31 -> layer_2_attn_softmax_31
	layer_2_attn_softmax_31 -> layer_2_attn_out_31
	layer_2_attn_out_31 -> layer_2_attn_allreduce_7
	layer_2_qkv_32 [label="Layer2_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_33 [label="Layer2_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_34 [label="Layer2_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_35 [label="Layer2_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_32 [label="Layer2_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_33 [label="Layer2_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_34 [label="Layer2_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_35 [label="Layer2_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_32 [label="Layer2_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_33 [label="Layer2_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_34 [label="Layer2_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_35 [label="Layer2_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_32 [label="Layer2_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_33 [label="Layer2_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_34 [label="Layer2_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_35 [label="Layer2_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_8 [label="Layer2_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_32
	layer_2_qkv_32 -> layer_2_attn_scores_32
	layer_2_attn_scores_32 -> layer_2_attn_softmax_32
	layer_2_attn_softmax_32 -> layer_2_attn_out_32
	layer_2_attn_out_32 -> layer_2_attn_allreduce_8
	layer_2_input -> layer_2_qkv_33
	layer_2_qkv_33 -> layer_2_attn_scores_33
	layer_2_attn_scores_33 -> layer_2_attn_softmax_33
	layer_2_attn_softmax_33 -> layer_2_attn_out_33
	layer_2_attn_out_33 -> layer_2_attn_allreduce_8
	layer_2_input -> layer_2_qkv_34
	layer_2_qkv_34 -> layer_2_attn_scores_34
	layer_2_attn_scores_34 -> layer_2_attn_softmax_34
	layer_2_attn_softmax_34 -> layer_2_attn_out_34
	layer_2_attn_out_34 -> layer_2_attn_allreduce_8
	layer_2_input -> layer_2_qkv_35
	layer_2_qkv_35 -> layer_2_attn_scores_35
	layer_2_attn_scores_35 -> layer_2_attn_softmax_35
	layer_2_attn_softmax_35 -> layer_2_attn_out_35
	layer_2_attn_out_35 -> layer_2_attn_allreduce_8
	layer_2_qkv_36 [label="Layer2_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_37 [label="Layer2_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_38 [label="Layer2_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_39 [label="Layer2_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_36 [label="Layer2_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_37 [label="Layer2_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_38 [label="Layer2_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_39 [label="Layer2_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_36 [label="Layer2_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_37 [label="Layer2_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_38 [label="Layer2_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_39 [label="Layer2_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_36 [label="Layer2_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_37 [label="Layer2_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_38 [label="Layer2_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_39 [label="Layer2_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_9 [label="Layer2_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_36
	layer_2_qkv_36 -> layer_2_attn_scores_36
	layer_2_attn_scores_36 -> layer_2_attn_softmax_36
	layer_2_attn_softmax_36 -> layer_2_attn_out_36
	layer_2_attn_out_36 -> layer_2_attn_allreduce_9
	layer_2_input -> layer_2_qkv_37
	layer_2_qkv_37 -> layer_2_attn_scores_37
	layer_2_attn_scores_37 -> layer_2_attn_softmax_37
	layer_2_attn_softmax_37 -> layer_2_attn_out_37
	layer_2_attn_out_37 -> layer_2_attn_allreduce_9
	layer_2_input -> layer_2_qkv_38
	layer_2_qkv_38 -> layer_2_attn_scores_38
	layer_2_attn_scores_38 -> layer_2_attn_softmax_38
	layer_2_attn_softmax_38 -> layer_2_attn_out_38
	layer_2_attn_out_38 -> layer_2_attn_allreduce_9
	layer_2_input -> layer_2_qkv_39
	layer_2_qkv_39 -> layer_2_attn_scores_39
	layer_2_attn_scores_39 -> layer_2_attn_softmax_39
	layer_2_attn_softmax_39 -> layer_2_attn_out_39
	layer_2_attn_out_39 -> layer_2_attn_allreduce_9
	layer_2_qkv_40 [label="Layer2_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_41 [label="Layer2_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_42 [label="Layer2_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_43 [label="Layer2_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_40 [label="Layer2_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_41 [label="Layer2_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_42 [label="Layer2_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_43 [label="Layer2_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_40 [label="Layer2_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_41 [label="Layer2_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_42 [label="Layer2_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_43 [label="Layer2_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_40 [label="Layer2_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_41 [label="Layer2_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_42 [label="Layer2_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_43 [label="Layer2_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_10 [label="Layer2_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_40
	layer_2_qkv_40 -> layer_2_attn_scores_40
	layer_2_attn_scores_40 -> layer_2_attn_softmax_40
	layer_2_attn_softmax_40 -> layer_2_attn_out_40
	layer_2_attn_out_40 -> layer_2_attn_allreduce_10
	layer_2_input -> layer_2_qkv_41
	layer_2_qkv_41 -> layer_2_attn_scores_41
	layer_2_attn_scores_41 -> layer_2_attn_softmax_41
	layer_2_attn_softmax_41 -> layer_2_attn_out_41
	layer_2_attn_out_41 -> layer_2_attn_allreduce_10
	layer_2_input -> layer_2_qkv_42
	layer_2_qkv_42 -> layer_2_attn_scores_42
	layer_2_attn_scores_42 -> layer_2_attn_softmax_42
	layer_2_attn_softmax_42 -> layer_2_attn_out_42
	layer_2_attn_out_42 -> layer_2_attn_allreduce_10
	layer_2_input -> layer_2_qkv_43
	layer_2_qkv_43 -> layer_2_attn_scores_43
	layer_2_attn_scores_43 -> layer_2_attn_softmax_43
	layer_2_attn_softmax_43 -> layer_2_attn_out_43
	layer_2_attn_out_43 -> layer_2_attn_allreduce_10
	layer_2_qkv_44 [label="Layer2_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_45 [label="Layer2_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_46 [label="Layer2_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_47 [label="Layer2_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_44 [label="Layer2_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_45 [label="Layer2_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_46 [label="Layer2_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_47 [label="Layer2_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_44 [label="Layer2_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_45 [label="Layer2_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_46 [label="Layer2_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_47 [label="Layer2_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_44 [label="Layer2_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_45 [label="Layer2_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_46 [label="Layer2_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_47 [label="Layer2_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_11 [label="Layer2_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_44
	layer_2_qkv_44 -> layer_2_attn_scores_44
	layer_2_attn_scores_44 -> layer_2_attn_softmax_44
	layer_2_attn_softmax_44 -> layer_2_attn_out_44
	layer_2_attn_out_44 -> layer_2_attn_allreduce_11
	layer_2_input -> layer_2_qkv_45
	layer_2_qkv_45 -> layer_2_attn_scores_45
	layer_2_attn_scores_45 -> layer_2_attn_softmax_45
	layer_2_attn_softmax_45 -> layer_2_attn_out_45
	layer_2_attn_out_45 -> layer_2_attn_allreduce_11
	layer_2_input -> layer_2_qkv_46
	layer_2_qkv_46 -> layer_2_attn_scores_46
	layer_2_attn_scores_46 -> layer_2_attn_softmax_46
	layer_2_attn_softmax_46 -> layer_2_attn_out_46
	layer_2_attn_out_46 -> layer_2_attn_allreduce_11
	layer_2_input -> layer_2_qkv_47
	layer_2_qkv_47 -> layer_2_attn_scores_47
	layer_2_attn_scores_47 -> layer_2_attn_softmax_47
	layer_2_attn_softmax_47 -> layer_2_attn_out_47
	layer_2_attn_out_47 -> layer_2_attn_allreduce_11
	layer_2_qkv_48 [label="Layer2_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_49 [label="Layer2_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_50 [label="Layer2_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_51 [label="Layer2_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_48 [label="Layer2_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_49 [label="Layer2_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_50 [label="Layer2_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_51 [label="Layer2_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_48 [label="Layer2_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_49 [label="Layer2_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_50 [label="Layer2_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_51 [label="Layer2_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_48 [label="Layer2_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_49 [label="Layer2_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_50 [label="Layer2_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_51 [label="Layer2_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_12 [label="Layer2_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_48
	layer_2_qkv_48 -> layer_2_attn_scores_48
	layer_2_attn_scores_48 -> layer_2_attn_softmax_48
	layer_2_attn_softmax_48 -> layer_2_attn_out_48
	layer_2_attn_out_48 -> layer_2_attn_allreduce_12
	layer_2_input -> layer_2_qkv_49
	layer_2_qkv_49 -> layer_2_attn_scores_49
	layer_2_attn_scores_49 -> layer_2_attn_softmax_49
	layer_2_attn_softmax_49 -> layer_2_attn_out_49
	layer_2_attn_out_49 -> layer_2_attn_allreduce_12
	layer_2_input -> layer_2_qkv_50
	layer_2_qkv_50 -> layer_2_attn_scores_50
	layer_2_attn_scores_50 -> layer_2_attn_softmax_50
	layer_2_attn_softmax_50 -> layer_2_attn_out_50
	layer_2_attn_out_50 -> layer_2_attn_allreduce_12
	layer_2_input -> layer_2_qkv_51
	layer_2_qkv_51 -> layer_2_attn_scores_51
	layer_2_attn_scores_51 -> layer_2_attn_softmax_51
	layer_2_attn_softmax_51 -> layer_2_attn_out_51
	layer_2_attn_out_51 -> layer_2_attn_allreduce_12
	layer_2_qkv_52 [label="Layer2_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_53 [label="Layer2_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_54 [label="Layer2_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_55 [label="Layer2_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_52 [label="Layer2_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_53 [label="Layer2_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_54 [label="Layer2_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_55 [label="Layer2_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_52 [label="Layer2_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_53 [label="Layer2_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_54 [label="Layer2_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_55 [label="Layer2_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_52 [label="Layer2_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_53 [label="Layer2_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_54 [label="Layer2_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_55 [label="Layer2_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_13 [label="Layer2_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_52
	layer_2_qkv_52 -> layer_2_attn_scores_52
	layer_2_attn_scores_52 -> layer_2_attn_softmax_52
	layer_2_attn_softmax_52 -> layer_2_attn_out_52
	layer_2_attn_out_52 -> layer_2_attn_allreduce_13
	layer_2_input -> layer_2_qkv_53
	layer_2_qkv_53 -> layer_2_attn_scores_53
	layer_2_attn_scores_53 -> layer_2_attn_softmax_53
	layer_2_attn_softmax_53 -> layer_2_attn_out_53
	layer_2_attn_out_53 -> layer_2_attn_allreduce_13
	layer_2_input -> layer_2_qkv_54
	layer_2_qkv_54 -> layer_2_attn_scores_54
	layer_2_attn_scores_54 -> layer_2_attn_softmax_54
	layer_2_attn_softmax_54 -> layer_2_attn_out_54
	layer_2_attn_out_54 -> layer_2_attn_allreduce_13
	layer_2_input -> layer_2_qkv_55
	layer_2_qkv_55 -> layer_2_attn_scores_55
	layer_2_attn_scores_55 -> layer_2_attn_softmax_55
	layer_2_attn_softmax_55 -> layer_2_attn_out_55
	layer_2_attn_out_55 -> layer_2_attn_allreduce_13
	layer_2_qkv_56 [label="Layer2_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_57 [label="Layer2_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_58 [label="Layer2_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_59 [label="Layer2_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_56 [label="Layer2_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_57 [label="Layer2_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_58 [label="Layer2_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_59 [label="Layer2_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_56 [label="Layer2_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_57 [label="Layer2_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_58 [label="Layer2_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_59 [label="Layer2_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_56 [label="Layer2_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_57 [label="Layer2_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_58 [label="Layer2_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_59 [label="Layer2_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_14 [label="Layer2_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_56
	layer_2_qkv_56 -> layer_2_attn_scores_56
	layer_2_attn_scores_56 -> layer_2_attn_softmax_56
	layer_2_attn_softmax_56 -> layer_2_attn_out_56
	layer_2_attn_out_56 -> layer_2_attn_allreduce_14
	layer_2_input -> layer_2_qkv_57
	layer_2_qkv_57 -> layer_2_attn_scores_57
	layer_2_attn_scores_57 -> layer_2_attn_softmax_57
	layer_2_attn_softmax_57 -> layer_2_attn_out_57
	layer_2_attn_out_57 -> layer_2_attn_allreduce_14
	layer_2_input -> layer_2_qkv_58
	layer_2_qkv_58 -> layer_2_attn_scores_58
	layer_2_attn_scores_58 -> layer_2_attn_softmax_58
	layer_2_attn_softmax_58 -> layer_2_attn_out_58
	layer_2_attn_out_58 -> layer_2_attn_allreduce_14
	layer_2_input -> layer_2_qkv_59
	layer_2_qkv_59 -> layer_2_attn_scores_59
	layer_2_attn_scores_59 -> layer_2_attn_softmax_59
	layer_2_attn_softmax_59 -> layer_2_attn_out_59
	layer_2_attn_out_59 -> layer_2_attn_allreduce_14
	layer_2_qkv_60 [label="Layer2_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_61 [label="Layer2_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_62 [label="Layer2_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_qkv_63 [label="Layer2_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_60 [label="Layer2_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_61 [label="Layer2_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_62 [label="Layer2_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_scores_63 [label="Layer2_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_60 [label="Layer2_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_61 [label="Layer2_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_62 [label="Layer2_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_softmax_63 [label="Layer2_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_60 [label="Layer2_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_61 [label="Layer2_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_62 [label="Layer2_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_out_63 [label="Layer2_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_15 [label="Layer2_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_input -> layer_2_qkv_60
	layer_2_qkv_60 -> layer_2_attn_scores_60
	layer_2_attn_scores_60 -> layer_2_attn_softmax_60
	layer_2_attn_softmax_60 -> layer_2_attn_out_60
	layer_2_attn_out_60 -> layer_2_attn_allreduce_15
	layer_2_input -> layer_2_qkv_61
	layer_2_qkv_61 -> layer_2_attn_scores_61
	layer_2_attn_scores_61 -> layer_2_attn_softmax_61
	layer_2_attn_softmax_61 -> layer_2_attn_out_61
	layer_2_attn_out_61 -> layer_2_attn_allreduce_15
	layer_2_input -> layer_2_qkv_62
	layer_2_qkv_62 -> layer_2_attn_scores_62
	layer_2_attn_scores_62 -> layer_2_attn_softmax_62
	layer_2_attn_softmax_62 -> layer_2_attn_out_62
	layer_2_attn_out_62 -> layer_2_attn_allreduce_15
	layer_2_input -> layer_2_qkv_63
	layer_2_qkv_63 -> layer_2_attn_scores_63
	layer_2_attn_scores_63 -> layer_2_attn_softmax_63
	layer_2_attn_softmax_63 -> layer_2_attn_out_63
	layer_2_attn_out_63 -> layer_2_attn_allreduce_15
	layer_2_gate_0 [label="Layer2_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_4 [label="Layer2_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_8 [label="Layer2_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_12 [label="Layer2_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_16 [label="Layer2_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_20 [label="Layer2_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_24 [label="Layer2_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_28 [label="Layer2_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_32 [label="Layer2_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_36 [label="Layer2_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_40 [label="Layer2_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_44 [label="Layer2_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_48 [label="Layer2_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_52 [label="Layer2_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_56 [label="Layer2_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_gate_60 [label="Layer2_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_2_alltoall [label="Layer2_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_0 [label="Layer2_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_1 [label="Layer2_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_2 [label="Layer2_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_3 [label="Layer2_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_0 [label="Layer2_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_1 [label="Layer2_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_2 [label="Layer2_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_3 [label="Layer2_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_0 [label="Layer2_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_1 [label="Layer2_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_2 [label="Layer2_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_3 [label="Layer2_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_0 [label="Layer2_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_4 [label="Layer2_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_5 [label="Layer2_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_6 [label="Layer2_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_7 [label="Layer2_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_4 [label="Layer2_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_5 [label="Layer2_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_6 [label="Layer2_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_7 [label="Layer2_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_4 [label="Layer2_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_5 [label="Layer2_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_6 [label="Layer2_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_7 [label="Layer2_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_1 [label="Layer2_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_8 [label="Layer2_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_9 [label="Layer2_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_10 [label="Layer2_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_11 [label="Layer2_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_8 [label="Layer2_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_9 [label="Layer2_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_10 [label="Layer2_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_11 [label="Layer2_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_8 [label="Layer2_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_9 [label="Layer2_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_10 [label="Layer2_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_11 [label="Layer2_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_2 [label="Layer2_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_12 [label="Layer2_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_13 [label="Layer2_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_14 [label="Layer2_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_15 [label="Layer2_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_12 [label="Layer2_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_13 [label="Layer2_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_14 [label="Layer2_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_15 [label="Layer2_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_12 [label="Layer2_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_13 [label="Layer2_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_14 [label="Layer2_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_15 [label="Layer2_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_3 [label="Layer2_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_16 [label="Layer2_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_17 [label="Layer2_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_18 [label="Layer2_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_19 [label="Layer2_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_16 [label="Layer2_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_17 [label="Layer2_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_18 [label="Layer2_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_19 [label="Layer2_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_16 [label="Layer2_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_17 [label="Layer2_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_18 [label="Layer2_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_19 [label="Layer2_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_4 [label="Layer2_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_20 [label="Layer2_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_21 [label="Layer2_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_22 [label="Layer2_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_23 [label="Layer2_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_20 [label="Layer2_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_21 [label="Layer2_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_22 [label="Layer2_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_23 [label="Layer2_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_20 [label="Layer2_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_21 [label="Layer2_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_22 [label="Layer2_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_23 [label="Layer2_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_5 [label="Layer2_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_24 [label="Layer2_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_25 [label="Layer2_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_26 [label="Layer2_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_27 [label="Layer2_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_24 [label="Layer2_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_25 [label="Layer2_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_26 [label="Layer2_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_27 [label="Layer2_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_24 [label="Layer2_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_25 [label="Layer2_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_26 [label="Layer2_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_27 [label="Layer2_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_6 [label="Layer2_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_28 [label="Layer2_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_29 [label="Layer2_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_30 [label="Layer2_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_31 [label="Layer2_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_28 [label="Layer2_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_29 [label="Layer2_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_30 [label="Layer2_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_31 [label="Layer2_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_28 [label="Layer2_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_29 [label="Layer2_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_30 [label="Layer2_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_31 [label="Layer2_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_7 [label="Layer2_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_32 [label="Layer2_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_33 [label="Layer2_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_34 [label="Layer2_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_35 [label="Layer2_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_32 [label="Layer2_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_33 [label="Layer2_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_34 [label="Layer2_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_35 [label="Layer2_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_32 [label="Layer2_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_33 [label="Layer2_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_34 [label="Layer2_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_35 [label="Layer2_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_8 [label="Layer2_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_36 [label="Layer2_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_37 [label="Layer2_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_38 [label="Layer2_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_39 [label="Layer2_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_36 [label="Layer2_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_37 [label="Layer2_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_38 [label="Layer2_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_39 [label="Layer2_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_36 [label="Layer2_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_37 [label="Layer2_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_38 [label="Layer2_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_39 [label="Layer2_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_9 [label="Layer2_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_40 [label="Layer2_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_41 [label="Layer2_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_42 [label="Layer2_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_43 [label="Layer2_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_40 [label="Layer2_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_41 [label="Layer2_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_42 [label="Layer2_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_43 [label="Layer2_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_40 [label="Layer2_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_41 [label="Layer2_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_42 [label="Layer2_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_43 [label="Layer2_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_10 [label="Layer2_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_44 [label="Layer2_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_45 [label="Layer2_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_46 [label="Layer2_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_47 [label="Layer2_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_44 [label="Layer2_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_45 [label="Layer2_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_46 [label="Layer2_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_47 [label="Layer2_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_44 [label="Layer2_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_45 [label="Layer2_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_46 [label="Layer2_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_47 [label="Layer2_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_11 [label="Layer2_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_48 [label="Layer2_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_49 [label="Layer2_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_50 [label="Layer2_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_51 [label="Layer2_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_48 [label="Layer2_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_49 [label="Layer2_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_50 [label="Layer2_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_51 [label="Layer2_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_48 [label="Layer2_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_49 [label="Layer2_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_50 [label="Layer2_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_51 [label="Layer2_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_12 [label="Layer2_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_52 [label="Layer2_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_53 [label="Layer2_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_54 [label="Layer2_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_55 [label="Layer2_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_52 [label="Layer2_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_53 [label="Layer2_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_54 [label="Layer2_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_55 [label="Layer2_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_52 [label="Layer2_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_53 [label="Layer2_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_54 [label="Layer2_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_55 [label="Layer2_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_13 [label="Layer2_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_56 [label="Layer2_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_57 [label="Layer2_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_58 [label="Layer2_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_59 [label="Layer2_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_56 [label="Layer2_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_57 [label="Layer2_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_58 [label="Layer2_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_59 [label="Layer2_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_56 [label="Layer2_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_57 [label="Layer2_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_58 [label="Layer2_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_59 [label="Layer2_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_14 [label="Layer2_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert1_60 [label="Layer2_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_61 [label="Layer2_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_62 [label="Layer2_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert1_63 [label="Layer2_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_60 [label="Layer2_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_61 [label="Layer2_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_62 [label="Layer2_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert_act_63 [label="Layer2_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_2_expert2_60 [label="Layer2_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_61 [label="Layer2_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_62 [label="Layer2_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert2_63 [label="Layer2_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_2_expert_allreduce_15 [label="Layer2_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_2_expert_agg [label="Layer2_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_2_norm [label="Layer2_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_2_attn_allreduce_0 -> layer_2_gate_0 [style=dashed]
	layer_2_gate_0 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_0
	layer_2_expert1_0 -> layer_2_expert_act_0
	layer_2_expert_act_0 -> layer_2_expert2_0
	layer_2_expert2_0 -> layer_2_expert_allreduce_0
	layer_2_alltoall -> layer_2_expert1_1
	layer_2_expert1_1 -> layer_2_expert_act_1
	layer_2_expert_act_1 -> layer_2_expert2_1
	layer_2_expert2_1 -> layer_2_expert_allreduce_0
	layer_2_alltoall -> layer_2_expert1_2
	layer_2_expert1_2 -> layer_2_expert_act_2
	layer_2_expert_act_2 -> layer_2_expert2_2
	layer_2_expert2_2 -> layer_2_expert_allreduce_0
	layer_2_alltoall -> layer_2_expert1_3
	layer_2_expert1_3 -> layer_2_expert_act_3
	layer_2_expert_act_3 -> layer_2_expert2_3
	layer_2_expert2_3 -> layer_2_expert_allreduce_0
	layer_2_expert_allreduce_0 -> layer_2_expert_agg
	layer_2_attn_allreduce_1 -> layer_2_gate_4 [style=dashed]
	layer_2_gate_4 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_4
	layer_2_expert1_4 -> layer_2_expert_act_4
	layer_2_expert_act_4 -> layer_2_expert2_4
	layer_2_expert2_4 -> layer_2_expert_allreduce_1
	layer_2_alltoall -> layer_2_expert1_5
	layer_2_expert1_5 -> layer_2_expert_act_5
	layer_2_expert_act_5 -> layer_2_expert2_5
	layer_2_expert2_5 -> layer_2_expert_allreduce_1
	layer_2_alltoall -> layer_2_expert1_6
	layer_2_expert1_6 -> layer_2_expert_act_6
	layer_2_expert_act_6 -> layer_2_expert2_6
	layer_2_expert2_6 -> layer_2_expert_allreduce_1
	layer_2_alltoall -> layer_2_expert1_7
	layer_2_expert1_7 -> layer_2_expert_act_7
	layer_2_expert_act_7 -> layer_2_expert2_7
	layer_2_expert2_7 -> layer_2_expert_allreduce_1
	layer_2_expert_allreduce_1 -> layer_2_expert_agg
	layer_2_attn_allreduce_2 -> layer_2_gate_8 [style=dashed]
	layer_2_gate_8 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_8
	layer_2_expert1_8 -> layer_2_expert_act_8
	layer_2_expert_act_8 -> layer_2_expert2_8
	layer_2_expert2_8 -> layer_2_expert_allreduce_2
	layer_2_alltoall -> layer_2_expert1_9
	layer_2_expert1_9 -> layer_2_expert_act_9
	layer_2_expert_act_9 -> layer_2_expert2_9
	layer_2_expert2_9 -> layer_2_expert_allreduce_2
	layer_2_alltoall -> layer_2_expert1_10
	layer_2_expert1_10 -> layer_2_expert_act_10
	layer_2_expert_act_10 -> layer_2_expert2_10
	layer_2_expert2_10 -> layer_2_expert_allreduce_2
	layer_2_alltoall -> layer_2_expert1_11
	layer_2_expert1_11 -> layer_2_expert_act_11
	layer_2_expert_act_11 -> layer_2_expert2_11
	layer_2_expert2_11 -> layer_2_expert_allreduce_2
	layer_2_expert_allreduce_2 -> layer_2_expert_agg
	layer_2_attn_allreduce_3 -> layer_2_gate_12 [style=dashed]
	layer_2_gate_12 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_12
	layer_2_expert1_12 -> layer_2_expert_act_12
	layer_2_expert_act_12 -> layer_2_expert2_12
	layer_2_expert2_12 -> layer_2_expert_allreduce_3
	layer_2_alltoall -> layer_2_expert1_13
	layer_2_expert1_13 -> layer_2_expert_act_13
	layer_2_expert_act_13 -> layer_2_expert2_13
	layer_2_expert2_13 -> layer_2_expert_allreduce_3
	layer_2_alltoall -> layer_2_expert1_14
	layer_2_expert1_14 -> layer_2_expert_act_14
	layer_2_expert_act_14 -> layer_2_expert2_14
	layer_2_expert2_14 -> layer_2_expert_allreduce_3
	layer_2_alltoall -> layer_2_expert1_15
	layer_2_expert1_15 -> layer_2_expert_act_15
	layer_2_expert_act_15 -> layer_2_expert2_15
	layer_2_expert2_15 -> layer_2_expert_allreduce_3
	layer_2_expert_allreduce_3 -> layer_2_expert_agg
	layer_2_attn_allreduce_4 -> layer_2_gate_16 [style=dashed]
	layer_2_gate_16 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_16
	layer_2_expert1_16 -> layer_2_expert_act_16
	layer_2_expert_act_16 -> layer_2_expert2_16
	layer_2_expert2_16 -> layer_2_expert_allreduce_4
	layer_2_alltoall -> layer_2_expert1_17
	layer_2_expert1_17 -> layer_2_expert_act_17
	layer_2_expert_act_17 -> layer_2_expert2_17
	layer_2_expert2_17 -> layer_2_expert_allreduce_4
	layer_2_alltoall -> layer_2_expert1_18
	layer_2_expert1_18 -> layer_2_expert_act_18
	layer_2_expert_act_18 -> layer_2_expert2_18
	layer_2_expert2_18 -> layer_2_expert_allreduce_4
	layer_2_alltoall -> layer_2_expert1_19
	layer_2_expert1_19 -> layer_2_expert_act_19
	layer_2_expert_act_19 -> layer_2_expert2_19
	layer_2_expert2_19 -> layer_2_expert_allreduce_4
	layer_2_expert_allreduce_4 -> layer_2_expert_agg
	layer_2_attn_allreduce_5 -> layer_2_gate_20 [style=dashed]
	layer_2_gate_20 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_20
	layer_2_expert1_20 -> layer_2_expert_act_20
	layer_2_expert_act_20 -> layer_2_expert2_20
	layer_2_expert2_20 -> layer_2_expert_allreduce_5
	layer_2_alltoall -> layer_2_expert1_21
	layer_2_expert1_21 -> layer_2_expert_act_21
	layer_2_expert_act_21 -> layer_2_expert2_21
	layer_2_expert2_21 -> layer_2_expert_allreduce_5
	layer_2_alltoall -> layer_2_expert1_22
	layer_2_expert1_22 -> layer_2_expert_act_22
	layer_2_expert_act_22 -> layer_2_expert2_22
	layer_2_expert2_22 -> layer_2_expert_allreduce_5
	layer_2_alltoall -> layer_2_expert1_23
	layer_2_expert1_23 -> layer_2_expert_act_23
	layer_2_expert_act_23 -> layer_2_expert2_23
	layer_2_expert2_23 -> layer_2_expert_allreduce_5
	layer_2_expert_allreduce_5 -> layer_2_expert_agg
	layer_2_attn_allreduce_6 -> layer_2_gate_24 [style=dashed]
	layer_2_gate_24 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_24
	layer_2_expert1_24 -> layer_2_expert_act_24
	layer_2_expert_act_24 -> layer_2_expert2_24
	layer_2_expert2_24 -> layer_2_expert_allreduce_6
	layer_2_alltoall -> layer_2_expert1_25
	layer_2_expert1_25 -> layer_2_expert_act_25
	layer_2_expert_act_25 -> layer_2_expert2_25
	layer_2_expert2_25 -> layer_2_expert_allreduce_6
	layer_2_alltoall -> layer_2_expert1_26
	layer_2_expert1_26 -> layer_2_expert_act_26
	layer_2_expert_act_26 -> layer_2_expert2_26
	layer_2_expert2_26 -> layer_2_expert_allreduce_6
	layer_2_alltoall -> layer_2_expert1_27
	layer_2_expert1_27 -> layer_2_expert_act_27
	layer_2_expert_act_27 -> layer_2_expert2_27
	layer_2_expert2_27 -> layer_2_expert_allreduce_6
	layer_2_expert_allreduce_6 -> layer_2_expert_agg
	layer_2_attn_allreduce_7 -> layer_2_gate_28 [style=dashed]
	layer_2_gate_28 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_28
	layer_2_expert1_28 -> layer_2_expert_act_28
	layer_2_expert_act_28 -> layer_2_expert2_28
	layer_2_expert2_28 -> layer_2_expert_allreduce_7
	layer_2_alltoall -> layer_2_expert1_29
	layer_2_expert1_29 -> layer_2_expert_act_29
	layer_2_expert_act_29 -> layer_2_expert2_29
	layer_2_expert2_29 -> layer_2_expert_allreduce_7
	layer_2_alltoall -> layer_2_expert1_30
	layer_2_expert1_30 -> layer_2_expert_act_30
	layer_2_expert_act_30 -> layer_2_expert2_30
	layer_2_expert2_30 -> layer_2_expert_allreduce_7
	layer_2_alltoall -> layer_2_expert1_31
	layer_2_expert1_31 -> layer_2_expert_act_31
	layer_2_expert_act_31 -> layer_2_expert2_31
	layer_2_expert2_31 -> layer_2_expert_allreduce_7
	layer_2_expert_allreduce_7 -> layer_2_expert_agg
	layer_2_attn_allreduce_8 -> layer_2_gate_32 [style=dashed]
	layer_2_gate_32 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_32
	layer_2_expert1_32 -> layer_2_expert_act_32
	layer_2_expert_act_32 -> layer_2_expert2_32
	layer_2_expert2_32 -> layer_2_expert_allreduce_8
	layer_2_alltoall -> layer_2_expert1_33
	layer_2_expert1_33 -> layer_2_expert_act_33
	layer_2_expert_act_33 -> layer_2_expert2_33
	layer_2_expert2_33 -> layer_2_expert_allreduce_8
	layer_2_alltoall -> layer_2_expert1_34
	layer_2_expert1_34 -> layer_2_expert_act_34
	layer_2_expert_act_34 -> layer_2_expert2_34
	layer_2_expert2_34 -> layer_2_expert_allreduce_8
	layer_2_alltoall -> layer_2_expert1_35
	layer_2_expert1_35 -> layer_2_expert_act_35
	layer_2_expert_act_35 -> layer_2_expert2_35
	layer_2_expert2_35 -> layer_2_expert_allreduce_8
	layer_2_expert_allreduce_8 -> layer_2_expert_agg
	layer_2_attn_allreduce_9 -> layer_2_gate_36 [style=dashed]
	layer_2_gate_36 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_36
	layer_2_expert1_36 -> layer_2_expert_act_36
	layer_2_expert_act_36 -> layer_2_expert2_36
	layer_2_expert2_36 -> layer_2_expert_allreduce_9
	layer_2_alltoall -> layer_2_expert1_37
	layer_2_expert1_37 -> layer_2_expert_act_37
	layer_2_expert_act_37 -> layer_2_expert2_37
	layer_2_expert2_37 -> layer_2_expert_allreduce_9
	layer_2_alltoall -> layer_2_expert1_38
	layer_2_expert1_38 -> layer_2_expert_act_38
	layer_2_expert_act_38 -> layer_2_expert2_38
	layer_2_expert2_38 -> layer_2_expert_allreduce_9
	layer_2_alltoall -> layer_2_expert1_39
	layer_2_expert1_39 -> layer_2_expert_act_39
	layer_2_expert_act_39 -> layer_2_expert2_39
	layer_2_expert2_39 -> layer_2_expert_allreduce_9
	layer_2_expert_allreduce_9 -> layer_2_expert_agg
	layer_2_attn_allreduce_10 -> layer_2_gate_40 [style=dashed]
	layer_2_gate_40 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_40
	layer_2_expert1_40 -> layer_2_expert_act_40
	layer_2_expert_act_40 -> layer_2_expert2_40
	layer_2_expert2_40 -> layer_2_expert_allreduce_10
	layer_2_alltoall -> layer_2_expert1_41
	layer_2_expert1_41 -> layer_2_expert_act_41
	layer_2_expert_act_41 -> layer_2_expert2_41
	layer_2_expert2_41 -> layer_2_expert_allreduce_10
	layer_2_alltoall -> layer_2_expert1_42
	layer_2_expert1_42 -> layer_2_expert_act_42
	layer_2_expert_act_42 -> layer_2_expert2_42
	layer_2_expert2_42 -> layer_2_expert_allreduce_10
	layer_2_alltoall -> layer_2_expert1_43
	layer_2_expert1_43 -> layer_2_expert_act_43
	layer_2_expert_act_43 -> layer_2_expert2_43
	layer_2_expert2_43 -> layer_2_expert_allreduce_10
	layer_2_expert_allreduce_10 -> layer_2_expert_agg
	layer_2_attn_allreduce_11 -> layer_2_gate_44 [style=dashed]
	layer_2_gate_44 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_44
	layer_2_expert1_44 -> layer_2_expert_act_44
	layer_2_expert_act_44 -> layer_2_expert2_44
	layer_2_expert2_44 -> layer_2_expert_allreduce_11
	layer_2_alltoall -> layer_2_expert1_45
	layer_2_expert1_45 -> layer_2_expert_act_45
	layer_2_expert_act_45 -> layer_2_expert2_45
	layer_2_expert2_45 -> layer_2_expert_allreduce_11
	layer_2_alltoall -> layer_2_expert1_46
	layer_2_expert1_46 -> layer_2_expert_act_46
	layer_2_expert_act_46 -> layer_2_expert2_46
	layer_2_expert2_46 -> layer_2_expert_allreduce_11
	layer_2_alltoall -> layer_2_expert1_47
	layer_2_expert1_47 -> layer_2_expert_act_47
	layer_2_expert_act_47 -> layer_2_expert2_47
	layer_2_expert2_47 -> layer_2_expert_allreduce_11
	layer_2_expert_allreduce_11 -> layer_2_expert_agg
	layer_2_attn_allreduce_12 -> layer_2_gate_48 [style=dashed]
	layer_2_gate_48 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_48
	layer_2_expert1_48 -> layer_2_expert_act_48
	layer_2_expert_act_48 -> layer_2_expert2_48
	layer_2_expert2_48 -> layer_2_expert_allreduce_12
	layer_2_alltoall -> layer_2_expert1_49
	layer_2_expert1_49 -> layer_2_expert_act_49
	layer_2_expert_act_49 -> layer_2_expert2_49
	layer_2_expert2_49 -> layer_2_expert_allreduce_12
	layer_2_alltoall -> layer_2_expert1_50
	layer_2_expert1_50 -> layer_2_expert_act_50
	layer_2_expert_act_50 -> layer_2_expert2_50
	layer_2_expert2_50 -> layer_2_expert_allreduce_12
	layer_2_alltoall -> layer_2_expert1_51
	layer_2_expert1_51 -> layer_2_expert_act_51
	layer_2_expert_act_51 -> layer_2_expert2_51
	layer_2_expert2_51 -> layer_2_expert_allreduce_12
	layer_2_expert_allreduce_12 -> layer_2_expert_agg
	layer_2_attn_allreduce_13 -> layer_2_gate_52 [style=dashed]
	layer_2_gate_52 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_52
	layer_2_expert1_52 -> layer_2_expert_act_52
	layer_2_expert_act_52 -> layer_2_expert2_52
	layer_2_expert2_52 -> layer_2_expert_allreduce_13
	layer_2_alltoall -> layer_2_expert1_53
	layer_2_expert1_53 -> layer_2_expert_act_53
	layer_2_expert_act_53 -> layer_2_expert2_53
	layer_2_expert2_53 -> layer_2_expert_allreduce_13
	layer_2_alltoall -> layer_2_expert1_54
	layer_2_expert1_54 -> layer_2_expert_act_54
	layer_2_expert_act_54 -> layer_2_expert2_54
	layer_2_expert2_54 -> layer_2_expert_allreduce_13
	layer_2_alltoall -> layer_2_expert1_55
	layer_2_expert1_55 -> layer_2_expert_act_55
	layer_2_expert_act_55 -> layer_2_expert2_55
	layer_2_expert2_55 -> layer_2_expert_allreduce_13
	layer_2_expert_allreduce_13 -> layer_2_expert_agg
	layer_2_attn_allreduce_14 -> layer_2_gate_56 [style=dashed]
	layer_2_gate_56 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_56
	layer_2_expert1_56 -> layer_2_expert_act_56
	layer_2_expert_act_56 -> layer_2_expert2_56
	layer_2_expert2_56 -> layer_2_expert_allreduce_14
	layer_2_alltoall -> layer_2_expert1_57
	layer_2_expert1_57 -> layer_2_expert_act_57
	layer_2_expert_act_57 -> layer_2_expert2_57
	layer_2_expert2_57 -> layer_2_expert_allreduce_14
	layer_2_alltoall -> layer_2_expert1_58
	layer_2_expert1_58 -> layer_2_expert_act_58
	layer_2_expert_act_58 -> layer_2_expert2_58
	layer_2_expert2_58 -> layer_2_expert_allreduce_14
	layer_2_alltoall -> layer_2_expert1_59
	layer_2_expert1_59 -> layer_2_expert_act_59
	layer_2_expert_act_59 -> layer_2_expert2_59
	layer_2_expert2_59 -> layer_2_expert_allreduce_14
	layer_2_expert_allreduce_14 -> layer_2_expert_agg
	layer_2_attn_allreduce_15 -> layer_2_gate_60 [style=dashed]
	layer_2_gate_60 -> layer_2_alltoall [style=dashed]
	layer_2_alltoall -> layer_2_expert1_60
	layer_2_expert1_60 -> layer_2_expert_act_60
	layer_2_expert_act_60 -> layer_2_expert2_60
	layer_2_expert2_60 -> layer_2_expert_allreduce_15
	layer_2_alltoall -> layer_2_expert1_61
	layer_2_expert1_61 -> layer_2_expert_act_61
	layer_2_expert_act_61 -> layer_2_expert2_61
	layer_2_expert2_61 -> layer_2_expert_allreduce_15
	layer_2_alltoall -> layer_2_expert1_62
	layer_2_expert1_62 -> layer_2_expert_act_62
	layer_2_expert_act_62 -> layer_2_expert2_62
	layer_2_expert2_62 -> layer_2_expert_allreduce_15
	layer_2_alltoall -> layer_2_expert1_63
	layer_2_expert1_63 -> layer_2_expert_act_63
	layer_2_expert_act_63 -> layer_2_expert2_63
	layer_2_expert2_63 -> layer_2_expert_allreduce_15
	layer_2_expert_allreduce_15 -> layer_2_expert_agg
	layer_2_expert_agg -> layer_2_norm
	layer_2_norm -> layer_3_input
	layer_3_input [label="Layer3_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_3_qkv_0 [label="Layer3_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_1 [label="Layer3_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_2 [label="Layer3_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_3 [label="Layer3_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_0 [label="Layer3_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_1 [label="Layer3_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_2 [label="Layer3_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_3 [label="Layer3_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_0 [label="Layer3_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_1 [label="Layer3_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_2 [label="Layer3_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_3 [label="Layer3_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_0 [label="Layer3_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_1 [label="Layer3_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_2 [label="Layer3_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_3 [label="Layer3_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_0 [label="Layer3_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_0
	layer_3_qkv_0 -> layer_3_attn_scores_0
	layer_3_attn_scores_0 -> layer_3_attn_softmax_0
	layer_3_attn_softmax_0 -> layer_3_attn_out_0
	layer_3_attn_out_0 -> layer_3_attn_allreduce_0
	layer_3_input -> layer_3_qkv_1
	layer_3_qkv_1 -> layer_3_attn_scores_1
	layer_3_attn_scores_1 -> layer_3_attn_softmax_1
	layer_3_attn_softmax_1 -> layer_3_attn_out_1
	layer_3_attn_out_1 -> layer_3_attn_allreduce_0
	layer_3_input -> layer_3_qkv_2
	layer_3_qkv_2 -> layer_3_attn_scores_2
	layer_3_attn_scores_2 -> layer_3_attn_softmax_2
	layer_3_attn_softmax_2 -> layer_3_attn_out_2
	layer_3_attn_out_2 -> layer_3_attn_allreduce_0
	layer_3_input -> layer_3_qkv_3
	layer_3_qkv_3 -> layer_3_attn_scores_3
	layer_3_attn_scores_3 -> layer_3_attn_softmax_3
	layer_3_attn_softmax_3 -> layer_3_attn_out_3
	layer_3_attn_out_3 -> layer_3_attn_allreduce_0
	layer_3_qkv_4 [label="Layer3_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_5 [label="Layer3_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_6 [label="Layer3_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_7 [label="Layer3_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_4 [label="Layer3_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_5 [label="Layer3_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_6 [label="Layer3_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_7 [label="Layer3_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_4 [label="Layer3_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_5 [label="Layer3_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_6 [label="Layer3_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_7 [label="Layer3_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_4 [label="Layer3_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_5 [label="Layer3_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_6 [label="Layer3_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_7 [label="Layer3_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_1 [label="Layer3_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_4
	layer_3_qkv_4 -> layer_3_attn_scores_4
	layer_3_attn_scores_4 -> layer_3_attn_softmax_4
	layer_3_attn_softmax_4 -> layer_3_attn_out_4
	layer_3_attn_out_4 -> layer_3_attn_allreduce_1
	layer_3_input -> layer_3_qkv_5
	layer_3_qkv_5 -> layer_3_attn_scores_5
	layer_3_attn_scores_5 -> layer_3_attn_softmax_5
	layer_3_attn_softmax_5 -> layer_3_attn_out_5
	layer_3_attn_out_5 -> layer_3_attn_allreduce_1
	layer_3_input -> layer_3_qkv_6
	layer_3_qkv_6 -> layer_3_attn_scores_6
	layer_3_attn_scores_6 -> layer_3_attn_softmax_6
	layer_3_attn_softmax_6 -> layer_3_attn_out_6
	layer_3_attn_out_6 -> layer_3_attn_allreduce_1
	layer_3_input -> layer_3_qkv_7
	layer_3_qkv_7 -> layer_3_attn_scores_7
	layer_3_attn_scores_7 -> layer_3_attn_softmax_7
	layer_3_attn_softmax_7 -> layer_3_attn_out_7
	layer_3_attn_out_7 -> layer_3_attn_allreduce_1
	layer_3_qkv_8 [label="Layer3_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_9 [label="Layer3_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_10 [label="Layer3_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_11 [label="Layer3_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_8 [label="Layer3_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_9 [label="Layer3_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_10 [label="Layer3_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_11 [label="Layer3_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_8 [label="Layer3_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_9 [label="Layer3_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_10 [label="Layer3_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_11 [label="Layer3_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_8 [label="Layer3_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_9 [label="Layer3_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_10 [label="Layer3_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_11 [label="Layer3_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_2 [label="Layer3_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_8
	layer_3_qkv_8 -> layer_3_attn_scores_8
	layer_3_attn_scores_8 -> layer_3_attn_softmax_8
	layer_3_attn_softmax_8 -> layer_3_attn_out_8
	layer_3_attn_out_8 -> layer_3_attn_allreduce_2
	layer_3_input -> layer_3_qkv_9
	layer_3_qkv_9 -> layer_3_attn_scores_9
	layer_3_attn_scores_9 -> layer_3_attn_softmax_9
	layer_3_attn_softmax_9 -> layer_3_attn_out_9
	layer_3_attn_out_9 -> layer_3_attn_allreduce_2
	layer_3_input -> layer_3_qkv_10
	layer_3_qkv_10 -> layer_3_attn_scores_10
	layer_3_attn_scores_10 -> layer_3_attn_softmax_10
	layer_3_attn_softmax_10 -> layer_3_attn_out_10
	layer_3_attn_out_10 -> layer_3_attn_allreduce_2
	layer_3_input -> layer_3_qkv_11
	layer_3_qkv_11 -> layer_3_attn_scores_11
	layer_3_attn_scores_11 -> layer_3_attn_softmax_11
	layer_3_attn_softmax_11 -> layer_3_attn_out_11
	layer_3_attn_out_11 -> layer_3_attn_allreduce_2
	layer_3_qkv_12 [label="Layer3_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_13 [label="Layer3_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_14 [label="Layer3_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_15 [label="Layer3_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_12 [label="Layer3_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_13 [label="Layer3_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_14 [label="Layer3_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_15 [label="Layer3_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_12 [label="Layer3_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_13 [label="Layer3_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_14 [label="Layer3_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_15 [label="Layer3_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_12 [label="Layer3_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_13 [label="Layer3_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_14 [label="Layer3_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_15 [label="Layer3_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_3 [label="Layer3_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_12
	layer_3_qkv_12 -> layer_3_attn_scores_12
	layer_3_attn_scores_12 -> layer_3_attn_softmax_12
	layer_3_attn_softmax_12 -> layer_3_attn_out_12
	layer_3_attn_out_12 -> layer_3_attn_allreduce_3
	layer_3_input -> layer_3_qkv_13
	layer_3_qkv_13 -> layer_3_attn_scores_13
	layer_3_attn_scores_13 -> layer_3_attn_softmax_13
	layer_3_attn_softmax_13 -> layer_3_attn_out_13
	layer_3_attn_out_13 -> layer_3_attn_allreduce_3
	layer_3_input -> layer_3_qkv_14
	layer_3_qkv_14 -> layer_3_attn_scores_14
	layer_3_attn_scores_14 -> layer_3_attn_softmax_14
	layer_3_attn_softmax_14 -> layer_3_attn_out_14
	layer_3_attn_out_14 -> layer_3_attn_allreduce_3
	layer_3_input -> layer_3_qkv_15
	layer_3_qkv_15 -> layer_3_attn_scores_15
	layer_3_attn_scores_15 -> layer_3_attn_softmax_15
	layer_3_attn_softmax_15 -> layer_3_attn_out_15
	layer_3_attn_out_15 -> layer_3_attn_allreduce_3
	layer_3_qkv_16 [label="Layer3_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_17 [label="Layer3_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_18 [label="Layer3_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_19 [label="Layer3_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_16 [label="Layer3_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_17 [label="Layer3_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_18 [label="Layer3_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_19 [label="Layer3_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_16 [label="Layer3_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_17 [label="Layer3_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_18 [label="Layer3_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_19 [label="Layer3_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_16 [label="Layer3_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_17 [label="Layer3_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_18 [label="Layer3_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_19 [label="Layer3_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_4 [label="Layer3_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_16
	layer_3_qkv_16 -> layer_3_attn_scores_16
	layer_3_attn_scores_16 -> layer_3_attn_softmax_16
	layer_3_attn_softmax_16 -> layer_3_attn_out_16
	layer_3_attn_out_16 -> layer_3_attn_allreduce_4
	layer_3_input -> layer_3_qkv_17
	layer_3_qkv_17 -> layer_3_attn_scores_17
	layer_3_attn_scores_17 -> layer_3_attn_softmax_17
	layer_3_attn_softmax_17 -> layer_3_attn_out_17
	layer_3_attn_out_17 -> layer_3_attn_allreduce_4
	layer_3_input -> layer_3_qkv_18
	layer_3_qkv_18 -> layer_3_attn_scores_18
	layer_3_attn_scores_18 -> layer_3_attn_softmax_18
	layer_3_attn_softmax_18 -> layer_3_attn_out_18
	layer_3_attn_out_18 -> layer_3_attn_allreduce_4
	layer_3_input -> layer_3_qkv_19
	layer_3_qkv_19 -> layer_3_attn_scores_19
	layer_3_attn_scores_19 -> layer_3_attn_softmax_19
	layer_3_attn_softmax_19 -> layer_3_attn_out_19
	layer_3_attn_out_19 -> layer_3_attn_allreduce_4
	layer_3_qkv_20 [label="Layer3_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_21 [label="Layer3_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_22 [label="Layer3_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_23 [label="Layer3_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_20 [label="Layer3_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_21 [label="Layer3_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_22 [label="Layer3_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_23 [label="Layer3_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_20 [label="Layer3_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_21 [label="Layer3_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_22 [label="Layer3_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_23 [label="Layer3_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_20 [label="Layer3_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_21 [label="Layer3_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_22 [label="Layer3_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_23 [label="Layer3_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_5 [label="Layer3_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_20
	layer_3_qkv_20 -> layer_3_attn_scores_20
	layer_3_attn_scores_20 -> layer_3_attn_softmax_20
	layer_3_attn_softmax_20 -> layer_3_attn_out_20
	layer_3_attn_out_20 -> layer_3_attn_allreduce_5
	layer_3_input -> layer_3_qkv_21
	layer_3_qkv_21 -> layer_3_attn_scores_21
	layer_3_attn_scores_21 -> layer_3_attn_softmax_21
	layer_3_attn_softmax_21 -> layer_3_attn_out_21
	layer_3_attn_out_21 -> layer_3_attn_allreduce_5
	layer_3_input -> layer_3_qkv_22
	layer_3_qkv_22 -> layer_3_attn_scores_22
	layer_3_attn_scores_22 -> layer_3_attn_softmax_22
	layer_3_attn_softmax_22 -> layer_3_attn_out_22
	layer_3_attn_out_22 -> layer_3_attn_allreduce_5
	layer_3_input -> layer_3_qkv_23
	layer_3_qkv_23 -> layer_3_attn_scores_23
	layer_3_attn_scores_23 -> layer_3_attn_softmax_23
	layer_3_attn_softmax_23 -> layer_3_attn_out_23
	layer_3_attn_out_23 -> layer_3_attn_allreduce_5
	layer_3_qkv_24 [label="Layer3_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_25 [label="Layer3_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_26 [label="Layer3_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_27 [label="Layer3_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_24 [label="Layer3_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_25 [label="Layer3_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_26 [label="Layer3_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_27 [label="Layer3_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_24 [label="Layer3_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_25 [label="Layer3_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_26 [label="Layer3_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_27 [label="Layer3_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_24 [label="Layer3_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_25 [label="Layer3_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_26 [label="Layer3_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_27 [label="Layer3_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_6 [label="Layer3_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_24
	layer_3_qkv_24 -> layer_3_attn_scores_24
	layer_3_attn_scores_24 -> layer_3_attn_softmax_24
	layer_3_attn_softmax_24 -> layer_3_attn_out_24
	layer_3_attn_out_24 -> layer_3_attn_allreduce_6
	layer_3_input -> layer_3_qkv_25
	layer_3_qkv_25 -> layer_3_attn_scores_25
	layer_3_attn_scores_25 -> layer_3_attn_softmax_25
	layer_3_attn_softmax_25 -> layer_3_attn_out_25
	layer_3_attn_out_25 -> layer_3_attn_allreduce_6
	layer_3_input -> layer_3_qkv_26
	layer_3_qkv_26 -> layer_3_attn_scores_26
	layer_3_attn_scores_26 -> layer_3_attn_softmax_26
	layer_3_attn_softmax_26 -> layer_3_attn_out_26
	layer_3_attn_out_26 -> layer_3_attn_allreduce_6
	layer_3_input -> layer_3_qkv_27
	layer_3_qkv_27 -> layer_3_attn_scores_27
	layer_3_attn_scores_27 -> layer_3_attn_softmax_27
	layer_3_attn_softmax_27 -> layer_3_attn_out_27
	layer_3_attn_out_27 -> layer_3_attn_allreduce_6
	layer_3_qkv_28 [label="Layer3_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_29 [label="Layer3_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_30 [label="Layer3_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_31 [label="Layer3_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_28 [label="Layer3_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_29 [label="Layer3_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_30 [label="Layer3_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_31 [label="Layer3_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_28 [label="Layer3_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_29 [label="Layer3_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_30 [label="Layer3_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_31 [label="Layer3_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_28 [label="Layer3_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_29 [label="Layer3_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_30 [label="Layer3_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_31 [label="Layer3_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_7 [label="Layer3_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_28
	layer_3_qkv_28 -> layer_3_attn_scores_28
	layer_3_attn_scores_28 -> layer_3_attn_softmax_28
	layer_3_attn_softmax_28 -> layer_3_attn_out_28
	layer_3_attn_out_28 -> layer_3_attn_allreduce_7
	layer_3_input -> layer_3_qkv_29
	layer_3_qkv_29 -> layer_3_attn_scores_29
	layer_3_attn_scores_29 -> layer_3_attn_softmax_29
	layer_3_attn_softmax_29 -> layer_3_attn_out_29
	layer_3_attn_out_29 -> layer_3_attn_allreduce_7
	layer_3_input -> layer_3_qkv_30
	layer_3_qkv_30 -> layer_3_attn_scores_30
	layer_3_attn_scores_30 -> layer_3_attn_softmax_30
	layer_3_attn_softmax_30 -> layer_3_attn_out_30
	layer_3_attn_out_30 -> layer_3_attn_allreduce_7
	layer_3_input -> layer_3_qkv_31
	layer_3_qkv_31 -> layer_3_attn_scores_31
	layer_3_attn_scores_31 -> layer_3_attn_softmax_31
	layer_3_attn_softmax_31 -> layer_3_attn_out_31
	layer_3_attn_out_31 -> layer_3_attn_allreduce_7
	layer_3_qkv_32 [label="Layer3_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_33 [label="Layer3_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_34 [label="Layer3_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_35 [label="Layer3_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_32 [label="Layer3_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_33 [label="Layer3_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_34 [label="Layer3_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_35 [label="Layer3_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_32 [label="Layer3_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_33 [label="Layer3_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_34 [label="Layer3_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_35 [label="Layer3_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_32 [label="Layer3_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_33 [label="Layer3_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_34 [label="Layer3_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_35 [label="Layer3_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_8 [label="Layer3_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_32
	layer_3_qkv_32 -> layer_3_attn_scores_32
	layer_3_attn_scores_32 -> layer_3_attn_softmax_32
	layer_3_attn_softmax_32 -> layer_3_attn_out_32
	layer_3_attn_out_32 -> layer_3_attn_allreduce_8
	layer_3_input -> layer_3_qkv_33
	layer_3_qkv_33 -> layer_3_attn_scores_33
	layer_3_attn_scores_33 -> layer_3_attn_softmax_33
	layer_3_attn_softmax_33 -> layer_3_attn_out_33
	layer_3_attn_out_33 -> layer_3_attn_allreduce_8
	layer_3_input -> layer_3_qkv_34
	layer_3_qkv_34 -> layer_3_attn_scores_34
	layer_3_attn_scores_34 -> layer_3_attn_softmax_34
	layer_3_attn_softmax_34 -> layer_3_attn_out_34
	layer_3_attn_out_34 -> layer_3_attn_allreduce_8
	layer_3_input -> layer_3_qkv_35
	layer_3_qkv_35 -> layer_3_attn_scores_35
	layer_3_attn_scores_35 -> layer_3_attn_softmax_35
	layer_3_attn_softmax_35 -> layer_3_attn_out_35
	layer_3_attn_out_35 -> layer_3_attn_allreduce_8
	layer_3_qkv_36 [label="Layer3_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_37 [label="Layer3_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_38 [label="Layer3_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_39 [label="Layer3_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_36 [label="Layer3_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_37 [label="Layer3_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_38 [label="Layer3_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_39 [label="Layer3_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_36 [label="Layer3_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_37 [label="Layer3_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_38 [label="Layer3_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_39 [label="Layer3_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_36 [label="Layer3_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_37 [label="Layer3_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_38 [label="Layer3_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_39 [label="Layer3_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_9 [label="Layer3_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_36
	layer_3_qkv_36 -> layer_3_attn_scores_36
	layer_3_attn_scores_36 -> layer_3_attn_softmax_36
	layer_3_attn_softmax_36 -> layer_3_attn_out_36
	layer_3_attn_out_36 -> layer_3_attn_allreduce_9
	layer_3_input -> layer_3_qkv_37
	layer_3_qkv_37 -> layer_3_attn_scores_37
	layer_3_attn_scores_37 -> layer_3_attn_softmax_37
	layer_3_attn_softmax_37 -> layer_3_attn_out_37
	layer_3_attn_out_37 -> layer_3_attn_allreduce_9
	layer_3_input -> layer_3_qkv_38
	layer_3_qkv_38 -> layer_3_attn_scores_38
	layer_3_attn_scores_38 -> layer_3_attn_softmax_38
	layer_3_attn_softmax_38 -> layer_3_attn_out_38
	layer_3_attn_out_38 -> layer_3_attn_allreduce_9
	layer_3_input -> layer_3_qkv_39
	layer_3_qkv_39 -> layer_3_attn_scores_39
	layer_3_attn_scores_39 -> layer_3_attn_softmax_39
	layer_3_attn_softmax_39 -> layer_3_attn_out_39
	layer_3_attn_out_39 -> layer_3_attn_allreduce_9
	layer_3_qkv_40 [label="Layer3_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_41 [label="Layer3_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_42 [label="Layer3_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_43 [label="Layer3_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_40 [label="Layer3_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_41 [label="Layer3_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_42 [label="Layer3_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_43 [label="Layer3_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_40 [label="Layer3_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_41 [label="Layer3_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_42 [label="Layer3_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_43 [label="Layer3_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_40 [label="Layer3_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_41 [label="Layer3_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_42 [label="Layer3_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_43 [label="Layer3_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_10 [label="Layer3_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_40
	layer_3_qkv_40 -> layer_3_attn_scores_40
	layer_3_attn_scores_40 -> layer_3_attn_softmax_40
	layer_3_attn_softmax_40 -> layer_3_attn_out_40
	layer_3_attn_out_40 -> layer_3_attn_allreduce_10
	layer_3_input -> layer_3_qkv_41
	layer_3_qkv_41 -> layer_3_attn_scores_41
	layer_3_attn_scores_41 -> layer_3_attn_softmax_41
	layer_3_attn_softmax_41 -> layer_3_attn_out_41
	layer_3_attn_out_41 -> layer_3_attn_allreduce_10
	layer_3_input -> layer_3_qkv_42
	layer_3_qkv_42 -> layer_3_attn_scores_42
	layer_3_attn_scores_42 -> layer_3_attn_softmax_42
	layer_3_attn_softmax_42 -> layer_3_attn_out_42
	layer_3_attn_out_42 -> layer_3_attn_allreduce_10
	layer_3_input -> layer_3_qkv_43
	layer_3_qkv_43 -> layer_3_attn_scores_43
	layer_3_attn_scores_43 -> layer_3_attn_softmax_43
	layer_3_attn_softmax_43 -> layer_3_attn_out_43
	layer_3_attn_out_43 -> layer_3_attn_allreduce_10
	layer_3_qkv_44 [label="Layer3_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_45 [label="Layer3_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_46 [label="Layer3_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_47 [label="Layer3_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_44 [label="Layer3_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_45 [label="Layer3_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_46 [label="Layer3_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_47 [label="Layer3_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_44 [label="Layer3_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_45 [label="Layer3_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_46 [label="Layer3_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_47 [label="Layer3_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_44 [label="Layer3_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_45 [label="Layer3_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_46 [label="Layer3_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_47 [label="Layer3_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_11 [label="Layer3_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_44
	layer_3_qkv_44 -> layer_3_attn_scores_44
	layer_3_attn_scores_44 -> layer_3_attn_softmax_44
	layer_3_attn_softmax_44 -> layer_3_attn_out_44
	layer_3_attn_out_44 -> layer_3_attn_allreduce_11
	layer_3_input -> layer_3_qkv_45
	layer_3_qkv_45 -> layer_3_attn_scores_45
	layer_3_attn_scores_45 -> layer_3_attn_softmax_45
	layer_3_attn_softmax_45 -> layer_3_attn_out_45
	layer_3_attn_out_45 -> layer_3_attn_allreduce_11
	layer_3_input -> layer_3_qkv_46
	layer_3_qkv_46 -> layer_3_attn_scores_46
	layer_3_attn_scores_46 -> layer_3_attn_softmax_46
	layer_3_attn_softmax_46 -> layer_3_attn_out_46
	layer_3_attn_out_46 -> layer_3_attn_allreduce_11
	layer_3_input -> layer_3_qkv_47
	layer_3_qkv_47 -> layer_3_attn_scores_47
	layer_3_attn_scores_47 -> layer_3_attn_softmax_47
	layer_3_attn_softmax_47 -> layer_3_attn_out_47
	layer_3_attn_out_47 -> layer_3_attn_allreduce_11
	layer_3_qkv_48 [label="Layer3_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_49 [label="Layer3_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_50 [label="Layer3_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_51 [label="Layer3_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_48 [label="Layer3_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_49 [label="Layer3_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_50 [label="Layer3_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_51 [label="Layer3_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_48 [label="Layer3_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_49 [label="Layer3_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_50 [label="Layer3_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_51 [label="Layer3_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_48 [label="Layer3_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_49 [label="Layer3_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_50 [label="Layer3_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_51 [label="Layer3_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_12 [label="Layer3_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_48
	layer_3_qkv_48 -> layer_3_attn_scores_48
	layer_3_attn_scores_48 -> layer_3_attn_softmax_48
	layer_3_attn_softmax_48 -> layer_3_attn_out_48
	layer_3_attn_out_48 -> layer_3_attn_allreduce_12
	layer_3_input -> layer_3_qkv_49
	layer_3_qkv_49 -> layer_3_attn_scores_49
	layer_3_attn_scores_49 -> layer_3_attn_softmax_49
	layer_3_attn_softmax_49 -> layer_3_attn_out_49
	layer_3_attn_out_49 -> layer_3_attn_allreduce_12
	layer_3_input -> layer_3_qkv_50
	layer_3_qkv_50 -> layer_3_attn_scores_50
	layer_3_attn_scores_50 -> layer_3_attn_softmax_50
	layer_3_attn_softmax_50 -> layer_3_attn_out_50
	layer_3_attn_out_50 -> layer_3_attn_allreduce_12
	layer_3_input -> layer_3_qkv_51
	layer_3_qkv_51 -> layer_3_attn_scores_51
	layer_3_attn_scores_51 -> layer_3_attn_softmax_51
	layer_3_attn_softmax_51 -> layer_3_attn_out_51
	layer_3_attn_out_51 -> layer_3_attn_allreduce_12
	layer_3_qkv_52 [label="Layer3_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_53 [label="Layer3_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_54 [label="Layer3_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_55 [label="Layer3_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_52 [label="Layer3_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_53 [label="Layer3_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_54 [label="Layer3_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_55 [label="Layer3_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_52 [label="Layer3_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_53 [label="Layer3_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_54 [label="Layer3_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_55 [label="Layer3_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_52 [label="Layer3_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_53 [label="Layer3_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_54 [label="Layer3_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_55 [label="Layer3_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_13 [label="Layer3_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_52
	layer_3_qkv_52 -> layer_3_attn_scores_52
	layer_3_attn_scores_52 -> layer_3_attn_softmax_52
	layer_3_attn_softmax_52 -> layer_3_attn_out_52
	layer_3_attn_out_52 -> layer_3_attn_allreduce_13
	layer_3_input -> layer_3_qkv_53
	layer_3_qkv_53 -> layer_3_attn_scores_53
	layer_3_attn_scores_53 -> layer_3_attn_softmax_53
	layer_3_attn_softmax_53 -> layer_3_attn_out_53
	layer_3_attn_out_53 -> layer_3_attn_allreduce_13
	layer_3_input -> layer_3_qkv_54
	layer_3_qkv_54 -> layer_3_attn_scores_54
	layer_3_attn_scores_54 -> layer_3_attn_softmax_54
	layer_3_attn_softmax_54 -> layer_3_attn_out_54
	layer_3_attn_out_54 -> layer_3_attn_allreduce_13
	layer_3_input -> layer_3_qkv_55
	layer_3_qkv_55 -> layer_3_attn_scores_55
	layer_3_attn_scores_55 -> layer_3_attn_softmax_55
	layer_3_attn_softmax_55 -> layer_3_attn_out_55
	layer_3_attn_out_55 -> layer_3_attn_allreduce_13
	layer_3_qkv_56 [label="Layer3_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_57 [label="Layer3_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_58 [label="Layer3_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_59 [label="Layer3_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_56 [label="Layer3_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_57 [label="Layer3_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_58 [label="Layer3_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_59 [label="Layer3_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_56 [label="Layer3_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_57 [label="Layer3_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_58 [label="Layer3_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_59 [label="Layer3_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_56 [label="Layer3_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_57 [label="Layer3_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_58 [label="Layer3_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_59 [label="Layer3_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_14 [label="Layer3_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_56
	layer_3_qkv_56 -> layer_3_attn_scores_56
	layer_3_attn_scores_56 -> layer_3_attn_softmax_56
	layer_3_attn_softmax_56 -> layer_3_attn_out_56
	layer_3_attn_out_56 -> layer_3_attn_allreduce_14
	layer_3_input -> layer_3_qkv_57
	layer_3_qkv_57 -> layer_3_attn_scores_57
	layer_3_attn_scores_57 -> layer_3_attn_softmax_57
	layer_3_attn_softmax_57 -> layer_3_attn_out_57
	layer_3_attn_out_57 -> layer_3_attn_allreduce_14
	layer_3_input -> layer_3_qkv_58
	layer_3_qkv_58 -> layer_3_attn_scores_58
	layer_3_attn_scores_58 -> layer_3_attn_softmax_58
	layer_3_attn_softmax_58 -> layer_3_attn_out_58
	layer_3_attn_out_58 -> layer_3_attn_allreduce_14
	layer_3_input -> layer_3_qkv_59
	layer_3_qkv_59 -> layer_3_attn_scores_59
	layer_3_attn_scores_59 -> layer_3_attn_softmax_59
	layer_3_attn_softmax_59 -> layer_3_attn_out_59
	layer_3_attn_out_59 -> layer_3_attn_allreduce_14
	layer_3_qkv_60 [label="Layer3_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_61 [label="Layer3_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_62 [label="Layer3_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_qkv_63 [label="Layer3_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_60 [label="Layer3_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_61 [label="Layer3_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_62 [label="Layer3_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_scores_63 [label="Layer3_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_60 [label="Layer3_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_61 [label="Layer3_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_62 [label="Layer3_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_softmax_63 [label="Layer3_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_60 [label="Layer3_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_61 [label="Layer3_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_62 [label="Layer3_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_out_63 [label="Layer3_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_15 [label="Layer3_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_input -> layer_3_qkv_60
	layer_3_qkv_60 -> layer_3_attn_scores_60
	layer_3_attn_scores_60 -> layer_3_attn_softmax_60
	layer_3_attn_softmax_60 -> layer_3_attn_out_60
	layer_3_attn_out_60 -> layer_3_attn_allreduce_15
	layer_3_input -> layer_3_qkv_61
	layer_3_qkv_61 -> layer_3_attn_scores_61
	layer_3_attn_scores_61 -> layer_3_attn_softmax_61
	layer_3_attn_softmax_61 -> layer_3_attn_out_61
	layer_3_attn_out_61 -> layer_3_attn_allreduce_15
	layer_3_input -> layer_3_qkv_62
	layer_3_qkv_62 -> layer_3_attn_scores_62
	layer_3_attn_scores_62 -> layer_3_attn_softmax_62
	layer_3_attn_softmax_62 -> layer_3_attn_out_62
	layer_3_attn_out_62 -> layer_3_attn_allreduce_15
	layer_3_input -> layer_3_qkv_63
	layer_3_qkv_63 -> layer_3_attn_scores_63
	layer_3_attn_scores_63 -> layer_3_attn_softmax_63
	layer_3_attn_softmax_63 -> layer_3_attn_out_63
	layer_3_attn_out_63 -> layer_3_attn_allreduce_15
	layer_3_gate_0 [label="Layer3_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_4 [label="Layer3_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_8 [label="Layer3_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_12 [label="Layer3_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_16 [label="Layer3_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_20 [label="Layer3_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_24 [label="Layer3_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_28 [label="Layer3_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_32 [label="Layer3_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_36 [label="Layer3_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_40 [label="Layer3_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_44 [label="Layer3_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_48 [label="Layer3_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_52 [label="Layer3_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_56 [label="Layer3_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_gate_60 [label="Layer3_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_3_alltoall [label="Layer3_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_0 [label="Layer3_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_1 [label="Layer3_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_2 [label="Layer3_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_3 [label="Layer3_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_0 [label="Layer3_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_1 [label="Layer3_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_2 [label="Layer3_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_3 [label="Layer3_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_0 [label="Layer3_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_1 [label="Layer3_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_2 [label="Layer3_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_3 [label="Layer3_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_0 [label="Layer3_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_4 [label="Layer3_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_5 [label="Layer3_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_6 [label="Layer3_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_7 [label="Layer3_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_4 [label="Layer3_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_5 [label="Layer3_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_6 [label="Layer3_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_7 [label="Layer3_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_4 [label="Layer3_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_5 [label="Layer3_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_6 [label="Layer3_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_7 [label="Layer3_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_1 [label="Layer3_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_8 [label="Layer3_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_9 [label="Layer3_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_10 [label="Layer3_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_11 [label="Layer3_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_8 [label="Layer3_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_9 [label="Layer3_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_10 [label="Layer3_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_11 [label="Layer3_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_8 [label="Layer3_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_9 [label="Layer3_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_10 [label="Layer3_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_11 [label="Layer3_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_2 [label="Layer3_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_12 [label="Layer3_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_13 [label="Layer3_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_14 [label="Layer3_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_15 [label="Layer3_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_12 [label="Layer3_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_13 [label="Layer3_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_14 [label="Layer3_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_15 [label="Layer3_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_12 [label="Layer3_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_13 [label="Layer3_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_14 [label="Layer3_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_15 [label="Layer3_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_3 [label="Layer3_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_16 [label="Layer3_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_17 [label="Layer3_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_18 [label="Layer3_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_19 [label="Layer3_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_16 [label="Layer3_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_17 [label="Layer3_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_18 [label="Layer3_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_19 [label="Layer3_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_16 [label="Layer3_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_17 [label="Layer3_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_18 [label="Layer3_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_19 [label="Layer3_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_4 [label="Layer3_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_20 [label="Layer3_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_21 [label="Layer3_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_22 [label="Layer3_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_23 [label="Layer3_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_20 [label="Layer3_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_21 [label="Layer3_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_22 [label="Layer3_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_23 [label="Layer3_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_20 [label="Layer3_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_21 [label="Layer3_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_22 [label="Layer3_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_23 [label="Layer3_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_5 [label="Layer3_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_24 [label="Layer3_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_25 [label="Layer3_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_26 [label="Layer3_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_27 [label="Layer3_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_24 [label="Layer3_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_25 [label="Layer3_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_26 [label="Layer3_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_27 [label="Layer3_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_24 [label="Layer3_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_25 [label="Layer3_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_26 [label="Layer3_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_27 [label="Layer3_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_6 [label="Layer3_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_28 [label="Layer3_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_29 [label="Layer3_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_30 [label="Layer3_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_31 [label="Layer3_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_28 [label="Layer3_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_29 [label="Layer3_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_30 [label="Layer3_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_31 [label="Layer3_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_28 [label="Layer3_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_29 [label="Layer3_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_30 [label="Layer3_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_31 [label="Layer3_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_7 [label="Layer3_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_32 [label="Layer3_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_33 [label="Layer3_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_34 [label="Layer3_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_35 [label="Layer3_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_32 [label="Layer3_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_33 [label="Layer3_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_34 [label="Layer3_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_35 [label="Layer3_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_32 [label="Layer3_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_33 [label="Layer3_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_34 [label="Layer3_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_35 [label="Layer3_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_8 [label="Layer3_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_36 [label="Layer3_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_37 [label="Layer3_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_38 [label="Layer3_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_39 [label="Layer3_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_36 [label="Layer3_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_37 [label="Layer3_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_38 [label="Layer3_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_39 [label="Layer3_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_36 [label="Layer3_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_37 [label="Layer3_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_38 [label="Layer3_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_39 [label="Layer3_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_9 [label="Layer3_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_40 [label="Layer3_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_41 [label="Layer3_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_42 [label="Layer3_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_43 [label="Layer3_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_40 [label="Layer3_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_41 [label="Layer3_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_42 [label="Layer3_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_43 [label="Layer3_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_40 [label="Layer3_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_41 [label="Layer3_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_42 [label="Layer3_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_43 [label="Layer3_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_10 [label="Layer3_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_44 [label="Layer3_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_45 [label="Layer3_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_46 [label="Layer3_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_47 [label="Layer3_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_44 [label="Layer3_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_45 [label="Layer3_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_46 [label="Layer3_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_47 [label="Layer3_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_44 [label="Layer3_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_45 [label="Layer3_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_46 [label="Layer3_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_47 [label="Layer3_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_11 [label="Layer3_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_48 [label="Layer3_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_49 [label="Layer3_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_50 [label="Layer3_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_51 [label="Layer3_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_48 [label="Layer3_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_49 [label="Layer3_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_50 [label="Layer3_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_51 [label="Layer3_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_48 [label="Layer3_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_49 [label="Layer3_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_50 [label="Layer3_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_51 [label="Layer3_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_12 [label="Layer3_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_52 [label="Layer3_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_53 [label="Layer3_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_54 [label="Layer3_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_55 [label="Layer3_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_52 [label="Layer3_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_53 [label="Layer3_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_54 [label="Layer3_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_55 [label="Layer3_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_52 [label="Layer3_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_53 [label="Layer3_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_54 [label="Layer3_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_55 [label="Layer3_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_13 [label="Layer3_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_56 [label="Layer3_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_57 [label="Layer3_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_58 [label="Layer3_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_59 [label="Layer3_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_56 [label="Layer3_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_57 [label="Layer3_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_58 [label="Layer3_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_59 [label="Layer3_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_56 [label="Layer3_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_57 [label="Layer3_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_58 [label="Layer3_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_59 [label="Layer3_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_14 [label="Layer3_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert1_60 [label="Layer3_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_61 [label="Layer3_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_62 [label="Layer3_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert1_63 [label="Layer3_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_60 [label="Layer3_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_61 [label="Layer3_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_62 [label="Layer3_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert_act_63 [label="Layer3_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_3_expert2_60 [label="Layer3_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_61 [label="Layer3_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_62 [label="Layer3_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert2_63 [label="Layer3_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_3_expert_allreduce_15 [label="Layer3_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_3_expert_agg [label="Layer3_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_3_norm [label="Layer3_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_3_attn_allreduce_0 -> layer_3_gate_0 [style=dashed]
	layer_3_gate_0 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_0
	layer_3_expert1_0 -> layer_3_expert_act_0
	layer_3_expert_act_0 -> layer_3_expert2_0
	layer_3_expert2_0 -> layer_3_expert_allreduce_0
	layer_3_alltoall -> layer_3_expert1_1
	layer_3_expert1_1 -> layer_3_expert_act_1
	layer_3_expert_act_1 -> layer_3_expert2_1
	layer_3_expert2_1 -> layer_3_expert_allreduce_0
	layer_3_alltoall -> layer_3_expert1_2
	layer_3_expert1_2 -> layer_3_expert_act_2
	layer_3_expert_act_2 -> layer_3_expert2_2
	layer_3_expert2_2 -> layer_3_expert_allreduce_0
	layer_3_alltoall -> layer_3_expert1_3
	layer_3_expert1_3 -> layer_3_expert_act_3
	layer_3_expert_act_3 -> layer_3_expert2_3
	layer_3_expert2_3 -> layer_3_expert_allreduce_0
	layer_3_expert_allreduce_0 -> layer_3_expert_agg
	layer_3_attn_allreduce_1 -> layer_3_gate_4 [style=dashed]
	layer_3_gate_4 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_4
	layer_3_expert1_4 -> layer_3_expert_act_4
	layer_3_expert_act_4 -> layer_3_expert2_4
	layer_3_expert2_4 -> layer_3_expert_allreduce_1
	layer_3_alltoall -> layer_3_expert1_5
	layer_3_expert1_5 -> layer_3_expert_act_5
	layer_3_expert_act_5 -> layer_3_expert2_5
	layer_3_expert2_5 -> layer_3_expert_allreduce_1
	layer_3_alltoall -> layer_3_expert1_6
	layer_3_expert1_6 -> layer_3_expert_act_6
	layer_3_expert_act_6 -> layer_3_expert2_6
	layer_3_expert2_6 -> layer_3_expert_allreduce_1
	layer_3_alltoall -> layer_3_expert1_7
	layer_3_expert1_7 -> layer_3_expert_act_7
	layer_3_expert_act_7 -> layer_3_expert2_7
	layer_3_expert2_7 -> layer_3_expert_allreduce_1
	layer_3_expert_allreduce_1 -> layer_3_expert_agg
	layer_3_attn_allreduce_2 -> layer_3_gate_8 [style=dashed]
	layer_3_gate_8 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_8
	layer_3_expert1_8 -> layer_3_expert_act_8
	layer_3_expert_act_8 -> layer_3_expert2_8
	layer_3_expert2_8 -> layer_3_expert_allreduce_2
	layer_3_alltoall -> layer_3_expert1_9
	layer_3_expert1_9 -> layer_3_expert_act_9
	layer_3_expert_act_9 -> layer_3_expert2_9
	layer_3_expert2_9 -> layer_3_expert_allreduce_2
	layer_3_alltoall -> layer_3_expert1_10
	layer_3_expert1_10 -> layer_3_expert_act_10
	layer_3_expert_act_10 -> layer_3_expert2_10
	layer_3_expert2_10 -> layer_3_expert_allreduce_2
	layer_3_alltoall -> layer_3_expert1_11
	layer_3_expert1_11 -> layer_3_expert_act_11
	layer_3_expert_act_11 -> layer_3_expert2_11
	layer_3_expert2_11 -> layer_3_expert_allreduce_2
	layer_3_expert_allreduce_2 -> layer_3_expert_agg
	layer_3_attn_allreduce_3 -> layer_3_gate_12 [style=dashed]
	layer_3_gate_12 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_12
	layer_3_expert1_12 -> layer_3_expert_act_12
	layer_3_expert_act_12 -> layer_3_expert2_12
	layer_3_expert2_12 -> layer_3_expert_allreduce_3
	layer_3_alltoall -> layer_3_expert1_13
	layer_3_expert1_13 -> layer_3_expert_act_13
	layer_3_expert_act_13 -> layer_3_expert2_13
	layer_3_expert2_13 -> layer_3_expert_allreduce_3
	layer_3_alltoall -> layer_3_expert1_14
	layer_3_expert1_14 -> layer_3_expert_act_14
	layer_3_expert_act_14 -> layer_3_expert2_14
	layer_3_expert2_14 -> layer_3_expert_allreduce_3
	layer_3_alltoall -> layer_3_expert1_15
	layer_3_expert1_15 -> layer_3_expert_act_15
	layer_3_expert_act_15 -> layer_3_expert2_15
	layer_3_expert2_15 -> layer_3_expert_allreduce_3
	layer_3_expert_allreduce_3 -> layer_3_expert_agg
	layer_3_attn_allreduce_4 -> layer_3_gate_16 [style=dashed]
	layer_3_gate_16 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_16
	layer_3_expert1_16 -> layer_3_expert_act_16
	layer_3_expert_act_16 -> layer_3_expert2_16
	layer_3_expert2_16 -> layer_3_expert_allreduce_4
	layer_3_alltoall -> layer_3_expert1_17
	layer_3_expert1_17 -> layer_3_expert_act_17
	layer_3_expert_act_17 -> layer_3_expert2_17
	layer_3_expert2_17 -> layer_3_expert_allreduce_4
	layer_3_alltoall -> layer_3_expert1_18
	layer_3_expert1_18 -> layer_3_expert_act_18
	layer_3_expert_act_18 -> layer_3_expert2_18
	layer_3_expert2_18 -> layer_3_expert_allreduce_4
	layer_3_alltoall -> layer_3_expert1_19
	layer_3_expert1_19 -> layer_3_expert_act_19
	layer_3_expert_act_19 -> layer_3_expert2_19
	layer_3_expert2_19 -> layer_3_expert_allreduce_4
	layer_3_expert_allreduce_4 -> layer_3_expert_agg
	layer_3_attn_allreduce_5 -> layer_3_gate_20 [style=dashed]
	layer_3_gate_20 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_20
	layer_3_expert1_20 -> layer_3_expert_act_20
	layer_3_expert_act_20 -> layer_3_expert2_20
	layer_3_expert2_20 -> layer_3_expert_allreduce_5
	layer_3_alltoall -> layer_3_expert1_21
	layer_3_expert1_21 -> layer_3_expert_act_21
	layer_3_expert_act_21 -> layer_3_expert2_21
	layer_3_expert2_21 -> layer_3_expert_allreduce_5
	layer_3_alltoall -> layer_3_expert1_22
	layer_3_expert1_22 -> layer_3_expert_act_22
	layer_3_expert_act_22 -> layer_3_expert2_22
	layer_3_expert2_22 -> layer_3_expert_allreduce_5
	layer_3_alltoall -> layer_3_expert1_23
	layer_3_expert1_23 -> layer_3_expert_act_23
	layer_3_expert_act_23 -> layer_3_expert2_23
	layer_3_expert2_23 -> layer_3_expert_allreduce_5
	layer_3_expert_allreduce_5 -> layer_3_expert_agg
	layer_3_attn_allreduce_6 -> layer_3_gate_24 [style=dashed]
	layer_3_gate_24 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_24
	layer_3_expert1_24 -> layer_3_expert_act_24
	layer_3_expert_act_24 -> layer_3_expert2_24
	layer_3_expert2_24 -> layer_3_expert_allreduce_6
	layer_3_alltoall -> layer_3_expert1_25
	layer_3_expert1_25 -> layer_3_expert_act_25
	layer_3_expert_act_25 -> layer_3_expert2_25
	layer_3_expert2_25 -> layer_3_expert_allreduce_6
	layer_3_alltoall -> layer_3_expert1_26
	layer_3_expert1_26 -> layer_3_expert_act_26
	layer_3_expert_act_26 -> layer_3_expert2_26
	layer_3_expert2_26 -> layer_3_expert_allreduce_6
	layer_3_alltoall -> layer_3_expert1_27
	layer_3_expert1_27 -> layer_3_expert_act_27
	layer_3_expert_act_27 -> layer_3_expert2_27
	layer_3_expert2_27 -> layer_3_expert_allreduce_6
	layer_3_expert_allreduce_6 -> layer_3_expert_agg
	layer_3_attn_allreduce_7 -> layer_3_gate_28 [style=dashed]
	layer_3_gate_28 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_28
	layer_3_expert1_28 -> layer_3_expert_act_28
	layer_3_expert_act_28 -> layer_3_expert2_28
	layer_3_expert2_28 -> layer_3_expert_allreduce_7
	layer_3_alltoall -> layer_3_expert1_29
	layer_3_expert1_29 -> layer_3_expert_act_29
	layer_3_expert_act_29 -> layer_3_expert2_29
	layer_3_expert2_29 -> layer_3_expert_allreduce_7
	layer_3_alltoall -> layer_3_expert1_30
	layer_3_expert1_30 -> layer_3_expert_act_30
	layer_3_expert_act_30 -> layer_3_expert2_30
	layer_3_expert2_30 -> layer_3_expert_allreduce_7
	layer_3_alltoall -> layer_3_expert1_31
	layer_3_expert1_31 -> layer_3_expert_act_31
	layer_3_expert_act_31 -> layer_3_expert2_31
	layer_3_expert2_31 -> layer_3_expert_allreduce_7
	layer_3_expert_allreduce_7 -> layer_3_expert_agg
	layer_3_attn_allreduce_8 -> layer_3_gate_32 [style=dashed]
	layer_3_gate_32 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_32
	layer_3_expert1_32 -> layer_3_expert_act_32
	layer_3_expert_act_32 -> layer_3_expert2_32
	layer_3_expert2_32 -> layer_3_expert_allreduce_8
	layer_3_alltoall -> layer_3_expert1_33
	layer_3_expert1_33 -> layer_3_expert_act_33
	layer_3_expert_act_33 -> layer_3_expert2_33
	layer_3_expert2_33 -> layer_3_expert_allreduce_8
	layer_3_alltoall -> layer_3_expert1_34
	layer_3_expert1_34 -> layer_3_expert_act_34
	layer_3_expert_act_34 -> layer_3_expert2_34
	layer_3_expert2_34 -> layer_3_expert_allreduce_8
	layer_3_alltoall -> layer_3_expert1_35
	layer_3_expert1_35 -> layer_3_expert_act_35
	layer_3_expert_act_35 -> layer_3_expert2_35
	layer_3_expert2_35 -> layer_3_expert_allreduce_8
	layer_3_expert_allreduce_8 -> layer_3_expert_agg
	layer_3_attn_allreduce_9 -> layer_3_gate_36 [style=dashed]
	layer_3_gate_36 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_36
	layer_3_expert1_36 -> layer_3_expert_act_36
	layer_3_expert_act_36 -> layer_3_expert2_36
	layer_3_expert2_36 -> layer_3_expert_allreduce_9
	layer_3_alltoall -> layer_3_expert1_37
	layer_3_expert1_37 -> layer_3_expert_act_37
	layer_3_expert_act_37 -> layer_3_expert2_37
	layer_3_expert2_37 -> layer_3_expert_allreduce_9
	layer_3_alltoall -> layer_3_expert1_38
	layer_3_expert1_38 -> layer_3_expert_act_38
	layer_3_expert_act_38 -> layer_3_expert2_38
	layer_3_expert2_38 -> layer_3_expert_allreduce_9
	layer_3_alltoall -> layer_3_expert1_39
	layer_3_expert1_39 -> layer_3_expert_act_39
	layer_3_expert_act_39 -> layer_3_expert2_39
	layer_3_expert2_39 -> layer_3_expert_allreduce_9
	layer_3_expert_allreduce_9 -> layer_3_expert_agg
	layer_3_attn_allreduce_10 -> layer_3_gate_40 [style=dashed]
	layer_3_gate_40 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_40
	layer_3_expert1_40 -> layer_3_expert_act_40
	layer_3_expert_act_40 -> layer_3_expert2_40
	layer_3_expert2_40 -> layer_3_expert_allreduce_10
	layer_3_alltoall -> layer_3_expert1_41
	layer_3_expert1_41 -> layer_3_expert_act_41
	layer_3_expert_act_41 -> layer_3_expert2_41
	layer_3_expert2_41 -> layer_3_expert_allreduce_10
	layer_3_alltoall -> layer_3_expert1_42
	layer_3_expert1_42 -> layer_3_expert_act_42
	layer_3_expert_act_42 -> layer_3_expert2_42
	layer_3_expert2_42 -> layer_3_expert_allreduce_10
	layer_3_alltoall -> layer_3_expert1_43
	layer_3_expert1_43 -> layer_3_expert_act_43
	layer_3_expert_act_43 -> layer_3_expert2_43
	layer_3_expert2_43 -> layer_3_expert_allreduce_10
	layer_3_expert_allreduce_10 -> layer_3_expert_agg
	layer_3_attn_allreduce_11 -> layer_3_gate_44 [style=dashed]
	layer_3_gate_44 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_44
	layer_3_expert1_44 -> layer_3_expert_act_44
	layer_3_expert_act_44 -> layer_3_expert2_44
	layer_3_expert2_44 -> layer_3_expert_allreduce_11
	layer_3_alltoall -> layer_3_expert1_45
	layer_3_expert1_45 -> layer_3_expert_act_45
	layer_3_expert_act_45 -> layer_3_expert2_45
	layer_3_expert2_45 -> layer_3_expert_allreduce_11
	layer_3_alltoall -> layer_3_expert1_46
	layer_3_expert1_46 -> layer_3_expert_act_46
	layer_3_expert_act_46 -> layer_3_expert2_46
	layer_3_expert2_46 -> layer_3_expert_allreduce_11
	layer_3_alltoall -> layer_3_expert1_47
	layer_3_expert1_47 -> layer_3_expert_act_47
	layer_3_expert_act_47 -> layer_3_expert2_47
	layer_3_expert2_47 -> layer_3_expert_allreduce_11
	layer_3_expert_allreduce_11 -> layer_3_expert_agg
	layer_3_attn_allreduce_12 -> layer_3_gate_48 [style=dashed]
	layer_3_gate_48 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_48
	layer_3_expert1_48 -> layer_3_expert_act_48
	layer_3_expert_act_48 -> layer_3_expert2_48
	layer_3_expert2_48 -> layer_3_expert_allreduce_12
	layer_3_alltoall -> layer_3_expert1_49
	layer_3_expert1_49 -> layer_3_expert_act_49
	layer_3_expert_act_49 -> layer_3_expert2_49
	layer_3_expert2_49 -> layer_3_expert_allreduce_12
	layer_3_alltoall -> layer_3_expert1_50
	layer_3_expert1_50 -> layer_3_expert_act_50
	layer_3_expert_act_50 -> layer_3_expert2_50
	layer_3_expert2_50 -> layer_3_expert_allreduce_12
	layer_3_alltoall -> layer_3_expert1_51
	layer_3_expert1_51 -> layer_3_expert_act_51
	layer_3_expert_act_51 -> layer_3_expert2_51
	layer_3_expert2_51 -> layer_3_expert_allreduce_12
	layer_3_expert_allreduce_12 -> layer_3_expert_agg
	layer_3_attn_allreduce_13 -> layer_3_gate_52 [style=dashed]
	layer_3_gate_52 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_52
	layer_3_expert1_52 -> layer_3_expert_act_52
	layer_3_expert_act_52 -> layer_3_expert2_52
	layer_3_expert2_52 -> layer_3_expert_allreduce_13
	layer_3_alltoall -> layer_3_expert1_53
	layer_3_expert1_53 -> layer_3_expert_act_53
	layer_3_expert_act_53 -> layer_3_expert2_53
	layer_3_expert2_53 -> layer_3_expert_allreduce_13
	layer_3_alltoall -> layer_3_expert1_54
	layer_3_expert1_54 -> layer_3_expert_act_54
	layer_3_expert_act_54 -> layer_3_expert2_54
	layer_3_expert2_54 -> layer_3_expert_allreduce_13
	layer_3_alltoall -> layer_3_expert1_55
	layer_3_expert1_55 -> layer_3_expert_act_55
	layer_3_expert_act_55 -> layer_3_expert2_55
	layer_3_expert2_55 -> layer_3_expert_allreduce_13
	layer_3_expert_allreduce_13 -> layer_3_expert_agg
	layer_3_attn_allreduce_14 -> layer_3_gate_56 [style=dashed]
	layer_3_gate_56 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_56
	layer_3_expert1_56 -> layer_3_expert_act_56
	layer_3_expert_act_56 -> layer_3_expert2_56
	layer_3_expert2_56 -> layer_3_expert_allreduce_14
	layer_3_alltoall -> layer_3_expert1_57
	layer_3_expert1_57 -> layer_3_expert_act_57
	layer_3_expert_act_57 -> layer_3_expert2_57
	layer_3_expert2_57 -> layer_3_expert_allreduce_14
	layer_3_alltoall -> layer_3_expert1_58
	layer_3_expert1_58 -> layer_3_expert_act_58
	layer_3_expert_act_58 -> layer_3_expert2_58
	layer_3_expert2_58 -> layer_3_expert_allreduce_14
	layer_3_alltoall -> layer_3_expert1_59
	layer_3_expert1_59 -> layer_3_expert_act_59
	layer_3_expert_act_59 -> layer_3_expert2_59
	layer_3_expert2_59 -> layer_3_expert_allreduce_14
	layer_3_expert_allreduce_14 -> layer_3_expert_agg
	layer_3_attn_allreduce_15 -> layer_3_gate_60 [style=dashed]
	layer_3_gate_60 -> layer_3_alltoall [style=dashed]
	layer_3_alltoall -> layer_3_expert1_60
	layer_3_expert1_60 -> layer_3_expert_act_60
	layer_3_expert_act_60 -> layer_3_expert2_60
	layer_3_expert2_60 -> layer_3_expert_allreduce_15
	layer_3_alltoall -> layer_3_expert1_61
	layer_3_expert1_61 -> layer_3_expert_act_61
	layer_3_expert_act_61 -> layer_3_expert2_61
	layer_3_expert2_61 -> layer_3_expert_allreduce_15
	layer_3_alltoall -> layer_3_expert1_62
	layer_3_expert1_62 -> layer_3_expert_act_62
	layer_3_expert_act_62 -> layer_3_expert2_62
	layer_3_expert2_62 -> layer_3_expert_allreduce_15
	layer_3_alltoall -> layer_3_expert1_63
	layer_3_expert1_63 -> layer_3_expert_act_63
	layer_3_expert_act_63 -> layer_3_expert2_63
	layer_3_expert2_63 -> layer_3_expert_allreduce_15
	layer_3_expert_allreduce_15 -> layer_3_expert_agg
	layer_3_expert_agg -> layer_3_norm
	layer_3_norm -> layer_4_input
	layer_4_input [label="Layer4_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_4_qkv_0 [label="Layer4_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_1 [label="Layer4_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_2 [label="Layer4_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_3 [label="Layer4_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_0 [label="Layer4_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_1 [label="Layer4_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_2 [label="Layer4_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_3 [label="Layer4_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_0 [label="Layer4_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_1 [label="Layer4_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_2 [label="Layer4_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_3 [label="Layer4_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_0 [label="Layer4_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_1 [label="Layer4_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_2 [label="Layer4_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_3 [label="Layer4_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_0 [label="Layer4_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_0
	layer_4_qkv_0 -> layer_4_attn_scores_0
	layer_4_attn_scores_0 -> layer_4_attn_softmax_0
	layer_4_attn_softmax_0 -> layer_4_attn_out_0
	layer_4_attn_out_0 -> layer_4_attn_allreduce_0
	layer_4_input -> layer_4_qkv_1
	layer_4_qkv_1 -> layer_4_attn_scores_1
	layer_4_attn_scores_1 -> layer_4_attn_softmax_1
	layer_4_attn_softmax_1 -> layer_4_attn_out_1
	layer_4_attn_out_1 -> layer_4_attn_allreduce_0
	layer_4_input -> layer_4_qkv_2
	layer_4_qkv_2 -> layer_4_attn_scores_2
	layer_4_attn_scores_2 -> layer_4_attn_softmax_2
	layer_4_attn_softmax_2 -> layer_4_attn_out_2
	layer_4_attn_out_2 -> layer_4_attn_allreduce_0
	layer_4_input -> layer_4_qkv_3
	layer_4_qkv_3 -> layer_4_attn_scores_3
	layer_4_attn_scores_3 -> layer_4_attn_softmax_3
	layer_4_attn_softmax_3 -> layer_4_attn_out_3
	layer_4_attn_out_3 -> layer_4_attn_allreduce_0
	layer_4_qkv_4 [label="Layer4_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_5 [label="Layer4_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_6 [label="Layer4_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_7 [label="Layer4_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_4 [label="Layer4_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_5 [label="Layer4_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_6 [label="Layer4_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_7 [label="Layer4_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_4 [label="Layer4_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_5 [label="Layer4_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_6 [label="Layer4_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_7 [label="Layer4_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_4 [label="Layer4_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_5 [label="Layer4_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_6 [label="Layer4_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_7 [label="Layer4_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_1 [label="Layer4_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_4
	layer_4_qkv_4 -> layer_4_attn_scores_4
	layer_4_attn_scores_4 -> layer_4_attn_softmax_4
	layer_4_attn_softmax_4 -> layer_4_attn_out_4
	layer_4_attn_out_4 -> layer_4_attn_allreduce_1
	layer_4_input -> layer_4_qkv_5
	layer_4_qkv_5 -> layer_4_attn_scores_5
	layer_4_attn_scores_5 -> layer_4_attn_softmax_5
	layer_4_attn_softmax_5 -> layer_4_attn_out_5
	layer_4_attn_out_5 -> layer_4_attn_allreduce_1
	layer_4_input -> layer_4_qkv_6
	layer_4_qkv_6 -> layer_4_attn_scores_6
	layer_4_attn_scores_6 -> layer_4_attn_softmax_6
	layer_4_attn_softmax_6 -> layer_4_attn_out_6
	layer_4_attn_out_6 -> layer_4_attn_allreduce_1
	layer_4_input -> layer_4_qkv_7
	layer_4_qkv_7 -> layer_4_attn_scores_7
	layer_4_attn_scores_7 -> layer_4_attn_softmax_7
	layer_4_attn_softmax_7 -> layer_4_attn_out_7
	layer_4_attn_out_7 -> layer_4_attn_allreduce_1
	layer_4_qkv_8 [label="Layer4_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_9 [label="Layer4_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_10 [label="Layer4_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_11 [label="Layer4_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_8 [label="Layer4_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_9 [label="Layer4_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_10 [label="Layer4_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_11 [label="Layer4_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_8 [label="Layer4_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_9 [label="Layer4_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_10 [label="Layer4_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_11 [label="Layer4_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_8 [label="Layer4_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_9 [label="Layer4_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_10 [label="Layer4_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_11 [label="Layer4_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_2 [label="Layer4_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_8
	layer_4_qkv_8 -> layer_4_attn_scores_8
	layer_4_attn_scores_8 -> layer_4_attn_softmax_8
	layer_4_attn_softmax_8 -> layer_4_attn_out_8
	layer_4_attn_out_8 -> layer_4_attn_allreduce_2
	layer_4_input -> layer_4_qkv_9
	layer_4_qkv_9 -> layer_4_attn_scores_9
	layer_4_attn_scores_9 -> layer_4_attn_softmax_9
	layer_4_attn_softmax_9 -> layer_4_attn_out_9
	layer_4_attn_out_9 -> layer_4_attn_allreduce_2
	layer_4_input -> layer_4_qkv_10
	layer_4_qkv_10 -> layer_4_attn_scores_10
	layer_4_attn_scores_10 -> layer_4_attn_softmax_10
	layer_4_attn_softmax_10 -> layer_4_attn_out_10
	layer_4_attn_out_10 -> layer_4_attn_allreduce_2
	layer_4_input -> layer_4_qkv_11
	layer_4_qkv_11 -> layer_4_attn_scores_11
	layer_4_attn_scores_11 -> layer_4_attn_softmax_11
	layer_4_attn_softmax_11 -> layer_4_attn_out_11
	layer_4_attn_out_11 -> layer_4_attn_allreduce_2
	layer_4_qkv_12 [label="Layer4_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_13 [label="Layer4_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_14 [label="Layer4_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_15 [label="Layer4_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_12 [label="Layer4_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_13 [label="Layer4_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_14 [label="Layer4_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_15 [label="Layer4_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_12 [label="Layer4_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_13 [label="Layer4_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_14 [label="Layer4_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_15 [label="Layer4_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_12 [label="Layer4_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_13 [label="Layer4_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_14 [label="Layer4_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_15 [label="Layer4_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_3 [label="Layer4_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_12
	layer_4_qkv_12 -> layer_4_attn_scores_12
	layer_4_attn_scores_12 -> layer_4_attn_softmax_12
	layer_4_attn_softmax_12 -> layer_4_attn_out_12
	layer_4_attn_out_12 -> layer_4_attn_allreduce_3
	layer_4_input -> layer_4_qkv_13
	layer_4_qkv_13 -> layer_4_attn_scores_13
	layer_4_attn_scores_13 -> layer_4_attn_softmax_13
	layer_4_attn_softmax_13 -> layer_4_attn_out_13
	layer_4_attn_out_13 -> layer_4_attn_allreduce_3
	layer_4_input -> layer_4_qkv_14
	layer_4_qkv_14 -> layer_4_attn_scores_14
	layer_4_attn_scores_14 -> layer_4_attn_softmax_14
	layer_4_attn_softmax_14 -> layer_4_attn_out_14
	layer_4_attn_out_14 -> layer_4_attn_allreduce_3
	layer_4_input -> layer_4_qkv_15
	layer_4_qkv_15 -> layer_4_attn_scores_15
	layer_4_attn_scores_15 -> layer_4_attn_softmax_15
	layer_4_attn_softmax_15 -> layer_4_attn_out_15
	layer_4_attn_out_15 -> layer_4_attn_allreduce_3
	layer_4_qkv_16 [label="Layer4_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_17 [label="Layer4_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_18 [label="Layer4_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_19 [label="Layer4_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_16 [label="Layer4_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_17 [label="Layer4_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_18 [label="Layer4_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_19 [label="Layer4_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_16 [label="Layer4_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_17 [label="Layer4_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_18 [label="Layer4_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_19 [label="Layer4_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_16 [label="Layer4_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_17 [label="Layer4_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_18 [label="Layer4_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_19 [label="Layer4_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_4 [label="Layer4_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_16
	layer_4_qkv_16 -> layer_4_attn_scores_16
	layer_4_attn_scores_16 -> layer_4_attn_softmax_16
	layer_4_attn_softmax_16 -> layer_4_attn_out_16
	layer_4_attn_out_16 -> layer_4_attn_allreduce_4
	layer_4_input -> layer_4_qkv_17
	layer_4_qkv_17 -> layer_4_attn_scores_17
	layer_4_attn_scores_17 -> layer_4_attn_softmax_17
	layer_4_attn_softmax_17 -> layer_4_attn_out_17
	layer_4_attn_out_17 -> layer_4_attn_allreduce_4
	layer_4_input -> layer_4_qkv_18
	layer_4_qkv_18 -> layer_4_attn_scores_18
	layer_4_attn_scores_18 -> layer_4_attn_softmax_18
	layer_4_attn_softmax_18 -> layer_4_attn_out_18
	layer_4_attn_out_18 -> layer_4_attn_allreduce_4
	layer_4_input -> layer_4_qkv_19
	layer_4_qkv_19 -> layer_4_attn_scores_19
	layer_4_attn_scores_19 -> layer_4_attn_softmax_19
	layer_4_attn_softmax_19 -> layer_4_attn_out_19
	layer_4_attn_out_19 -> layer_4_attn_allreduce_4
	layer_4_qkv_20 [label="Layer4_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_21 [label="Layer4_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_22 [label="Layer4_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_23 [label="Layer4_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_20 [label="Layer4_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_21 [label="Layer4_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_22 [label="Layer4_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_23 [label="Layer4_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_20 [label="Layer4_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_21 [label="Layer4_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_22 [label="Layer4_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_23 [label="Layer4_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_20 [label="Layer4_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_21 [label="Layer4_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_22 [label="Layer4_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_23 [label="Layer4_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_5 [label="Layer4_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_20
	layer_4_qkv_20 -> layer_4_attn_scores_20
	layer_4_attn_scores_20 -> layer_4_attn_softmax_20
	layer_4_attn_softmax_20 -> layer_4_attn_out_20
	layer_4_attn_out_20 -> layer_4_attn_allreduce_5
	layer_4_input -> layer_4_qkv_21
	layer_4_qkv_21 -> layer_4_attn_scores_21
	layer_4_attn_scores_21 -> layer_4_attn_softmax_21
	layer_4_attn_softmax_21 -> layer_4_attn_out_21
	layer_4_attn_out_21 -> layer_4_attn_allreduce_5
	layer_4_input -> layer_4_qkv_22
	layer_4_qkv_22 -> layer_4_attn_scores_22
	layer_4_attn_scores_22 -> layer_4_attn_softmax_22
	layer_4_attn_softmax_22 -> layer_4_attn_out_22
	layer_4_attn_out_22 -> layer_4_attn_allreduce_5
	layer_4_input -> layer_4_qkv_23
	layer_4_qkv_23 -> layer_4_attn_scores_23
	layer_4_attn_scores_23 -> layer_4_attn_softmax_23
	layer_4_attn_softmax_23 -> layer_4_attn_out_23
	layer_4_attn_out_23 -> layer_4_attn_allreduce_5
	layer_4_qkv_24 [label="Layer4_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_25 [label="Layer4_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_26 [label="Layer4_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_27 [label="Layer4_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_24 [label="Layer4_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_25 [label="Layer4_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_26 [label="Layer4_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_27 [label="Layer4_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_24 [label="Layer4_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_25 [label="Layer4_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_26 [label="Layer4_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_27 [label="Layer4_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_24 [label="Layer4_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_25 [label="Layer4_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_26 [label="Layer4_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_27 [label="Layer4_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_6 [label="Layer4_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_24
	layer_4_qkv_24 -> layer_4_attn_scores_24
	layer_4_attn_scores_24 -> layer_4_attn_softmax_24
	layer_4_attn_softmax_24 -> layer_4_attn_out_24
	layer_4_attn_out_24 -> layer_4_attn_allreduce_6
	layer_4_input -> layer_4_qkv_25
	layer_4_qkv_25 -> layer_4_attn_scores_25
	layer_4_attn_scores_25 -> layer_4_attn_softmax_25
	layer_4_attn_softmax_25 -> layer_4_attn_out_25
	layer_4_attn_out_25 -> layer_4_attn_allreduce_6
	layer_4_input -> layer_4_qkv_26
	layer_4_qkv_26 -> layer_4_attn_scores_26
	layer_4_attn_scores_26 -> layer_4_attn_softmax_26
	layer_4_attn_softmax_26 -> layer_4_attn_out_26
	layer_4_attn_out_26 -> layer_4_attn_allreduce_6
	layer_4_input -> layer_4_qkv_27
	layer_4_qkv_27 -> layer_4_attn_scores_27
	layer_4_attn_scores_27 -> layer_4_attn_softmax_27
	layer_4_attn_softmax_27 -> layer_4_attn_out_27
	layer_4_attn_out_27 -> layer_4_attn_allreduce_6
	layer_4_qkv_28 [label="Layer4_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_29 [label="Layer4_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_30 [label="Layer4_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_31 [label="Layer4_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_28 [label="Layer4_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_29 [label="Layer4_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_30 [label="Layer4_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_31 [label="Layer4_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_28 [label="Layer4_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_29 [label="Layer4_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_30 [label="Layer4_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_31 [label="Layer4_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_28 [label="Layer4_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_29 [label="Layer4_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_30 [label="Layer4_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_31 [label="Layer4_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_7 [label="Layer4_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_28
	layer_4_qkv_28 -> layer_4_attn_scores_28
	layer_4_attn_scores_28 -> layer_4_attn_softmax_28
	layer_4_attn_softmax_28 -> layer_4_attn_out_28
	layer_4_attn_out_28 -> layer_4_attn_allreduce_7
	layer_4_input -> layer_4_qkv_29
	layer_4_qkv_29 -> layer_4_attn_scores_29
	layer_4_attn_scores_29 -> layer_4_attn_softmax_29
	layer_4_attn_softmax_29 -> layer_4_attn_out_29
	layer_4_attn_out_29 -> layer_4_attn_allreduce_7
	layer_4_input -> layer_4_qkv_30
	layer_4_qkv_30 -> layer_4_attn_scores_30
	layer_4_attn_scores_30 -> layer_4_attn_softmax_30
	layer_4_attn_softmax_30 -> layer_4_attn_out_30
	layer_4_attn_out_30 -> layer_4_attn_allreduce_7
	layer_4_input -> layer_4_qkv_31
	layer_4_qkv_31 -> layer_4_attn_scores_31
	layer_4_attn_scores_31 -> layer_4_attn_softmax_31
	layer_4_attn_softmax_31 -> layer_4_attn_out_31
	layer_4_attn_out_31 -> layer_4_attn_allreduce_7
	layer_4_qkv_32 [label="Layer4_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_33 [label="Layer4_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_34 [label="Layer4_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_35 [label="Layer4_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_32 [label="Layer4_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_33 [label="Layer4_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_34 [label="Layer4_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_35 [label="Layer4_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_32 [label="Layer4_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_33 [label="Layer4_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_34 [label="Layer4_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_35 [label="Layer4_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_32 [label="Layer4_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_33 [label="Layer4_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_34 [label="Layer4_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_35 [label="Layer4_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_8 [label="Layer4_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_32
	layer_4_qkv_32 -> layer_4_attn_scores_32
	layer_4_attn_scores_32 -> layer_4_attn_softmax_32
	layer_4_attn_softmax_32 -> layer_4_attn_out_32
	layer_4_attn_out_32 -> layer_4_attn_allreduce_8
	layer_4_input -> layer_4_qkv_33
	layer_4_qkv_33 -> layer_4_attn_scores_33
	layer_4_attn_scores_33 -> layer_4_attn_softmax_33
	layer_4_attn_softmax_33 -> layer_4_attn_out_33
	layer_4_attn_out_33 -> layer_4_attn_allreduce_8
	layer_4_input -> layer_4_qkv_34
	layer_4_qkv_34 -> layer_4_attn_scores_34
	layer_4_attn_scores_34 -> layer_4_attn_softmax_34
	layer_4_attn_softmax_34 -> layer_4_attn_out_34
	layer_4_attn_out_34 -> layer_4_attn_allreduce_8
	layer_4_input -> layer_4_qkv_35
	layer_4_qkv_35 -> layer_4_attn_scores_35
	layer_4_attn_scores_35 -> layer_4_attn_softmax_35
	layer_4_attn_softmax_35 -> layer_4_attn_out_35
	layer_4_attn_out_35 -> layer_4_attn_allreduce_8
	layer_4_qkv_36 [label="Layer4_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_37 [label="Layer4_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_38 [label="Layer4_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_39 [label="Layer4_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_36 [label="Layer4_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_37 [label="Layer4_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_38 [label="Layer4_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_39 [label="Layer4_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_36 [label="Layer4_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_37 [label="Layer4_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_38 [label="Layer4_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_39 [label="Layer4_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_36 [label="Layer4_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_37 [label="Layer4_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_38 [label="Layer4_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_39 [label="Layer4_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_9 [label="Layer4_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_36
	layer_4_qkv_36 -> layer_4_attn_scores_36
	layer_4_attn_scores_36 -> layer_4_attn_softmax_36
	layer_4_attn_softmax_36 -> layer_4_attn_out_36
	layer_4_attn_out_36 -> layer_4_attn_allreduce_9
	layer_4_input -> layer_4_qkv_37
	layer_4_qkv_37 -> layer_4_attn_scores_37
	layer_4_attn_scores_37 -> layer_4_attn_softmax_37
	layer_4_attn_softmax_37 -> layer_4_attn_out_37
	layer_4_attn_out_37 -> layer_4_attn_allreduce_9
	layer_4_input -> layer_4_qkv_38
	layer_4_qkv_38 -> layer_4_attn_scores_38
	layer_4_attn_scores_38 -> layer_4_attn_softmax_38
	layer_4_attn_softmax_38 -> layer_4_attn_out_38
	layer_4_attn_out_38 -> layer_4_attn_allreduce_9
	layer_4_input -> layer_4_qkv_39
	layer_4_qkv_39 -> layer_4_attn_scores_39
	layer_4_attn_scores_39 -> layer_4_attn_softmax_39
	layer_4_attn_softmax_39 -> layer_4_attn_out_39
	layer_4_attn_out_39 -> layer_4_attn_allreduce_9
	layer_4_qkv_40 [label="Layer4_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_41 [label="Layer4_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_42 [label="Layer4_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_43 [label="Layer4_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_40 [label="Layer4_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_41 [label="Layer4_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_42 [label="Layer4_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_43 [label="Layer4_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_40 [label="Layer4_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_41 [label="Layer4_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_42 [label="Layer4_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_43 [label="Layer4_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_40 [label="Layer4_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_41 [label="Layer4_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_42 [label="Layer4_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_43 [label="Layer4_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_10 [label="Layer4_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_40
	layer_4_qkv_40 -> layer_4_attn_scores_40
	layer_4_attn_scores_40 -> layer_4_attn_softmax_40
	layer_4_attn_softmax_40 -> layer_4_attn_out_40
	layer_4_attn_out_40 -> layer_4_attn_allreduce_10
	layer_4_input -> layer_4_qkv_41
	layer_4_qkv_41 -> layer_4_attn_scores_41
	layer_4_attn_scores_41 -> layer_4_attn_softmax_41
	layer_4_attn_softmax_41 -> layer_4_attn_out_41
	layer_4_attn_out_41 -> layer_4_attn_allreduce_10
	layer_4_input -> layer_4_qkv_42
	layer_4_qkv_42 -> layer_4_attn_scores_42
	layer_4_attn_scores_42 -> layer_4_attn_softmax_42
	layer_4_attn_softmax_42 -> layer_4_attn_out_42
	layer_4_attn_out_42 -> layer_4_attn_allreduce_10
	layer_4_input -> layer_4_qkv_43
	layer_4_qkv_43 -> layer_4_attn_scores_43
	layer_4_attn_scores_43 -> layer_4_attn_softmax_43
	layer_4_attn_softmax_43 -> layer_4_attn_out_43
	layer_4_attn_out_43 -> layer_4_attn_allreduce_10
	layer_4_qkv_44 [label="Layer4_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_45 [label="Layer4_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_46 [label="Layer4_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_47 [label="Layer4_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_44 [label="Layer4_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_45 [label="Layer4_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_46 [label="Layer4_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_47 [label="Layer4_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_44 [label="Layer4_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_45 [label="Layer4_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_46 [label="Layer4_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_47 [label="Layer4_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_44 [label="Layer4_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_45 [label="Layer4_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_46 [label="Layer4_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_47 [label="Layer4_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_11 [label="Layer4_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_44
	layer_4_qkv_44 -> layer_4_attn_scores_44
	layer_4_attn_scores_44 -> layer_4_attn_softmax_44
	layer_4_attn_softmax_44 -> layer_4_attn_out_44
	layer_4_attn_out_44 -> layer_4_attn_allreduce_11
	layer_4_input -> layer_4_qkv_45
	layer_4_qkv_45 -> layer_4_attn_scores_45
	layer_4_attn_scores_45 -> layer_4_attn_softmax_45
	layer_4_attn_softmax_45 -> layer_4_attn_out_45
	layer_4_attn_out_45 -> layer_4_attn_allreduce_11
	layer_4_input -> layer_4_qkv_46
	layer_4_qkv_46 -> layer_4_attn_scores_46
	layer_4_attn_scores_46 -> layer_4_attn_softmax_46
	layer_4_attn_softmax_46 -> layer_4_attn_out_46
	layer_4_attn_out_46 -> layer_4_attn_allreduce_11
	layer_4_input -> layer_4_qkv_47
	layer_4_qkv_47 -> layer_4_attn_scores_47
	layer_4_attn_scores_47 -> layer_4_attn_softmax_47
	layer_4_attn_softmax_47 -> layer_4_attn_out_47
	layer_4_attn_out_47 -> layer_4_attn_allreduce_11
	layer_4_qkv_48 [label="Layer4_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_49 [label="Layer4_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_50 [label="Layer4_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_51 [label="Layer4_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_48 [label="Layer4_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_49 [label="Layer4_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_50 [label="Layer4_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_51 [label="Layer4_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_48 [label="Layer4_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_49 [label="Layer4_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_50 [label="Layer4_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_51 [label="Layer4_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_48 [label="Layer4_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_49 [label="Layer4_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_50 [label="Layer4_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_51 [label="Layer4_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_12 [label="Layer4_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_48
	layer_4_qkv_48 -> layer_4_attn_scores_48
	layer_4_attn_scores_48 -> layer_4_attn_softmax_48
	layer_4_attn_softmax_48 -> layer_4_attn_out_48
	layer_4_attn_out_48 -> layer_4_attn_allreduce_12
	layer_4_input -> layer_4_qkv_49
	layer_4_qkv_49 -> layer_4_attn_scores_49
	layer_4_attn_scores_49 -> layer_4_attn_softmax_49
	layer_4_attn_softmax_49 -> layer_4_attn_out_49
	layer_4_attn_out_49 -> layer_4_attn_allreduce_12
	layer_4_input -> layer_4_qkv_50
	layer_4_qkv_50 -> layer_4_attn_scores_50
	layer_4_attn_scores_50 -> layer_4_attn_softmax_50
	layer_4_attn_softmax_50 -> layer_4_attn_out_50
	layer_4_attn_out_50 -> layer_4_attn_allreduce_12
	layer_4_input -> layer_4_qkv_51
	layer_4_qkv_51 -> layer_4_attn_scores_51
	layer_4_attn_scores_51 -> layer_4_attn_softmax_51
	layer_4_attn_softmax_51 -> layer_4_attn_out_51
	layer_4_attn_out_51 -> layer_4_attn_allreduce_12
	layer_4_qkv_52 [label="Layer4_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_53 [label="Layer4_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_54 [label="Layer4_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_55 [label="Layer4_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_52 [label="Layer4_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_53 [label="Layer4_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_54 [label="Layer4_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_55 [label="Layer4_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_52 [label="Layer4_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_53 [label="Layer4_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_54 [label="Layer4_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_55 [label="Layer4_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_52 [label="Layer4_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_53 [label="Layer4_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_54 [label="Layer4_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_55 [label="Layer4_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_13 [label="Layer4_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_52
	layer_4_qkv_52 -> layer_4_attn_scores_52
	layer_4_attn_scores_52 -> layer_4_attn_softmax_52
	layer_4_attn_softmax_52 -> layer_4_attn_out_52
	layer_4_attn_out_52 -> layer_4_attn_allreduce_13
	layer_4_input -> layer_4_qkv_53
	layer_4_qkv_53 -> layer_4_attn_scores_53
	layer_4_attn_scores_53 -> layer_4_attn_softmax_53
	layer_4_attn_softmax_53 -> layer_4_attn_out_53
	layer_4_attn_out_53 -> layer_4_attn_allreduce_13
	layer_4_input -> layer_4_qkv_54
	layer_4_qkv_54 -> layer_4_attn_scores_54
	layer_4_attn_scores_54 -> layer_4_attn_softmax_54
	layer_4_attn_softmax_54 -> layer_4_attn_out_54
	layer_4_attn_out_54 -> layer_4_attn_allreduce_13
	layer_4_input -> layer_4_qkv_55
	layer_4_qkv_55 -> layer_4_attn_scores_55
	layer_4_attn_scores_55 -> layer_4_attn_softmax_55
	layer_4_attn_softmax_55 -> layer_4_attn_out_55
	layer_4_attn_out_55 -> layer_4_attn_allreduce_13
	layer_4_qkv_56 [label="Layer4_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_57 [label="Layer4_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_58 [label="Layer4_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_59 [label="Layer4_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_56 [label="Layer4_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_57 [label="Layer4_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_58 [label="Layer4_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_59 [label="Layer4_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_56 [label="Layer4_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_57 [label="Layer4_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_58 [label="Layer4_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_59 [label="Layer4_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_56 [label="Layer4_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_57 [label="Layer4_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_58 [label="Layer4_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_59 [label="Layer4_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_14 [label="Layer4_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_56
	layer_4_qkv_56 -> layer_4_attn_scores_56
	layer_4_attn_scores_56 -> layer_4_attn_softmax_56
	layer_4_attn_softmax_56 -> layer_4_attn_out_56
	layer_4_attn_out_56 -> layer_4_attn_allreduce_14
	layer_4_input -> layer_4_qkv_57
	layer_4_qkv_57 -> layer_4_attn_scores_57
	layer_4_attn_scores_57 -> layer_4_attn_softmax_57
	layer_4_attn_softmax_57 -> layer_4_attn_out_57
	layer_4_attn_out_57 -> layer_4_attn_allreduce_14
	layer_4_input -> layer_4_qkv_58
	layer_4_qkv_58 -> layer_4_attn_scores_58
	layer_4_attn_scores_58 -> layer_4_attn_softmax_58
	layer_4_attn_softmax_58 -> layer_4_attn_out_58
	layer_4_attn_out_58 -> layer_4_attn_allreduce_14
	layer_4_input -> layer_4_qkv_59
	layer_4_qkv_59 -> layer_4_attn_scores_59
	layer_4_attn_scores_59 -> layer_4_attn_softmax_59
	layer_4_attn_softmax_59 -> layer_4_attn_out_59
	layer_4_attn_out_59 -> layer_4_attn_allreduce_14
	layer_4_qkv_60 [label="Layer4_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_61 [label="Layer4_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_62 [label="Layer4_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_qkv_63 [label="Layer4_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_60 [label="Layer4_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_61 [label="Layer4_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_62 [label="Layer4_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_scores_63 [label="Layer4_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_60 [label="Layer4_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_61 [label="Layer4_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_62 [label="Layer4_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_softmax_63 [label="Layer4_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_60 [label="Layer4_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_61 [label="Layer4_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_62 [label="Layer4_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_out_63 [label="Layer4_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_15 [label="Layer4_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_input -> layer_4_qkv_60
	layer_4_qkv_60 -> layer_4_attn_scores_60
	layer_4_attn_scores_60 -> layer_4_attn_softmax_60
	layer_4_attn_softmax_60 -> layer_4_attn_out_60
	layer_4_attn_out_60 -> layer_4_attn_allreduce_15
	layer_4_input -> layer_4_qkv_61
	layer_4_qkv_61 -> layer_4_attn_scores_61
	layer_4_attn_scores_61 -> layer_4_attn_softmax_61
	layer_4_attn_softmax_61 -> layer_4_attn_out_61
	layer_4_attn_out_61 -> layer_4_attn_allreduce_15
	layer_4_input -> layer_4_qkv_62
	layer_4_qkv_62 -> layer_4_attn_scores_62
	layer_4_attn_scores_62 -> layer_4_attn_softmax_62
	layer_4_attn_softmax_62 -> layer_4_attn_out_62
	layer_4_attn_out_62 -> layer_4_attn_allreduce_15
	layer_4_input -> layer_4_qkv_63
	layer_4_qkv_63 -> layer_4_attn_scores_63
	layer_4_attn_scores_63 -> layer_4_attn_softmax_63
	layer_4_attn_softmax_63 -> layer_4_attn_out_63
	layer_4_attn_out_63 -> layer_4_attn_allreduce_15
	layer_4_gate_0 [label="Layer4_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_4 [label="Layer4_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_8 [label="Layer4_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_12 [label="Layer4_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_16 [label="Layer4_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_20 [label="Layer4_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_24 [label="Layer4_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_28 [label="Layer4_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_32 [label="Layer4_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_36 [label="Layer4_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_40 [label="Layer4_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_44 [label="Layer4_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_48 [label="Layer4_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_52 [label="Layer4_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_56 [label="Layer4_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_gate_60 [label="Layer4_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_4_alltoall [label="Layer4_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_0 [label="Layer4_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_1 [label="Layer4_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_2 [label="Layer4_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_3 [label="Layer4_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_0 [label="Layer4_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_1 [label="Layer4_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_2 [label="Layer4_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_3 [label="Layer4_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_0 [label="Layer4_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_1 [label="Layer4_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_2 [label="Layer4_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_3 [label="Layer4_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_0 [label="Layer4_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_4 [label="Layer4_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_5 [label="Layer4_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_6 [label="Layer4_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_7 [label="Layer4_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_4 [label="Layer4_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_5 [label="Layer4_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_6 [label="Layer4_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_7 [label="Layer4_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_4 [label="Layer4_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_5 [label="Layer4_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_6 [label="Layer4_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_7 [label="Layer4_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_1 [label="Layer4_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_8 [label="Layer4_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_9 [label="Layer4_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_10 [label="Layer4_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_11 [label="Layer4_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_8 [label="Layer4_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_9 [label="Layer4_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_10 [label="Layer4_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_11 [label="Layer4_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_8 [label="Layer4_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_9 [label="Layer4_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_10 [label="Layer4_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_11 [label="Layer4_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_2 [label="Layer4_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_12 [label="Layer4_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_13 [label="Layer4_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_14 [label="Layer4_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_15 [label="Layer4_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_12 [label="Layer4_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_13 [label="Layer4_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_14 [label="Layer4_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_15 [label="Layer4_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_12 [label="Layer4_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_13 [label="Layer4_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_14 [label="Layer4_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_15 [label="Layer4_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_3 [label="Layer4_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_16 [label="Layer4_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_17 [label="Layer4_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_18 [label="Layer4_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_19 [label="Layer4_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_16 [label="Layer4_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_17 [label="Layer4_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_18 [label="Layer4_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_19 [label="Layer4_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_16 [label="Layer4_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_17 [label="Layer4_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_18 [label="Layer4_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_19 [label="Layer4_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_4 [label="Layer4_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_20 [label="Layer4_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_21 [label="Layer4_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_22 [label="Layer4_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_23 [label="Layer4_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_20 [label="Layer4_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_21 [label="Layer4_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_22 [label="Layer4_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_23 [label="Layer4_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_20 [label="Layer4_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_21 [label="Layer4_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_22 [label="Layer4_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_23 [label="Layer4_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_5 [label="Layer4_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_24 [label="Layer4_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_25 [label="Layer4_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_26 [label="Layer4_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_27 [label="Layer4_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_24 [label="Layer4_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_25 [label="Layer4_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_26 [label="Layer4_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_27 [label="Layer4_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_24 [label="Layer4_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_25 [label="Layer4_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_26 [label="Layer4_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_27 [label="Layer4_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_6 [label="Layer4_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_28 [label="Layer4_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_29 [label="Layer4_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_30 [label="Layer4_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_31 [label="Layer4_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_28 [label="Layer4_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_29 [label="Layer4_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_30 [label="Layer4_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_31 [label="Layer4_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_28 [label="Layer4_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_29 [label="Layer4_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_30 [label="Layer4_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_31 [label="Layer4_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_7 [label="Layer4_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_32 [label="Layer4_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_33 [label="Layer4_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_34 [label="Layer4_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_35 [label="Layer4_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_32 [label="Layer4_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_33 [label="Layer4_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_34 [label="Layer4_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_35 [label="Layer4_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_32 [label="Layer4_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_33 [label="Layer4_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_34 [label="Layer4_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_35 [label="Layer4_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_8 [label="Layer4_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_36 [label="Layer4_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_37 [label="Layer4_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_38 [label="Layer4_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_39 [label="Layer4_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_36 [label="Layer4_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_37 [label="Layer4_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_38 [label="Layer4_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_39 [label="Layer4_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_36 [label="Layer4_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_37 [label="Layer4_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_38 [label="Layer4_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_39 [label="Layer4_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_9 [label="Layer4_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_40 [label="Layer4_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_41 [label="Layer4_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_42 [label="Layer4_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_43 [label="Layer4_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_40 [label="Layer4_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_41 [label="Layer4_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_42 [label="Layer4_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_43 [label="Layer4_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_40 [label="Layer4_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_41 [label="Layer4_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_42 [label="Layer4_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_43 [label="Layer4_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_10 [label="Layer4_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_44 [label="Layer4_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_45 [label="Layer4_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_46 [label="Layer4_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_47 [label="Layer4_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_44 [label="Layer4_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_45 [label="Layer4_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_46 [label="Layer4_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_47 [label="Layer4_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_44 [label="Layer4_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_45 [label="Layer4_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_46 [label="Layer4_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_47 [label="Layer4_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_11 [label="Layer4_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_48 [label="Layer4_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_49 [label="Layer4_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_50 [label="Layer4_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_51 [label="Layer4_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_48 [label="Layer4_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_49 [label="Layer4_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_50 [label="Layer4_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_51 [label="Layer4_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_48 [label="Layer4_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_49 [label="Layer4_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_50 [label="Layer4_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_51 [label="Layer4_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_12 [label="Layer4_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_52 [label="Layer4_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_53 [label="Layer4_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_54 [label="Layer4_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_55 [label="Layer4_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_52 [label="Layer4_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_53 [label="Layer4_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_54 [label="Layer4_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_55 [label="Layer4_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_52 [label="Layer4_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_53 [label="Layer4_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_54 [label="Layer4_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_55 [label="Layer4_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_13 [label="Layer4_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_56 [label="Layer4_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_57 [label="Layer4_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_58 [label="Layer4_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_59 [label="Layer4_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_56 [label="Layer4_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_57 [label="Layer4_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_58 [label="Layer4_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_59 [label="Layer4_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_56 [label="Layer4_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_57 [label="Layer4_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_58 [label="Layer4_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_59 [label="Layer4_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_14 [label="Layer4_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert1_60 [label="Layer4_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_61 [label="Layer4_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_62 [label="Layer4_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert1_63 [label="Layer4_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_60 [label="Layer4_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_61 [label="Layer4_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_62 [label="Layer4_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert_act_63 [label="Layer4_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_4_expert2_60 [label="Layer4_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_61 [label="Layer4_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_62 [label="Layer4_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert2_63 [label="Layer4_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_4_expert_allreduce_15 [label="Layer4_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_4_expert_agg [label="Layer4_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_4_norm [label="Layer4_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_4_attn_allreduce_0 -> layer_4_gate_0 [style=dashed]
	layer_4_gate_0 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_0
	layer_4_expert1_0 -> layer_4_expert_act_0
	layer_4_expert_act_0 -> layer_4_expert2_0
	layer_4_expert2_0 -> layer_4_expert_allreduce_0
	layer_4_alltoall -> layer_4_expert1_1
	layer_4_expert1_1 -> layer_4_expert_act_1
	layer_4_expert_act_1 -> layer_4_expert2_1
	layer_4_expert2_1 -> layer_4_expert_allreduce_0
	layer_4_alltoall -> layer_4_expert1_2
	layer_4_expert1_2 -> layer_4_expert_act_2
	layer_4_expert_act_2 -> layer_4_expert2_2
	layer_4_expert2_2 -> layer_4_expert_allreduce_0
	layer_4_alltoall -> layer_4_expert1_3
	layer_4_expert1_3 -> layer_4_expert_act_3
	layer_4_expert_act_3 -> layer_4_expert2_3
	layer_4_expert2_3 -> layer_4_expert_allreduce_0
	layer_4_expert_allreduce_0 -> layer_4_expert_agg
	layer_4_attn_allreduce_1 -> layer_4_gate_4 [style=dashed]
	layer_4_gate_4 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_4
	layer_4_expert1_4 -> layer_4_expert_act_4
	layer_4_expert_act_4 -> layer_4_expert2_4
	layer_4_expert2_4 -> layer_4_expert_allreduce_1
	layer_4_alltoall -> layer_4_expert1_5
	layer_4_expert1_5 -> layer_4_expert_act_5
	layer_4_expert_act_5 -> layer_4_expert2_5
	layer_4_expert2_5 -> layer_4_expert_allreduce_1
	layer_4_alltoall -> layer_4_expert1_6
	layer_4_expert1_6 -> layer_4_expert_act_6
	layer_4_expert_act_6 -> layer_4_expert2_6
	layer_4_expert2_6 -> layer_4_expert_allreduce_1
	layer_4_alltoall -> layer_4_expert1_7
	layer_4_expert1_7 -> layer_4_expert_act_7
	layer_4_expert_act_7 -> layer_4_expert2_7
	layer_4_expert2_7 -> layer_4_expert_allreduce_1
	layer_4_expert_allreduce_1 -> layer_4_expert_agg
	layer_4_attn_allreduce_2 -> layer_4_gate_8 [style=dashed]
	layer_4_gate_8 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_8
	layer_4_expert1_8 -> layer_4_expert_act_8
	layer_4_expert_act_8 -> layer_4_expert2_8
	layer_4_expert2_8 -> layer_4_expert_allreduce_2
	layer_4_alltoall -> layer_4_expert1_9
	layer_4_expert1_9 -> layer_4_expert_act_9
	layer_4_expert_act_9 -> layer_4_expert2_9
	layer_4_expert2_9 -> layer_4_expert_allreduce_2
	layer_4_alltoall -> layer_4_expert1_10
	layer_4_expert1_10 -> layer_4_expert_act_10
	layer_4_expert_act_10 -> layer_4_expert2_10
	layer_4_expert2_10 -> layer_4_expert_allreduce_2
	layer_4_alltoall -> layer_4_expert1_11
	layer_4_expert1_11 -> layer_4_expert_act_11
	layer_4_expert_act_11 -> layer_4_expert2_11
	layer_4_expert2_11 -> layer_4_expert_allreduce_2
	layer_4_expert_allreduce_2 -> layer_4_expert_agg
	layer_4_attn_allreduce_3 -> layer_4_gate_12 [style=dashed]
	layer_4_gate_12 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_12
	layer_4_expert1_12 -> layer_4_expert_act_12
	layer_4_expert_act_12 -> layer_4_expert2_12
	layer_4_expert2_12 -> layer_4_expert_allreduce_3
	layer_4_alltoall -> layer_4_expert1_13
	layer_4_expert1_13 -> layer_4_expert_act_13
	layer_4_expert_act_13 -> layer_4_expert2_13
	layer_4_expert2_13 -> layer_4_expert_allreduce_3
	layer_4_alltoall -> layer_4_expert1_14
	layer_4_expert1_14 -> layer_4_expert_act_14
	layer_4_expert_act_14 -> layer_4_expert2_14
	layer_4_expert2_14 -> layer_4_expert_allreduce_3
	layer_4_alltoall -> layer_4_expert1_15
	layer_4_expert1_15 -> layer_4_expert_act_15
	layer_4_expert_act_15 -> layer_4_expert2_15
	layer_4_expert2_15 -> layer_4_expert_allreduce_3
	layer_4_expert_allreduce_3 -> layer_4_expert_agg
	layer_4_attn_allreduce_4 -> layer_4_gate_16 [style=dashed]
	layer_4_gate_16 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_16
	layer_4_expert1_16 -> layer_4_expert_act_16
	layer_4_expert_act_16 -> layer_4_expert2_16
	layer_4_expert2_16 -> layer_4_expert_allreduce_4
	layer_4_alltoall -> layer_4_expert1_17
	layer_4_expert1_17 -> layer_4_expert_act_17
	layer_4_expert_act_17 -> layer_4_expert2_17
	layer_4_expert2_17 -> layer_4_expert_allreduce_4
	layer_4_alltoall -> layer_4_expert1_18
	layer_4_expert1_18 -> layer_4_expert_act_18
	layer_4_expert_act_18 -> layer_4_expert2_18
	layer_4_expert2_18 -> layer_4_expert_allreduce_4
	layer_4_alltoall -> layer_4_expert1_19
	layer_4_expert1_19 -> layer_4_expert_act_19
	layer_4_expert_act_19 -> layer_4_expert2_19
	layer_4_expert2_19 -> layer_4_expert_allreduce_4
	layer_4_expert_allreduce_4 -> layer_4_expert_agg
	layer_4_attn_allreduce_5 -> layer_4_gate_20 [style=dashed]
	layer_4_gate_20 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_20
	layer_4_expert1_20 -> layer_4_expert_act_20
	layer_4_expert_act_20 -> layer_4_expert2_20
	layer_4_expert2_20 -> layer_4_expert_allreduce_5
	layer_4_alltoall -> layer_4_expert1_21
	layer_4_expert1_21 -> layer_4_expert_act_21
	layer_4_expert_act_21 -> layer_4_expert2_21
	layer_4_expert2_21 -> layer_4_expert_allreduce_5
	layer_4_alltoall -> layer_4_expert1_22
	layer_4_expert1_22 -> layer_4_expert_act_22
	layer_4_expert_act_22 -> layer_4_expert2_22
	layer_4_expert2_22 -> layer_4_expert_allreduce_5
	layer_4_alltoall -> layer_4_expert1_23
	layer_4_expert1_23 -> layer_4_expert_act_23
	layer_4_expert_act_23 -> layer_4_expert2_23
	layer_4_expert2_23 -> layer_4_expert_allreduce_5
	layer_4_expert_allreduce_5 -> layer_4_expert_agg
	layer_4_attn_allreduce_6 -> layer_4_gate_24 [style=dashed]
	layer_4_gate_24 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_24
	layer_4_expert1_24 -> layer_4_expert_act_24
	layer_4_expert_act_24 -> layer_4_expert2_24
	layer_4_expert2_24 -> layer_4_expert_allreduce_6
	layer_4_alltoall -> layer_4_expert1_25
	layer_4_expert1_25 -> layer_4_expert_act_25
	layer_4_expert_act_25 -> layer_4_expert2_25
	layer_4_expert2_25 -> layer_4_expert_allreduce_6
	layer_4_alltoall -> layer_4_expert1_26
	layer_4_expert1_26 -> layer_4_expert_act_26
	layer_4_expert_act_26 -> layer_4_expert2_26
	layer_4_expert2_26 -> layer_4_expert_allreduce_6
	layer_4_alltoall -> layer_4_expert1_27
	layer_4_expert1_27 -> layer_4_expert_act_27
	layer_4_expert_act_27 -> layer_4_expert2_27
	layer_4_expert2_27 -> layer_4_expert_allreduce_6
	layer_4_expert_allreduce_6 -> layer_4_expert_agg
	layer_4_attn_allreduce_7 -> layer_4_gate_28 [style=dashed]
	layer_4_gate_28 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_28
	layer_4_expert1_28 -> layer_4_expert_act_28
	layer_4_expert_act_28 -> layer_4_expert2_28
	layer_4_expert2_28 -> layer_4_expert_allreduce_7
	layer_4_alltoall -> layer_4_expert1_29
	layer_4_expert1_29 -> layer_4_expert_act_29
	layer_4_expert_act_29 -> layer_4_expert2_29
	layer_4_expert2_29 -> layer_4_expert_allreduce_7
	layer_4_alltoall -> layer_4_expert1_30
	layer_4_expert1_30 -> layer_4_expert_act_30
	layer_4_expert_act_30 -> layer_4_expert2_30
	layer_4_expert2_30 -> layer_4_expert_allreduce_7
	layer_4_alltoall -> layer_4_expert1_31
	layer_4_expert1_31 -> layer_4_expert_act_31
	layer_4_expert_act_31 -> layer_4_expert2_31
	layer_4_expert2_31 -> layer_4_expert_allreduce_7
	layer_4_expert_allreduce_7 -> layer_4_expert_agg
	layer_4_attn_allreduce_8 -> layer_4_gate_32 [style=dashed]
	layer_4_gate_32 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_32
	layer_4_expert1_32 -> layer_4_expert_act_32
	layer_4_expert_act_32 -> layer_4_expert2_32
	layer_4_expert2_32 -> layer_4_expert_allreduce_8
	layer_4_alltoall -> layer_4_expert1_33
	layer_4_expert1_33 -> layer_4_expert_act_33
	layer_4_expert_act_33 -> layer_4_expert2_33
	layer_4_expert2_33 -> layer_4_expert_allreduce_8
	layer_4_alltoall -> layer_4_expert1_34
	layer_4_expert1_34 -> layer_4_expert_act_34
	layer_4_expert_act_34 -> layer_4_expert2_34
	layer_4_expert2_34 -> layer_4_expert_allreduce_8
	layer_4_alltoall -> layer_4_expert1_35
	layer_4_expert1_35 -> layer_4_expert_act_35
	layer_4_expert_act_35 -> layer_4_expert2_35
	layer_4_expert2_35 -> layer_4_expert_allreduce_8
	layer_4_expert_allreduce_8 -> layer_4_expert_agg
	layer_4_attn_allreduce_9 -> layer_4_gate_36 [style=dashed]
	layer_4_gate_36 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_36
	layer_4_expert1_36 -> layer_4_expert_act_36
	layer_4_expert_act_36 -> layer_4_expert2_36
	layer_4_expert2_36 -> layer_4_expert_allreduce_9
	layer_4_alltoall -> layer_4_expert1_37
	layer_4_expert1_37 -> layer_4_expert_act_37
	layer_4_expert_act_37 -> layer_4_expert2_37
	layer_4_expert2_37 -> layer_4_expert_allreduce_9
	layer_4_alltoall -> layer_4_expert1_38
	layer_4_expert1_38 -> layer_4_expert_act_38
	layer_4_expert_act_38 -> layer_4_expert2_38
	layer_4_expert2_38 -> layer_4_expert_allreduce_9
	layer_4_alltoall -> layer_4_expert1_39
	layer_4_expert1_39 -> layer_4_expert_act_39
	layer_4_expert_act_39 -> layer_4_expert2_39
	layer_4_expert2_39 -> layer_4_expert_allreduce_9
	layer_4_expert_allreduce_9 -> layer_4_expert_agg
	layer_4_attn_allreduce_10 -> layer_4_gate_40 [style=dashed]
	layer_4_gate_40 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_40
	layer_4_expert1_40 -> layer_4_expert_act_40
	layer_4_expert_act_40 -> layer_4_expert2_40
	layer_4_expert2_40 -> layer_4_expert_allreduce_10
	layer_4_alltoall -> layer_4_expert1_41
	layer_4_expert1_41 -> layer_4_expert_act_41
	layer_4_expert_act_41 -> layer_4_expert2_41
	layer_4_expert2_41 -> layer_4_expert_allreduce_10
	layer_4_alltoall -> layer_4_expert1_42
	layer_4_expert1_42 -> layer_4_expert_act_42
	layer_4_expert_act_42 -> layer_4_expert2_42
	layer_4_expert2_42 -> layer_4_expert_allreduce_10
	layer_4_alltoall -> layer_4_expert1_43
	layer_4_expert1_43 -> layer_4_expert_act_43
	layer_4_expert_act_43 -> layer_4_expert2_43
	layer_4_expert2_43 -> layer_4_expert_allreduce_10
	layer_4_expert_allreduce_10 -> layer_4_expert_agg
	layer_4_attn_allreduce_11 -> layer_4_gate_44 [style=dashed]
	layer_4_gate_44 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_44
	layer_4_expert1_44 -> layer_4_expert_act_44
	layer_4_expert_act_44 -> layer_4_expert2_44
	layer_4_expert2_44 -> layer_4_expert_allreduce_11
	layer_4_alltoall -> layer_4_expert1_45
	layer_4_expert1_45 -> layer_4_expert_act_45
	layer_4_expert_act_45 -> layer_4_expert2_45
	layer_4_expert2_45 -> layer_4_expert_allreduce_11
	layer_4_alltoall -> layer_4_expert1_46
	layer_4_expert1_46 -> layer_4_expert_act_46
	layer_4_expert_act_46 -> layer_4_expert2_46
	layer_4_expert2_46 -> layer_4_expert_allreduce_11
	layer_4_alltoall -> layer_4_expert1_47
	layer_4_expert1_47 -> layer_4_expert_act_47
	layer_4_expert_act_47 -> layer_4_expert2_47
	layer_4_expert2_47 -> layer_4_expert_allreduce_11
	layer_4_expert_allreduce_11 -> layer_4_expert_agg
	layer_4_attn_allreduce_12 -> layer_4_gate_48 [style=dashed]
	layer_4_gate_48 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_48
	layer_4_expert1_48 -> layer_4_expert_act_48
	layer_4_expert_act_48 -> layer_4_expert2_48
	layer_4_expert2_48 -> layer_4_expert_allreduce_12
	layer_4_alltoall -> layer_4_expert1_49
	layer_4_expert1_49 -> layer_4_expert_act_49
	layer_4_expert_act_49 -> layer_4_expert2_49
	layer_4_expert2_49 -> layer_4_expert_allreduce_12
	layer_4_alltoall -> layer_4_expert1_50
	layer_4_expert1_50 -> layer_4_expert_act_50
	layer_4_expert_act_50 -> layer_4_expert2_50
	layer_4_expert2_50 -> layer_4_expert_allreduce_12
	layer_4_alltoall -> layer_4_expert1_51
	layer_4_expert1_51 -> layer_4_expert_act_51
	layer_4_expert_act_51 -> layer_4_expert2_51
	layer_4_expert2_51 -> layer_4_expert_allreduce_12
	layer_4_expert_allreduce_12 -> layer_4_expert_agg
	layer_4_attn_allreduce_13 -> layer_4_gate_52 [style=dashed]
	layer_4_gate_52 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_52
	layer_4_expert1_52 -> layer_4_expert_act_52
	layer_4_expert_act_52 -> layer_4_expert2_52
	layer_4_expert2_52 -> layer_4_expert_allreduce_13
	layer_4_alltoall -> layer_4_expert1_53
	layer_4_expert1_53 -> layer_4_expert_act_53
	layer_4_expert_act_53 -> layer_4_expert2_53
	layer_4_expert2_53 -> layer_4_expert_allreduce_13
	layer_4_alltoall -> layer_4_expert1_54
	layer_4_expert1_54 -> layer_4_expert_act_54
	layer_4_expert_act_54 -> layer_4_expert2_54
	layer_4_expert2_54 -> layer_4_expert_allreduce_13
	layer_4_alltoall -> layer_4_expert1_55
	layer_4_expert1_55 -> layer_4_expert_act_55
	layer_4_expert_act_55 -> layer_4_expert2_55
	layer_4_expert2_55 -> layer_4_expert_allreduce_13
	layer_4_expert_allreduce_13 -> layer_4_expert_agg
	layer_4_attn_allreduce_14 -> layer_4_gate_56 [style=dashed]
	layer_4_gate_56 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_56
	layer_4_expert1_56 -> layer_4_expert_act_56
	layer_4_expert_act_56 -> layer_4_expert2_56
	layer_4_expert2_56 -> layer_4_expert_allreduce_14
	layer_4_alltoall -> layer_4_expert1_57
	layer_4_expert1_57 -> layer_4_expert_act_57
	layer_4_expert_act_57 -> layer_4_expert2_57
	layer_4_expert2_57 -> layer_4_expert_allreduce_14
	layer_4_alltoall -> layer_4_expert1_58
	layer_4_expert1_58 -> layer_4_expert_act_58
	layer_4_expert_act_58 -> layer_4_expert2_58
	layer_4_expert2_58 -> layer_4_expert_allreduce_14
	layer_4_alltoall -> layer_4_expert1_59
	layer_4_expert1_59 -> layer_4_expert_act_59
	layer_4_expert_act_59 -> layer_4_expert2_59
	layer_4_expert2_59 -> layer_4_expert_allreduce_14
	layer_4_expert_allreduce_14 -> layer_4_expert_agg
	layer_4_attn_allreduce_15 -> layer_4_gate_60 [style=dashed]
	layer_4_gate_60 -> layer_4_alltoall [style=dashed]
	layer_4_alltoall -> layer_4_expert1_60
	layer_4_expert1_60 -> layer_4_expert_act_60
	layer_4_expert_act_60 -> layer_4_expert2_60
	layer_4_expert2_60 -> layer_4_expert_allreduce_15
	layer_4_alltoall -> layer_4_expert1_61
	layer_4_expert1_61 -> layer_4_expert_act_61
	layer_4_expert_act_61 -> layer_4_expert2_61
	layer_4_expert2_61 -> layer_4_expert_allreduce_15
	layer_4_alltoall -> layer_4_expert1_62
	layer_4_expert1_62 -> layer_4_expert_act_62
	layer_4_expert_act_62 -> layer_4_expert2_62
	layer_4_expert2_62 -> layer_4_expert_allreduce_15
	layer_4_alltoall -> layer_4_expert1_63
	layer_4_expert1_63 -> layer_4_expert_act_63
	layer_4_expert_act_63 -> layer_4_expert2_63
	layer_4_expert2_63 -> layer_4_expert_allreduce_15
	layer_4_expert_allreduce_15 -> layer_4_expert_agg
	layer_4_expert_agg -> layer_4_norm
	layer_4_norm -> layer_5_input
	layer_5_input [label="Layer5_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_5_qkv_0 [label="Layer5_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_1 [label="Layer5_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_2 [label="Layer5_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_3 [label="Layer5_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_0 [label="Layer5_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_1 [label="Layer5_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_2 [label="Layer5_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_3 [label="Layer5_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_0 [label="Layer5_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_1 [label="Layer5_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_2 [label="Layer5_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_3 [label="Layer5_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_0 [label="Layer5_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_1 [label="Layer5_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_2 [label="Layer5_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_3 [label="Layer5_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_0 [label="Layer5_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_0
	layer_5_qkv_0 -> layer_5_attn_scores_0
	layer_5_attn_scores_0 -> layer_5_attn_softmax_0
	layer_5_attn_softmax_0 -> layer_5_attn_out_0
	layer_5_attn_out_0 -> layer_5_attn_allreduce_0
	layer_5_input -> layer_5_qkv_1
	layer_5_qkv_1 -> layer_5_attn_scores_1
	layer_5_attn_scores_1 -> layer_5_attn_softmax_1
	layer_5_attn_softmax_1 -> layer_5_attn_out_1
	layer_5_attn_out_1 -> layer_5_attn_allreduce_0
	layer_5_input -> layer_5_qkv_2
	layer_5_qkv_2 -> layer_5_attn_scores_2
	layer_5_attn_scores_2 -> layer_5_attn_softmax_2
	layer_5_attn_softmax_2 -> layer_5_attn_out_2
	layer_5_attn_out_2 -> layer_5_attn_allreduce_0
	layer_5_input -> layer_5_qkv_3
	layer_5_qkv_3 -> layer_5_attn_scores_3
	layer_5_attn_scores_3 -> layer_5_attn_softmax_3
	layer_5_attn_softmax_3 -> layer_5_attn_out_3
	layer_5_attn_out_3 -> layer_5_attn_allreduce_0
	layer_5_qkv_4 [label="Layer5_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_5 [label="Layer5_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_6 [label="Layer5_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_7 [label="Layer5_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_4 [label="Layer5_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_5 [label="Layer5_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_6 [label="Layer5_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_7 [label="Layer5_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_4 [label="Layer5_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_5 [label="Layer5_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_6 [label="Layer5_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_7 [label="Layer5_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_4 [label="Layer5_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_5 [label="Layer5_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_6 [label="Layer5_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_7 [label="Layer5_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_1 [label="Layer5_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_4
	layer_5_qkv_4 -> layer_5_attn_scores_4
	layer_5_attn_scores_4 -> layer_5_attn_softmax_4
	layer_5_attn_softmax_4 -> layer_5_attn_out_4
	layer_5_attn_out_4 -> layer_5_attn_allreduce_1
	layer_5_input -> layer_5_qkv_5
	layer_5_qkv_5 -> layer_5_attn_scores_5
	layer_5_attn_scores_5 -> layer_5_attn_softmax_5
	layer_5_attn_softmax_5 -> layer_5_attn_out_5
	layer_5_attn_out_5 -> layer_5_attn_allreduce_1
	layer_5_input -> layer_5_qkv_6
	layer_5_qkv_6 -> layer_5_attn_scores_6
	layer_5_attn_scores_6 -> layer_5_attn_softmax_6
	layer_5_attn_softmax_6 -> layer_5_attn_out_6
	layer_5_attn_out_6 -> layer_5_attn_allreduce_1
	layer_5_input -> layer_5_qkv_7
	layer_5_qkv_7 -> layer_5_attn_scores_7
	layer_5_attn_scores_7 -> layer_5_attn_softmax_7
	layer_5_attn_softmax_7 -> layer_5_attn_out_7
	layer_5_attn_out_7 -> layer_5_attn_allreduce_1
	layer_5_qkv_8 [label="Layer5_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_9 [label="Layer5_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_10 [label="Layer5_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_11 [label="Layer5_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_8 [label="Layer5_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_9 [label="Layer5_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_10 [label="Layer5_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_11 [label="Layer5_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_8 [label="Layer5_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_9 [label="Layer5_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_10 [label="Layer5_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_11 [label="Layer5_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_8 [label="Layer5_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_9 [label="Layer5_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_10 [label="Layer5_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_11 [label="Layer5_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_2 [label="Layer5_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_8
	layer_5_qkv_8 -> layer_5_attn_scores_8
	layer_5_attn_scores_8 -> layer_5_attn_softmax_8
	layer_5_attn_softmax_8 -> layer_5_attn_out_8
	layer_5_attn_out_8 -> layer_5_attn_allreduce_2
	layer_5_input -> layer_5_qkv_9
	layer_5_qkv_9 -> layer_5_attn_scores_9
	layer_5_attn_scores_9 -> layer_5_attn_softmax_9
	layer_5_attn_softmax_9 -> layer_5_attn_out_9
	layer_5_attn_out_9 -> layer_5_attn_allreduce_2
	layer_5_input -> layer_5_qkv_10
	layer_5_qkv_10 -> layer_5_attn_scores_10
	layer_5_attn_scores_10 -> layer_5_attn_softmax_10
	layer_5_attn_softmax_10 -> layer_5_attn_out_10
	layer_5_attn_out_10 -> layer_5_attn_allreduce_2
	layer_5_input -> layer_5_qkv_11
	layer_5_qkv_11 -> layer_5_attn_scores_11
	layer_5_attn_scores_11 -> layer_5_attn_softmax_11
	layer_5_attn_softmax_11 -> layer_5_attn_out_11
	layer_5_attn_out_11 -> layer_5_attn_allreduce_2
	layer_5_qkv_12 [label="Layer5_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_13 [label="Layer5_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_14 [label="Layer5_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_15 [label="Layer5_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_12 [label="Layer5_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_13 [label="Layer5_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_14 [label="Layer5_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_15 [label="Layer5_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_12 [label="Layer5_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_13 [label="Layer5_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_14 [label="Layer5_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_15 [label="Layer5_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_12 [label="Layer5_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_13 [label="Layer5_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_14 [label="Layer5_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_15 [label="Layer5_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_3 [label="Layer5_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_12
	layer_5_qkv_12 -> layer_5_attn_scores_12
	layer_5_attn_scores_12 -> layer_5_attn_softmax_12
	layer_5_attn_softmax_12 -> layer_5_attn_out_12
	layer_5_attn_out_12 -> layer_5_attn_allreduce_3
	layer_5_input -> layer_5_qkv_13
	layer_5_qkv_13 -> layer_5_attn_scores_13
	layer_5_attn_scores_13 -> layer_5_attn_softmax_13
	layer_5_attn_softmax_13 -> layer_5_attn_out_13
	layer_5_attn_out_13 -> layer_5_attn_allreduce_3
	layer_5_input -> layer_5_qkv_14
	layer_5_qkv_14 -> layer_5_attn_scores_14
	layer_5_attn_scores_14 -> layer_5_attn_softmax_14
	layer_5_attn_softmax_14 -> layer_5_attn_out_14
	layer_5_attn_out_14 -> layer_5_attn_allreduce_3
	layer_5_input -> layer_5_qkv_15
	layer_5_qkv_15 -> layer_5_attn_scores_15
	layer_5_attn_scores_15 -> layer_5_attn_softmax_15
	layer_5_attn_softmax_15 -> layer_5_attn_out_15
	layer_5_attn_out_15 -> layer_5_attn_allreduce_3
	layer_5_qkv_16 [label="Layer5_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_17 [label="Layer5_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_18 [label="Layer5_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_19 [label="Layer5_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_16 [label="Layer5_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_17 [label="Layer5_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_18 [label="Layer5_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_19 [label="Layer5_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_16 [label="Layer5_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_17 [label="Layer5_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_18 [label="Layer5_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_19 [label="Layer5_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_16 [label="Layer5_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_17 [label="Layer5_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_18 [label="Layer5_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_19 [label="Layer5_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_4 [label="Layer5_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_16
	layer_5_qkv_16 -> layer_5_attn_scores_16
	layer_5_attn_scores_16 -> layer_5_attn_softmax_16
	layer_5_attn_softmax_16 -> layer_5_attn_out_16
	layer_5_attn_out_16 -> layer_5_attn_allreduce_4
	layer_5_input -> layer_5_qkv_17
	layer_5_qkv_17 -> layer_5_attn_scores_17
	layer_5_attn_scores_17 -> layer_5_attn_softmax_17
	layer_5_attn_softmax_17 -> layer_5_attn_out_17
	layer_5_attn_out_17 -> layer_5_attn_allreduce_4
	layer_5_input -> layer_5_qkv_18
	layer_5_qkv_18 -> layer_5_attn_scores_18
	layer_5_attn_scores_18 -> layer_5_attn_softmax_18
	layer_5_attn_softmax_18 -> layer_5_attn_out_18
	layer_5_attn_out_18 -> layer_5_attn_allreduce_4
	layer_5_input -> layer_5_qkv_19
	layer_5_qkv_19 -> layer_5_attn_scores_19
	layer_5_attn_scores_19 -> layer_5_attn_softmax_19
	layer_5_attn_softmax_19 -> layer_5_attn_out_19
	layer_5_attn_out_19 -> layer_5_attn_allreduce_4
	layer_5_qkv_20 [label="Layer5_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_21 [label="Layer5_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_22 [label="Layer5_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_23 [label="Layer5_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_20 [label="Layer5_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_21 [label="Layer5_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_22 [label="Layer5_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_23 [label="Layer5_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_20 [label="Layer5_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_21 [label="Layer5_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_22 [label="Layer5_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_23 [label="Layer5_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_20 [label="Layer5_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_21 [label="Layer5_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_22 [label="Layer5_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_23 [label="Layer5_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_5 [label="Layer5_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_20
	layer_5_qkv_20 -> layer_5_attn_scores_20
	layer_5_attn_scores_20 -> layer_5_attn_softmax_20
	layer_5_attn_softmax_20 -> layer_5_attn_out_20
	layer_5_attn_out_20 -> layer_5_attn_allreduce_5
	layer_5_input -> layer_5_qkv_21
	layer_5_qkv_21 -> layer_5_attn_scores_21
	layer_5_attn_scores_21 -> layer_5_attn_softmax_21
	layer_5_attn_softmax_21 -> layer_5_attn_out_21
	layer_5_attn_out_21 -> layer_5_attn_allreduce_5
	layer_5_input -> layer_5_qkv_22
	layer_5_qkv_22 -> layer_5_attn_scores_22
	layer_5_attn_scores_22 -> layer_5_attn_softmax_22
	layer_5_attn_softmax_22 -> layer_5_attn_out_22
	layer_5_attn_out_22 -> layer_5_attn_allreduce_5
	layer_5_input -> layer_5_qkv_23
	layer_5_qkv_23 -> layer_5_attn_scores_23
	layer_5_attn_scores_23 -> layer_5_attn_softmax_23
	layer_5_attn_softmax_23 -> layer_5_attn_out_23
	layer_5_attn_out_23 -> layer_5_attn_allreduce_5
	layer_5_qkv_24 [label="Layer5_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_25 [label="Layer5_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_26 [label="Layer5_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_27 [label="Layer5_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_24 [label="Layer5_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_25 [label="Layer5_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_26 [label="Layer5_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_27 [label="Layer5_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_24 [label="Layer5_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_25 [label="Layer5_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_26 [label="Layer5_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_27 [label="Layer5_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_24 [label="Layer5_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_25 [label="Layer5_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_26 [label="Layer5_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_27 [label="Layer5_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_6 [label="Layer5_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_24
	layer_5_qkv_24 -> layer_5_attn_scores_24
	layer_5_attn_scores_24 -> layer_5_attn_softmax_24
	layer_5_attn_softmax_24 -> layer_5_attn_out_24
	layer_5_attn_out_24 -> layer_5_attn_allreduce_6
	layer_5_input -> layer_5_qkv_25
	layer_5_qkv_25 -> layer_5_attn_scores_25
	layer_5_attn_scores_25 -> layer_5_attn_softmax_25
	layer_5_attn_softmax_25 -> layer_5_attn_out_25
	layer_5_attn_out_25 -> layer_5_attn_allreduce_6
	layer_5_input -> layer_5_qkv_26
	layer_5_qkv_26 -> layer_5_attn_scores_26
	layer_5_attn_scores_26 -> layer_5_attn_softmax_26
	layer_5_attn_softmax_26 -> layer_5_attn_out_26
	layer_5_attn_out_26 -> layer_5_attn_allreduce_6
	layer_5_input -> layer_5_qkv_27
	layer_5_qkv_27 -> layer_5_attn_scores_27
	layer_5_attn_scores_27 -> layer_5_attn_softmax_27
	layer_5_attn_softmax_27 -> layer_5_attn_out_27
	layer_5_attn_out_27 -> layer_5_attn_allreduce_6
	layer_5_qkv_28 [label="Layer5_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_29 [label="Layer5_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_30 [label="Layer5_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_31 [label="Layer5_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_28 [label="Layer5_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_29 [label="Layer5_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_30 [label="Layer5_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_31 [label="Layer5_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_28 [label="Layer5_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_29 [label="Layer5_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_30 [label="Layer5_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_31 [label="Layer5_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_28 [label="Layer5_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_29 [label="Layer5_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_30 [label="Layer5_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_31 [label="Layer5_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_7 [label="Layer5_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_28
	layer_5_qkv_28 -> layer_5_attn_scores_28
	layer_5_attn_scores_28 -> layer_5_attn_softmax_28
	layer_5_attn_softmax_28 -> layer_5_attn_out_28
	layer_5_attn_out_28 -> layer_5_attn_allreduce_7
	layer_5_input -> layer_5_qkv_29
	layer_5_qkv_29 -> layer_5_attn_scores_29
	layer_5_attn_scores_29 -> layer_5_attn_softmax_29
	layer_5_attn_softmax_29 -> layer_5_attn_out_29
	layer_5_attn_out_29 -> layer_5_attn_allreduce_7
	layer_5_input -> layer_5_qkv_30
	layer_5_qkv_30 -> layer_5_attn_scores_30
	layer_5_attn_scores_30 -> layer_5_attn_softmax_30
	layer_5_attn_softmax_30 -> layer_5_attn_out_30
	layer_5_attn_out_30 -> layer_5_attn_allreduce_7
	layer_5_input -> layer_5_qkv_31
	layer_5_qkv_31 -> layer_5_attn_scores_31
	layer_5_attn_scores_31 -> layer_5_attn_softmax_31
	layer_5_attn_softmax_31 -> layer_5_attn_out_31
	layer_5_attn_out_31 -> layer_5_attn_allreduce_7
	layer_5_qkv_32 [label="Layer5_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_33 [label="Layer5_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_34 [label="Layer5_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_35 [label="Layer5_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_32 [label="Layer5_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_33 [label="Layer5_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_34 [label="Layer5_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_35 [label="Layer5_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_32 [label="Layer5_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_33 [label="Layer5_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_34 [label="Layer5_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_35 [label="Layer5_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_32 [label="Layer5_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_33 [label="Layer5_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_34 [label="Layer5_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_35 [label="Layer5_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_8 [label="Layer5_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_32
	layer_5_qkv_32 -> layer_5_attn_scores_32
	layer_5_attn_scores_32 -> layer_5_attn_softmax_32
	layer_5_attn_softmax_32 -> layer_5_attn_out_32
	layer_5_attn_out_32 -> layer_5_attn_allreduce_8
	layer_5_input -> layer_5_qkv_33
	layer_5_qkv_33 -> layer_5_attn_scores_33
	layer_5_attn_scores_33 -> layer_5_attn_softmax_33
	layer_5_attn_softmax_33 -> layer_5_attn_out_33
	layer_5_attn_out_33 -> layer_5_attn_allreduce_8
	layer_5_input -> layer_5_qkv_34
	layer_5_qkv_34 -> layer_5_attn_scores_34
	layer_5_attn_scores_34 -> layer_5_attn_softmax_34
	layer_5_attn_softmax_34 -> layer_5_attn_out_34
	layer_5_attn_out_34 -> layer_5_attn_allreduce_8
	layer_5_input -> layer_5_qkv_35
	layer_5_qkv_35 -> layer_5_attn_scores_35
	layer_5_attn_scores_35 -> layer_5_attn_softmax_35
	layer_5_attn_softmax_35 -> layer_5_attn_out_35
	layer_5_attn_out_35 -> layer_5_attn_allreduce_8
	layer_5_qkv_36 [label="Layer5_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_37 [label="Layer5_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_38 [label="Layer5_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_39 [label="Layer5_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_36 [label="Layer5_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_37 [label="Layer5_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_38 [label="Layer5_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_39 [label="Layer5_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_36 [label="Layer5_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_37 [label="Layer5_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_38 [label="Layer5_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_39 [label="Layer5_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_36 [label="Layer5_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_37 [label="Layer5_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_38 [label="Layer5_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_39 [label="Layer5_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_9 [label="Layer5_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_36
	layer_5_qkv_36 -> layer_5_attn_scores_36
	layer_5_attn_scores_36 -> layer_5_attn_softmax_36
	layer_5_attn_softmax_36 -> layer_5_attn_out_36
	layer_5_attn_out_36 -> layer_5_attn_allreduce_9
	layer_5_input -> layer_5_qkv_37
	layer_5_qkv_37 -> layer_5_attn_scores_37
	layer_5_attn_scores_37 -> layer_5_attn_softmax_37
	layer_5_attn_softmax_37 -> layer_5_attn_out_37
	layer_5_attn_out_37 -> layer_5_attn_allreduce_9
	layer_5_input -> layer_5_qkv_38
	layer_5_qkv_38 -> layer_5_attn_scores_38
	layer_5_attn_scores_38 -> layer_5_attn_softmax_38
	layer_5_attn_softmax_38 -> layer_5_attn_out_38
	layer_5_attn_out_38 -> layer_5_attn_allreduce_9
	layer_5_input -> layer_5_qkv_39
	layer_5_qkv_39 -> layer_5_attn_scores_39
	layer_5_attn_scores_39 -> layer_5_attn_softmax_39
	layer_5_attn_softmax_39 -> layer_5_attn_out_39
	layer_5_attn_out_39 -> layer_5_attn_allreduce_9
	layer_5_qkv_40 [label="Layer5_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_41 [label="Layer5_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_42 [label="Layer5_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_43 [label="Layer5_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_40 [label="Layer5_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_41 [label="Layer5_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_42 [label="Layer5_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_43 [label="Layer5_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_40 [label="Layer5_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_41 [label="Layer5_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_42 [label="Layer5_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_43 [label="Layer5_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_40 [label="Layer5_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_41 [label="Layer5_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_42 [label="Layer5_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_43 [label="Layer5_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_10 [label="Layer5_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_40
	layer_5_qkv_40 -> layer_5_attn_scores_40
	layer_5_attn_scores_40 -> layer_5_attn_softmax_40
	layer_5_attn_softmax_40 -> layer_5_attn_out_40
	layer_5_attn_out_40 -> layer_5_attn_allreduce_10
	layer_5_input -> layer_5_qkv_41
	layer_5_qkv_41 -> layer_5_attn_scores_41
	layer_5_attn_scores_41 -> layer_5_attn_softmax_41
	layer_5_attn_softmax_41 -> layer_5_attn_out_41
	layer_5_attn_out_41 -> layer_5_attn_allreduce_10
	layer_5_input -> layer_5_qkv_42
	layer_5_qkv_42 -> layer_5_attn_scores_42
	layer_5_attn_scores_42 -> layer_5_attn_softmax_42
	layer_5_attn_softmax_42 -> layer_5_attn_out_42
	layer_5_attn_out_42 -> layer_5_attn_allreduce_10
	layer_5_input -> layer_5_qkv_43
	layer_5_qkv_43 -> layer_5_attn_scores_43
	layer_5_attn_scores_43 -> layer_5_attn_softmax_43
	layer_5_attn_softmax_43 -> layer_5_attn_out_43
	layer_5_attn_out_43 -> layer_5_attn_allreduce_10
	layer_5_qkv_44 [label="Layer5_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_45 [label="Layer5_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_46 [label="Layer5_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_47 [label="Layer5_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_44 [label="Layer5_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_45 [label="Layer5_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_46 [label="Layer5_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_47 [label="Layer5_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_44 [label="Layer5_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_45 [label="Layer5_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_46 [label="Layer5_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_47 [label="Layer5_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_44 [label="Layer5_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_45 [label="Layer5_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_46 [label="Layer5_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_47 [label="Layer5_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_11 [label="Layer5_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_44
	layer_5_qkv_44 -> layer_5_attn_scores_44
	layer_5_attn_scores_44 -> layer_5_attn_softmax_44
	layer_5_attn_softmax_44 -> layer_5_attn_out_44
	layer_5_attn_out_44 -> layer_5_attn_allreduce_11
	layer_5_input -> layer_5_qkv_45
	layer_5_qkv_45 -> layer_5_attn_scores_45
	layer_5_attn_scores_45 -> layer_5_attn_softmax_45
	layer_5_attn_softmax_45 -> layer_5_attn_out_45
	layer_5_attn_out_45 -> layer_5_attn_allreduce_11
	layer_5_input -> layer_5_qkv_46
	layer_5_qkv_46 -> layer_5_attn_scores_46
	layer_5_attn_scores_46 -> layer_5_attn_softmax_46
	layer_5_attn_softmax_46 -> layer_5_attn_out_46
	layer_5_attn_out_46 -> layer_5_attn_allreduce_11
	layer_5_input -> layer_5_qkv_47
	layer_5_qkv_47 -> layer_5_attn_scores_47
	layer_5_attn_scores_47 -> layer_5_attn_softmax_47
	layer_5_attn_softmax_47 -> layer_5_attn_out_47
	layer_5_attn_out_47 -> layer_5_attn_allreduce_11
	layer_5_qkv_48 [label="Layer5_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_49 [label="Layer5_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_50 [label="Layer5_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_51 [label="Layer5_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_48 [label="Layer5_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_49 [label="Layer5_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_50 [label="Layer5_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_51 [label="Layer5_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_48 [label="Layer5_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_49 [label="Layer5_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_50 [label="Layer5_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_51 [label="Layer5_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_48 [label="Layer5_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_49 [label="Layer5_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_50 [label="Layer5_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_51 [label="Layer5_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_12 [label="Layer5_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_48
	layer_5_qkv_48 -> layer_5_attn_scores_48
	layer_5_attn_scores_48 -> layer_5_attn_softmax_48
	layer_5_attn_softmax_48 -> layer_5_attn_out_48
	layer_5_attn_out_48 -> layer_5_attn_allreduce_12
	layer_5_input -> layer_5_qkv_49
	layer_5_qkv_49 -> layer_5_attn_scores_49
	layer_5_attn_scores_49 -> layer_5_attn_softmax_49
	layer_5_attn_softmax_49 -> layer_5_attn_out_49
	layer_5_attn_out_49 -> layer_5_attn_allreduce_12
	layer_5_input -> layer_5_qkv_50
	layer_5_qkv_50 -> layer_5_attn_scores_50
	layer_5_attn_scores_50 -> layer_5_attn_softmax_50
	layer_5_attn_softmax_50 -> layer_5_attn_out_50
	layer_5_attn_out_50 -> layer_5_attn_allreduce_12
	layer_5_input -> layer_5_qkv_51
	layer_5_qkv_51 -> layer_5_attn_scores_51
	layer_5_attn_scores_51 -> layer_5_attn_softmax_51
	layer_5_attn_softmax_51 -> layer_5_attn_out_51
	layer_5_attn_out_51 -> layer_5_attn_allreduce_12
	layer_5_qkv_52 [label="Layer5_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_53 [label="Layer5_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_54 [label="Layer5_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_55 [label="Layer5_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_52 [label="Layer5_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_53 [label="Layer5_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_54 [label="Layer5_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_55 [label="Layer5_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_52 [label="Layer5_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_53 [label="Layer5_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_54 [label="Layer5_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_55 [label="Layer5_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_52 [label="Layer5_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_53 [label="Layer5_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_54 [label="Layer5_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_55 [label="Layer5_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_13 [label="Layer5_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_52
	layer_5_qkv_52 -> layer_5_attn_scores_52
	layer_5_attn_scores_52 -> layer_5_attn_softmax_52
	layer_5_attn_softmax_52 -> layer_5_attn_out_52
	layer_5_attn_out_52 -> layer_5_attn_allreduce_13
	layer_5_input -> layer_5_qkv_53
	layer_5_qkv_53 -> layer_5_attn_scores_53
	layer_5_attn_scores_53 -> layer_5_attn_softmax_53
	layer_5_attn_softmax_53 -> layer_5_attn_out_53
	layer_5_attn_out_53 -> layer_5_attn_allreduce_13
	layer_5_input -> layer_5_qkv_54
	layer_5_qkv_54 -> layer_5_attn_scores_54
	layer_5_attn_scores_54 -> layer_5_attn_softmax_54
	layer_5_attn_softmax_54 -> layer_5_attn_out_54
	layer_5_attn_out_54 -> layer_5_attn_allreduce_13
	layer_5_input -> layer_5_qkv_55
	layer_5_qkv_55 -> layer_5_attn_scores_55
	layer_5_attn_scores_55 -> layer_5_attn_softmax_55
	layer_5_attn_softmax_55 -> layer_5_attn_out_55
	layer_5_attn_out_55 -> layer_5_attn_allreduce_13
	layer_5_qkv_56 [label="Layer5_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_57 [label="Layer5_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_58 [label="Layer5_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_59 [label="Layer5_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_56 [label="Layer5_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_57 [label="Layer5_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_58 [label="Layer5_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_59 [label="Layer5_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_56 [label="Layer5_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_57 [label="Layer5_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_58 [label="Layer5_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_59 [label="Layer5_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_56 [label="Layer5_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_57 [label="Layer5_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_58 [label="Layer5_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_59 [label="Layer5_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_14 [label="Layer5_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_56
	layer_5_qkv_56 -> layer_5_attn_scores_56
	layer_5_attn_scores_56 -> layer_5_attn_softmax_56
	layer_5_attn_softmax_56 -> layer_5_attn_out_56
	layer_5_attn_out_56 -> layer_5_attn_allreduce_14
	layer_5_input -> layer_5_qkv_57
	layer_5_qkv_57 -> layer_5_attn_scores_57
	layer_5_attn_scores_57 -> layer_5_attn_softmax_57
	layer_5_attn_softmax_57 -> layer_5_attn_out_57
	layer_5_attn_out_57 -> layer_5_attn_allreduce_14
	layer_5_input -> layer_5_qkv_58
	layer_5_qkv_58 -> layer_5_attn_scores_58
	layer_5_attn_scores_58 -> layer_5_attn_softmax_58
	layer_5_attn_softmax_58 -> layer_5_attn_out_58
	layer_5_attn_out_58 -> layer_5_attn_allreduce_14
	layer_5_input -> layer_5_qkv_59
	layer_5_qkv_59 -> layer_5_attn_scores_59
	layer_5_attn_scores_59 -> layer_5_attn_softmax_59
	layer_5_attn_softmax_59 -> layer_5_attn_out_59
	layer_5_attn_out_59 -> layer_5_attn_allreduce_14
	layer_5_qkv_60 [label="Layer5_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_61 [label="Layer5_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_62 [label="Layer5_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_qkv_63 [label="Layer5_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_60 [label="Layer5_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_61 [label="Layer5_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_62 [label="Layer5_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_scores_63 [label="Layer5_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_60 [label="Layer5_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_61 [label="Layer5_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_62 [label="Layer5_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_softmax_63 [label="Layer5_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_60 [label="Layer5_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_61 [label="Layer5_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_62 [label="Layer5_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_out_63 [label="Layer5_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_15 [label="Layer5_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_input -> layer_5_qkv_60
	layer_5_qkv_60 -> layer_5_attn_scores_60
	layer_5_attn_scores_60 -> layer_5_attn_softmax_60
	layer_5_attn_softmax_60 -> layer_5_attn_out_60
	layer_5_attn_out_60 -> layer_5_attn_allreduce_15
	layer_5_input -> layer_5_qkv_61
	layer_5_qkv_61 -> layer_5_attn_scores_61
	layer_5_attn_scores_61 -> layer_5_attn_softmax_61
	layer_5_attn_softmax_61 -> layer_5_attn_out_61
	layer_5_attn_out_61 -> layer_5_attn_allreduce_15
	layer_5_input -> layer_5_qkv_62
	layer_5_qkv_62 -> layer_5_attn_scores_62
	layer_5_attn_scores_62 -> layer_5_attn_softmax_62
	layer_5_attn_softmax_62 -> layer_5_attn_out_62
	layer_5_attn_out_62 -> layer_5_attn_allreduce_15
	layer_5_input -> layer_5_qkv_63
	layer_5_qkv_63 -> layer_5_attn_scores_63
	layer_5_attn_scores_63 -> layer_5_attn_softmax_63
	layer_5_attn_softmax_63 -> layer_5_attn_out_63
	layer_5_attn_out_63 -> layer_5_attn_allreduce_15
	layer_5_gate_0 [label="Layer5_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_4 [label="Layer5_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_8 [label="Layer5_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_12 [label="Layer5_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_16 [label="Layer5_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_20 [label="Layer5_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_24 [label="Layer5_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_28 [label="Layer5_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_32 [label="Layer5_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_36 [label="Layer5_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_40 [label="Layer5_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_44 [label="Layer5_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_48 [label="Layer5_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_52 [label="Layer5_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_56 [label="Layer5_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_gate_60 [label="Layer5_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_5_alltoall [label="Layer5_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_0 [label="Layer5_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_1 [label="Layer5_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_2 [label="Layer5_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_3 [label="Layer5_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_0 [label="Layer5_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_1 [label="Layer5_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_2 [label="Layer5_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_3 [label="Layer5_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_0 [label="Layer5_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_1 [label="Layer5_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_2 [label="Layer5_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_3 [label="Layer5_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_0 [label="Layer5_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_4 [label="Layer5_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_5 [label="Layer5_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_6 [label="Layer5_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_7 [label="Layer5_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_4 [label="Layer5_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_5 [label="Layer5_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_6 [label="Layer5_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_7 [label="Layer5_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_4 [label="Layer5_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_5 [label="Layer5_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_6 [label="Layer5_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_7 [label="Layer5_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_1 [label="Layer5_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_8 [label="Layer5_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_9 [label="Layer5_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_10 [label="Layer5_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_11 [label="Layer5_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_8 [label="Layer5_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_9 [label="Layer5_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_10 [label="Layer5_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_11 [label="Layer5_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_8 [label="Layer5_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_9 [label="Layer5_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_10 [label="Layer5_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_11 [label="Layer5_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_2 [label="Layer5_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_12 [label="Layer5_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_13 [label="Layer5_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_14 [label="Layer5_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_15 [label="Layer5_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_12 [label="Layer5_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_13 [label="Layer5_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_14 [label="Layer5_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_15 [label="Layer5_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_12 [label="Layer5_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_13 [label="Layer5_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_14 [label="Layer5_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_15 [label="Layer5_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_3 [label="Layer5_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_16 [label="Layer5_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_17 [label="Layer5_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_18 [label="Layer5_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_19 [label="Layer5_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_16 [label="Layer5_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_17 [label="Layer5_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_18 [label="Layer5_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_19 [label="Layer5_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_16 [label="Layer5_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_17 [label="Layer5_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_18 [label="Layer5_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_19 [label="Layer5_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_4 [label="Layer5_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_20 [label="Layer5_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_21 [label="Layer5_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_22 [label="Layer5_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_23 [label="Layer5_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_20 [label="Layer5_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_21 [label="Layer5_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_22 [label="Layer5_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_23 [label="Layer5_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_20 [label="Layer5_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_21 [label="Layer5_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_22 [label="Layer5_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_23 [label="Layer5_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_5 [label="Layer5_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_24 [label="Layer5_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_25 [label="Layer5_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_26 [label="Layer5_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_27 [label="Layer5_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_24 [label="Layer5_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_25 [label="Layer5_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_26 [label="Layer5_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_27 [label="Layer5_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_24 [label="Layer5_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_25 [label="Layer5_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_26 [label="Layer5_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_27 [label="Layer5_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_6 [label="Layer5_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_28 [label="Layer5_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_29 [label="Layer5_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_30 [label="Layer5_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_31 [label="Layer5_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_28 [label="Layer5_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_29 [label="Layer5_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_30 [label="Layer5_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_31 [label="Layer5_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_28 [label="Layer5_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_29 [label="Layer5_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_30 [label="Layer5_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_31 [label="Layer5_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_7 [label="Layer5_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_32 [label="Layer5_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_33 [label="Layer5_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_34 [label="Layer5_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_35 [label="Layer5_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_32 [label="Layer5_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_33 [label="Layer5_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_34 [label="Layer5_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_35 [label="Layer5_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_32 [label="Layer5_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_33 [label="Layer5_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_34 [label="Layer5_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_35 [label="Layer5_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_8 [label="Layer5_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_36 [label="Layer5_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_37 [label="Layer5_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_38 [label="Layer5_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_39 [label="Layer5_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_36 [label="Layer5_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_37 [label="Layer5_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_38 [label="Layer5_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_39 [label="Layer5_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_36 [label="Layer5_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_37 [label="Layer5_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_38 [label="Layer5_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_39 [label="Layer5_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_9 [label="Layer5_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_40 [label="Layer5_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_41 [label="Layer5_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_42 [label="Layer5_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_43 [label="Layer5_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_40 [label="Layer5_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_41 [label="Layer5_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_42 [label="Layer5_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_43 [label="Layer5_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_40 [label="Layer5_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_41 [label="Layer5_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_42 [label="Layer5_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_43 [label="Layer5_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_10 [label="Layer5_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_44 [label="Layer5_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_45 [label="Layer5_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_46 [label="Layer5_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_47 [label="Layer5_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_44 [label="Layer5_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_45 [label="Layer5_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_46 [label="Layer5_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_47 [label="Layer5_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_44 [label="Layer5_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_45 [label="Layer5_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_46 [label="Layer5_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_47 [label="Layer5_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_11 [label="Layer5_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_48 [label="Layer5_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_49 [label="Layer5_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_50 [label="Layer5_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_51 [label="Layer5_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_48 [label="Layer5_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_49 [label="Layer5_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_50 [label="Layer5_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_51 [label="Layer5_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_48 [label="Layer5_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_49 [label="Layer5_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_50 [label="Layer5_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_51 [label="Layer5_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_12 [label="Layer5_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_52 [label="Layer5_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_53 [label="Layer5_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_54 [label="Layer5_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_55 [label="Layer5_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_52 [label="Layer5_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_53 [label="Layer5_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_54 [label="Layer5_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_55 [label="Layer5_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_52 [label="Layer5_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_53 [label="Layer5_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_54 [label="Layer5_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_55 [label="Layer5_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_13 [label="Layer5_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_56 [label="Layer5_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_57 [label="Layer5_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_58 [label="Layer5_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_59 [label="Layer5_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_56 [label="Layer5_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_57 [label="Layer5_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_58 [label="Layer5_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_59 [label="Layer5_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_56 [label="Layer5_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_57 [label="Layer5_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_58 [label="Layer5_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_59 [label="Layer5_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_14 [label="Layer5_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert1_60 [label="Layer5_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_61 [label="Layer5_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_62 [label="Layer5_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert1_63 [label="Layer5_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_60 [label="Layer5_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_61 [label="Layer5_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_62 [label="Layer5_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert_act_63 [label="Layer5_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_5_expert2_60 [label="Layer5_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_61 [label="Layer5_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_62 [label="Layer5_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert2_63 [label="Layer5_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_5_expert_allreduce_15 [label="Layer5_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_5_expert_agg [label="Layer5_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_5_norm [label="Layer5_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_5_attn_allreduce_0 -> layer_5_gate_0 [style=dashed]
	layer_5_gate_0 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_0
	layer_5_expert1_0 -> layer_5_expert_act_0
	layer_5_expert_act_0 -> layer_5_expert2_0
	layer_5_expert2_0 -> layer_5_expert_allreduce_0
	layer_5_alltoall -> layer_5_expert1_1
	layer_5_expert1_1 -> layer_5_expert_act_1
	layer_5_expert_act_1 -> layer_5_expert2_1
	layer_5_expert2_1 -> layer_5_expert_allreduce_0
	layer_5_alltoall -> layer_5_expert1_2
	layer_5_expert1_2 -> layer_5_expert_act_2
	layer_5_expert_act_2 -> layer_5_expert2_2
	layer_5_expert2_2 -> layer_5_expert_allreduce_0
	layer_5_alltoall -> layer_5_expert1_3
	layer_5_expert1_3 -> layer_5_expert_act_3
	layer_5_expert_act_3 -> layer_5_expert2_3
	layer_5_expert2_3 -> layer_5_expert_allreduce_0
	layer_5_expert_allreduce_0 -> layer_5_expert_agg
	layer_5_attn_allreduce_1 -> layer_5_gate_4 [style=dashed]
	layer_5_gate_4 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_4
	layer_5_expert1_4 -> layer_5_expert_act_4
	layer_5_expert_act_4 -> layer_5_expert2_4
	layer_5_expert2_4 -> layer_5_expert_allreduce_1
	layer_5_alltoall -> layer_5_expert1_5
	layer_5_expert1_5 -> layer_5_expert_act_5
	layer_5_expert_act_5 -> layer_5_expert2_5
	layer_5_expert2_5 -> layer_5_expert_allreduce_1
	layer_5_alltoall -> layer_5_expert1_6
	layer_5_expert1_6 -> layer_5_expert_act_6
	layer_5_expert_act_6 -> layer_5_expert2_6
	layer_5_expert2_6 -> layer_5_expert_allreduce_1
	layer_5_alltoall -> layer_5_expert1_7
	layer_5_expert1_7 -> layer_5_expert_act_7
	layer_5_expert_act_7 -> layer_5_expert2_7
	layer_5_expert2_7 -> layer_5_expert_allreduce_1
	layer_5_expert_allreduce_1 -> layer_5_expert_agg
	layer_5_attn_allreduce_2 -> layer_5_gate_8 [style=dashed]
	layer_5_gate_8 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_8
	layer_5_expert1_8 -> layer_5_expert_act_8
	layer_5_expert_act_8 -> layer_5_expert2_8
	layer_5_expert2_8 -> layer_5_expert_allreduce_2
	layer_5_alltoall -> layer_5_expert1_9
	layer_5_expert1_9 -> layer_5_expert_act_9
	layer_5_expert_act_9 -> layer_5_expert2_9
	layer_5_expert2_9 -> layer_5_expert_allreduce_2
	layer_5_alltoall -> layer_5_expert1_10
	layer_5_expert1_10 -> layer_5_expert_act_10
	layer_5_expert_act_10 -> layer_5_expert2_10
	layer_5_expert2_10 -> layer_5_expert_allreduce_2
	layer_5_alltoall -> layer_5_expert1_11
	layer_5_expert1_11 -> layer_5_expert_act_11
	layer_5_expert_act_11 -> layer_5_expert2_11
	layer_5_expert2_11 -> layer_5_expert_allreduce_2
	layer_5_expert_allreduce_2 -> layer_5_expert_agg
	layer_5_attn_allreduce_3 -> layer_5_gate_12 [style=dashed]
	layer_5_gate_12 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_12
	layer_5_expert1_12 -> layer_5_expert_act_12
	layer_5_expert_act_12 -> layer_5_expert2_12
	layer_5_expert2_12 -> layer_5_expert_allreduce_3
	layer_5_alltoall -> layer_5_expert1_13
	layer_5_expert1_13 -> layer_5_expert_act_13
	layer_5_expert_act_13 -> layer_5_expert2_13
	layer_5_expert2_13 -> layer_5_expert_allreduce_3
	layer_5_alltoall -> layer_5_expert1_14
	layer_5_expert1_14 -> layer_5_expert_act_14
	layer_5_expert_act_14 -> layer_5_expert2_14
	layer_5_expert2_14 -> layer_5_expert_allreduce_3
	layer_5_alltoall -> layer_5_expert1_15
	layer_5_expert1_15 -> layer_5_expert_act_15
	layer_5_expert_act_15 -> layer_5_expert2_15
	layer_5_expert2_15 -> layer_5_expert_allreduce_3
	layer_5_expert_allreduce_3 -> layer_5_expert_agg
	layer_5_attn_allreduce_4 -> layer_5_gate_16 [style=dashed]
	layer_5_gate_16 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_16
	layer_5_expert1_16 -> layer_5_expert_act_16
	layer_5_expert_act_16 -> layer_5_expert2_16
	layer_5_expert2_16 -> layer_5_expert_allreduce_4
	layer_5_alltoall -> layer_5_expert1_17
	layer_5_expert1_17 -> layer_5_expert_act_17
	layer_5_expert_act_17 -> layer_5_expert2_17
	layer_5_expert2_17 -> layer_5_expert_allreduce_4
	layer_5_alltoall -> layer_5_expert1_18
	layer_5_expert1_18 -> layer_5_expert_act_18
	layer_5_expert_act_18 -> layer_5_expert2_18
	layer_5_expert2_18 -> layer_5_expert_allreduce_4
	layer_5_alltoall -> layer_5_expert1_19
	layer_5_expert1_19 -> layer_5_expert_act_19
	layer_5_expert_act_19 -> layer_5_expert2_19
	layer_5_expert2_19 -> layer_5_expert_allreduce_4
	layer_5_expert_allreduce_4 -> layer_5_expert_agg
	layer_5_attn_allreduce_5 -> layer_5_gate_20 [style=dashed]
	layer_5_gate_20 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_20
	layer_5_expert1_20 -> layer_5_expert_act_20
	layer_5_expert_act_20 -> layer_5_expert2_20
	layer_5_expert2_20 -> layer_5_expert_allreduce_5
	layer_5_alltoall -> layer_5_expert1_21
	layer_5_expert1_21 -> layer_5_expert_act_21
	layer_5_expert_act_21 -> layer_5_expert2_21
	layer_5_expert2_21 -> layer_5_expert_allreduce_5
	layer_5_alltoall -> layer_5_expert1_22
	layer_5_expert1_22 -> layer_5_expert_act_22
	layer_5_expert_act_22 -> layer_5_expert2_22
	layer_5_expert2_22 -> layer_5_expert_allreduce_5
	layer_5_alltoall -> layer_5_expert1_23
	layer_5_expert1_23 -> layer_5_expert_act_23
	layer_5_expert_act_23 -> layer_5_expert2_23
	layer_5_expert2_23 -> layer_5_expert_allreduce_5
	layer_5_expert_allreduce_5 -> layer_5_expert_agg
	layer_5_attn_allreduce_6 -> layer_5_gate_24 [style=dashed]
	layer_5_gate_24 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_24
	layer_5_expert1_24 -> layer_5_expert_act_24
	layer_5_expert_act_24 -> layer_5_expert2_24
	layer_5_expert2_24 -> layer_5_expert_allreduce_6
	layer_5_alltoall -> layer_5_expert1_25
	layer_5_expert1_25 -> layer_5_expert_act_25
	layer_5_expert_act_25 -> layer_5_expert2_25
	layer_5_expert2_25 -> layer_5_expert_allreduce_6
	layer_5_alltoall -> layer_5_expert1_26
	layer_5_expert1_26 -> layer_5_expert_act_26
	layer_5_expert_act_26 -> layer_5_expert2_26
	layer_5_expert2_26 -> layer_5_expert_allreduce_6
	layer_5_alltoall -> layer_5_expert1_27
	layer_5_expert1_27 -> layer_5_expert_act_27
	layer_5_expert_act_27 -> layer_5_expert2_27
	layer_5_expert2_27 -> layer_5_expert_allreduce_6
	layer_5_expert_allreduce_6 -> layer_5_expert_agg
	layer_5_attn_allreduce_7 -> layer_5_gate_28 [style=dashed]
	layer_5_gate_28 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_28
	layer_5_expert1_28 -> layer_5_expert_act_28
	layer_5_expert_act_28 -> layer_5_expert2_28
	layer_5_expert2_28 -> layer_5_expert_allreduce_7
	layer_5_alltoall -> layer_5_expert1_29
	layer_5_expert1_29 -> layer_5_expert_act_29
	layer_5_expert_act_29 -> layer_5_expert2_29
	layer_5_expert2_29 -> layer_5_expert_allreduce_7
	layer_5_alltoall -> layer_5_expert1_30
	layer_5_expert1_30 -> layer_5_expert_act_30
	layer_5_expert_act_30 -> layer_5_expert2_30
	layer_5_expert2_30 -> layer_5_expert_allreduce_7
	layer_5_alltoall -> layer_5_expert1_31
	layer_5_expert1_31 -> layer_5_expert_act_31
	layer_5_expert_act_31 -> layer_5_expert2_31
	layer_5_expert2_31 -> layer_5_expert_allreduce_7
	layer_5_expert_allreduce_7 -> layer_5_expert_agg
	layer_5_attn_allreduce_8 -> layer_5_gate_32 [style=dashed]
	layer_5_gate_32 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_32
	layer_5_expert1_32 -> layer_5_expert_act_32
	layer_5_expert_act_32 -> layer_5_expert2_32
	layer_5_expert2_32 -> layer_5_expert_allreduce_8
	layer_5_alltoall -> layer_5_expert1_33
	layer_5_expert1_33 -> layer_5_expert_act_33
	layer_5_expert_act_33 -> layer_5_expert2_33
	layer_5_expert2_33 -> layer_5_expert_allreduce_8
	layer_5_alltoall -> layer_5_expert1_34
	layer_5_expert1_34 -> layer_5_expert_act_34
	layer_5_expert_act_34 -> layer_5_expert2_34
	layer_5_expert2_34 -> layer_5_expert_allreduce_8
	layer_5_alltoall -> layer_5_expert1_35
	layer_5_expert1_35 -> layer_5_expert_act_35
	layer_5_expert_act_35 -> layer_5_expert2_35
	layer_5_expert2_35 -> layer_5_expert_allreduce_8
	layer_5_expert_allreduce_8 -> layer_5_expert_agg
	layer_5_attn_allreduce_9 -> layer_5_gate_36 [style=dashed]
	layer_5_gate_36 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_36
	layer_5_expert1_36 -> layer_5_expert_act_36
	layer_5_expert_act_36 -> layer_5_expert2_36
	layer_5_expert2_36 -> layer_5_expert_allreduce_9
	layer_5_alltoall -> layer_5_expert1_37
	layer_5_expert1_37 -> layer_5_expert_act_37
	layer_5_expert_act_37 -> layer_5_expert2_37
	layer_5_expert2_37 -> layer_5_expert_allreduce_9
	layer_5_alltoall -> layer_5_expert1_38
	layer_5_expert1_38 -> layer_5_expert_act_38
	layer_5_expert_act_38 -> layer_5_expert2_38
	layer_5_expert2_38 -> layer_5_expert_allreduce_9
	layer_5_alltoall -> layer_5_expert1_39
	layer_5_expert1_39 -> layer_5_expert_act_39
	layer_5_expert_act_39 -> layer_5_expert2_39
	layer_5_expert2_39 -> layer_5_expert_allreduce_9
	layer_5_expert_allreduce_9 -> layer_5_expert_agg
	layer_5_attn_allreduce_10 -> layer_5_gate_40 [style=dashed]
	layer_5_gate_40 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_40
	layer_5_expert1_40 -> layer_5_expert_act_40
	layer_5_expert_act_40 -> layer_5_expert2_40
	layer_5_expert2_40 -> layer_5_expert_allreduce_10
	layer_5_alltoall -> layer_5_expert1_41
	layer_5_expert1_41 -> layer_5_expert_act_41
	layer_5_expert_act_41 -> layer_5_expert2_41
	layer_5_expert2_41 -> layer_5_expert_allreduce_10
	layer_5_alltoall -> layer_5_expert1_42
	layer_5_expert1_42 -> layer_5_expert_act_42
	layer_5_expert_act_42 -> layer_5_expert2_42
	layer_5_expert2_42 -> layer_5_expert_allreduce_10
	layer_5_alltoall -> layer_5_expert1_43
	layer_5_expert1_43 -> layer_5_expert_act_43
	layer_5_expert_act_43 -> layer_5_expert2_43
	layer_5_expert2_43 -> layer_5_expert_allreduce_10
	layer_5_expert_allreduce_10 -> layer_5_expert_agg
	layer_5_attn_allreduce_11 -> layer_5_gate_44 [style=dashed]
	layer_5_gate_44 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_44
	layer_5_expert1_44 -> layer_5_expert_act_44
	layer_5_expert_act_44 -> layer_5_expert2_44
	layer_5_expert2_44 -> layer_5_expert_allreduce_11
	layer_5_alltoall -> layer_5_expert1_45
	layer_5_expert1_45 -> layer_5_expert_act_45
	layer_5_expert_act_45 -> layer_5_expert2_45
	layer_5_expert2_45 -> layer_5_expert_allreduce_11
	layer_5_alltoall -> layer_5_expert1_46
	layer_5_expert1_46 -> layer_5_expert_act_46
	layer_5_expert_act_46 -> layer_5_expert2_46
	layer_5_expert2_46 -> layer_5_expert_allreduce_11
	layer_5_alltoall -> layer_5_expert1_47
	layer_5_expert1_47 -> layer_5_expert_act_47
	layer_5_expert_act_47 -> layer_5_expert2_47
	layer_5_expert2_47 -> layer_5_expert_allreduce_11
	layer_5_expert_allreduce_11 -> layer_5_expert_agg
	layer_5_attn_allreduce_12 -> layer_5_gate_48 [style=dashed]
	layer_5_gate_48 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_48
	layer_5_expert1_48 -> layer_5_expert_act_48
	layer_5_expert_act_48 -> layer_5_expert2_48
	layer_5_expert2_48 -> layer_5_expert_allreduce_12
	layer_5_alltoall -> layer_5_expert1_49
	layer_5_expert1_49 -> layer_5_expert_act_49
	layer_5_expert_act_49 -> layer_5_expert2_49
	layer_5_expert2_49 -> layer_5_expert_allreduce_12
	layer_5_alltoall -> layer_5_expert1_50
	layer_5_expert1_50 -> layer_5_expert_act_50
	layer_5_expert_act_50 -> layer_5_expert2_50
	layer_5_expert2_50 -> layer_5_expert_allreduce_12
	layer_5_alltoall -> layer_5_expert1_51
	layer_5_expert1_51 -> layer_5_expert_act_51
	layer_5_expert_act_51 -> layer_5_expert2_51
	layer_5_expert2_51 -> layer_5_expert_allreduce_12
	layer_5_expert_allreduce_12 -> layer_5_expert_agg
	layer_5_attn_allreduce_13 -> layer_5_gate_52 [style=dashed]
	layer_5_gate_52 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_52
	layer_5_expert1_52 -> layer_5_expert_act_52
	layer_5_expert_act_52 -> layer_5_expert2_52
	layer_5_expert2_52 -> layer_5_expert_allreduce_13
	layer_5_alltoall -> layer_5_expert1_53
	layer_5_expert1_53 -> layer_5_expert_act_53
	layer_5_expert_act_53 -> layer_5_expert2_53
	layer_5_expert2_53 -> layer_5_expert_allreduce_13
	layer_5_alltoall -> layer_5_expert1_54
	layer_5_expert1_54 -> layer_5_expert_act_54
	layer_5_expert_act_54 -> layer_5_expert2_54
	layer_5_expert2_54 -> layer_5_expert_allreduce_13
	layer_5_alltoall -> layer_5_expert1_55
	layer_5_expert1_55 -> layer_5_expert_act_55
	layer_5_expert_act_55 -> layer_5_expert2_55
	layer_5_expert2_55 -> layer_5_expert_allreduce_13
	layer_5_expert_allreduce_13 -> layer_5_expert_agg
	layer_5_attn_allreduce_14 -> layer_5_gate_56 [style=dashed]
	layer_5_gate_56 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_56
	layer_5_expert1_56 -> layer_5_expert_act_56
	layer_5_expert_act_56 -> layer_5_expert2_56
	layer_5_expert2_56 -> layer_5_expert_allreduce_14
	layer_5_alltoall -> layer_5_expert1_57
	layer_5_expert1_57 -> layer_5_expert_act_57
	layer_5_expert_act_57 -> layer_5_expert2_57
	layer_5_expert2_57 -> layer_5_expert_allreduce_14
	layer_5_alltoall -> layer_5_expert1_58
	layer_5_expert1_58 -> layer_5_expert_act_58
	layer_5_expert_act_58 -> layer_5_expert2_58
	layer_5_expert2_58 -> layer_5_expert_allreduce_14
	layer_5_alltoall -> layer_5_expert1_59
	layer_5_expert1_59 -> layer_5_expert_act_59
	layer_5_expert_act_59 -> layer_5_expert2_59
	layer_5_expert2_59 -> layer_5_expert_allreduce_14
	layer_5_expert_allreduce_14 -> layer_5_expert_agg
	layer_5_attn_allreduce_15 -> layer_5_gate_60 [style=dashed]
	layer_5_gate_60 -> layer_5_alltoall [style=dashed]
	layer_5_alltoall -> layer_5_expert1_60
	layer_5_expert1_60 -> layer_5_expert_act_60
	layer_5_expert_act_60 -> layer_5_expert2_60
	layer_5_expert2_60 -> layer_5_expert_allreduce_15
	layer_5_alltoall -> layer_5_expert1_61
	layer_5_expert1_61 -> layer_5_expert_act_61
	layer_5_expert_act_61 -> layer_5_expert2_61
	layer_5_expert2_61 -> layer_5_expert_allreduce_15
	layer_5_alltoall -> layer_5_expert1_62
	layer_5_expert1_62 -> layer_5_expert_act_62
	layer_5_expert_act_62 -> layer_5_expert2_62
	layer_5_expert2_62 -> layer_5_expert_allreduce_15
	layer_5_alltoall -> layer_5_expert1_63
	layer_5_expert1_63 -> layer_5_expert_act_63
	layer_5_expert_act_63 -> layer_5_expert2_63
	layer_5_expert2_63 -> layer_5_expert_allreduce_15
	layer_5_expert_allreduce_15 -> layer_5_expert_agg
	layer_5_expert_agg -> layer_5_norm
	layer_5_norm -> layer_6_input
	layer_6_input [label="Layer6_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_6_qkv_0 [label="Layer6_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_1 [label="Layer6_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_2 [label="Layer6_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_3 [label="Layer6_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_0 [label="Layer6_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_1 [label="Layer6_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_2 [label="Layer6_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_3 [label="Layer6_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_0 [label="Layer6_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_1 [label="Layer6_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_2 [label="Layer6_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_3 [label="Layer6_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_0 [label="Layer6_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_1 [label="Layer6_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_2 [label="Layer6_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_3 [label="Layer6_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_0 [label="Layer6_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_0
	layer_6_qkv_0 -> layer_6_attn_scores_0
	layer_6_attn_scores_0 -> layer_6_attn_softmax_0
	layer_6_attn_softmax_0 -> layer_6_attn_out_0
	layer_6_attn_out_0 -> layer_6_attn_allreduce_0
	layer_6_input -> layer_6_qkv_1
	layer_6_qkv_1 -> layer_6_attn_scores_1
	layer_6_attn_scores_1 -> layer_6_attn_softmax_1
	layer_6_attn_softmax_1 -> layer_6_attn_out_1
	layer_6_attn_out_1 -> layer_6_attn_allreduce_0
	layer_6_input -> layer_6_qkv_2
	layer_6_qkv_2 -> layer_6_attn_scores_2
	layer_6_attn_scores_2 -> layer_6_attn_softmax_2
	layer_6_attn_softmax_2 -> layer_6_attn_out_2
	layer_6_attn_out_2 -> layer_6_attn_allreduce_0
	layer_6_input -> layer_6_qkv_3
	layer_6_qkv_3 -> layer_6_attn_scores_3
	layer_6_attn_scores_3 -> layer_6_attn_softmax_3
	layer_6_attn_softmax_3 -> layer_6_attn_out_3
	layer_6_attn_out_3 -> layer_6_attn_allreduce_0
	layer_6_qkv_4 [label="Layer6_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_5 [label="Layer6_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_6 [label="Layer6_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_7 [label="Layer6_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_4 [label="Layer6_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_5 [label="Layer6_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_6 [label="Layer6_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_7 [label="Layer6_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_4 [label="Layer6_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_5 [label="Layer6_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_6 [label="Layer6_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_7 [label="Layer6_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_4 [label="Layer6_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_5 [label="Layer6_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_6 [label="Layer6_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_7 [label="Layer6_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_1 [label="Layer6_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_4
	layer_6_qkv_4 -> layer_6_attn_scores_4
	layer_6_attn_scores_4 -> layer_6_attn_softmax_4
	layer_6_attn_softmax_4 -> layer_6_attn_out_4
	layer_6_attn_out_4 -> layer_6_attn_allreduce_1
	layer_6_input -> layer_6_qkv_5
	layer_6_qkv_5 -> layer_6_attn_scores_5
	layer_6_attn_scores_5 -> layer_6_attn_softmax_5
	layer_6_attn_softmax_5 -> layer_6_attn_out_5
	layer_6_attn_out_5 -> layer_6_attn_allreduce_1
	layer_6_input -> layer_6_qkv_6
	layer_6_qkv_6 -> layer_6_attn_scores_6
	layer_6_attn_scores_6 -> layer_6_attn_softmax_6
	layer_6_attn_softmax_6 -> layer_6_attn_out_6
	layer_6_attn_out_6 -> layer_6_attn_allreduce_1
	layer_6_input -> layer_6_qkv_7
	layer_6_qkv_7 -> layer_6_attn_scores_7
	layer_6_attn_scores_7 -> layer_6_attn_softmax_7
	layer_6_attn_softmax_7 -> layer_6_attn_out_7
	layer_6_attn_out_7 -> layer_6_attn_allreduce_1
	layer_6_qkv_8 [label="Layer6_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_9 [label="Layer6_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_10 [label="Layer6_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_11 [label="Layer6_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_8 [label="Layer6_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_9 [label="Layer6_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_10 [label="Layer6_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_11 [label="Layer6_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_8 [label="Layer6_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_9 [label="Layer6_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_10 [label="Layer6_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_11 [label="Layer6_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_8 [label="Layer6_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_9 [label="Layer6_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_10 [label="Layer6_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_11 [label="Layer6_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_2 [label="Layer6_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_8
	layer_6_qkv_8 -> layer_6_attn_scores_8
	layer_6_attn_scores_8 -> layer_6_attn_softmax_8
	layer_6_attn_softmax_8 -> layer_6_attn_out_8
	layer_6_attn_out_8 -> layer_6_attn_allreduce_2
	layer_6_input -> layer_6_qkv_9
	layer_6_qkv_9 -> layer_6_attn_scores_9
	layer_6_attn_scores_9 -> layer_6_attn_softmax_9
	layer_6_attn_softmax_9 -> layer_6_attn_out_9
	layer_6_attn_out_9 -> layer_6_attn_allreduce_2
	layer_6_input -> layer_6_qkv_10
	layer_6_qkv_10 -> layer_6_attn_scores_10
	layer_6_attn_scores_10 -> layer_6_attn_softmax_10
	layer_6_attn_softmax_10 -> layer_6_attn_out_10
	layer_6_attn_out_10 -> layer_6_attn_allreduce_2
	layer_6_input -> layer_6_qkv_11
	layer_6_qkv_11 -> layer_6_attn_scores_11
	layer_6_attn_scores_11 -> layer_6_attn_softmax_11
	layer_6_attn_softmax_11 -> layer_6_attn_out_11
	layer_6_attn_out_11 -> layer_6_attn_allreduce_2
	layer_6_qkv_12 [label="Layer6_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_13 [label="Layer6_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_14 [label="Layer6_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_15 [label="Layer6_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_12 [label="Layer6_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_13 [label="Layer6_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_14 [label="Layer6_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_15 [label="Layer6_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_12 [label="Layer6_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_13 [label="Layer6_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_14 [label="Layer6_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_15 [label="Layer6_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_12 [label="Layer6_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_13 [label="Layer6_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_14 [label="Layer6_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_15 [label="Layer6_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_3 [label="Layer6_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_12
	layer_6_qkv_12 -> layer_6_attn_scores_12
	layer_6_attn_scores_12 -> layer_6_attn_softmax_12
	layer_6_attn_softmax_12 -> layer_6_attn_out_12
	layer_6_attn_out_12 -> layer_6_attn_allreduce_3
	layer_6_input -> layer_6_qkv_13
	layer_6_qkv_13 -> layer_6_attn_scores_13
	layer_6_attn_scores_13 -> layer_6_attn_softmax_13
	layer_6_attn_softmax_13 -> layer_6_attn_out_13
	layer_6_attn_out_13 -> layer_6_attn_allreduce_3
	layer_6_input -> layer_6_qkv_14
	layer_6_qkv_14 -> layer_6_attn_scores_14
	layer_6_attn_scores_14 -> layer_6_attn_softmax_14
	layer_6_attn_softmax_14 -> layer_6_attn_out_14
	layer_6_attn_out_14 -> layer_6_attn_allreduce_3
	layer_6_input -> layer_6_qkv_15
	layer_6_qkv_15 -> layer_6_attn_scores_15
	layer_6_attn_scores_15 -> layer_6_attn_softmax_15
	layer_6_attn_softmax_15 -> layer_6_attn_out_15
	layer_6_attn_out_15 -> layer_6_attn_allreduce_3
	layer_6_qkv_16 [label="Layer6_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_17 [label="Layer6_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_18 [label="Layer6_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_19 [label="Layer6_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_16 [label="Layer6_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_17 [label="Layer6_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_18 [label="Layer6_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_19 [label="Layer6_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_16 [label="Layer6_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_17 [label="Layer6_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_18 [label="Layer6_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_19 [label="Layer6_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_16 [label="Layer6_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_17 [label="Layer6_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_18 [label="Layer6_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_19 [label="Layer6_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_4 [label="Layer6_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_16
	layer_6_qkv_16 -> layer_6_attn_scores_16
	layer_6_attn_scores_16 -> layer_6_attn_softmax_16
	layer_6_attn_softmax_16 -> layer_6_attn_out_16
	layer_6_attn_out_16 -> layer_6_attn_allreduce_4
	layer_6_input -> layer_6_qkv_17
	layer_6_qkv_17 -> layer_6_attn_scores_17
	layer_6_attn_scores_17 -> layer_6_attn_softmax_17
	layer_6_attn_softmax_17 -> layer_6_attn_out_17
	layer_6_attn_out_17 -> layer_6_attn_allreduce_4
	layer_6_input -> layer_6_qkv_18
	layer_6_qkv_18 -> layer_6_attn_scores_18
	layer_6_attn_scores_18 -> layer_6_attn_softmax_18
	layer_6_attn_softmax_18 -> layer_6_attn_out_18
	layer_6_attn_out_18 -> layer_6_attn_allreduce_4
	layer_6_input -> layer_6_qkv_19
	layer_6_qkv_19 -> layer_6_attn_scores_19
	layer_6_attn_scores_19 -> layer_6_attn_softmax_19
	layer_6_attn_softmax_19 -> layer_6_attn_out_19
	layer_6_attn_out_19 -> layer_6_attn_allreduce_4
	layer_6_qkv_20 [label="Layer6_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_21 [label="Layer6_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_22 [label="Layer6_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_23 [label="Layer6_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_20 [label="Layer6_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_21 [label="Layer6_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_22 [label="Layer6_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_23 [label="Layer6_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_20 [label="Layer6_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_21 [label="Layer6_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_22 [label="Layer6_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_23 [label="Layer6_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_20 [label="Layer6_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_21 [label="Layer6_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_22 [label="Layer6_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_23 [label="Layer6_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_5 [label="Layer6_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_20
	layer_6_qkv_20 -> layer_6_attn_scores_20
	layer_6_attn_scores_20 -> layer_6_attn_softmax_20
	layer_6_attn_softmax_20 -> layer_6_attn_out_20
	layer_6_attn_out_20 -> layer_6_attn_allreduce_5
	layer_6_input -> layer_6_qkv_21
	layer_6_qkv_21 -> layer_6_attn_scores_21
	layer_6_attn_scores_21 -> layer_6_attn_softmax_21
	layer_6_attn_softmax_21 -> layer_6_attn_out_21
	layer_6_attn_out_21 -> layer_6_attn_allreduce_5
	layer_6_input -> layer_6_qkv_22
	layer_6_qkv_22 -> layer_6_attn_scores_22
	layer_6_attn_scores_22 -> layer_6_attn_softmax_22
	layer_6_attn_softmax_22 -> layer_6_attn_out_22
	layer_6_attn_out_22 -> layer_6_attn_allreduce_5
	layer_6_input -> layer_6_qkv_23
	layer_6_qkv_23 -> layer_6_attn_scores_23
	layer_6_attn_scores_23 -> layer_6_attn_softmax_23
	layer_6_attn_softmax_23 -> layer_6_attn_out_23
	layer_6_attn_out_23 -> layer_6_attn_allreduce_5
	layer_6_qkv_24 [label="Layer6_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_25 [label="Layer6_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_26 [label="Layer6_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_27 [label="Layer6_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_24 [label="Layer6_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_25 [label="Layer6_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_26 [label="Layer6_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_27 [label="Layer6_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_24 [label="Layer6_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_25 [label="Layer6_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_26 [label="Layer6_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_27 [label="Layer6_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_24 [label="Layer6_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_25 [label="Layer6_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_26 [label="Layer6_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_27 [label="Layer6_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_6 [label="Layer6_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_24
	layer_6_qkv_24 -> layer_6_attn_scores_24
	layer_6_attn_scores_24 -> layer_6_attn_softmax_24
	layer_6_attn_softmax_24 -> layer_6_attn_out_24
	layer_6_attn_out_24 -> layer_6_attn_allreduce_6
	layer_6_input -> layer_6_qkv_25
	layer_6_qkv_25 -> layer_6_attn_scores_25
	layer_6_attn_scores_25 -> layer_6_attn_softmax_25
	layer_6_attn_softmax_25 -> layer_6_attn_out_25
	layer_6_attn_out_25 -> layer_6_attn_allreduce_6
	layer_6_input -> layer_6_qkv_26
	layer_6_qkv_26 -> layer_6_attn_scores_26
	layer_6_attn_scores_26 -> layer_6_attn_softmax_26
	layer_6_attn_softmax_26 -> layer_6_attn_out_26
	layer_6_attn_out_26 -> layer_6_attn_allreduce_6
	layer_6_input -> layer_6_qkv_27
	layer_6_qkv_27 -> layer_6_attn_scores_27
	layer_6_attn_scores_27 -> layer_6_attn_softmax_27
	layer_6_attn_softmax_27 -> layer_6_attn_out_27
	layer_6_attn_out_27 -> layer_6_attn_allreduce_6
	layer_6_qkv_28 [label="Layer6_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_29 [label="Layer6_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_30 [label="Layer6_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_31 [label="Layer6_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_28 [label="Layer6_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_29 [label="Layer6_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_30 [label="Layer6_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_31 [label="Layer6_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_28 [label="Layer6_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_29 [label="Layer6_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_30 [label="Layer6_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_31 [label="Layer6_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_28 [label="Layer6_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_29 [label="Layer6_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_30 [label="Layer6_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_31 [label="Layer6_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_7 [label="Layer6_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_28
	layer_6_qkv_28 -> layer_6_attn_scores_28
	layer_6_attn_scores_28 -> layer_6_attn_softmax_28
	layer_6_attn_softmax_28 -> layer_6_attn_out_28
	layer_6_attn_out_28 -> layer_6_attn_allreduce_7
	layer_6_input -> layer_6_qkv_29
	layer_6_qkv_29 -> layer_6_attn_scores_29
	layer_6_attn_scores_29 -> layer_6_attn_softmax_29
	layer_6_attn_softmax_29 -> layer_6_attn_out_29
	layer_6_attn_out_29 -> layer_6_attn_allreduce_7
	layer_6_input -> layer_6_qkv_30
	layer_6_qkv_30 -> layer_6_attn_scores_30
	layer_6_attn_scores_30 -> layer_6_attn_softmax_30
	layer_6_attn_softmax_30 -> layer_6_attn_out_30
	layer_6_attn_out_30 -> layer_6_attn_allreduce_7
	layer_6_input -> layer_6_qkv_31
	layer_6_qkv_31 -> layer_6_attn_scores_31
	layer_6_attn_scores_31 -> layer_6_attn_softmax_31
	layer_6_attn_softmax_31 -> layer_6_attn_out_31
	layer_6_attn_out_31 -> layer_6_attn_allreduce_7
	layer_6_qkv_32 [label="Layer6_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_33 [label="Layer6_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_34 [label="Layer6_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_35 [label="Layer6_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_32 [label="Layer6_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_33 [label="Layer6_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_34 [label="Layer6_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_35 [label="Layer6_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_32 [label="Layer6_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_33 [label="Layer6_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_34 [label="Layer6_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_35 [label="Layer6_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_32 [label="Layer6_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_33 [label="Layer6_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_34 [label="Layer6_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_35 [label="Layer6_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_8 [label="Layer6_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_32
	layer_6_qkv_32 -> layer_6_attn_scores_32
	layer_6_attn_scores_32 -> layer_6_attn_softmax_32
	layer_6_attn_softmax_32 -> layer_6_attn_out_32
	layer_6_attn_out_32 -> layer_6_attn_allreduce_8
	layer_6_input -> layer_6_qkv_33
	layer_6_qkv_33 -> layer_6_attn_scores_33
	layer_6_attn_scores_33 -> layer_6_attn_softmax_33
	layer_6_attn_softmax_33 -> layer_6_attn_out_33
	layer_6_attn_out_33 -> layer_6_attn_allreduce_8
	layer_6_input -> layer_6_qkv_34
	layer_6_qkv_34 -> layer_6_attn_scores_34
	layer_6_attn_scores_34 -> layer_6_attn_softmax_34
	layer_6_attn_softmax_34 -> layer_6_attn_out_34
	layer_6_attn_out_34 -> layer_6_attn_allreduce_8
	layer_6_input -> layer_6_qkv_35
	layer_6_qkv_35 -> layer_6_attn_scores_35
	layer_6_attn_scores_35 -> layer_6_attn_softmax_35
	layer_6_attn_softmax_35 -> layer_6_attn_out_35
	layer_6_attn_out_35 -> layer_6_attn_allreduce_8
	layer_6_qkv_36 [label="Layer6_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_37 [label="Layer6_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_38 [label="Layer6_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_39 [label="Layer6_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_36 [label="Layer6_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_37 [label="Layer6_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_38 [label="Layer6_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_39 [label="Layer6_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_36 [label="Layer6_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_37 [label="Layer6_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_38 [label="Layer6_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_39 [label="Layer6_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_36 [label="Layer6_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_37 [label="Layer6_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_38 [label="Layer6_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_39 [label="Layer6_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_9 [label="Layer6_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_36
	layer_6_qkv_36 -> layer_6_attn_scores_36
	layer_6_attn_scores_36 -> layer_6_attn_softmax_36
	layer_6_attn_softmax_36 -> layer_6_attn_out_36
	layer_6_attn_out_36 -> layer_6_attn_allreduce_9
	layer_6_input -> layer_6_qkv_37
	layer_6_qkv_37 -> layer_6_attn_scores_37
	layer_6_attn_scores_37 -> layer_6_attn_softmax_37
	layer_6_attn_softmax_37 -> layer_6_attn_out_37
	layer_6_attn_out_37 -> layer_6_attn_allreduce_9
	layer_6_input -> layer_6_qkv_38
	layer_6_qkv_38 -> layer_6_attn_scores_38
	layer_6_attn_scores_38 -> layer_6_attn_softmax_38
	layer_6_attn_softmax_38 -> layer_6_attn_out_38
	layer_6_attn_out_38 -> layer_6_attn_allreduce_9
	layer_6_input -> layer_6_qkv_39
	layer_6_qkv_39 -> layer_6_attn_scores_39
	layer_6_attn_scores_39 -> layer_6_attn_softmax_39
	layer_6_attn_softmax_39 -> layer_6_attn_out_39
	layer_6_attn_out_39 -> layer_6_attn_allreduce_9
	layer_6_qkv_40 [label="Layer6_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_41 [label="Layer6_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_42 [label="Layer6_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_43 [label="Layer6_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_40 [label="Layer6_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_41 [label="Layer6_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_42 [label="Layer6_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_43 [label="Layer6_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_40 [label="Layer6_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_41 [label="Layer6_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_42 [label="Layer6_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_43 [label="Layer6_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_40 [label="Layer6_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_41 [label="Layer6_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_42 [label="Layer6_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_43 [label="Layer6_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_10 [label="Layer6_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_40
	layer_6_qkv_40 -> layer_6_attn_scores_40
	layer_6_attn_scores_40 -> layer_6_attn_softmax_40
	layer_6_attn_softmax_40 -> layer_6_attn_out_40
	layer_6_attn_out_40 -> layer_6_attn_allreduce_10
	layer_6_input -> layer_6_qkv_41
	layer_6_qkv_41 -> layer_6_attn_scores_41
	layer_6_attn_scores_41 -> layer_6_attn_softmax_41
	layer_6_attn_softmax_41 -> layer_6_attn_out_41
	layer_6_attn_out_41 -> layer_6_attn_allreduce_10
	layer_6_input -> layer_6_qkv_42
	layer_6_qkv_42 -> layer_6_attn_scores_42
	layer_6_attn_scores_42 -> layer_6_attn_softmax_42
	layer_6_attn_softmax_42 -> layer_6_attn_out_42
	layer_6_attn_out_42 -> layer_6_attn_allreduce_10
	layer_6_input -> layer_6_qkv_43
	layer_6_qkv_43 -> layer_6_attn_scores_43
	layer_6_attn_scores_43 -> layer_6_attn_softmax_43
	layer_6_attn_softmax_43 -> layer_6_attn_out_43
	layer_6_attn_out_43 -> layer_6_attn_allreduce_10
	layer_6_qkv_44 [label="Layer6_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_45 [label="Layer6_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_46 [label="Layer6_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_47 [label="Layer6_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_44 [label="Layer6_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_45 [label="Layer6_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_46 [label="Layer6_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_47 [label="Layer6_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_44 [label="Layer6_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_45 [label="Layer6_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_46 [label="Layer6_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_47 [label="Layer6_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_44 [label="Layer6_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_45 [label="Layer6_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_46 [label="Layer6_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_47 [label="Layer6_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_11 [label="Layer6_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_44
	layer_6_qkv_44 -> layer_6_attn_scores_44
	layer_6_attn_scores_44 -> layer_6_attn_softmax_44
	layer_6_attn_softmax_44 -> layer_6_attn_out_44
	layer_6_attn_out_44 -> layer_6_attn_allreduce_11
	layer_6_input -> layer_6_qkv_45
	layer_6_qkv_45 -> layer_6_attn_scores_45
	layer_6_attn_scores_45 -> layer_6_attn_softmax_45
	layer_6_attn_softmax_45 -> layer_6_attn_out_45
	layer_6_attn_out_45 -> layer_6_attn_allreduce_11
	layer_6_input -> layer_6_qkv_46
	layer_6_qkv_46 -> layer_6_attn_scores_46
	layer_6_attn_scores_46 -> layer_6_attn_softmax_46
	layer_6_attn_softmax_46 -> layer_6_attn_out_46
	layer_6_attn_out_46 -> layer_6_attn_allreduce_11
	layer_6_input -> layer_6_qkv_47
	layer_6_qkv_47 -> layer_6_attn_scores_47
	layer_6_attn_scores_47 -> layer_6_attn_softmax_47
	layer_6_attn_softmax_47 -> layer_6_attn_out_47
	layer_6_attn_out_47 -> layer_6_attn_allreduce_11
	layer_6_qkv_48 [label="Layer6_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_49 [label="Layer6_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_50 [label="Layer6_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_51 [label="Layer6_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_48 [label="Layer6_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_49 [label="Layer6_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_50 [label="Layer6_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_51 [label="Layer6_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_48 [label="Layer6_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_49 [label="Layer6_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_50 [label="Layer6_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_51 [label="Layer6_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_48 [label="Layer6_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_49 [label="Layer6_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_50 [label="Layer6_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_51 [label="Layer6_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_12 [label="Layer6_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_48
	layer_6_qkv_48 -> layer_6_attn_scores_48
	layer_6_attn_scores_48 -> layer_6_attn_softmax_48
	layer_6_attn_softmax_48 -> layer_6_attn_out_48
	layer_6_attn_out_48 -> layer_6_attn_allreduce_12
	layer_6_input -> layer_6_qkv_49
	layer_6_qkv_49 -> layer_6_attn_scores_49
	layer_6_attn_scores_49 -> layer_6_attn_softmax_49
	layer_6_attn_softmax_49 -> layer_6_attn_out_49
	layer_6_attn_out_49 -> layer_6_attn_allreduce_12
	layer_6_input -> layer_6_qkv_50
	layer_6_qkv_50 -> layer_6_attn_scores_50
	layer_6_attn_scores_50 -> layer_6_attn_softmax_50
	layer_6_attn_softmax_50 -> layer_6_attn_out_50
	layer_6_attn_out_50 -> layer_6_attn_allreduce_12
	layer_6_input -> layer_6_qkv_51
	layer_6_qkv_51 -> layer_6_attn_scores_51
	layer_6_attn_scores_51 -> layer_6_attn_softmax_51
	layer_6_attn_softmax_51 -> layer_6_attn_out_51
	layer_6_attn_out_51 -> layer_6_attn_allreduce_12
	layer_6_qkv_52 [label="Layer6_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_53 [label="Layer6_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_54 [label="Layer6_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_55 [label="Layer6_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_52 [label="Layer6_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_53 [label="Layer6_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_54 [label="Layer6_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_55 [label="Layer6_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_52 [label="Layer6_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_53 [label="Layer6_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_54 [label="Layer6_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_55 [label="Layer6_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_52 [label="Layer6_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_53 [label="Layer6_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_54 [label="Layer6_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_55 [label="Layer6_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_13 [label="Layer6_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_52
	layer_6_qkv_52 -> layer_6_attn_scores_52
	layer_6_attn_scores_52 -> layer_6_attn_softmax_52
	layer_6_attn_softmax_52 -> layer_6_attn_out_52
	layer_6_attn_out_52 -> layer_6_attn_allreduce_13
	layer_6_input -> layer_6_qkv_53
	layer_6_qkv_53 -> layer_6_attn_scores_53
	layer_6_attn_scores_53 -> layer_6_attn_softmax_53
	layer_6_attn_softmax_53 -> layer_6_attn_out_53
	layer_6_attn_out_53 -> layer_6_attn_allreduce_13
	layer_6_input -> layer_6_qkv_54
	layer_6_qkv_54 -> layer_6_attn_scores_54
	layer_6_attn_scores_54 -> layer_6_attn_softmax_54
	layer_6_attn_softmax_54 -> layer_6_attn_out_54
	layer_6_attn_out_54 -> layer_6_attn_allreduce_13
	layer_6_input -> layer_6_qkv_55
	layer_6_qkv_55 -> layer_6_attn_scores_55
	layer_6_attn_scores_55 -> layer_6_attn_softmax_55
	layer_6_attn_softmax_55 -> layer_6_attn_out_55
	layer_6_attn_out_55 -> layer_6_attn_allreduce_13
	layer_6_qkv_56 [label="Layer6_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_57 [label="Layer6_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_58 [label="Layer6_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_59 [label="Layer6_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_56 [label="Layer6_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_57 [label="Layer6_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_58 [label="Layer6_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_59 [label="Layer6_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_56 [label="Layer6_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_57 [label="Layer6_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_58 [label="Layer6_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_59 [label="Layer6_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_56 [label="Layer6_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_57 [label="Layer6_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_58 [label="Layer6_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_59 [label="Layer6_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_14 [label="Layer6_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_56
	layer_6_qkv_56 -> layer_6_attn_scores_56
	layer_6_attn_scores_56 -> layer_6_attn_softmax_56
	layer_6_attn_softmax_56 -> layer_6_attn_out_56
	layer_6_attn_out_56 -> layer_6_attn_allreduce_14
	layer_6_input -> layer_6_qkv_57
	layer_6_qkv_57 -> layer_6_attn_scores_57
	layer_6_attn_scores_57 -> layer_6_attn_softmax_57
	layer_6_attn_softmax_57 -> layer_6_attn_out_57
	layer_6_attn_out_57 -> layer_6_attn_allreduce_14
	layer_6_input -> layer_6_qkv_58
	layer_6_qkv_58 -> layer_6_attn_scores_58
	layer_6_attn_scores_58 -> layer_6_attn_softmax_58
	layer_6_attn_softmax_58 -> layer_6_attn_out_58
	layer_6_attn_out_58 -> layer_6_attn_allreduce_14
	layer_6_input -> layer_6_qkv_59
	layer_6_qkv_59 -> layer_6_attn_scores_59
	layer_6_attn_scores_59 -> layer_6_attn_softmax_59
	layer_6_attn_softmax_59 -> layer_6_attn_out_59
	layer_6_attn_out_59 -> layer_6_attn_allreduce_14
	layer_6_qkv_60 [label="Layer6_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_61 [label="Layer6_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_62 [label="Layer6_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_qkv_63 [label="Layer6_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_60 [label="Layer6_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_61 [label="Layer6_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_62 [label="Layer6_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_scores_63 [label="Layer6_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_60 [label="Layer6_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_61 [label="Layer6_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_62 [label="Layer6_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_softmax_63 [label="Layer6_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_60 [label="Layer6_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_61 [label="Layer6_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_62 [label="Layer6_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_out_63 [label="Layer6_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_15 [label="Layer6_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_input -> layer_6_qkv_60
	layer_6_qkv_60 -> layer_6_attn_scores_60
	layer_6_attn_scores_60 -> layer_6_attn_softmax_60
	layer_6_attn_softmax_60 -> layer_6_attn_out_60
	layer_6_attn_out_60 -> layer_6_attn_allreduce_15
	layer_6_input -> layer_6_qkv_61
	layer_6_qkv_61 -> layer_6_attn_scores_61
	layer_6_attn_scores_61 -> layer_6_attn_softmax_61
	layer_6_attn_softmax_61 -> layer_6_attn_out_61
	layer_6_attn_out_61 -> layer_6_attn_allreduce_15
	layer_6_input -> layer_6_qkv_62
	layer_6_qkv_62 -> layer_6_attn_scores_62
	layer_6_attn_scores_62 -> layer_6_attn_softmax_62
	layer_6_attn_softmax_62 -> layer_6_attn_out_62
	layer_6_attn_out_62 -> layer_6_attn_allreduce_15
	layer_6_input -> layer_6_qkv_63
	layer_6_qkv_63 -> layer_6_attn_scores_63
	layer_6_attn_scores_63 -> layer_6_attn_softmax_63
	layer_6_attn_softmax_63 -> layer_6_attn_out_63
	layer_6_attn_out_63 -> layer_6_attn_allreduce_15
	layer_6_gate_0 [label="Layer6_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_4 [label="Layer6_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_8 [label="Layer6_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_12 [label="Layer6_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_16 [label="Layer6_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_20 [label="Layer6_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_24 [label="Layer6_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_28 [label="Layer6_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_32 [label="Layer6_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_36 [label="Layer6_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_40 [label="Layer6_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_44 [label="Layer6_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_48 [label="Layer6_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_52 [label="Layer6_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_56 [label="Layer6_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_gate_60 [label="Layer6_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_6_alltoall [label="Layer6_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_0 [label="Layer6_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_1 [label="Layer6_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_2 [label="Layer6_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_3 [label="Layer6_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_0 [label="Layer6_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_1 [label="Layer6_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_2 [label="Layer6_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_3 [label="Layer6_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_0 [label="Layer6_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_1 [label="Layer6_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_2 [label="Layer6_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_3 [label="Layer6_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_0 [label="Layer6_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_4 [label="Layer6_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_5 [label="Layer6_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_6 [label="Layer6_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_7 [label="Layer6_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_4 [label="Layer6_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_5 [label="Layer6_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_6 [label="Layer6_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_7 [label="Layer6_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_4 [label="Layer6_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_5 [label="Layer6_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_6 [label="Layer6_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_7 [label="Layer6_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_1 [label="Layer6_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_8 [label="Layer6_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_9 [label="Layer6_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_10 [label="Layer6_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_11 [label="Layer6_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_8 [label="Layer6_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_9 [label="Layer6_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_10 [label="Layer6_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_11 [label="Layer6_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_8 [label="Layer6_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_9 [label="Layer6_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_10 [label="Layer6_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_11 [label="Layer6_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_2 [label="Layer6_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_12 [label="Layer6_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_13 [label="Layer6_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_14 [label="Layer6_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_15 [label="Layer6_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_12 [label="Layer6_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_13 [label="Layer6_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_14 [label="Layer6_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_15 [label="Layer6_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_12 [label="Layer6_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_13 [label="Layer6_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_14 [label="Layer6_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_15 [label="Layer6_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_3 [label="Layer6_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_16 [label="Layer6_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_17 [label="Layer6_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_18 [label="Layer6_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_19 [label="Layer6_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_16 [label="Layer6_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_17 [label="Layer6_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_18 [label="Layer6_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_19 [label="Layer6_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_16 [label="Layer6_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_17 [label="Layer6_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_18 [label="Layer6_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_19 [label="Layer6_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_4 [label="Layer6_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_20 [label="Layer6_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_21 [label="Layer6_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_22 [label="Layer6_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_23 [label="Layer6_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_20 [label="Layer6_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_21 [label="Layer6_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_22 [label="Layer6_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_23 [label="Layer6_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_20 [label="Layer6_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_21 [label="Layer6_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_22 [label="Layer6_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_23 [label="Layer6_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_5 [label="Layer6_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_24 [label="Layer6_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_25 [label="Layer6_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_26 [label="Layer6_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_27 [label="Layer6_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_24 [label="Layer6_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_25 [label="Layer6_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_26 [label="Layer6_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_27 [label="Layer6_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_24 [label="Layer6_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_25 [label="Layer6_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_26 [label="Layer6_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_27 [label="Layer6_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_6 [label="Layer6_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_28 [label="Layer6_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_29 [label="Layer6_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_30 [label="Layer6_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_31 [label="Layer6_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_28 [label="Layer6_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_29 [label="Layer6_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_30 [label="Layer6_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_31 [label="Layer6_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_28 [label="Layer6_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_29 [label="Layer6_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_30 [label="Layer6_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_31 [label="Layer6_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_7 [label="Layer6_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_32 [label="Layer6_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_33 [label="Layer6_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_34 [label="Layer6_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_35 [label="Layer6_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_32 [label="Layer6_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_33 [label="Layer6_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_34 [label="Layer6_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_35 [label="Layer6_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_32 [label="Layer6_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_33 [label="Layer6_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_34 [label="Layer6_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_35 [label="Layer6_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_8 [label="Layer6_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_36 [label="Layer6_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_37 [label="Layer6_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_38 [label="Layer6_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_39 [label="Layer6_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_36 [label="Layer6_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_37 [label="Layer6_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_38 [label="Layer6_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_39 [label="Layer6_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_36 [label="Layer6_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_37 [label="Layer6_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_38 [label="Layer6_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_39 [label="Layer6_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_9 [label="Layer6_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_40 [label="Layer6_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_41 [label="Layer6_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_42 [label="Layer6_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_43 [label="Layer6_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_40 [label="Layer6_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_41 [label="Layer6_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_42 [label="Layer6_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_43 [label="Layer6_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_40 [label="Layer6_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_41 [label="Layer6_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_42 [label="Layer6_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_43 [label="Layer6_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_10 [label="Layer6_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_44 [label="Layer6_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_45 [label="Layer6_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_46 [label="Layer6_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_47 [label="Layer6_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_44 [label="Layer6_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_45 [label="Layer6_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_46 [label="Layer6_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_47 [label="Layer6_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_44 [label="Layer6_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_45 [label="Layer6_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_46 [label="Layer6_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_47 [label="Layer6_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_11 [label="Layer6_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_48 [label="Layer6_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_49 [label="Layer6_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_50 [label="Layer6_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_51 [label="Layer6_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_48 [label="Layer6_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_49 [label="Layer6_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_50 [label="Layer6_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_51 [label="Layer6_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_48 [label="Layer6_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_49 [label="Layer6_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_50 [label="Layer6_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_51 [label="Layer6_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_12 [label="Layer6_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_52 [label="Layer6_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_53 [label="Layer6_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_54 [label="Layer6_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_55 [label="Layer6_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_52 [label="Layer6_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_53 [label="Layer6_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_54 [label="Layer6_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_55 [label="Layer6_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_52 [label="Layer6_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_53 [label="Layer6_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_54 [label="Layer6_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_55 [label="Layer6_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_13 [label="Layer6_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_56 [label="Layer6_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_57 [label="Layer6_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_58 [label="Layer6_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_59 [label="Layer6_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_56 [label="Layer6_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_57 [label="Layer6_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_58 [label="Layer6_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_59 [label="Layer6_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_56 [label="Layer6_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_57 [label="Layer6_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_58 [label="Layer6_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_59 [label="Layer6_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_14 [label="Layer6_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert1_60 [label="Layer6_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_61 [label="Layer6_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_62 [label="Layer6_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert1_63 [label="Layer6_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_60 [label="Layer6_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_61 [label="Layer6_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_62 [label="Layer6_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert_act_63 [label="Layer6_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_6_expert2_60 [label="Layer6_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_61 [label="Layer6_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_62 [label="Layer6_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert2_63 [label="Layer6_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_6_expert_allreduce_15 [label="Layer6_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_6_expert_agg [label="Layer6_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_6_norm [label="Layer6_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_6_attn_allreduce_0 -> layer_6_gate_0 [style=dashed]
	layer_6_gate_0 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_0
	layer_6_expert1_0 -> layer_6_expert_act_0
	layer_6_expert_act_0 -> layer_6_expert2_0
	layer_6_expert2_0 -> layer_6_expert_allreduce_0
	layer_6_alltoall -> layer_6_expert1_1
	layer_6_expert1_1 -> layer_6_expert_act_1
	layer_6_expert_act_1 -> layer_6_expert2_1
	layer_6_expert2_1 -> layer_6_expert_allreduce_0
	layer_6_alltoall -> layer_6_expert1_2
	layer_6_expert1_2 -> layer_6_expert_act_2
	layer_6_expert_act_2 -> layer_6_expert2_2
	layer_6_expert2_2 -> layer_6_expert_allreduce_0
	layer_6_alltoall -> layer_6_expert1_3
	layer_6_expert1_3 -> layer_6_expert_act_3
	layer_6_expert_act_3 -> layer_6_expert2_3
	layer_6_expert2_3 -> layer_6_expert_allreduce_0
	layer_6_expert_allreduce_0 -> layer_6_expert_agg
	layer_6_attn_allreduce_1 -> layer_6_gate_4 [style=dashed]
	layer_6_gate_4 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_4
	layer_6_expert1_4 -> layer_6_expert_act_4
	layer_6_expert_act_4 -> layer_6_expert2_4
	layer_6_expert2_4 -> layer_6_expert_allreduce_1
	layer_6_alltoall -> layer_6_expert1_5
	layer_6_expert1_5 -> layer_6_expert_act_5
	layer_6_expert_act_5 -> layer_6_expert2_5
	layer_6_expert2_5 -> layer_6_expert_allreduce_1
	layer_6_alltoall -> layer_6_expert1_6
	layer_6_expert1_6 -> layer_6_expert_act_6
	layer_6_expert_act_6 -> layer_6_expert2_6
	layer_6_expert2_6 -> layer_6_expert_allreduce_1
	layer_6_alltoall -> layer_6_expert1_7
	layer_6_expert1_7 -> layer_6_expert_act_7
	layer_6_expert_act_7 -> layer_6_expert2_7
	layer_6_expert2_7 -> layer_6_expert_allreduce_1
	layer_6_expert_allreduce_1 -> layer_6_expert_agg
	layer_6_attn_allreduce_2 -> layer_6_gate_8 [style=dashed]
	layer_6_gate_8 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_8
	layer_6_expert1_8 -> layer_6_expert_act_8
	layer_6_expert_act_8 -> layer_6_expert2_8
	layer_6_expert2_8 -> layer_6_expert_allreduce_2
	layer_6_alltoall -> layer_6_expert1_9
	layer_6_expert1_9 -> layer_6_expert_act_9
	layer_6_expert_act_9 -> layer_6_expert2_9
	layer_6_expert2_9 -> layer_6_expert_allreduce_2
	layer_6_alltoall -> layer_6_expert1_10
	layer_6_expert1_10 -> layer_6_expert_act_10
	layer_6_expert_act_10 -> layer_6_expert2_10
	layer_6_expert2_10 -> layer_6_expert_allreduce_2
	layer_6_alltoall -> layer_6_expert1_11
	layer_6_expert1_11 -> layer_6_expert_act_11
	layer_6_expert_act_11 -> layer_6_expert2_11
	layer_6_expert2_11 -> layer_6_expert_allreduce_2
	layer_6_expert_allreduce_2 -> layer_6_expert_agg
	layer_6_attn_allreduce_3 -> layer_6_gate_12 [style=dashed]
	layer_6_gate_12 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_12
	layer_6_expert1_12 -> layer_6_expert_act_12
	layer_6_expert_act_12 -> layer_6_expert2_12
	layer_6_expert2_12 -> layer_6_expert_allreduce_3
	layer_6_alltoall -> layer_6_expert1_13
	layer_6_expert1_13 -> layer_6_expert_act_13
	layer_6_expert_act_13 -> layer_6_expert2_13
	layer_6_expert2_13 -> layer_6_expert_allreduce_3
	layer_6_alltoall -> layer_6_expert1_14
	layer_6_expert1_14 -> layer_6_expert_act_14
	layer_6_expert_act_14 -> layer_6_expert2_14
	layer_6_expert2_14 -> layer_6_expert_allreduce_3
	layer_6_alltoall -> layer_6_expert1_15
	layer_6_expert1_15 -> layer_6_expert_act_15
	layer_6_expert_act_15 -> layer_6_expert2_15
	layer_6_expert2_15 -> layer_6_expert_allreduce_3
	layer_6_expert_allreduce_3 -> layer_6_expert_agg
	layer_6_attn_allreduce_4 -> layer_6_gate_16 [style=dashed]
	layer_6_gate_16 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_16
	layer_6_expert1_16 -> layer_6_expert_act_16
	layer_6_expert_act_16 -> layer_6_expert2_16
	layer_6_expert2_16 -> layer_6_expert_allreduce_4
	layer_6_alltoall -> layer_6_expert1_17
	layer_6_expert1_17 -> layer_6_expert_act_17
	layer_6_expert_act_17 -> layer_6_expert2_17
	layer_6_expert2_17 -> layer_6_expert_allreduce_4
	layer_6_alltoall -> layer_6_expert1_18
	layer_6_expert1_18 -> layer_6_expert_act_18
	layer_6_expert_act_18 -> layer_6_expert2_18
	layer_6_expert2_18 -> layer_6_expert_allreduce_4
	layer_6_alltoall -> layer_6_expert1_19
	layer_6_expert1_19 -> layer_6_expert_act_19
	layer_6_expert_act_19 -> layer_6_expert2_19
	layer_6_expert2_19 -> layer_6_expert_allreduce_4
	layer_6_expert_allreduce_4 -> layer_6_expert_agg
	layer_6_attn_allreduce_5 -> layer_6_gate_20 [style=dashed]
	layer_6_gate_20 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_20
	layer_6_expert1_20 -> layer_6_expert_act_20
	layer_6_expert_act_20 -> layer_6_expert2_20
	layer_6_expert2_20 -> layer_6_expert_allreduce_5
	layer_6_alltoall -> layer_6_expert1_21
	layer_6_expert1_21 -> layer_6_expert_act_21
	layer_6_expert_act_21 -> layer_6_expert2_21
	layer_6_expert2_21 -> layer_6_expert_allreduce_5
	layer_6_alltoall -> layer_6_expert1_22
	layer_6_expert1_22 -> layer_6_expert_act_22
	layer_6_expert_act_22 -> layer_6_expert2_22
	layer_6_expert2_22 -> layer_6_expert_allreduce_5
	layer_6_alltoall -> layer_6_expert1_23
	layer_6_expert1_23 -> layer_6_expert_act_23
	layer_6_expert_act_23 -> layer_6_expert2_23
	layer_6_expert2_23 -> layer_6_expert_allreduce_5
	layer_6_expert_allreduce_5 -> layer_6_expert_agg
	layer_6_attn_allreduce_6 -> layer_6_gate_24 [style=dashed]
	layer_6_gate_24 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_24
	layer_6_expert1_24 -> layer_6_expert_act_24
	layer_6_expert_act_24 -> layer_6_expert2_24
	layer_6_expert2_24 -> layer_6_expert_allreduce_6
	layer_6_alltoall -> layer_6_expert1_25
	layer_6_expert1_25 -> layer_6_expert_act_25
	layer_6_expert_act_25 -> layer_6_expert2_25
	layer_6_expert2_25 -> layer_6_expert_allreduce_6
	layer_6_alltoall -> layer_6_expert1_26
	layer_6_expert1_26 -> layer_6_expert_act_26
	layer_6_expert_act_26 -> layer_6_expert2_26
	layer_6_expert2_26 -> layer_6_expert_allreduce_6
	layer_6_alltoall -> layer_6_expert1_27
	layer_6_expert1_27 -> layer_6_expert_act_27
	layer_6_expert_act_27 -> layer_6_expert2_27
	layer_6_expert2_27 -> layer_6_expert_allreduce_6
	layer_6_expert_allreduce_6 -> layer_6_expert_agg
	layer_6_attn_allreduce_7 -> layer_6_gate_28 [style=dashed]
	layer_6_gate_28 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_28
	layer_6_expert1_28 -> layer_6_expert_act_28
	layer_6_expert_act_28 -> layer_6_expert2_28
	layer_6_expert2_28 -> layer_6_expert_allreduce_7
	layer_6_alltoall -> layer_6_expert1_29
	layer_6_expert1_29 -> layer_6_expert_act_29
	layer_6_expert_act_29 -> layer_6_expert2_29
	layer_6_expert2_29 -> layer_6_expert_allreduce_7
	layer_6_alltoall -> layer_6_expert1_30
	layer_6_expert1_30 -> layer_6_expert_act_30
	layer_6_expert_act_30 -> layer_6_expert2_30
	layer_6_expert2_30 -> layer_6_expert_allreduce_7
	layer_6_alltoall -> layer_6_expert1_31
	layer_6_expert1_31 -> layer_6_expert_act_31
	layer_6_expert_act_31 -> layer_6_expert2_31
	layer_6_expert2_31 -> layer_6_expert_allreduce_7
	layer_6_expert_allreduce_7 -> layer_6_expert_agg
	layer_6_attn_allreduce_8 -> layer_6_gate_32 [style=dashed]
	layer_6_gate_32 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_32
	layer_6_expert1_32 -> layer_6_expert_act_32
	layer_6_expert_act_32 -> layer_6_expert2_32
	layer_6_expert2_32 -> layer_6_expert_allreduce_8
	layer_6_alltoall -> layer_6_expert1_33
	layer_6_expert1_33 -> layer_6_expert_act_33
	layer_6_expert_act_33 -> layer_6_expert2_33
	layer_6_expert2_33 -> layer_6_expert_allreduce_8
	layer_6_alltoall -> layer_6_expert1_34
	layer_6_expert1_34 -> layer_6_expert_act_34
	layer_6_expert_act_34 -> layer_6_expert2_34
	layer_6_expert2_34 -> layer_6_expert_allreduce_8
	layer_6_alltoall -> layer_6_expert1_35
	layer_6_expert1_35 -> layer_6_expert_act_35
	layer_6_expert_act_35 -> layer_6_expert2_35
	layer_6_expert2_35 -> layer_6_expert_allreduce_8
	layer_6_expert_allreduce_8 -> layer_6_expert_agg
	layer_6_attn_allreduce_9 -> layer_6_gate_36 [style=dashed]
	layer_6_gate_36 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_36
	layer_6_expert1_36 -> layer_6_expert_act_36
	layer_6_expert_act_36 -> layer_6_expert2_36
	layer_6_expert2_36 -> layer_6_expert_allreduce_9
	layer_6_alltoall -> layer_6_expert1_37
	layer_6_expert1_37 -> layer_6_expert_act_37
	layer_6_expert_act_37 -> layer_6_expert2_37
	layer_6_expert2_37 -> layer_6_expert_allreduce_9
	layer_6_alltoall -> layer_6_expert1_38
	layer_6_expert1_38 -> layer_6_expert_act_38
	layer_6_expert_act_38 -> layer_6_expert2_38
	layer_6_expert2_38 -> layer_6_expert_allreduce_9
	layer_6_alltoall -> layer_6_expert1_39
	layer_6_expert1_39 -> layer_6_expert_act_39
	layer_6_expert_act_39 -> layer_6_expert2_39
	layer_6_expert2_39 -> layer_6_expert_allreduce_9
	layer_6_expert_allreduce_9 -> layer_6_expert_agg
	layer_6_attn_allreduce_10 -> layer_6_gate_40 [style=dashed]
	layer_6_gate_40 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_40
	layer_6_expert1_40 -> layer_6_expert_act_40
	layer_6_expert_act_40 -> layer_6_expert2_40
	layer_6_expert2_40 -> layer_6_expert_allreduce_10
	layer_6_alltoall -> layer_6_expert1_41
	layer_6_expert1_41 -> layer_6_expert_act_41
	layer_6_expert_act_41 -> layer_6_expert2_41
	layer_6_expert2_41 -> layer_6_expert_allreduce_10
	layer_6_alltoall -> layer_6_expert1_42
	layer_6_expert1_42 -> layer_6_expert_act_42
	layer_6_expert_act_42 -> layer_6_expert2_42
	layer_6_expert2_42 -> layer_6_expert_allreduce_10
	layer_6_alltoall -> layer_6_expert1_43
	layer_6_expert1_43 -> layer_6_expert_act_43
	layer_6_expert_act_43 -> layer_6_expert2_43
	layer_6_expert2_43 -> layer_6_expert_allreduce_10
	layer_6_expert_allreduce_10 -> layer_6_expert_agg
	layer_6_attn_allreduce_11 -> layer_6_gate_44 [style=dashed]
	layer_6_gate_44 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_44
	layer_6_expert1_44 -> layer_6_expert_act_44
	layer_6_expert_act_44 -> layer_6_expert2_44
	layer_6_expert2_44 -> layer_6_expert_allreduce_11
	layer_6_alltoall -> layer_6_expert1_45
	layer_6_expert1_45 -> layer_6_expert_act_45
	layer_6_expert_act_45 -> layer_6_expert2_45
	layer_6_expert2_45 -> layer_6_expert_allreduce_11
	layer_6_alltoall -> layer_6_expert1_46
	layer_6_expert1_46 -> layer_6_expert_act_46
	layer_6_expert_act_46 -> layer_6_expert2_46
	layer_6_expert2_46 -> layer_6_expert_allreduce_11
	layer_6_alltoall -> layer_6_expert1_47
	layer_6_expert1_47 -> layer_6_expert_act_47
	layer_6_expert_act_47 -> layer_6_expert2_47
	layer_6_expert2_47 -> layer_6_expert_allreduce_11
	layer_6_expert_allreduce_11 -> layer_6_expert_agg
	layer_6_attn_allreduce_12 -> layer_6_gate_48 [style=dashed]
	layer_6_gate_48 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_48
	layer_6_expert1_48 -> layer_6_expert_act_48
	layer_6_expert_act_48 -> layer_6_expert2_48
	layer_6_expert2_48 -> layer_6_expert_allreduce_12
	layer_6_alltoall -> layer_6_expert1_49
	layer_6_expert1_49 -> layer_6_expert_act_49
	layer_6_expert_act_49 -> layer_6_expert2_49
	layer_6_expert2_49 -> layer_6_expert_allreduce_12
	layer_6_alltoall -> layer_6_expert1_50
	layer_6_expert1_50 -> layer_6_expert_act_50
	layer_6_expert_act_50 -> layer_6_expert2_50
	layer_6_expert2_50 -> layer_6_expert_allreduce_12
	layer_6_alltoall -> layer_6_expert1_51
	layer_6_expert1_51 -> layer_6_expert_act_51
	layer_6_expert_act_51 -> layer_6_expert2_51
	layer_6_expert2_51 -> layer_6_expert_allreduce_12
	layer_6_expert_allreduce_12 -> layer_6_expert_agg
	layer_6_attn_allreduce_13 -> layer_6_gate_52 [style=dashed]
	layer_6_gate_52 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_52
	layer_6_expert1_52 -> layer_6_expert_act_52
	layer_6_expert_act_52 -> layer_6_expert2_52
	layer_6_expert2_52 -> layer_6_expert_allreduce_13
	layer_6_alltoall -> layer_6_expert1_53
	layer_6_expert1_53 -> layer_6_expert_act_53
	layer_6_expert_act_53 -> layer_6_expert2_53
	layer_6_expert2_53 -> layer_6_expert_allreduce_13
	layer_6_alltoall -> layer_6_expert1_54
	layer_6_expert1_54 -> layer_6_expert_act_54
	layer_6_expert_act_54 -> layer_6_expert2_54
	layer_6_expert2_54 -> layer_6_expert_allreduce_13
	layer_6_alltoall -> layer_6_expert1_55
	layer_6_expert1_55 -> layer_6_expert_act_55
	layer_6_expert_act_55 -> layer_6_expert2_55
	layer_6_expert2_55 -> layer_6_expert_allreduce_13
	layer_6_expert_allreduce_13 -> layer_6_expert_agg
	layer_6_attn_allreduce_14 -> layer_6_gate_56 [style=dashed]
	layer_6_gate_56 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_56
	layer_6_expert1_56 -> layer_6_expert_act_56
	layer_6_expert_act_56 -> layer_6_expert2_56
	layer_6_expert2_56 -> layer_6_expert_allreduce_14
	layer_6_alltoall -> layer_6_expert1_57
	layer_6_expert1_57 -> layer_6_expert_act_57
	layer_6_expert_act_57 -> layer_6_expert2_57
	layer_6_expert2_57 -> layer_6_expert_allreduce_14
	layer_6_alltoall -> layer_6_expert1_58
	layer_6_expert1_58 -> layer_6_expert_act_58
	layer_6_expert_act_58 -> layer_6_expert2_58
	layer_6_expert2_58 -> layer_6_expert_allreduce_14
	layer_6_alltoall -> layer_6_expert1_59
	layer_6_expert1_59 -> layer_6_expert_act_59
	layer_6_expert_act_59 -> layer_6_expert2_59
	layer_6_expert2_59 -> layer_6_expert_allreduce_14
	layer_6_expert_allreduce_14 -> layer_6_expert_agg
	layer_6_attn_allreduce_15 -> layer_6_gate_60 [style=dashed]
	layer_6_gate_60 -> layer_6_alltoall [style=dashed]
	layer_6_alltoall -> layer_6_expert1_60
	layer_6_expert1_60 -> layer_6_expert_act_60
	layer_6_expert_act_60 -> layer_6_expert2_60
	layer_6_expert2_60 -> layer_6_expert_allreduce_15
	layer_6_alltoall -> layer_6_expert1_61
	layer_6_expert1_61 -> layer_6_expert_act_61
	layer_6_expert_act_61 -> layer_6_expert2_61
	layer_6_expert2_61 -> layer_6_expert_allreduce_15
	layer_6_alltoall -> layer_6_expert1_62
	layer_6_expert1_62 -> layer_6_expert_act_62
	layer_6_expert_act_62 -> layer_6_expert2_62
	layer_6_expert2_62 -> layer_6_expert_allreduce_15
	layer_6_alltoall -> layer_6_expert1_63
	layer_6_expert1_63 -> layer_6_expert_act_63
	layer_6_expert_act_63 -> layer_6_expert2_63
	layer_6_expert2_63 -> layer_6_expert_allreduce_15
	layer_6_expert_allreduce_15 -> layer_6_expert_agg
	layer_6_expert_agg -> layer_6_norm
	layer_6_norm -> layer_7_input
	layer_7_input [label="Layer7_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_7_qkv_0 [label="Layer7_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_1 [label="Layer7_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_2 [label="Layer7_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_3 [label="Layer7_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_0 [label="Layer7_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_1 [label="Layer7_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_2 [label="Layer7_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_3 [label="Layer7_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_0 [label="Layer7_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_1 [label="Layer7_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_2 [label="Layer7_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_3 [label="Layer7_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_0 [label="Layer7_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_1 [label="Layer7_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_2 [label="Layer7_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_3 [label="Layer7_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_0 [label="Layer7_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_0
	layer_7_qkv_0 -> layer_7_attn_scores_0
	layer_7_attn_scores_0 -> layer_7_attn_softmax_0
	layer_7_attn_softmax_0 -> layer_7_attn_out_0
	layer_7_attn_out_0 -> layer_7_attn_allreduce_0
	layer_7_input -> layer_7_qkv_1
	layer_7_qkv_1 -> layer_7_attn_scores_1
	layer_7_attn_scores_1 -> layer_7_attn_softmax_1
	layer_7_attn_softmax_1 -> layer_7_attn_out_1
	layer_7_attn_out_1 -> layer_7_attn_allreduce_0
	layer_7_input -> layer_7_qkv_2
	layer_7_qkv_2 -> layer_7_attn_scores_2
	layer_7_attn_scores_2 -> layer_7_attn_softmax_2
	layer_7_attn_softmax_2 -> layer_7_attn_out_2
	layer_7_attn_out_2 -> layer_7_attn_allreduce_0
	layer_7_input -> layer_7_qkv_3
	layer_7_qkv_3 -> layer_7_attn_scores_3
	layer_7_attn_scores_3 -> layer_7_attn_softmax_3
	layer_7_attn_softmax_3 -> layer_7_attn_out_3
	layer_7_attn_out_3 -> layer_7_attn_allreduce_0
	layer_7_qkv_4 [label="Layer7_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_5 [label="Layer7_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_6 [label="Layer7_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_7 [label="Layer7_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_4 [label="Layer7_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_5 [label="Layer7_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_6 [label="Layer7_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_7 [label="Layer7_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_4 [label="Layer7_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_5 [label="Layer7_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_6 [label="Layer7_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_7 [label="Layer7_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_4 [label="Layer7_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_5 [label="Layer7_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_6 [label="Layer7_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_7 [label="Layer7_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_1 [label="Layer7_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_4
	layer_7_qkv_4 -> layer_7_attn_scores_4
	layer_7_attn_scores_4 -> layer_7_attn_softmax_4
	layer_7_attn_softmax_4 -> layer_7_attn_out_4
	layer_7_attn_out_4 -> layer_7_attn_allreduce_1
	layer_7_input -> layer_7_qkv_5
	layer_7_qkv_5 -> layer_7_attn_scores_5
	layer_7_attn_scores_5 -> layer_7_attn_softmax_5
	layer_7_attn_softmax_5 -> layer_7_attn_out_5
	layer_7_attn_out_5 -> layer_7_attn_allreduce_1
	layer_7_input -> layer_7_qkv_6
	layer_7_qkv_6 -> layer_7_attn_scores_6
	layer_7_attn_scores_6 -> layer_7_attn_softmax_6
	layer_7_attn_softmax_6 -> layer_7_attn_out_6
	layer_7_attn_out_6 -> layer_7_attn_allreduce_1
	layer_7_input -> layer_7_qkv_7
	layer_7_qkv_7 -> layer_7_attn_scores_7
	layer_7_attn_scores_7 -> layer_7_attn_softmax_7
	layer_7_attn_softmax_7 -> layer_7_attn_out_7
	layer_7_attn_out_7 -> layer_7_attn_allreduce_1
	layer_7_qkv_8 [label="Layer7_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_9 [label="Layer7_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_10 [label="Layer7_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_11 [label="Layer7_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_8 [label="Layer7_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_9 [label="Layer7_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_10 [label="Layer7_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_11 [label="Layer7_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_8 [label="Layer7_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_9 [label="Layer7_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_10 [label="Layer7_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_11 [label="Layer7_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_8 [label="Layer7_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_9 [label="Layer7_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_10 [label="Layer7_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_11 [label="Layer7_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_2 [label="Layer7_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_8
	layer_7_qkv_8 -> layer_7_attn_scores_8
	layer_7_attn_scores_8 -> layer_7_attn_softmax_8
	layer_7_attn_softmax_8 -> layer_7_attn_out_8
	layer_7_attn_out_8 -> layer_7_attn_allreduce_2
	layer_7_input -> layer_7_qkv_9
	layer_7_qkv_9 -> layer_7_attn_scores_9
	layer_7_attn_scores_9 -> layer_7_attn_softmax_9
	layer_7_attn_softmax_9 -> layer_7_attn_out_9
	layer_7_attn_out_9 -> layer_7_attn_allreduce_2
	layer_7_input -> layer_7_qkv_10
	layer_7_qkv_10 -> layer_7_attn_scores_10
	layer_7_attn_scores_10 -> layer_7_attn_softmax_10
	layer_7_attn_softmax_10 -> layer_7_attn_out_10
	layer_7_attn_out_10 -> layer_7_attn_allreduce_2
	layer_7_input -> layer_7_qkv_11
	layer_7_qkv_11 -> layer_7_attn_scores_11
	layer_7_attn_scores_11 -> layer_7_attn_softmax_11
	layer_7_attn_softmax_11 -> layer_7_attn_out_11
	layer_7_attn_out_11 -> layer_7_attn_allreduce_2
	layer_7_qkv_12 [label="Layer7_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_13 [label="Layer7_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_14 [label="Layer7_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_15 [label="Layer7_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_12 [label="Layer7_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_13 [label="Layer7_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_14 [label="Layer7_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_15 [label="Layer7_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_12 [label="Layer7_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_13 [label="Layer7_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_14 [label="Layer7_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_15 [label="Layer7_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_12 [label="Layer7_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_13 [label="Layer7_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_14 [label="Layer7_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_15 [label="Layer7_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_3 [label="Layer7_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_12
	layer_7_qkv_12 -> layer_7_attn_scores_12
	layer_7_attn_scores_12 -> layer_7_attn_softmax_12
	layer_7_attn_softmax_12 -> layer_7_attn_out_12
	layer_7_attn_out_12 -> layer_7_attn_allreduce_3
	layer_7_input -> layer_7_qkv_13
	layer_7_qkv_13 -> layer_7_attn_scores_13
	layer_7_attn_scores_13 -> layer_7_attn_softmax_13
	layer_7_attn_softmax_13 -> layer_7_attn_out_13
	layer_7_attn_out_13 -> layer_7_attn_allreduce_3
	layer_7_input -> layer_7_qkv_14
	layer_7_qkv_14 -> layer_7_attn_scores_14
	layer_7_attn_scores_14 -> layer_7_attn_softmax_14
	layer_7_attn_softmax_14 -> layer_7_attn_out_14
	layer_7_attn_out_14 -> layer_7_attn_allreduce_3
	layer_7_input -> layer_7_qkv_15
	layer_7_qkv_15 -> layer_7_attn_scores_15
	layer_7_attn_scores_15 -> layer_7_attn_softmax_15
	layer_7_attn_softmax_15 -> layer_7_attn_out_15
	layer_7_attn_out_15 -> layer_7_attn_allreduce_3
	layer_7_qkv_16 [label="Layer7_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_17 [label="Layer7_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_18 [label="Layer7_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_19 [label="Layer7_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_16 [label="Layer7_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_17 [label="Layer7_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_18 [label="Layer7_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_19 [label="Layer7_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_16 [label="Layer7_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_17 [label="Layer7_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_18 [label="Layer7_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_19 [label="Layer7_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_16 [label="Layer7_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_17 [label="Layer7_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_18 [label="Layer7_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_19 [label="Layer7_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_4 [label="Layer7_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_16
	layer_7_qkv_16 -> layer_7_attn_scores_16
	layer_7_attn_scores_16 -> layer_7_attn_softmax_16
	layer_7_attn_softmax_16 -> layer_7_attn_out_16
	layer_7_attn_out_16 -> layer_7_attn_allreduce_4
	layer_7_input -> layer_7_qkv_17
	layer_7_qkv_17 -> layer_7_attn_scores_17
	layer_7_attn_scores_17 -> layer_7_attn_softmax_17
	layer_7_attn_softmax_17 -> layer_7_attn_out_17
	layer_7_attn_out_17 -> layer_7_attn_allreduce_4
	layer_7_input -> layer_7_qkv_18
	layer_7_qkv_18 -> layer_7_attn_scores_18
	layer_7_attn_scores_18 -> layer_7_attn_softmax_18
	layer_7_attn_softmax_18 -> layer_7_attn_out_18
	layer_7_attn_out_18 -> layer_7_attn_allreduce_4
	layer_7_input -> layer_7_qkv_19
	layer_7_qkv_19 -> layer_7_attn_scores_19
	layer_7_attn_scores_19 -> layer_7_attn_softmax_19
	layer_7_attn_softmax_19 -> layer_7_attn_out_19
	layer_7_attn_out_19 -> layer_7_attn_allreduce_4
	layer_7_qkv_20 [label="Layer7_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_21 [label="Layer7_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_22 [label="Layer7_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_23 [label="Layer7_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_20 [label="Layer7_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_21 [label="Layer7_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_22 [label="Layer7_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_23 [label="Layer7_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_20 [label="Layer7_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_21 [label="Layer7_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_22 [label="Layer7_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_23 [label="Layer7_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_20 [label="Layer7_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_21 [label="Layer7_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_22 [label="Layer7_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_23 [label="Layer7_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_5 [label="Layer7_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_20
	layer_7_qkv_20 -> layer_7_attn_scores_20
	layer_7_attn_scores_20 -> layer_7_attn_softmax_20
	layer_7_attn_softmax_20 -> layer_7_attn_out_20
	layer_7_attn_out_20 -> layer_7_attn_allreduce_5
	layer_7_input -> layer_7_qkv_21
	layer_7_qkv_21 -> layer_7_attn_scores_21
	layer_7_attn_scores_21 -> layer_7_attn_softmax_21
	layer_7_attn_softmax_21 -> layer_7_attn_out_21
	layer_7_attn_out_21 -> layer_7_attn_allreduce_5
	layer_7_input -> layer_7_qkv_22
	layer_7_qkv_22 -> layer_7_attn_scores_22
	layer_7_attn_scores_22 -> layer_7_attn_softmax_22
	layer_7_attn_softmax_22 -> layer_7_attn_out_22
	layer_7_attn_out_22 -> layer_7_attn_allreduce_5
	layer_7_input -> layer_7_qkv_23
	layer_7_qkv_23 -> layer_7_attn_scores_23
	layer_7_attn_scores_23 -> layer_7_attn_softmax_23
	layer_7_attn_softmax_23 -> layer_7_attn_out_23
	layer_7_attn_out_23 -> layer_7_attn_allreduce_5
	layer_7_qkv_24 [label="Layer7_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_25 [label="Layer7_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_26 [label="Layer7_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_27 [label="Layer7_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_24 [label="Layer7_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_25 [label="Layer7_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_26 [label="Layer7_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_27 [label="Layer7_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_24 [label="Layer7_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_25 [label="Layer7_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_26 [label="Layer7_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_27 [label="Layer7_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_24 [label="Layer7_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_25 [label="Layer7_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_26 [label="Layer7_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_27 [label="Layer7_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_6 [label="Layer7_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_24
	layer_7_qkv_24 -> layer_7_attn_scores_24
	layer_7_attn_scores_24 -> layer_7_attn_softmax_24
	layer_7_attn_softmax_24 -> layer_7_attn_out_24
	layer_7_attn_out_24 -> layer_7_attn_allreduce_6
	layer_7_input -> layer_7_qkv_25
	layer_7_qkv_25 -> layer_7_attn_scores_25
	layer_7_attn_scores_25 -> layer_7_attn_softmax_25
	layer_7_attn_softmax_25 -> layer_7_attn_out_25
	layer_7_attn_out_25 -> layer_7_attn_allreduce_6
	layer_7_input -> layer_7_qkv_26
	layer_7_qkv_26 -> layer_7_attn_scores_26
	layer_7_attn_scores_26 -> layer_7_attn_softmax_26
	layer_7_attn_softmax_26 -> layer_7_attn_out_26
	layer_7_attn_out_26 -> layer_7_attn_allreduce_6
	layer_7_input -> layer_7_qkv_27
	layer_7_qkv_27 -> layer_7_attn_scores_27
	layer_7_attn_scores_27 -> layer_7_attn_softmax_27
	layer_7_attn_softmax_27 -> layer_7_attn_out_27
	layer_7_attn_out_27 -> layer_7_attn_allreduce_6
	layer_7_qkv_28 [label="Layer7_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_29 [label="Layer7_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_30 [label="Layer7_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_31 [label="Layer7_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_28 [label="Layer7_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_29 [label="Layer7_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_30 [label="Layer7_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_31 [label="Layer7_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_28 [label="Layer7_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_29 [label="Layer7_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_30 [label="Layer7_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_31 [label="Layer7_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_28 [label="Layer7_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_29 [label="Layer7_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_30 [label="Layer7_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_31 [label="Layer7_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_7 [label="Layer7_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_28
	layer_7_qkv_28 -> layer_7_attn_scores_28
	layer_7_attn_scores_28 -> layer_7_attn_softmax_28
	layer_7_attn_softmax_28 -> layer_7_attn_out_28
	layer_7_attn_out_28 -> layer_7_attn_allreduce_7
	layer_7_input -> layer_7_qkv_29
	layer_7_qkv_29 -> layer_7_attn_scores_29
	layer_7_attn_scores_29 -> layer_7_attn_softmax_29
	layer_7_attn_softmax_29 -> layer_7_attn_out_29
	layer_7_attn_out_29 -> layer_7_attn_allreduce_7
	layer_7_input -> layer_7_qkv_30
	layer_7_qkv_30 -> layer_7_attn_scores_30
	layer_7_attn_scores_30 -> layer_7_attn_softmax_30
	layer_7_attn_softmax_30 -> layer_7_attn_out_30
	layer_7_attn_out_30 -> layer_7_attn_allreduce_7
	layer_7_input -> layer_7_qkv_31
	layer_7_qkv_31 -> layer_7_attn_scores_31
	layer_7_attn_scores_31 -> layer_7_attn_softmax_31
	layer_7_attn_softmax_31 -> layer_7_attn_out_31
	layer_7_attn_out_31 -> layer_7_attn_allreduce_7
	layer_7_qkv_32 [label="Layer7_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_33 [label="Layer7_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_34 [label="Layer7_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_35 [label="Layer7_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_32 [label="Layer7_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_33 [label="Layer7_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_34 [label="Layer7_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_35 [label="Layer7_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_32 [label="Layer7_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_33 [label="Layer7_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_34 [label="Layer7_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_35 [label="Layer7_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_32 [label="Layer7_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_33 [label="Layer7_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_34 [label="Layer7_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_35 [label="Layer7_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_8 [label="Layer7_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_32
	layer_7_qkv_32 -> layer_7_attn_scores_32
	layer_7_attn_scores_32 -> layer_7_attn_softmax_32
	layer_7_attn_softmax_32 -> layer_7_attn_out_32
	layer_7_attn_out_32 -> layer_7_attn_allreduce_8
	layer_7_input -> layer_7_qkv_33
	layer_7_qkv_33 -> layer_7_attn_scores_33
	layer_7_attn_scores_33 -> layer_7_attn_softmax_33
	layer_7_attn_softmax_33 -> layer_7_attn_out_33
	layer_7_attn_out_33 -> layer_7_attn_allreduce_8
	layer_7_input -> layer_7_qkv_34
	layer_7_qkv_34 -> layer_7_attn_scores_34
	layer_7_attn_scores_34 -> layer_7_attn_softmax_34
	layer_7_attn_softmax_34 -> layer_7_attn_out_34
	layer_7_attn_out_34 -> layer_7_attn_allreduce_8
	layer_7_input -> layer_7_qkv_35
	layer_7_qkv_35 -> layer_7_attn_scores_35
	layer_7_attn_scores_35 -> layer_7_attn_softmax_35
	layer_7_attn_softmax_35 -> layer_7_attn_out_35
	layer_7_attn_out_35 -> layer_7_attn_allreduce_8
	layer_7_qkv_36 [label="Layer7_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_37 [label="Layer7_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_38 [label="Layer7_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_39 [label="Layer7_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_36 [label="Layer7_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_37 [label="Layer7_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_38 [label="Layer7_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_39 [label="Layer7_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_36 [label="Layer7_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_37 [label="Layer7_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_38 [label="Layer7_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_39 [label="Layer7_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_36 [label="Layer7_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_37 [label="Layer7_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_38 [label="Layer7_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_39 [label="Layer7_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_9 [label="Layer7_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_36
	layer_7_qkv_36 -> layer_7_attn_scores_36
	layer_7_attn_scores_36 -> layer_7_attn_softmax_36
	layer_7_attn_softmax_36 -> layer_7_attn_out_36
	layer_7_attn_out_36 -> layer_7_attn_allreduce_9
	layer_7_input -> layer_7_qkv_37
	layer_7_qkv_37 -> layer_7_attn_scores_37
	layer_7_attn_scores_37 -> layer_7_attn_softmax_37
	layer_7_attn_softmax_37 -> layer_7_attn_out_37
	layer_7_attn_out_37 -> layer_7_attn_allreduce_9
	layer_7_input -> layer_7_qkv_38
	layer_7_qkv_38 -> layer_7_attn_scores_38
	layer_7_attn_scores_38 -> layer_7_attn_softmax_38
	layer_7_attn_softmax_38 -> layer_7_attn_out_38
	layer_7_attn_out_38 -> layer_7_attn_allreduce_9
	layer_7_input -> layer_7_qkv_39
	layer_7_qkv_39 -> layer_7_attn_scores_39
	layer_7_attn_scores_39 -> layer_7_attn_softmax_39
	layer_7_attn_softmax_39 -> layer_7_attn_out_39
	layer_7_attn_out_39 -> layer_7_attn_allreduce_9
	layer_7_qkv_40 [label="Layer7_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_41 [label="Layer7_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_42 [label="Layer7_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_43 [label="Layer7_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_40 [label="Layer7_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_41 [label="Layer7_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_42 [label="Layer7_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_43 [label="Layer7_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_40 [label="Layer7_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_41 [label="Layer7_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_42 [label="Layer7_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_43 [label="Layer7_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_40 [label="Layer7_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_41 [label="Layer7_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_42 [label="Layer7_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_43 [label="Layer7_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_10 [label="Layer7_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_40
	layer_7_qkv_40 -> layer_7_attn_scores_40
	layer_7_attn_scores_40 -> layer_7_attn_softmax_40
	layer_7_attn_softmax_40 -> layer_7_attn_out_40
	layer_7_attn_out_40 -> layer_7_attn_allreduce_10
	layer_7_input -> layer_7_qkv_41
	layer_7_qkv_41 -> layer_7_attn_scores_41
	layer_7_attn_scores_41 -> layer_7_attn_softmax_41
	layer_7_attn_softmax_41 -> layer_7_attn_out_41
	layer_7_attn_out_41 -> layer_7_attn_allreduce_10
	layer_7_input -> layer_7_qkv_42
	layer_7_qkv_42 -> layer_7_attn_scores_42
	layer_7_attn_scores_42 -> layer_7_attn_softmax_42
	layer_7_attn_softmax_42 -> layer_7_attn_out_42
	layer_7_attn_out_42 -> layer_7_attn_allreduce_10
	layer_7_input -> layer_7_qkv_43
	layer_7_qkv_43 -> layer_7_attn_scores_43
	layer_7_attn_scores_43 -> layer_7_attn_softmax_43
	layer_7_attn_softmax_43 -> layer_7_attn_out_43
	layer_7_attn_out_43 -> layer_7_attn_allreduce_10
	layer_7_qkv_44 [label="Layer7_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_45 [label="Layer7_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_46 [label="Layer7_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_47 [label="Layer7_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_44 [label="Layer7_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_45 [label="Layer7_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_46 [label="Layer7_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_47 [label="Layer7_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_44 [label="Layer7_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_45 [label="Layer7_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_46 [label="Layer7_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_47 [label="Layer7_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_44 [label="Layer7_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_45 [label="Layer7_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_46 [label="Layer7_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_47 [label="Layer7_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_11 [label="Layer7_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_44
	layer_7_qkv_44 -> layer_7_attn_scores_44
	layer_7_attn_scores_44 -> layer_7_attn_softmax_44
	layer_7_attn_softmax_44 -> layer_7_attn_out_44
	layer_7_attn_out_44 -> layer_7_attn_allreduce_11
	layer_7_input -> layer_7_qkv_45
	layer_7_qkv_45 -> layer_7_attn_scores_45
	layer_7_attn_scores_45 -> layer_7_attn_softmax_45
	layer_7_attn_softmax_45 -> layer_7_attn_out_45
	layer_7_attn_out_45 -> layer_7_attn_allreduce_11
	layer_7_input -> layer_7_qkv_46
	layer_7_qkv_46 -> layer_7_attn_scores_46
	layer_7_attn_scores_46 -> layer_7_attn_softmax_46
	layer_7_attn_softmax_46 -> layer_7_attn_out_46
	layer_7_attn_out_46 -> layer_7_attn_allreduce_11
	layer_7_input -> layer_7_qkv_47
	layer_7_qkv_47 -> layer_7_attn_scores_47
	layer_7_attn_scores_47 -> layer_7_attn_softmax_47
	layer_7_attn_softmax_47 -> layer_7_attn_out_47
	layer_7_attn_out_47 -> layer_7_attn_allreduce_11
	layer_7_qkv_48 [label="Layer7_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_49 [label="Layer7_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_50 [label="Layer7_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_51 [label="Layer7_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_48 [label="Layer7_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_49 [label="Layer7_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_50 [label="Layer7_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_51 [label="Layer7_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_48 [label="Layer7_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_49 [label="Layer7_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_50 [label="Layer7_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_51 [label="Layer7_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_48 [label="Layer7_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_49 [label="Layer7_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_50 [label="Layer7_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_51 [label="Layer7_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_12 [label="Layer7_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_48
	layer_7_qkv_48 -> layer_7_attn_scores_48
	layer_7_attn_scores_48 -> layer_7_attn_softmax_48
	layer_7_attn_softmax_48 -> layer_7_attn_out_48
	layer_7_attn_out_48 -> layer_7_attn_allreduce_12
	layer_7_input -> layer_7_qkv_49
	layer_7_qkv_49 -> layer_7_attn_scores_49
	layer_7_attn_scores_49 -> layer_7_attn_softmax_49
	layer_7_attn_softmax_49 -> layer_7_attn_out_49
	layer_7_attn_out_49 -> layer_7_attn_allreduce_12
	layer_7_input -> layer_7_qkv_50
	layer_7_qkv_50 -> layer_7_attn_scores_50
	layer_7_attn_scores_50 -> layer_7_attn_softmax_50
	layer_7_attn_softmax_50 -> layer_7_attn_out_50
	layer_7_attn_out_50 -> layer_7_attn_allreduce_12
	layer_7_input -> layer_7_qkv_51
	layer_7_qkv_51 -> layer_7_attn_scores_51
	layer_7_attn_scores_51 -> layer_7_attn_softmax_51
	layer_7_attn_softmax_51 -> layer_7_attn_out_51
	layer_7_attn_out_51 -> layer_7_attn_allreduce_12
	layer_7_qkv_52 [label="Layer7_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_53 [label="Layer7_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_54 [label="Layer7_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_55 [label="Layer7_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_52 [label="Layer7_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_53 [label="Layer7_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_54 [label="Layer7_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_55 [label="Layer7_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_52 [label="Layer7_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_53 [label="Layer7_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_54 [label="Layer7_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_55 [label="Layer7_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_52 [label="Layer7_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_53 [label="Layer7_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_54 [label="Layer7_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_55 [label="Layer7_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_13 [label="Layer7_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_52
	layer_7_qkv_52 -> layer_7_attn_scores_52
	layer_7_attn_scores_52 -> layer_7_attn_softmax_52
	layer_7_attn_softmax_52 -> layer_7_attn_out_52
	layer_7_attn_out_52 -> layer_7_attn_allreduce_13
	layer_7_input -> layer_7_qkv_53
	layer_7_qkv_53 -> layer_7_attn_scores_53
	layer_7_attn_scores_53 -> layer_7_attn_softmax_53
	layer_7_attn_softmax_53 -> layer_7_attn_out_53
	layer_7_attn_out_53 -> layer_7_attn_allreduce_13
	layer_7_input -> layer_7_qkv_54
	layer_7_qkv_54 -> layer_7_attn_scores_54
	layer_7_attn_scores_54 -> layer_7_attn_softmax_54
	layer_7_attn_softmax_54 -> layer_7_attn_out_54
	layer_7_attn_out_54 -> layer_7_attn_allreduce_13
	layer_7_input -> layer_7_qkv_55
	layer_7_qkv_55 -> layer_7_attn_scores_55
	layer_7_attn_scores_55 -> layer_7_attn_softmax_55
	layer_7_attn_softmax_55 -> layer_7_attn_out_55
	layer_7_attn_out_55 -> layer_7_attn_allreduce_13
	layer_7_qkv_56 [label="Layer7_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_57 [label="Layer7_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_58 [label="Layer7_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_59 [label="Layer7_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_56 [label="Layer7_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_57 [label="Layer7_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_58 [label="Layer7_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_59 [label="Layer7_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_56 [label="Layer7_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_57 [label="Layer7_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_58 [label="Layer7_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_59 [label="Layer7_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_56 [label="Layer7_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_57 [label="Layer7_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_58 [label="Layer7_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_59 [label="Layer7_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_14 [label="Layer7_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_56
	layer_7_qkv_56 -> layer_7_attn_scores_56
	layer_7_attn_scores_56 -> layer_7_attn_softmax_56
	layer_7_attn_softmax_56 -> layer_7_attn_out_56
	layer_7_attn_out_56 -> layer_7_attn_allreduce_14
	layer_7_input -> layer_7_qkv_57
	layer_7_qkv_57 -> layer_7_attn_scores_57
	layer_7_attn_scores_57 -> layer_7_attn_softmax_57
	layer_7_attn_softmax_57 -> layer_7_attn_out_57
	layer_7_attn_out_57 -> layer_7_attn_allreduce_14
	layer_7_input -> layer_7_qkv_58
	layer_7_qkv_58 -> layer_7_attn_scores_58
	layer_7_attn_scores_58 -> layer_7_attn_softmax_58
	layer_7_attn_softmax_58 -> layer_7_attn_out_58
	layer_7_attn_out_58 -> layer_7_attn_allreduce_14
	layer_7_input -> layer_7_qkv_59
	layer_7_qkv_59 -> layer_7_attn_scores_59
	layer_7_attn_scores_59 -> layer_7_attn_softmax_59
	layer_7_attn_softmax_59 -> layer_7_attn_out_59
	layer_7_attn_out_59 -> layer_7_attn_allreduce_14
	layer_7_qkv_60 [label="Layer7_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_61 [label="Layer7_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_62 [label="Layer7_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_qkv_63 [label="Layer7_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_60 [label="Layer7_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_61 [label="Layer7_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_62 [label="Layer7_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_scores_63 [label="Layer7_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_60 [label="Layer7_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_61 [label="Layer7_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_62 [label="Layer7_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_softmax_63 [label="Layer7_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_60 [label="Layer7_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_61 [label="Layer7_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_62 [label="Layer7_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_out_63 [label="Layer7_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_15 [label="Layer7_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_input -> layer_7_qkv_60
	layer_7_qkv_60 -> layer_7_attn_scores_60
	layer_7_attn_scores_60 -> layer_7_attn_softmax_60
	layer_7_attn_softmax_60 -> layer_7_attn_out_60
	layer_7_attn_out_60 -> layer_7_attn_allreduce_15
	layer_7_input -> layer_7_qkv_61
	layer_7_qkv_61 -> layer_7_attn_scores_61
	layer_7_attn_scores_61 -> layer_7_attn_softmax_61
	layer_7_attn_softmax_61 -> layer_7_attn_out_61
	layer_7_attn_out_61 -> layer_7_attn_allreduce_15
	layer_7_input -> layer_7_qkv_62
	layer_7_qkv_62 -> layer_7_attn_scores_62
	layer_7_attn_scores_62 -> layer_7_attn_softmax_62
	layer_7_attn_softmax_62 -> layer_7_attn_out_62
	layer_7_attn_out_62 -> layer_7_attn_allreduce_15
	layer_7_input -> layer_7_qkv_63
	layer_7_qkv_63 -> layer_7_attn_scores_63
	layer_7_attn_scores_63 -> layer_7_attn_softmax_63
	layer_7_attn_softmax_63 -> layer_7_attn_out_63
	layer_7_attn_out_63 -> layer_7_attn_allreduce_15
	layer_7_gate_0 [label="Layer7_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_4 [label="Layer7_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_8 [label="Layer7_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_12 [label="Layer7_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_16 [label="Layer7_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_20 [label="Layer7_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_24 [label="Layer7_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_28 [label="Layer7_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_32 [label="Layer7_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_36 [label="Layer7_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_40 [label="Layer7_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_44 [label="Layer7_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_48 [label="Layer7_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_52 [label="Layer7_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_56 [label="Layer7_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_gate_60 [label="Layer7_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_7_alltoall [label="Layer7_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_0 [label="Layer7_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_1 [label="Layer7_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_2 [label="Layer7_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_3 [label="Layer7_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_0 [label="Layer7_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_1 [label="Layer7_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_2 [label="Layer7_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_3 [label="Layer7_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_0 [label="Layer7_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_1 [label="Layer7_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_2 [label="Layer7_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_3 [label="Layer7_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_0 [label="Layer7_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_4 [label="Layer7_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_5 [label="Layer7_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_6 [label="Layer7_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_7 [label="Layer7_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_4 [label="Layer7_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_5 [label="Layer7_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_6 [label="Layer7_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_7 [label="Layer7_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_4 [label="Layer7_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_5 [label="Layer7_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_6 [label="Layer7_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_7 [label="Layer7_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_1 [label="Layer7_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_8 [label="Layer7_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_9 [label="Layer7_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_10 [label="Layer7_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_11 [label="Layer7_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_8 [label="Layer7_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_9 [label="Layer7_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_10 [label="Layer7_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_11 [label="Layer7_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_8 [label="Layer7_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_9 [label="Layer7_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_10 [label="Layer7_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_11 [label="Layer7_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_2 [label="Layer7_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_12 [label="Layer7_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_13 [label="Layer7_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_14 [label="Layer7_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_15 [label="Layer7_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_12 [label="Layer7_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_13 [label="Layer7_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_14 [label="Layer7_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_15 [label="Layer7_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_12 [label="Layer7_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_13 [label="Layer7_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_14 [label="Layer7_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_15 [label="Layer7_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_3 [label="Layer7_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_16 [label="Layer7_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_17 [label="Layer7_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_18 [label="Layer7_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_19 [label="Layer7_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_16 [label="Layer7_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_17 [label="Layer7_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_18 [label="Layer7_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_19 [label="Layer7_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_16 [label="Layer7_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_17 [label="Layer7_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_18 [label="Layer7_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_19 [label="Layer7_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_4 [label="Layer7_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_20 [label="Layer7_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_21 [label="Layer7_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_22 [label="Layer7_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_23 [label="Layer7_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_20 [label="Layer7_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_21 [label="Layer7_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_22 [label="Layer7_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_23 [label="Layer7_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_20 [label="Layer7_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_21 [label="Layer7_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_22 [label="Layer7_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_23 [label="Layer7_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_5 [label="Layer7_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_24 [label="Layer7_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_25 [label="Layer7_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_26 [label="Layer7_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_27 [label="Layer7_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_24 [label="Layer7_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_25 [label="Layer7_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_26 [label="Layer7_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_27 [label="Layer7_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_24 [label="Layer7_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_25 [label="Layer7_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_26 [label="Layer7_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_27 [label="Layer7_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_6 [label="Layer7_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_28 [label="Layer7_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_29 [label="Layer7_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_30 [label="Layer7_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_31 [label="Layer7_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_28 [label="Layer7_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_29 [label="Layer7_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_30 [label="Layer7_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_31 [label="Layer7_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_28 [label="Layer7_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_29 [label="Layer7_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_30 [label="Layer7_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_31 [label="Layer7_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_7 [label="Layer7_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_32 [label="Layer7_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_33 [label="Layer7_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_34 [label="Layer7_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_35 [label="Layer7_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_32 [label="Layer7_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_33 [label="Layer7_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_34 [label="Layer7_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_35 [label="Layer7_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_32 [label="Layer7_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_33 [label="Layer7_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_34 [label="Layer7_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_35 [label="Layer7_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_8 [label="Layer7_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_36 [label="Layer7_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_37 [label="Layer7_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_38 [label="Layer7_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_39 [label="Layer7_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_36 [label="Layer7_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_37 [label="Layer7_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_38 [label="Layer7_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_39 [label="Layer7_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_36 [label="Layer7_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_37 [label="Layer7_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_38 [label="Layer7_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_39 [label="Layer7_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_9 [label="Layer7_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_40 [label="Layer7_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_41 [label="Layer7_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_42 [label="Layer7_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_43 [label="Layer7_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_40 [label="Layer7_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_41 [label="Layer7_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_42 [label="Layer7_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_43 [label="Layer7_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_40 [label="Layer7_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_41 [label="Layer7_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_42 [label="Layer7_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_43 [label="Layer7_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_10 [label="Layer7_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_44 [label="Layer7_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_45 [label="Layer7_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_46 [label="Layer7_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_47 [label="Layer7_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_44 [label="Layer7_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_45 [label="Layer7_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_46 [label="Layer7_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_47 [label="Layer7_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_44 [label="Layer7_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_45 [label="Layer7_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_46 [label="Layer7_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_47 [label="Layer7_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_11 [label="Layer7_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_48 [label="Layer7_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_49 [label="Layer7_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_50 [label="Layer7_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_51 [label="Layer7_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_48 [label="Layer7_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_49 [label="Layer7_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_50 [label="Layer7_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_51 [label="Layer7_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_48 [label="Layer7_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_49 [label="Layer7_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_50 [label="Layer7_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_51 [label="Layer7_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_12 [label="Layer7_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_52 [label="Layer7_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_53 [label="Layer7_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_54 [label="Layer7_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_55 [label="Layer7_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_52 [label="Layer7_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_53 [label="Layer7_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_54 [label="Layer7_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_55 [label="Layer7_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_52 [label="Layer7_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_53 [label="Layer7_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_54 [label="Layer7_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_55 [label="Layer7_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_13 [label="Layer7_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_56 [label="Layer7_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_57 [label="Layer7_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_58 [label="Layer7_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_59 [label="Layer7_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_56 [label="Layer7_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_57 [label="Layer7_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_58 [label="Layer7_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_59 [label="Layer7_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_56 [label="Layer7_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_57 [label="Layer7_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_58 [label="Layer7_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_59 [label="Layer7_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_14 [label="Layer7_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert1_60 [label="Layer7_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_61 [label="Layer7_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_62 [label="Layer7_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert1_63 [label="Layer7_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_60 [label="Layer7_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_61 [label="Layer7_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_62 [label="Layer7_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert_act_63 [label="Layer7_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_7_expert2_60 [label="Layer7_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_61 [label="Layer7_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_62 [label="Layer7_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert2_63 [label="Layer7_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_7_expert_allreduce_15 [label="Layer7_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_7_expert_agg [label="Layer7_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_7_norm [label="Layer7_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_7_attn_allreduce_0 -> layer_7_gate_0 [style=dashed]
	layer_7_gate_0 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_0
	layer_7_expert1_0 -> layer_7_expert_act_0
	layer_7_expert_act_0 -> layer_7_expert2_0
	layer_7_expert2_0 -> layer_7_expert_allreduce_0
	layer_7_alltoall -> layer_7_expert1_1
	layer_7_expert1_1 -> layer_7_expert_act_1
	layer_7_expert_act_1 -> layer_7_expert2_1
	layer_7_expert2_1 -> layer_7_expert_allreduce_0
	layer_7_alltoall -> layer_7_expert1_2
	layer_7_expert1_2 -> layer_7_expert_act_2
	layer_7_expert_act_2 -> layer_7_expert2_2
	layer_7_expert2_2 -> layer_7_expert_allreduce_0
	layer_7_alltoall -> layer_7_expert1_3
	layer_7_expert1_3 -> layer_7_expert_act_3
	layer_7_expert_act_3 -> layer_7_expert2_3
	layer_7_expert2_3 -> layer_7_expert_allreduce_0
	layer_7_expert_allreduce_0 -> layer_7_expert_agg
	layer_7_attn_allreduce_1 -> layer_7_gate_4 [style=dashed]
	layer_7_gate_4 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_4
	layer_7_expert1_4 -> layer_7_expert_act_4
	layer_7_expert_act_4 -> layer_7_expert2_4
	layer_7_expert2_4 -> layer_7_expert_allreduce_1
	layer_7_alltoall -> layer_7_expert1_5
	layer_7_expert1_5 -> layer_7_expert_act_5
	layer_7_expert_act_5 -> layer_7_expert2_5
	layer_7_expert2_5 -> layer_7_expert_allreduce_1
	layer_7_alltoall -> layer_7_expert1_6
	layer_7_expert1_6 -> layer_7_expert_act_6
	layer_7_expert_act_6 -> layer_7_expert2_6
	layer_7_expert2_6 -> layer_7_expert_allreduce_1
	layer_7_alltoall -> layer_7_expert1_7
	layer_7_expert1_7 -> layer_7_expert_act_7
	layer_7_expert_act_7 -> layer_7_expert2_7
	layer_7_expert2_7 -> layer_7_expert_allreduce_1
	layer_7_expert_allreduce_1 -> layer_7_expert_agg
	layer_7_attn_allreduce_2 -> layer_7_gate_8 [style=dashed]
	layer_7_gate_8 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_8
	layer_7_expert1_8 -> layer_7_expert_act_8
	layer_7_expert_act_8 -> layer_7_expert2_8
	layer_7_expert2_8 -> layer_7_expert_allreduce_2
	layer_7_alltoall -> layer_7_expert1_9
	layer_7_expert1_9 -> layer_7_expert_act_9
	layer_7_expert_act_9 -> layer_7_expert2_9
	layer_7_expert2_9 -> layer_7_expert_allreduce_2
	layer_7_alltoall -> layer_7_expert1_10
	layer_7_expert1_10 -> layer_7_expert_act_10
	layer_7_expert_act_10 -> layer_7_expert2_10
	layer_7_expert2_10 -> layer_7_expert_allreduce_2
	layer_7_alltoall -> layer_7_expert1_11
	layer_7_expert1_11 -> layer_7_expert_act_11
	layer_7_expert_act_11 -> layer_7_expert2_11
	layer_7_expert2_11 -> layer_7_expert_allreduce_2
	layer_7_expert_allreduce_2 -> layer_7_expert_agg
	layer_7_attn_allreduce_3 -> layer_7_gate_12 [style=dashed]
	layer_7_gate_12 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_12
	layer_7_expert1_12 -> layer_7_expert_act_12
	layer_7_expert_act_12 -> layer_7_expert2_12
	layer_7_expert2_12 -> layer_7_expert_allreduce_3
	layer_7_alltoall -> layer_7_expert1_13
	layer_7_expert1_13 -> layer_7_expert_act_13
	layer_7_expert_act_13 -> layer_7_expert2_13
	layer_7_expert2_13 -> layer_7_expert_allreduce_3
	layer_7_alltoall -> layer_7_expert1_14
	layer_7_expert1_14 -> layer_7_expert_act_14
	layer_7_expert_act_14 -> layer_7_expert2_14
	layer_7_expert2_14 -> layer_7_expert_allreduce_3
	layer_7_alltoall -> layer_7_expert1_15
	layer_7_expert1_15 -> layer_7_expert_act_15
	layer_7_expert_act_15 -> layer_7_expert2_15
	layer_7_expert2_15 -> layer_7_expert_allreduce_3
	layer_7_expert_allreduce_3 -> layer_7_expert_agg
	layer_7_attn_allreduce_4 -> layer_7_gate_16 [style=dashed]
	layer_7_gate_16 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_16
	layer_7_expert1_16 -> layer_7_expert_act_16
	layer_7_expert_act_16 -> layer_7_expert2_16
	layer_7_expert2_16 -> layer_7_expert_allreduce_4
	layer_7_alltoall -> layer_7_expert1_17
	layer_7_expert1_17 -> layer_7_expert_act_17
	layer_7_expert_act_17 -> layer_7_expert2_17
	layer_7_expert2_17 -> layer_7_expert_allreduce_4
	layer_7_alltoall -> layer_7_expert1_18
	layer_7_expert1_18 -> layer_7_expert_act_18
	layer_7_expert_act_18 -> layer_7_expert2_18
	layer_7_expert2_18 -> layer_7_expert_allreduce_4
	layer_7_alltoall -> layer_7_expert1_19
	layer_7_expert1_19 -> layer_7_expert_act_19
	layer_7_expert_act_19 -> layer_7_expert2_19
	layer_7_expert2_19 -> layer_7_expert_allreduce_4
	layer_7_expert_allreduce_4 -> layer_7_expert_agg
	layer_7_attn_allreduce_5 -> layer_7_gate_20 [style=dashed]
	layer_7_gate_20 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_20
	layer_7_expert1_20 -> layer_7_expert_act_20
	layer_7_expert_act_20 -> layer_7_expert2_20
	layer_7_expert2_20 -> layer_7_expert_allreduce_5
	layer_7_alltoall -> layer_7_expert1_21
	layer_7_expert1_21 -> layer_7_expert_act_21
	layer_7_expert_act_21 -> layer_7_expert2_21
	layer_7_expert2_21 -> layer_7_expert_allreduce_5
	layer_7_alltoall -> layer_7_expert1_22
	layer_7_expert1_22 -> layer_7_expert_act_22
	layer_7_expert_act_22 -> layer_7_expert2_22
	layer_7_expert2_22 -> layer_7_expert_allreduce_5
	layer_7_alltoall -> layer_7_expert1_23
	layer_7_expert1_23 -> layer_7_expert_act_23
	layer_7_expert_act_23 -> layer_7_expert2_23
	layer_7_expert2_23 -> layer_7_expert_allreduce_5
	layer_7_expert_allreduce_5 -> layer_7_expert_agg
	layer_7_attn_allreduce_6 -> layer_7_gate_24 [style=dashed]
	layer_7_gate_24 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_24
	layer_7_expert1_24 -> layer_7_expert_act_24
	layer_7_expert_act_24 -> layer_7_expert2_24
	layer_7_expert2_24 -> layer_7_expert_allreduce_6
	layer_7_alltoall -> layer_7_expert1_25
	layer_7_expert1_25 -> layer_7_expert_act_25
	layer_7_expert_act_25 -> layer_7_expert2_25
	layer_7_expert2_25 -> layer_7_expert_allreduce_6
	layer_7_alltoall -> layer_7_expert1_26
	layer_7_expert1_26 -> layer_7_expert_act_26
	layer_7_expert_act_26 -> layer_7_expert2_26
	layer_7_expert2_26 -> layer_7_expert_allreduce_6
	layer_7_alltoall -> layer_7_expert1_27
	layer_7_expert1_27 -> layer_7_expert_act_27
	layer_7_expert_act_27 -> layer_7_expert2_27
	layer_7_expert2_27 -> layer_7_expert_allreduce_6
	layer_7_expert_allreduce_6 -> layer_7_expert_agg
	layer_7_attn_allreduce_7 -> layer_7_gate_28 [style=dashed]
	layer_7_gate_28 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_28
	layer_7_expert1_28 -> layer_7_expert_act_28
	layer_7_expert_act_28 -> layer_7_expert2_28
	layer_7_expert2_28 -> layer_7_expert_allreduce_7
	layer_7_alltoall -> layer_7_expert1_29
	layer_7_expert1_29 -> layer_7_expert_act_29
	layer_7_expert_act_29 -> layer_7_expert2_29
	layer_7_expert2_29 -> layer_7_expert_allreduce_7
	layer_7_alltoall -> layer_7_expert1_30
	layer_7_expert1_30 -> layer_7_expert_act_30
	layer_7_expert_act_30 -> layer_7_expert2_30
	layer_7_expert2_30 -> layer_7_expert_allreduce_7
	layer_7_alltoall -> layer_7_expert1_31
	layer_7_expert1_31 -> layer_7_expert_act_31
	layer_7_expert_act_31 -> layer_7_expert2_31
	layer_7_expert2_31 -> layer_7_expert_allreduce_7
	layer_7_expert_allreduce_7 -> layer_7_expert_agg
	layer_7_attn_allreduce_8 -> layer_7_gate_32 [style=dashed]
	layer_7_gate_32 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_32
	layer_7_expert1_32 -> layer_7_expert_act_32
	layer_7_expert_act_32 -> layer_7_expert2_32
	layer_7_expert2_32 -> layer_7_expert_allreduce_8
	layer_7_alltoall -> layer_7_expert1_33
	layer_7_expert1_33 -> layer_7_expert_act_33
	layer_7_expert_act_33 -> layer_7_expert2_33
	layer_7_expert2_33 -> layer_7_expert_allreduce_8
	layer_7_alltoall -> layer_7_expert1_34
	layer_7_expert1_34 -> layer_7_expert_act_34
	layer_7_expert_act_34 -> layer_7_expert2_34
	layer_7_expert2_34 -> layer_7_expert_allreduce_8
	layer_7_alltoall -> layer_7_expert1_35
	layer_7_expert1_35 -> layer_7_expert_act_35
	layer_7_expert_act_35 -> layer_7_expert2_35
	layer_7_expert2_35 -> layer_7_expert_allreduce_8
	layer_7_expert_allreduce_8 -> layer_7_expert_agg
	layer_7_attn_allreduce_9 -> layer_7_gate_36 [style=dashed]
	layer_7_gate_36 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_36
	layer_7_expert1_36 -> layer_7_expert_act_36
	layer_7_expert_act_36 -> layer_7_expert2_36
	layer_7_expert2_36 -> layer_7_expert_allreduce_9
	layer_7_alltoall -> layer_7_expert1_37
	layer_7_expert1_37 -> layer_7_expert_act_37
	layer_7_expert_act_37 -> layer_7_expert2_37
	layer_7_expert2_37 -> layer_7_expert_allreduce_9
	layer_7_alltoall -> layer_7_expert1_38
	layer_7_expert1_38 -> layer_7_expert_act_38
	layer_7_expert_act_38 -> layer_7_expert2_38
	layer_7_expert2_38 -> layer_7_expert_allreduce_9
	layer_7_alltoall -> layer_7_expert1_39
	layer_7_expert1_39 -> layer_7_expert_act_39
	layer_7_expert_act_39 -> layer_7_expert2_39
	layer_7_expert2_39 -> layer_7_expert_allreduce_9
	layer_7_expert_allreduce_9 -> layer_7_expert_agg
	layer_7_attn_allreduce_10 -> layer_7_gate_40 [style=dashed]
	layer_7_gate_40 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_40
	layer_7_expert1_40 -> layer_7_expert_act_40
	layer_7_expert_act_40 -> layer_7_expert2_40
	layer_7_expert2_40 -> layer_7_expert_allreduce_10
	layer_7_alltoall -> layer_7_expert1_41
	layer_7_expert1_41 -> layer_7_expert_act_41
	layer_7_expert_act_41 -> layer_7_expert2_41
	layer_7_expert2_41 -> layer_7_expert_allreduce_10
	layer_7_alltoall -> layer_7_expert1_42
	layer_7_expert1_42 -> layer_7_expert_act_42
	layer_7_expert_act_42 -> layer_7_expert2_42
	layer_7_expert2_42 -> layer_7_expert_allreduce_10
	layer_7_alltoall -> layer_7_expert1_43
	layer_7_expert1_43 -> layer_7_expert_act_43
	layer_7_expert_act_43 -> layer_7_expert2_43
	layer_7_expert2_43 -> layer_7_expert_allreduce_10
	layer_7_expert_allreduce_10 -> layer_7_expert_agg
	layer_7_attn_allreduce_11 -> layer_7_gate_44 [style=dashed]
	layer_7_gate_44 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_44
	layer_7_expert1_44 -> layer_7_expert_act_44
	layer_7_expert_act_44 -> layer_7_expert2_44
	layer_7_expert2_44 -> layer_7_expert_allreduce_11
	layer_7_alltoall -> layer_7_expert1_45
	layer_7_expert1_45 -> layer_7_expert_act_45
	layer_7_expert_act_45 -> layer_7_expert2_45
	layer_7_expert2_45 -> layer_7_expert_allreduce_11
	layer_7_alltoall -> layer_7_expert1_46
	layer_7_expert1_46 -> layer_7_expert_act_46
	layer_7_expert_act_46 -> layer_7_expert2_46
	layer_7_expert2_46 -> layer_7_expert_allreduce_11
	layer_7_alltoall -> layer_7_expert1_47
	layer_7_expert1_47 -> layer_7_expert_act_47
	layer_7_expert_act_47 -> layer_7_expert2_47
	layer_7_expert2_47 -> layer_7_expert_allreduce_11
	layer_7_expert_allreduce_11 -> layer_7_expert_agg
	layer_7_attn_allreduce_12 -> layer_7_gate_48 [style=dashed]
	layer_7_gate_48 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_48
	layer_7_expert1_48 -> layer_7_expert_act_48
	layer_7_expert_act_48 -> layer_7_expert2_48
	layer_7_expert2_48 -> layer_7_expert_allreduce_12
	layer_7_alltoall -> layer_7_expert1_49
	layer_7_expert1_49 -> layer_7_expert_act_49
	layer_7_expert_act_49 -> layer_7_expert2_49
	layer_7_expert2_49 -> layer_7_expert_allreduce_12
	layer_7_alltoall -> layer_7_expert1_50
	layer_7_expert1_50 -> layer_7_expert_act_50
	layer_7_expert_act_50 -> layer_7_expert2_50
	layer_7_expert2_50 -> layer_7_expert_allreduce_12
	layer_7_alltoall -> layer_7_expert1_51
	layer_7_expert1_51 -> layer_7_expert_act_51
	layer_7_expert_act_51 -> layer_7_expert2_51
	layer_7_expert2_51 -> layer_7_expert_allreduce_12
	layer_7_expert_allreduce_12 -> layer_7_expert_agg
	layer_7_attn_allreduce_13 -> layer_7_gate_52 [style=dashed]
	layer_7_gate_52 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_52
	layer_7_expert1_52 -> layer_7_expert_act_52
	layer_7_expert_act_52 -> layer_7_expert2_52
	layer_7_expert2_52 -> layer_7_expert_allreduce_13
	layer_7_alltoall -> layer_7_expert1_53
	layer_7_expert1_53 -> layer_7_expert_act_53
	layer_7_expert_act_53 -> layer_7_expert2_53
	layer_7_expert2_53 -> layer_7_expert_allreduce_13
	layer_7_alltoall -> layer_7_expert1_54
	layer_7_expert1_54 -> layer_7_expert_act_54
	layer_7_expert_act_54 -> layer_7_expert2_54
	layer_7_expert2_54 -> layer_7_expert_allreduce_13
	layer_7_alltoall -> layer_7_expert1_55
	layer_7_expert1_55 -> layer_7_expert_act_55
	layer_7_expert_act_55 -> layer_7_expert2_55
	layer_7_expert2_55 -> layer_7_expert_allreduce_13
	layer_7_expert_allreduce_13 -> layer_7_expert_agg
	layer_7_attn_allreduce_14 -> layer_7_gate_56 [style=dashed]
	layer_7_gate_56 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_56
	layer_7_expert1_56 -> layer_7_expert_act_56
	layer_7_expert_act_56 -> layer_7_expert2_56
	layer_7_expert2_56 -> layer_7_expert_allreduce_14
	layer_7_alltoall -> layer_7_expert1_57
	layer_7_expert1_57 -> layer_7_expert_act_57
	layer_7_expert_act_57 -> layer_7_expert2_57
	layer_7_expert2_57 -> layer_7_expert_allreduce_14
	layer_7_alltoall -> layer_7_expert1_58
	layer_7_expert1_58 -> layer_7_expert_act_58
	layer_7_expert_act_58 -> layer_7_expert2_58
	layer_7_expert2_58 -> layer_7_expert_allreduce_14
	layer_7_alltoall -> layer_7_expert1_59
	layer_7_expert1_59 -> layer_7_expert_act_59
	layer_7_expert_act_59 -> layer_7_expert2_59
	layer_7_expert2_59 -> layer_7_expert_allreduce_14
	layer_7_expert_allreduce_14 -> layer_7_expert_agg
	layer_7_attn_allreduce_15 -> layer_7_gate_60 [style=dashed]
	layer_7_gate_60 -> layer_7_alltoall [style=dashed]
	layer_7_alltoall -> layer_7_expert1_60
	layer_7_expert1_60 -> layer_7_expert_act_60
	layer_7_expert_act_60 -> layer_7_expert2_60
	layer_7_expert2_60 -> layer_7_expert_allreduce_15
	layer_7_alltoall -> layer_7_expert1_61
	layer_7_expert1_61 -> layer_7_expert_act_61
	layer_7_expert_act_61 -> layer_7_expert2_61
	layer_7_expert2_61 -> layer_7_expert_allreduce_15
	layer_7_alltoall -> layer_7_expert1_62
	layer_7_expert1_62 -> layer_7_expert_act_62
	layer_7_expert_act_62 -> layer_7_expert2_62
	layer_7_expert2_62 -> layer_7_expert_allreduce_15
	layer_7_alltoall -> layer_7_expert1_63
	layer_7_expert1_63 -> layer_7_expert_act_63
	layer_7_expert_act_63 -> layer_7_expert2_63
	layer_7_expert2_63 -> layer_7_expert_allreduce_15
	layer_7_expert_allreduce_15 -> layer_7_expert_agg
	layer_7_expert_agg -> layer_7_norm
	layer_7_norm -> layer_8_input
	layer_8_input [label="Layer8_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_8_qkv_0 [label="Layer8_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_1 [label="Layer8_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_2 [label="Layer8_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_3 [label="Layer8_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_0 [label="Layer8_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_1 [label="Layer8_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_2 [label="Layer8_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_3 [label="Layer8_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_0 [label="Layer8_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_1 [label="Layer8_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_2 [label="Layer8_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_3 [label="Layer8_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_0 [label="Layer8_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_1 [label="Layer8_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_2 [label="Layer8_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_3 [label="Layer8_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_0 [label="Layer8_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_0
	layer_8_qkv_0 -> layer_8_attn_scores_0
	layer_8_attn_scores_0 -> layer_8_attn_softmax_0
	layer_8_attn_softmax_0 -> layer_8_attn_out_0
	layer_8_attn_out_0 -> layer_8_attn_allreduce_0
	layer_8_input -> layer_8_qkv_1
	layer_8_qkv_1 -> layer_8_attn_scores_1
	layer_8_attn_scores_1 -> layer_8_attn_softmax_1
	layer_8_attn_softmax_1 -> layer_8_attn_out_1
	layer_8_attn_out_1 -> layer_8_attn_allreduce_0
	layer_8_input -> layer_8_qkv_2
	layer_8_qkv_2 -> layer_8_attn_scores_2
	layer_8_attn_scores_2 -> layer_8_attn_softmax_2
	layer_8_attn_softmax_2 -> layer_8_attn_out_2
	layer_8_attn_out_2 -> layer_8_attn_allreduce_0
	layer_8_input -> layer_8_qkv_3
	layer_8_qkv_3 -> layer_8_attn_scores_3
	layer_8_attn_scores_3 -> layer_8_attn_softmax_3
	layer_8_attn_softmax_3 -> layer_8_attn_out_3
	layer_8_attn_out_3 -> layer_8_attn_allreduce_0
	layer_8_qkv_4 [label="Layer8_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_5 [label="Layer8_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_6 [label="Layer8_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_7 [label="Layer8_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_4 [label="Layer8_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_5 [label="Layer8_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_6 [label="Layer8_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_7 [label="Layer8_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_4 [label="Layer8_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_5 [label="Layer8_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_6 [label="Layer8_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_7 [label="Layer8_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_4 [label="Layer8_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_5 [label="Layer8_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_6 [label="Layer8_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_7 [label="Layer8_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_1 [label="Layer8_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_4
	layer_8_qkv_4 -> layer_8_attn_scores_4
	layer_8_attn_scores_4 -> layer_8_attn_softmax_4
	layer_8_attn_softmax_4 -> layer_8_attn_out_4
	layer_8_attn_out_4 -> layer_8_attn_allreduce_1
	layer_8_input -> layer_8_qkv_5
	layer_8_qkv_5 -> layer_8_attn_scores_5
	layer_8_attn_scores_5 -> layer_8_attn_softmax_5
	layer_8_attn_softmax_5 -> layer_8_attn_out_5
	layer_8_attn_out_5 -> layer_8_attn_allreduce_1
	layer_8_input -> layer_8_qkv_6
	layer_8_qkv_6 -> layer_8_attn_scores_6
	layer_8_attn_scores_6 -> layer_8_attn_softmax_6
	layer_8_attn_softmax_6 -> layer_8_attn_out_6
	layer_8_attn_out_6 -> layer_8_attn_allreduce_1
	layer_8_input -> layer_8_qkv_7
	layer_8_qkv_7 -> layer_8_attn_scores_7
	layer_8_attn_scores_7 -> layer_8_attn_softmax_7
	layer_8_attn_softmax_7 -> layer_8_attn_out_7
	layer_8_attn_out_7 -> layer_8_attn_allreduce_1
	layer_8_qkv_8 [label="Layer8_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_9 [label="Layer8_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_10 [label="Layer8_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_11 [label="Layer8_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_8 [label="Layer8_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_9 [label="Layer8_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_10 [label="Layer8_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_11 [label="Layer8_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_8 [label="Layer8_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_9 [label="Layer8_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_10 [label="Layer8_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_11 [label="Layer8_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_8 [label="Layer8_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_9 [label="Layer8_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_10 [label="Layer8_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_11 [label="Layer8_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_2 [label="Layer8_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_8
	layer_8_qkv_8 -> layer_8_attn_scores_8
	layer_8_attn_scores_8 -> layer_8_attn_softmax_8
	layer_8_attn_softmax_8 -> layer_8_attn_out_8
	layer_8_attn_out_8 -> layer_8_attn_allreduce_2
	layer_8_input -> layer_8_qkv_9
	layer_8_qkv_9 -> layer_8_attn_scores_9
	layer_8_attn_scores_9 -> layer_8_attn_softmax_9
	layer_8_attn_softmax_9 -> layer_8_attn_out_9
	layer_8_attn_out_9 -> layer_8_attn_allreduce_2
	layer_8_input -> layer_8_qkv_10
	layer_8_qkv_10 -> layer_8_attn_scores_10
	layer_8_attn_scores_10 -> layer_8_attn_softmax_10
	layer_8_attn_softmax_10 -> layer_8_attn_out_10
	layer_8_attn_out_10 -> layer_8_attn_allreduce_2
	layer_8_input -> layer_8_qkv_11
	layer_8_qkv_11 -> layer_8_attn_scores_11
	layer_8_attn_scores_11 -> layer_8_attn_softmax_11
	layer_8_attn_softmax_11 -> layer_8_attn_out_11
	layer_8_attn_out_11 -> layer_8_attn_allreduce_2
	layer_8_qkv_12 [label="Layer8_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_13 [label="Layer8_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_14 [label="Layer8_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_15 [label="Layer8_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_12 [label="Layer8_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_13 [label="Layer8_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_14 [label="Layer8_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_15 [label="Layer8_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_12 [label="Layer8_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_13 [label="Layer8_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_14 [label="Layer8_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_15 [label="Layer8_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_12 [label="Layer8_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_13 [label="Layer8_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_14 [label="Layer8_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_15 [label="Layer8_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_3 [label="Layer8_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_12
	layer_8_qkv_12 -> layer_8_attn_scores_12
	layer_8_attn_scores_12 -> layer_8_attn_softmax_12
	layer_8_attn_softmax_12 -> layer_8_attn_out_12
	layer_8_attn_out_12 -> layer_8_attn_allreduce_3
	layer_8_input -> layer_8_qkv_13
	layer_8_qkv_13 -> layer_8_attn_scores_13
	layer_8_attn_scores_13 -> layer_8_attn_softmax_13
	layer_8_attn_softmax_13 -> layer_8_attn_out_13
	layer_8_attn_out_13 -> layer_8_attn_allreduce_3
	layer_8_input -> layer_8_qkv_14
	layer_8_qkv_14 -> layer_8_attn_scores_14
	layer_8_attn_scores_14 -> layer_8_attn_softmax_14
	layer_8_attn_softmax_14 -> layer_8_attn_out_14
	layer_8_attn_out_14 -> layer_8_attn_allreduce_3
	layer_8_input -> layer_8_qkv_15
	layer_8_qkv_15 -> layer_8_attn_scores_15
	layer_8_attn_scores_15 -> layer_8_attn_softmax_15
	layer_8_attn_softmax_15 -> layer_8_attn_out_15
	layer_8_attn_out_15 -> layer_8_attn_allreduce_3
	layer_8_qkv_16 [label="Layer8_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_17 [label="Layer8_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_18 [label="Layer8_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_19 [label="Layer8_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_16 [label="Layer8_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_17 [label="Layer8_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_18 [label="Layer8_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_19 [label="Layer8_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_16 [label="Layer8_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_17 [label="Layer8_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_18 [label="Layer8_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_19 [label="Layer8_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_16 [label="Layer8_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_17 [label="Layer8_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_18 [label="Layer8_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_19 [label="Layer8_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_4 [label="Layer8_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_16
	layer_8_qkv_16 -> layer_8_attn_scores_16
	layer_8_attn_scores_16 -> layer_8_attn_softmax_16
	layer_8_attn_softmax_16 -> layer_8_attn_out_16
	layer_8_attn_out_16 -> layer_8_attn_allreduce_4
	layer_8_input -> layer_8_qkv_17
	layer_8_qkv_17 -> layer_8_attn_scores_17
	layer_8_attn_scores_17 -> layer_8_attn_softmax_17
	layer_8_attn_softmax_17 -> layer_8_attn_out_17
	layer_8_attn_out_17 -> layer_8_attn_allreduce_4
	layer_8_input -> layer_8_qkv_18
	layer_8_qkv_18 -> layer_8_attn_scores_18
	layer_8_attn_scores_18 -> layer_8_attn_softmax_18
	layer_8_attn_softmax_18 -> layer_8_attn_out_18
	layer_8_attn_out_18 -> layer_8_attn_allreduce_4
	layer_8_input -> layer_8_qkv_19
	layer_8_qkv_19 -> layer_8_attn_scores_19
	layer_8_attn_scores_19 -> layer_8_attn_softmax_19
	layer_8_attn_softmax_19 -> layer_8_attn_out_19
	layer_8_attn_out_19 -> layer_8_attn_allreduce_4
	layer_8_qkv_20 [label="Layer8_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_21 [label="Layer8_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_22 [label="Layer8_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_23 [label="Layer8_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_20 [label="Layer8_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_21 [label="Layer8_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_22 [label="Layer8_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_23 [label="Layer8_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_20 [label="Layer8_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_21 [label="Layer8_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_22 [label="Layer8_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_23 [label="Layer8_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_20 [label="Layer8_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_21 [label="Layer8_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_22 [label="Layer8_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_23 [label="Layer8_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_5 [label="Layer8_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_20
	layer_8_qkv_20 -> layer_8_attn_scores_20
	layer_8_attn_scores_20 -> layer_8_attn_softmax_20
	layer_8_attn_softmax_20 -> layer_8_attn_out_20
	layer_8_attn_out_20 -> layer_8_attn_allreduce_5
	layer_8_input -> layer_8_qkv_21
	layer_8_qkv_21 -> layer_8_attn_scores_21
	layer_8_attn_scores_21 -> layer_8_attn_softmax_21
	layer_8_attn_softmax_21 -> layer_8_attn_out_21
	layer_8_attn_out_21 -> layer_8_attn_allreduce_5
	layer_8_input -> layer_8_qkv_22
	layer_8_qkv_22 -> layer_8_attn_scores_22
	layer_8_attn_scores_22 -> layer_8_attn_softmax_22
	layer_8_attn_softmax_22 -> layer_8_attn_out_22
	layer_8_attn_out_22 -> layer_8_attn_allreduce_5
	layer_8_input -> layer_8_qkv_23
	layer_8_qkv_23 -> layer_8_attn_scores_23
	layer_8_attn_scores_23 -> layer_8_attn_softmax_23
	layer_8_attn_softmax_23 -> layer_8_attn_out_23
	layer_8_attn_out_23 -> layer_8_attn_allreduce_5
	layer_8_qkv_24 [label="Layer8_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_25 [label="Layer8_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_26 [label="Layer8_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_27 [label="Layer8_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_24 [label="Layer8_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_25 [label="Layer8_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_26 [label="Layer8_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_27 [label="Layer8_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_24 [label="Layer8_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_25 [label="Layer8_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_26 [label="Layer8_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_27 [label="Layer8_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_24 [label="Layer8_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_25 [label="Layer8_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_26 [label="Layer8_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_27 [label="Layer8_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_6 [label="Layer8_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_24
	layer_8_qkv_24 -> layer_8_attn_scores_24
	layer_8_attn_scores_24 -> layer_8_attn_softmax_24
	layer_8_attn_softmax_24 -> layer_8_attn_out_24
	layer_8_attn_out_24 -> layer_8_attn_allreduce_6
	layer_8_input -> layer_8_qkv_25
	layer_8_qkv_25 -> layer_8_attn_scores_25
	layer_8_attn_scores_25 -> layer_8_attn_softmax_25
	layer_8_attn_softmax_25 -> layer_8_attn_out_25
	layer_8_attn_out_25 -> layer_8_attn_allreduce_6
	layer_8_input -> layer_8_qkv_26
	layer_8_qkv_26 -> layer_8_attn_scores_26
	layer_8_attn_scores_26 -> layer_8_attn_softmax_26
	layer_8_attn_softmax_26 -> layer_8_attn_out_26
	layer_8_attn_out_26 -> layer_8_attn_allreduce_6
	layer_8_input -> layer_8_qkv_27
	layer_8_qkv_27 -> layer_8_attn_scores_27
	layer_8_attn_scores_27 -> layer_8_attn_softmax_27
	layer_8_attn_softmax_27 -> layer_8_attn_out_27
	layer_8_attn_out_27 -> layer_8_attn_allreduce_6
	layer_8_qkv_28 [label="Layer8_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_29 [label="Layer8_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_30 [label="Layer8_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_31 [label="Layer8_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_28 [label="Layer8_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_29 [label="Layer8_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_30 [label="Layer8_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_31 [label="Layer8_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_28 [label="Layer8_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_29 [label="Layer8_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_30 [label="Layer8_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_31 [label="Layer8_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_28 [label="Layer8_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_29 [label="Layer8_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_30 [label="Layer8_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_31 [label="Layer8_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_7 [label="Layer8_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_28
	layer_8_qkv_28 -> layer_8_attn_scores_28
	layer_8_attn_scores_28 -> layer_8_attn_softmax_28
	layer_8_attn_softmax_28 -> layer_8_attn_out_28
	layer_8_attn_out_28 -> layer_8_attn_allreduce_7
	layer_8_input -> layer_8_qkv_29
	layer_8_qkv_29 -> layer_8_attn_scores_29
	layer_8_attn_scores_29 -> layer_8_attn_softmax_29
	layer_8_attn_softmax_29 -> layer_8_attn_out_29
	layer_8_attn_out_29 -> layer_8_attn_allreduce_7
	layer_8_input -> layer_8_qkv_30
	layer_8_qkv_30 -> layer_8_attn_scores_30
	layer_8_attn_scores_30 -> layer_8_attn_softmax_30
	layer_8_attn_softmax_30 -> layer_8_attn_out_30
	layer_8_attn_out_30 -> layer_8_attn_allreduce_7
	layer_8_input -> layer_8_qkv_31
	layer_8_qkv_31 -> layer_8_attn_scores_31
	layer_8_attn_scores_31 -> layer_8_attn_softmax_31
	layer_8_attn_softmax_31 -> layer_8_attn_out_31
	layer_8_attn_out_31 -> layer_8_attn_allreduce_7
	layer_8_qkv_32 [label="Layer8_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_33 [label="Layer8_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_34 [label="Layer8_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_35 [label="Layer8_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_32 [label="Layer8_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_33 [label="Layer8_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_34 [label="Layer8_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_35 [label="Layer8_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_32 [label="Layer8_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_33 [label="Layer8_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_34 [label="Layer8_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_35 [label="Layer8_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_32 [label="Layer8_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_33 [label="Layer8_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_34 [label="Layer8_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_35 [label="Layer8_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_8 [label="Layer8_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_32
	layer_8_qkv_32 -> layer_8_attn_scores_32
	layer_8_attn_scores_32 -> layer_8_attn_softmax_32
	layer_8_attn_softmax_32 -> layer_8_attn_out_32
	layer_8_attn_out_32 -> layer_8_attn_allreduce_8
	layer_8_input -> layer_8_qkv_33
	layer_8_qkv_33 -> layer_8_attn_scores_33
	layer_8_attn_scores_33 -> layer_8_attn_softmax_33
	layer_8_attn_softmax_33 -> layer_8_attn_out_33
	layer_8_attn_out_33 -> layer_8_attn_allreduce_8
	layer_8_input -> layer_8_qkv_34
	layer_8_qkv_34 -> layer_8_attn_scores_34
	layer_8_attn_scores_34 -> layer_8_attn_softmax_34
	layer_8_attn_softmax_34 -> layer_8_attn_out_34
	layer_8_attn_out_34 -> layer_8_attn_allreduce_8
	layer_8_input -> layer_8_qkv_35
	layer_8_qkv_35 -> layer_8_attn_scores_35
	layer_8_attn_scores_35 -> layer_8_attn_softmax_35
	layer_8_attn_softmax_35 -> layer_8_attn_out_35
	layer_8_attn_out_35 -> layer_8_attn_allreduce_8
	layer_8_qkv_36 [label="Layer8_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_37 [label="Layer8_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_38 [label="Layer8_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_39 [label="Layer8_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_36 [label="Layer8_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_37 [label="Layer8_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_38 [label="Layer8_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_39 [label="Layer8_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_36 [label="Layer8_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_37 [label="Layer8_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_38 [label="Layer8_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_39 [label="Layer8_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_36 [label="Layer8_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_37 [label="Layer8_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_38 [label="Layer8_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_39 [label="Layer8_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_9 [label="Layer8_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_36
	layer_8_qkv_36 -> layer_8_attn_scores_36
	layer_8_attn_scores_36 -> layer_8_attn_softmax_36
	layer_8_attn_softmax_36 -> layer_8_attn_out_36
	layer_8_attn_out_36 -> layer_8_attn_allreduce_9
	layer_8_input -> layer_8_qkv_37
	layer_8_qkv_37 -> layer_8_attn_scores_37
	layer_8_attn_scores_37 -> layer_8_attn_softmax_37
	layer_8_attn_softmax_37 -> layer_8_attn_out_37
	layer_8_attn_out_37 -> layer_8_attn_allreduce_9
	layer_8_input -> layer_8_qkv_38
	layer_8_qkv_38 -> layer_8_attn_scores_38
	layer_8_attn_scores_38 -> layer_8_attn_softmax_38
	layer_8_attn_softmax_38 -> layer_8_attn_out_38
	layer_8_attn_out_38 -> layer_8_attn_allreduce_9
	layer_8_input -> layer_8_qkv_39
	layer_8_qkv_39 -> layer_8_attn_scores_39
	layer_8_attn_scores_39 -> layer_8_attn_softmax_39
	layer_8_attn_softmax_39 -> layer_8_attn_out_39
	layer_8_attn_out_39 -> layer_8_attn_allreduce_9
	layer_8_qkv_40 [label="Layer8_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_41 [label="Layer8_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_42 [label="Layer8_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_43 [label="Layer8_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_40 [label="Layer8_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_41 [label="Layer8_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_42 [label="Layer8_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_43 [label="Layer8_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_40 [label="Layer8_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_41 [label="Layer8_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_42 [label="Layer8_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_43 [label="Layer8_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_40 [label="Layer8_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_41 [label="Layer8_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_42 [label="Layer8_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_43 [label="Layer8_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_10 [label="Layer8_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_40
	layer_8_qkv_40 -> layer_8_attn_scores_40
	layer_8_attn_scores_40 -> layer_8_attn_softmax_40
	layer_8_attn_softmax_40 -> layer_8_attn_out_40
	layer_8_attn_out_40 -> layer_8_attn_allreduce_10
	layer_8_input -> layer_8_qkv_41
	layer_8_qkv_41 -> layer_8_attn_scores_41
	layer_8_attn_scores_41 -> layer_8_attn_softmax_41
	layer_8_attn_softmax_41 -> layer_8_attn_out_41
	layer_8_attn_out_41 -> layer_8_attn_allreduce_10
	layer_8_input -> layer_8_qkv_42
	layer_8_qkv_42 -> layer_8_attn_scores_42
	layer_8_attn_scores_42 -> layer_8_attn_softmax_42
	layer_8_attn_softmax_42 -> layer_8_attn_out_42
	layer_8_attn_out_42 -> layer_8_attn_allreduce_10
	layer_8_input -> layer_8_qkv_43
	layer_8_qkv_43 -> layer_8_attn_scores_43
	layer_8_attn_scores_43 -> layer_8_attn_softmax_43
	layer_8_attn_softmax_43 -> layer_8_attn_out_43
	layer_8_attn_out_43 -> layer_8_attn_allreduce_10
	layer_8_qkv_44 [label="Layer8_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_45 [label="Layer8_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_46 [label="Layer8_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_47 [label="Layer8_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_44 [label="Layer8_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_45 [label="Layer8_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_46 [label="Layer8_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_47 [label="Layer8_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_44 [label="Layer8_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_45 [label="Layer8_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_46 [label="Layer8_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_47 [label="Layer8_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_44 [label="Layer8_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_45 [label="Layer8_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_46 [label="Layer8_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_47 [label="Layer8_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_11 [label="Layer8_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_44
	layer_8_qkv_44 -> layer_8_attn_scores_44
	layer_8_attn_scores_44 -> layer_8_attn_softmax_44
	layer_8_attn_softmax_44 -> layer_8_attn_out_44
	layer_8_attn_out_44 -> layer_8_attn_allreduce_11
	layer_8_input -> layer_8_qkv_45
	layer_8_qkv_45 -> layer_8_attn_scores_45
	layer_8_attn_scores_45 -> layer_8_attn_softmax_45
	layer_8_attn_softmax_45 -> layer_8_attn_out_45
	layer_8_attn_out_45 -> layer_8_attn_allreduce_11
	layer_8_input -> layer_8_qkv_46
	layer_8_qkv_46 -> layer_8_attn_scores_46
	layer_8_attn_scores_46 -> layer_8_attn_softmax_46
	layer_8_attn_softmax_46 -> layer_8_attn_out_46
	layer_8_attn_out_46 -> layer_8_attn_allreduce_11
	layer_8_input -> layer_8_qkv_47
	layer_8_qkv_47 -> layer_8_attn_scores_47
	layer_8_attn_scores_47 -> layer_8_attn_softmax_47
	layer_8_attn_softmax_47 -> layer_8_attn_out_47
	layer_8_attn_out_47 -> layer_8_attn_allreduce_11
	layer_8_qkv_48 [label="Layer8_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_49 [label="Layer8_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_50 [label="Layer8_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_51 [label="Layer8_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_48 [label="Layer8_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_49 [label="Layer8_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_50 [label="Layer8_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_51 [label="Layer8_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_48 [label="Layer8_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_49 [label="Layer8_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_50 [label="Layer8_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_51 [label="Layer8_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_48 [label="Layer8_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_49 [label="Layer8_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_50 [label="Layer8_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_51 [label="Layer8_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_12 [label="Layer8_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_48
	layer_8_qkv_48 -> layer_8_attn_scores_48
	layer_8_attn_scores_48 -> layer_8_attn_softmax_48
	layer_8_attn_softmax_48 -> layer_8_attn_out_48
	layer_8_attn_out_48 -> layer_8_attn_allreduce_12
	layer_8_input -> layer_8_qkv_49
	layer_8_qkv_49 -> layer_8_attn_scores_49
	layer_8_attn_scores_49 -> layer_8_attn_softmax_49
	layer_8_attn_softmax_49 -> layer_8_attn_out_49
	layer_8_attn_out_49 -> layer_8_attn_allreduce_12
	layer_8_input -> layer_8_qkv_50
	layer_8_qkv_50 -> layer_8_attn_scores_50
	layer_8_attn_scores_50 -> layer_8_attn_softmax_50
	layer_8_attn_softmax_50 -> layer_8_attn_out_50
	layer_8_attn_out_50 -> layer_8_attn_allreduce_12
	layer_8_input -> layer_8_qkv_51
	layer_8_qkv_51 -> layer_8_attn_scores_51
	layer_8_attn_scores_51 -> layer_8_attn_softmax_51
	layer_8_attn_softmax_51 -> layer_8_attn_out_51
	layer_8_attn_out_51 -> layer_8_attn_allreduce_12
	layer_8_qkv_52 [label="Layer8_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_53 [label="Layer8_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_54 [label="Layer8_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_55 [label="Layer8_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_52 [label="Layer8_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_53 [label="Layer8_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_54 [label="Layer8_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_55 [label="Layer8_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_52 [label="Layer8_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_53 [label="Layer8_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_54 [label="Layer8_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_55 [label="Layer8_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_52 [label="Layer8_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_53 [label="Layer8_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_54 [label="Layer8_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_55 [label="Layer8_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_13 [label="Layer8_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_52
	layer_8_qkv_52 -> layer_8_attn_scores_52
	layer_8_attn_scores_52 -> layer_8_attn_softmax_52
	layer_8_attn_softmax_52 -> layer_8_attn_out_52
	layer_8_attn_out_52 -> layer_8_attn_allreduce_13
	layer_8_input -> layer_8_qkv_53
	layer_8_qkv_53 -> layer_8_attn_scores_53
	layer_8_attn_scores_53 -> layer_8_attn_softmax_53
	layer_8_attn_softmax_53 -> layer_8_attn_out_53
	layer_8_attn_out_53 -> layer_8_attn_allreduce_13
	layer_8_input -> layer_8_qkv_54
	layer_8_qkv_54 -> layer_8_attn_scores_54
	layer_8_attn_scores_54 -> layer_8_attn_softmax_54
	layer_8_attn_softmax_54 -> layer_8_attn_out_54
	layer_8_attn_out_54 -> layer_8_attn_allreduce_13
	layer_8_input -> layer_8_qkv_55
	layer_8_qkv_55 -> layer_8_attn_scores_55
	layer_8_attn_scores_55 -> layer_8_attn_softmax_55
	layer_8_attn_softmax_55 -> layer_8_attn_out_55
	layer_8_attn_out_55 -> layer_8_attn_allreduce_13
	layer_8_qkv_56 [label="Layer8_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_57 [label="Layer8_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_58 [label="Layer8_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_59 [label="Layer8_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_56 [label="Layer8_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_57 [label="Layer8_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_58 [label="Layer8_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_59 [label="Layer8_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_56 [label="Layer8_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_57 [label="Layer8_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_58 [label="Layer8_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_59 [label="Layer8_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_56 [label="Layer8_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_57 [label="Layer8_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_58 [label="Layer8_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_59 [label="Layer8_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_14 [label="Layer8_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_56
	layer_8_qkv_56 -> layer_8_attn_scores_56
	layer_8_attn_scores_56 -> layer_8_attn_softmax_56
	layer_8_attn_softmax_56 -> layer_8_attn_out_56
	layer_8_attn_out_56 -> layer_8_attn_allreduce_14
	layer_8_input -> layer_8_qkv_57
	layer_8_qkv_57 -> layer_8_attn_scores_57
	layer_8_attn_scores_57 -> layer_8_attn_softmax_57
	layer_8_attn_softmax_57 -> layer_8_attn_out_57
	layer_8_attn_out_57 -> layer_8_attn_allreduce_14
	layer_8_input -> layer_8_qkv_58
	layer_8_qkv_58 -> layer_8_attn_scores_58
	layer_8_attn_scores_58 -> layer_8_attn_softmax_58
	layer_8_attn_softmax_58 -> layer_8_attn_out_58
	layer_8_attn_out_58 -> layer_8_attn_allreduce_14
	layer_8_input -> layer_8_qkv_59
	layer_8_qkv_59 -> layer_8_attn_scores_59
	layer_8_attn_scores_59 -> layer_8_attn_softmax_59
	layer_8_attn_softmax_59 -> layer_8_attn_out_59
	layer_8_attn_out_59 -> layer_8_attn_allreduce_14
	layer_8_qkv_60 [label="Layer8_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_61 [label="Layer8_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_62 [label="Layer8_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_qkv_63 [label="Layer8_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_60 [label="Layer8_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_61 [label="Layer8_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_62 [label="Layer8_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_scores_63 [label="Layer8_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_60 [label="Layer8_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_61 [label="Layer8_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_62 [label="Layer8_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_softmax_63 [label="Layer8_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_60 [label="Layer8_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_61 [label="Layer8_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_62 [label="Layer8_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_out_63 [label="Layer8_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_15 [label="Layer8_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_input -> layer_8_qkv_60
	layer_8_qkv_60 -> layer_8_attn_scores_60
	layer_8_attn_scores_60 -> layer_8_attn_softmax_60
	layer_8_attn_softmax_60 -> layer_8_attn_out_60
	layer_8_attn_out_60 -> layer_8_attn_allreduce_15
	layer_8_input -> layer_8_qkv_61
	layer_8_qkv_61 -> layer_8_attn_scores_61
	layer_8_attn_scores_61 -> layer_8_attn_softmax_61
	layer_8_attn_softmax_61 -> layer_8_attn_out_61
	layer_8_attn_out_61 -> layer_8_attn_allreduce_15
	layer_8_input -> layer_8_qkv_62
	layer_8_qkv_62 -> layer_8_attn_scores_62
	layer_8_attn_scores_62 -> layer_8_attn_softmax_62
	layer_8_attn_softmax_62 -> layer_8_attn_out_62
	layer_8_attn_out_62 -> layer_8_attn_allreduce_15
	layer_8_input -> layer_8_qkv_63
	layer_8_qkv_63 -> layer_8_attn_scores_63
	layer_8_attn_scores_63 -> layer_8_attn_softmax_63
	layer_8_attn_softmax_63 -> layer_8_attn_out_63
	layer_8_attn_out_63 -> layer_8_attn_allreduce_15
	layer_8_gate_0 [label="Layer8_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_4 [label="Layer8_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_8 [label="Layer8_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_12 [label="Layer8_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_16 [label="Layer8_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_20 [label="Layer8_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_24 [label="Layer8_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_28 [label="Layer8_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_32 [label="Layer8_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_36 [label="Layer8_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_40 [label="Layer8_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_44 [label="Layer8_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_48 [label="Layer8_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_52 [label="Layer8_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_56 [label="Layer8_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_gate_60 [label="Layer8_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_8_alltoall [label="Layer8_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_0 [label="Layer8_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_1 [label="Layer8_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_2 [label="Layer8_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_3 [label="Layer8_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_0 [label="Layer8_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_1 [label="Layer8_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_2 [label="Layer8_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_3 [label="Layer8_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_0 [label="Layer8_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_1 [label="Layer8_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_2 [label="Layer8_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_3 [label="Layer8_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_0 [label="Layer8_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_4 [label="Layer8_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_5 [label="Layer8_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_6 [label="Layer8_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_7 [label="Layer8_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_4 [label="Layer8_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_5 [label="Layer8_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_6 [label="Layer8_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_7 [label="Layer8_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_4 [label="Layer8_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_5 [label="Layer8_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_6 [label="Layer8_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_7 [label="Layer8_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_1 [label="Layer8_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_8 [label="Layer8_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_9 [label="Layer8_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_10 [label="Layer8_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_11 [label="Layer8_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_8 [label="Layer8_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_9 [label="Layer8_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_10 [label="Layer8_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_11 [label="Layer8_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_8 [label="Layer8_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_9 [label="Layer8_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_10 [label="Layer8_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_11 [label="Layer8_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_2 [label="Layer8_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_12 [label="Layer8_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_13 [label="Layer8_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_14 [label="Layer8_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_15 [label="Layer8_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_12 [label="Layer8_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_13 [label="Layer8_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_14 [label="Layer8_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_15 [label="Layer8_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_12 [label="Layer8_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_13 [label="Layer8_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_14 [label="Layer8_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_15 [label="Layer8_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_3 [label="Layer8_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_16 [label="Layer8_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_17 [label="Layer8_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_18 [label="Layer8_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_19 [label="Layer8_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_16 [label="Layer8_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_17 [label="Layer8_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_18 [label="Layer8_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_19 [label="Layer8_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_16 [label="Layer8_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_17 [label="Layer8_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_18 [label="Layer8_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_19 [label="Layer8_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_4 [label="Layer8_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_20 [label="Layer8_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_21 [label="Layer8_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_22 [label="Layer8_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_23 [label="Layer8_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_20 [label="Layer8_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_21 [label="Layer8_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_22 [label="Layer8_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_23 [label="Layer8_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_20 [label="Layer8_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_21 [label="Layer8_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_22 [label="Layer8_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_23 [label="Layer8_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_5 [label="Layer8_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_24 [label="Layer8_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_25 [label="Layer8_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_26 [label="Layer8_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_27 [label="Layer8_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_24 [label="Layer8_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_25 [label="Layer8_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_26 [label="Layer8_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_27 [label="Layer8_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_24 [label="Layer8_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_25 [label="Layer8_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_26 [label="Layer8_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_27 [label="Layer8_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_6 [label="Layer8_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_28 [label="Layer8_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_29 [label="Layer8_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_30 [label="Layer8_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_31 [label="Layer8_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_28 [label="Layer8_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_29 [label="Layer8_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_30 [label="Layer8_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_31 [label="Layer8_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_28 [label="Layer8_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_29 [label="Layer8_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_30 [label="Layer8_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_31 [label="Layer8_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_7 [label="Layer8_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_32 [label="Layer8_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_33 [label="Layer8_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_34 [label="Layer8_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_35 [label="Layer8_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_32 [label="Layer8_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_33 [label="Layer8_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_34 [label="Layer8_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_35 [label="Layer8_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_32 [label="Layer8_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_33 [label="Layer8_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_34 [label="Layer8_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_35 [label="Layer8_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_8 [label="Layer8_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_36 [label="Layer8_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_37 [label="Layer8_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_38 [label="Layer8_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_39 [label="Layer8_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_36 [label="Layer8_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_37 [label="Layer8_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_38 [label="Layer8_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_39 [label="Layer8_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_36 [label="Layer8_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_37 [label="Layer8_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_38 [label="Layer8_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_39 [label="Layer8_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_9 [label="Layer8_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_40 [label="Layer8_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_41 [label="Layer8_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_42 [label="Layer8_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_43 [label="Layer8_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_40 [label="Layer8_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_41 [label="Layer8_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_42 [label="Layer8_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_43 [label="Layer8_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_40 [label="Layer8_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_41 [label="Layer8_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_42 [label="Layer8_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_43 [label="Layer8_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_10 [label="Layer8_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_44 [label="Layer8_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_45 [label="Layer8_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_46 [label="Layer8_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_47 [label="Layer8_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_44 [label="Layer8_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_45 [label="Layer8_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_46 [label="Layer8_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_47 [label="Layer8_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_44 [label="Layer8_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_45 [label="Layer8_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_46 [label="Layer8_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_47 [label="Layer8_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_11 [label="Layer8_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_48 [label="Layer8_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_49 [label="Layer8_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_50 [label="Layer8_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_51 [label="Layer8_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_48 [label="Layer8_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_49 [label="Layer8_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_50 [label="Layer8_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_51 [label="Layer8_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_48 [label="Layer8_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_49 [label="Layer8_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_50 [label="Layer8_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_51 [label="Layer8_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_12 [label="Layer8_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_52 [label="Layer8_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_53 [label="Layer8_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_54 [label="Layer8_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_55 [label="Layer8_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_52 [label="Layer8_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_53 [label="Layer8_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_54 [label="Layer8_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_55 [label="Layer8_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_52 [label="Layer8_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_53 [label="Layer8_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_54 [label="Layer8_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_55 [label="Layer8_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_13 [label="Layer8_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_56 [label="Layer8_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_57 [label="Layer8_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_58 [label="Layer8_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_59 [label="Layer8_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_56 [label="Layer8_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_57 [label="Layer8_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_58 [label="Layer8_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_59 [label="Layer8_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_56 [label="Layer8_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_57 [label="Layer8_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_58 [label="Layer8_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_59 [label="Layer8_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_14 [label="Layer8_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert1_60 [label="Layer8_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_61 [label="Layer8_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_62 [label="Layer8_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert1_63 [label="Layer8_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_60 [label="Layer8_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_61 [label="Layer8_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_62 [label="Layer8_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert_act_63 [label="Layer8_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_8_expert2_60 [label="Layer8_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_61 [label="Layer8_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_62 [label="Layer8_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert2_63 [label="Layer8_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_8_expert_allreduce_15 [label="Layer8_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_8_expert_agg [label="Layer8_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_8_norm [label="Layer8_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_8_attn_allreduce_0 -> layer_8_gate_0 [style=dashed]
	layer_8_gate_0 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_0
	layer_8_expert1_0 -> layer_8_expert_act_0
	layer_8_expert_act_0 -> layer_8_expert2_0
	layer_8_expert2_0 -> layer_8_expert_allreduce_0
	layer_8_alltoall -> layer_8_expert1_1
	layer_8_expert1_1 -> layer_8_expert_act_1
	layer_8_expert_act_1 -> layer_8_expert2_1
	layer_8_expert2_1 -> layer_8_expert_allreduce_0
	layer_8_alltoall -> layer_8_expert1_2
	layer_8_expert1_2 -> layer_8_expert_act_2
	layer_8_expert_act_2 -> layer_8_expert2_2
	layer_8_expert2_2 -> layer_8_expert_allreduce_0
	layer_8_alltoall -> layer_8_expert1_3
	layer_8_expert1_3 -> layer_8_expert_act_3
	layer_8_expert_act_3 -> layer_8_expert2_3
	layer_8_expert2_3 -> layer_8_expert_allreduce_0
	layer_8_expert_allreduce_0 -> layer_8_expert_agg
	layer_8_attn_allreduce_1 -> layer_8_gate_4 [style=dashed]
	layer_8_gate_4 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_4
	layer_8_expert1_4 -> layer_8_expert_act_4
	layer_8_expert_act_4 -> layer_8_expert2_4
	layer_8_expert2_4 -> layer_8_expert_allreduce_1
	layer_8_alltoall -> layer_8_expert1_5
	layer_8_expert1_5 -> layer_8_expert_act_5
	layer_8_expert_act_5 -> layer_8_expert2_5
	layer_8_expert2_5 -> layer_8_expert_allreduce_1
	layer_8_alltoall -> layer_8_expert1_6
	layer_8_expert1_6 -> layer_8_expert_act_6
	layer_8_expert_act_6 -> layer_8_expert2_6
	layer_8_expert2_6 -> layer_8_expert_allreduce_1
	layer_8_alltoall -> layer_8_expert1_7
	layer_8_expert1_7 -> layer_8_expert_act_7
	layer_8_expert_act_7 -> layer_8_expert2_7
	layer_8_expert2_7 -> layer_8_expert_allreduce_1
	layer_8_expert_allreduce_1 -> layer_8_expert_agg
	layer_8_attn_allreduce_2 -> layer_8_gate_8 [style=dashed]
	layer_8_gate_8 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_8
	layer_8_expert1_8 -> layer_8_expert_act_8
	layer_8_expert_act_8 -> layer_8_expert2_8
	layer_8_expert2_8 -> layer_8_expert_allreduce_2
	layer_8_alltoall -> layer_8_expert1_9
	layer_8_expert1_9 -> layer_8_expert_act_9
	layer_8_expert_act_9 -> layer_8_expert2_9
	layer_8_expert2_9 -> layer_8_expert_allreduce_2
	layer_8_alltoall -> layer_8_expert1_10
	layer_8_expert1_10 -> layer_8_expert_act_10
	layer_8_expert_act_10 -> layer_8_expert2_10
	layer_8_expert2_10 -> layer_8_expert_allreduce_2
	layer_8_alltoall -> layer_8_expert1_11
	layer_8_expert1_11 -> layer_8_expert_act_11
	layer_8_expert_act_11 -> layer_8_expert2_11
	layer_8_expert2_11 -> layer_8_expert_allreduce_2
	layer_8_expert_allreduce_2 -> layer_8_expert_agg
	layer_8_attn_allreduce_3 -> layer_8_gate_12 [style=dashed]
	layer_8_gate_12 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_12
	layer_8_expert1_12 -> layer_8_expert_act_12
	layer_8_expert_act_12 -> layer_8_expert2_12
	layer_8_expert2_12 -> layer_8_expert_allreduce_3
	layer_8_alltoall -> layer_8_expert1_13
	layer_8_expert1_13 -> layer_8_expert_act_13
	layer_8_expert_act_13 -> layer_8_expert2_13
	layer_8_expert2_13 -> layer_8_expert_allreduce_3
	layer_8_alltoall -> layer_8_expert1_14
	layer_8_expert1_14 -> layer_8_expert_act_14
	layer_8_expert_act_14 -> layer_8_expert2_14
	layer_8_expert2_14 -> layer_8_expert_allreduce_3
	layer_8_alltoall -> layer_8_expert1_15
	layer_8_expert1_15 -> layer_8_expert_act_15
	layer_8_expert_act_15 -> layer_8_expert2_15
	layer_8_expert2_15 -> layer_8_expert_allreduce_3
	layer_8_expert_allreduce_3 -> layer_8_expert_agg
	layer_8_attn_allreduce_4 -> layer_8_gate_16 [style=dashed]
	layer_8_gate_16 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_16
	layer_8_expert1_16 -> layer_8_expert_act_16
	layer_8_expert_act_16 -> layer_8_expert2_16
	layer_8_expert2_16 -> layer_8_expert_allreduce_4
	layer_8_alltoall -> layer_8_expert1_17
	layer_8_expert1_17 -> layer_8_expert_act_17
	layer_8_expert_act_17 -> layer_8_expert2_17
	layer_8_expert2_17 -> layer_8_expert_allreduce_4
	layer_8_alltoall -> layer_8_expert1_18
	layer_8_expert1_18 -> layer_8_expert_act_18
	layer_8_expert_act_18 -> layer_8_expert2_18
	layer_8_expert2_18 -> layer_8_expert_allreduce_4
	layer_8_alltoall -> layer_8_expert1_19
	layer_8_expert1_19 -> layer_8_expert_act_19
	layer_8_expert_act_19 -> layer_8_expert2_19
	layer_8_expert2_19 -> layer_8_expert_allreduce_4
	layer_8_expert_allreduce_4 -> layer_8_expert_agg
	layer_8_attn_allreduce_5 -> layer_8_gate_20 [style=dashed]
	layer_8_gate_20 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_20
	layer_8_expert1_20 -> layer_8_expert_act_20
	layer_8_expert_act_20 -> layer_8_expert2_20
	layer_8_expert2_20 -> layer_8_expert_allreduce_5
	layer_8_alltoall -> layer_8_expert1_21
	layer_8_expert1_21 -> layer_8_expert_act_21
	layer_8_expert_act_21 -> layer_8_expert2_21
	layer_8_expert2_21 -> layer_8_expert_allreduce_5
	layer_8_alltoall -> layer_8_expert1_22
	layer_8_expert1_22 -> layer_8_expert_act_22
	layer_8_expert_act_22 -> layer_8_expert2_22
	layer_8_expert2_22 -> layer_8_expert_allreduce_5
	layer_8_alltoall -> layer_8_expert1_23
	layer_8_expert1_23 -> layer_8_expert_act_23
	layer_8_expert_act_23 -> layer_8_expert2_23
	layer_8_expert2_23 -> layer_8_expert_allreduce_5
	layer_8_expert_allreduce_5 -> layer_8_expert_agg
	layer_8_attn_allreduce_6 -> layer_8_gate_24 [style=dashed]
	layer_8_gate_24 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_24
	layer_8_expert1_24 -> layer_8_expert_act_24
	layer_8_expert_act_24 -> layer_8_expert2_24
	layer_8_expert2_24 -> layer_8_expert_allreduce_6
	layer_8_alltoall -> layer_8_expert1_25
	layer_8_expert1_25 -> layer_8_expert_act_25
	layer_8_expert_act_25 -> layer_8_expert2_25
	layer_8_expert2_25 -> layer_8_expert_allreduce_6
	layer_8_alltoall -> layer_8_expert1_26
	layer_8_expert1_26 -> layer_8_expert_act_26
	layer_8_expert_act_26 -> layer_8_expert2_26
	layer_8_expert2_26 -> layer_8_expert_allreduce_6
	layer_8_alltoall -> layer_8_expert1_27
	layer_8_expert1_27 -> layer_8_expert_act_27
	layer_8_expert_act_27 -> layer_8_expert2_27
	layer_8_expert2_27 -> layer_8_expert_allreduce_6
	layer_8_expert_allreduce_6 -> layer_8_expert_agg
	layer_8_attn_allreduce_7 -> layer_8_gate_28 [style=dashed]
	layer_8_gate_28 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_28
	layer_8_expert1_28 -> layer_8_expert_act_28
	layer_8_expert_act_28 -> layer_8_expert2_28
	layer_8_expert2_28 -> layer_8_expert_allreduce_7
	layer_8_alltoall -> layer_8_expert1_29
	layer_8_expert1_29 -> layer_8_expert_act_29
	layer_8_expert_act_29 -> layer_8_expert2_29
	layer_8_expert2_29 -> layer_8_expert_allreduce_7
	layer_8_alltoall -> layer_8_expert1_30
	layer_8_expert1_30 -> layer_8_expert_act_30
	layer_8_expert_act_30 -> layer_8_expert2_30
	layer_8_expert2_30 -> layer_8_expert_allreduce_7
	layer_8_alltoall -> layer_8_expert1_31
	layer_8_expert1_31 -> layer_8_expert_act_31
	layer_8_expert_act_31 -> layer_8_expert2_31
	layer_8_expert2_31 -> layer_8_expert_allreduce_7
	layer_8_expert_allreduce_7 -> layer_8_expert_agg
	layer_8_attn_allreduce_8 -> layer_8_gate_32 [style=dashed]
	layer_8_gate_32 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_32
	layer_8_expert1_32 -> layer_8_expert_act_32
	layer_8_expert_act_32 -> layer_8_expert2_32
	layer_8_expert2_32 -> layer_8_expert_allreduce_8
	layer_8_alltoall -> layer_8_expert1_33
	layer_8_expert1_33 -> layer_8_expert_act_33
	layer_8_expert_act_33 -> layer_8_expert2_33
	layer_8_expert2_33 -> layer_8_expert_allreduce_8
	layer_8_alltoall -> layer_8_expert1_34
	layer_8_expert1_34 -> layer_8_expert_act_34
	layer_8_expert_act_34 -> layer_8_expert2_34
	layer_8_expert2_34 -> layer_8_expert_allreduce_8
	layer_8_alltoall -> layer_8_expert1_35
	layer_8_expert1_35 -> layer_8_expert_act_35
	layer_8_expert_act_35 -> layer_8_expert2_35
	layer_8_expert2_35 -> layer_8_expert_allreduce_8
	layer_8_expert_allreduce_8 -> layer_8_expert_agg
	layer_8_attn_allreduce_9 -> layer_8_gate_36 [style=dashed]
	layer_8_gate_36 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_36
	layer_8_expert1_36 -> layer_8_expert_act_36
	layer_8_expert_act_36 -> layer_8_expert2_36
	layer_8_expert2_36 -> layer_8_expert_allreduce_9
	layer_8_alltoall -> layer_8_expert1_37
	layer_8_expert1_37 -> layer_8_expert_act_37
	layer_8_expert_act_37 -> layer_8_expert2_37
	layer_8_expert2_37 -> layer_8_expert_allreduce_9
	layer_8_alltoall -> layer_8_expert1_38
	layer_8_expert1_38 -> layer_8_expert_act_38
	layer_8_expert_act_38 -> layer_8_expert2_38
	layer_8_expert2_38 -> layer_8_expert_allreduce_9
	layer_8_alltoall -> layer_8_expert1_39
	layer_8_expert1_39 -> layer_8_expert_act_39
	layer_8_expert_act_39 -> layer_8_expert2_39
	layer_8_expert2_39 -> layer_8_expert_allreduce_9
	layer_8_expert_allreduce_9 -> layer_8_expert_agg
	layer_8_attn_allreduce_10 -> layer_8_gate_40 [style=dashed]
	layer_8_gate_40 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_40
	layer_8_expert1_40 -> layer_8_expert_act_40
	layer_8_expert_act_40 -> layer_8_expert2_40
	layer_8_expert2_40 -> layer_8_expert_allreduce_10
	layer_8_alltoall -> layer_8_expert1_41
	layer_8_expert1_41 -> layer_8_expert_act_41
	layer_8_expert_act_41 -> layer_8_expert2_41
	layer_8_expert2_41 -> layer_8_expert_allreduce_10
	layer_8_alltoall -> layer_8_expert1_42
	layer_8_expert1_42 -> layer_8_expert_act_42
	layer_8_expert_act_42 -> layer_8_expert2_42
	layer_8_expert2_42 -> layer_8_expert_allreduce_10
	layer_8_alltoall -> layer_8_expert1_43
	layer_8_expert1_43 -> layer_8_expert_act_43
	layer_8_expert_act_43 -> layer_8_expert2_43
	layer_8_expert2_43 -> layer_8_expert_allreduce_10
	layer_8_expert_allreduce_10 -> layer_8_expert_agg
	layer_8_attn_allreduce_11 -> layer_8_gate_44 [style=dashed]
	layer_8_gate_44 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_44
	layer_8_expert1_44 -> layer_8_expert_act_44
	layer_8_expert_act_44 -> layer_8_expert2_44
	layer_8_expert2_44 -> layer_8_expert_allreduce_11
	layer_8_alltoall -> layer_8_expert1_45
	layer_8_expert1_45 -> layer_8_expert_act_45
	layer_8_expert_act_45 -> layer_8_expert2_45
	layer_8_expert2_45 -> layer_8_expert_allreduce_11
	layer_8_alltoall -> layer_8_expert1_46
	layer_8_expert1_46 -> layer_8_expert_act_46
	layer_8_expert_act_46 -> layer_8_expert2_46
	layer_8_expert2_46 -> layer_8_expert_allreduce_11
	layer_8_alltoall -> layer_8_expert1_47
	layer_8_expert1_47 -> layer_8_expert_act_47
	layer_8_expert_act_47 -> layer_8_expert2_47
	layer_8_expert2_47 -> layer_8_expert_allreduce_11
	layer_8_expert_allreduce_11 -> layer_8_expert_agg
	layer_8_attn_allreduce_12 -> layer_8_gate_48 [style=dashed]
	layer_8_gate_48 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_48
	layer_8_expert1_48 -> layer_8_expert_act_48
	layer_8_expert_act_48 -> layer_8_expert2_48
	layer_8_expert2_48 -> layer_8_expert_allreduce_12
	layer_8_alltoall -> layer_8_expert1_49
	layer_8_expert1_49 -> layer_8_expert_act_49
	layer_8_expert_act_49 -> layer_8_expert2_49
	layer_8_expert2_49 -> layer_8_expert_allreduce_12
	layer_8_alltoall -> layer_8_expert1_50
	layer_8_expert1_50 -> layer_8_expert_act_50
	layer_8_expert_act_50 -> layer_8_expert2_50
	layer_8_expert2_50 -> layer_8_expert_allreduce_12
	layer_8_alltoall -> layer_8_expert1_51
	layer_8_expert1_51 -> layer_8_expert_act_51
	layer_8_expert_act_51 -> layer_8_expert2_51
	layer_8_expert2_51 -> layer_8_expert_allreduce_12
	layer_8_expert_allreduce_12 -> layer_8_expert_agg
	layer_8_attn_allreduce_13 -> layer_8_gate_52 [style=dashed]
	layer_8_gate_52 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_52
	layer_8_expert1_52 -> layer_8_expert_act_52
	layer_8_expert_act_52 -> layer_8_expert2_52
	layer_8_expert2_52 -> layer_8_expert_allreduce_13
	layer_8_alltoall -> layer_8_expert1_53
	layer_8_expert1_53 -> layer_8_expert_act_53
	layer_8_expert_act_53 -> layer_8_expert2_53
	layer_8_expert2_53 -> layer_8_expert_allreduce_13
	layer_8_alltoall -> layer_8_expert1_54
	layer_8_expert1_54 -> layer_8_expert_act_54
	layer_8_expert_act_54 -> layer_8_expert2_54
	layer_8_expert2_54 -> layer_8_expert_allreduce_13
	layer_8_alltoall -> layer_8_expert1_55
	layer_8_expert1_55 -> layer_8_expert_act_55
	layer_8_expert_act_55 -> layer_8_expert2_55
	layer_8_expert2_55 -> layer_8_expert_allreduce_13
	layer_8_expert_allreduce_13 -> layer_8_expert_agg
	layer_8_attn_allreduce_14 -> layer_8_gate_56 [style=dashed]
	layer_8_gate_56 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_56
	layer_8_expert1_56 -> layer_8_expert_act_56
	layer_8_expert_act_56 -> layer_8_expert2_56
	layer_8_expert2_56 -> layer_8_expert_allreduce_14
	layer_8_alltoall -> layer_8_expert1_57
	layer_8_expert1_57 -> layer_8_expert_act_57
	layer_8_expert_act_57 -> layer_8_expert2_57
	layer_8_expert2_57 -> layer_8_expert_allreduce_14
	layer_8_alltoall -> layer_8_expert1_58
	layer_8_expert1_58 -> layer_8_expert_act_58
	layer_8_expert_act_58 -> layer_8_expert2_58
	layer_8_expert2_58 -> layer_8_expert_allreduce_14
	layer_8_alltoall -> layer_8_expert1_59
	layer_8_expert1_59 -> layer_8_expert_act_59
	layer_8_expert_act_59 -> layer_8_expert2_59
	layer_8_expert2_59 -> layer_8_expert_allreduce_14
	layer_8_expert_allreduce_14 -> layer_8_expert_agg
	layer_8_attn_allreduce_15 -> layer_8_gate_60 [style=dashed]
	layer_8_gate_60 -> layer_8_alltoall [style=dashed]
	layer_8_alltoall -> layer_8_expert1_60
	layer_8_expert1_60 -> layer_8_expert_act_60
	layer_8_expert_act_60 -> layer_8_expert2_60
	layer_8_expert2_60 -> layer_8_expert_allreduce_15
	layer_8_alltoall -> layer_8_expert1_61
	layer_8_expert1_61 -> layer_8_expert_act_61
	layer_8_expert_act_61 -> layer_8_expert2_61
	layer_8_expert2_61 -> layer_8_expert_allreduce_15
	layer_8_alltoall -> layer_8_expert1_62
	layer_8_expert1_62 -> layer_8_expert_act_62
	layer_8_expert_act_62 -> layer_8_expert2_62
	layer_8_expert2_62 -> layer_8_expert_allreduce_15
	layer_8_alltoall -> layer_8_expert1_63
	layer_8_expert1_63 -> layer_8_expert_act_63
	layer_8_expert_act_63 -> layer_8_expert2_63
	layer_8_expert2_63 -> layer_8_expert_allreduce_15
	layer_8_expert_allreduce_15 -> layer_8_expert_agg
	layer_8_expert_agg -> layer_8_norm
	layer_8_norm -> layer_9_input
	layer_9_input [label="Layer9_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_9_qkv_0 [label="Layer9_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_1 [label="Layer9_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_2 [label="Layer9_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_3 [label="Layer9_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_0 [label="Layer9_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_1 [label="Layer9_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_2 [label="Layer9_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_3 [label="Layer9_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_0 [label="Layer9_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_1 [label="Layer9_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_2 [label="Layer9_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_3 [label="Layer9_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_0 [label="Layer9_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_1 [label="Layer9_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_2 [label="Layer9_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_3 [label="Layer9_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_0 [label="Layer9_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_0
	layer_9_qkv_0 -> layer_9_attn_scores_0
	layer_9_attn_scores_0 -> layer_9_attn_softmax_0
	layer_9_attn_softmax_0 -> layer_9_attn_out_0
	layer_9_attn_out_0 -> layer_9_attn_allreduce_0
	layer_9_input -> layer_9_qkv_1
	layer_9_qkv_1 -> layer_9_attn_scores_1
	layer_9_attn_scores_1 -> layer_9_attn_softmax_1
	layer_9_attn_softmax_1 -> layer_9_attn_out_1
	layer_9_attn_out_1 -> layer_9_attn_allreduce_0
	layer_9_input -> layer_9_qkv_2
	layer_9_qkv_2 -> layer_9_attn_scores_2
	layer_9_attn_scores_2 -> layer_9_attn_softmax_2
	layer_9_attn_softmax_2 -> layer_9_attn_out_2
	layer_9_attn_out_2 -> layer_9_attn_allreduce_0
	layer_9_input -> layer_9_qkv_3
	layer_9_qkv_3 -> layer_9_attn_scores_3
	layer_9_attn_scores_3 -> layer_9_attn_softmax_3
	layer_9_attn_softmax_3 -> layer_9_attn_out_3
	layer_9_attn_out_3 -> layer_9_attn_allreduce_0
	layer_9_qkv_4 [label="Layer9_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_5 [label="Layer9_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_6 [label="Layer9_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_7 [label="Layer9_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_4 [label="Layer9_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_5 [label="Layer9_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_6 [label="Layer9_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_7 [label="Layer9_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_4 [label="Layer9_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_5 [label="Layer9_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_6 [label="Layer9_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_7 [label="Layer9_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_4 [label="Layer9_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_5 [label="Layer9_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_6 [label="Layer9_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_7 [label="Layer9_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_1 [label="Layer9_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_4
	layer_9_qkv_4 -> layer_9_attn_scores_4
	layer_9_attn_scores_4 -> layer_9_attn_softmax_4
	layer_9_attn_softmax_4 -> layer_9_attn_out_4
	layer_9_attn_out_4 -> layer_9_attn_allreduce_1
	layer_9_input -> layer_9_qkv_5
	layer_9_qkv_5 -> layer_9_attn_scores_5
	layer_9_attn_scores_5 -> layer_9_attn_softmax_5
	layer_9_attn_softmax_5 -> layer_9_attn_out_5
	layer_9_attn_out_5 -> layer_9_attn_allreduce_1
	layer_9_input -> layer_9_qkv_6
	layer_9_qkv_6 -> layer_9_attn_scores_6
	layer_9_attn_scores_6 -> layer_9_attn_softmax_6
	layer_9_attn_softmax_6 -> layer_9_attn_out_6
	layer_9_attn_out_6 -> layer_9_attn_allreduce_1
	layer_9_input -> layer_9_qkv_7
	layer_9_qkv_7 -> layer_9_attn_scores_7
	layer_9_attn_scores_7 -> layer_9_attn_softmax_7
	layer_9_attn_softmax_7 -> layer_9_attn_out_7
	layer_9_attn_out_7 -> layer_9_attn_allreduce_1
	layer_9_qkv_8 [label="Layer9_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_9 [label="Layer9_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_10 [label="Layer9_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_11 [label="Layer9_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_8 [label="Layer9_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_9 [label="Layer9_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_10 [label="Layer9_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_11 [label="Layer9_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_8 [label="Layer9_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_9 [label="Layer9_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_10 [label="Layer9_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_11 [label="Layer9_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_8 [label="Layer9_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_9 [label="Layer9_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_10 [label="Layer9_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_11 [label="Layer9_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_2 [label="Layer9_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_8
	layer_9_qkv_8 -> layer_9_attn_scores_8
	layer_9_attn_scores_8 -> layer_9_attn_softmax_8
	layer_9_attn_softmax_8 -> layer_9_attn_out_8
	layer_9_attn_out_8 -> layer_9_attn_allreduce_2
	layer_9_input -> layer_9_qkv_9
	layer_9_qkv_9 -> layer_9_attn_scores_9
	layer_9_attn_scores_9 -> layer_9_attn_softmax_9
	layer_9_attn_softmax_9 -> layer_9_attn_out_9
	layer_9_attn_out_9 -> layer_9_attn_allreduce_2
	layer_9_input -> layer_9_qkv_10
	layer_9_qkv_10 -> layer_9_attn_scores_10
	layer_9_attn_scores_10 -> layer_9_attn_softmax_10
	layer_9_attn_softmax_10 -> layer_9_attn_out_10
	layer_9_attn_out_10 -> layer_9_attn_allreduce_2
	layer_9_input -> layer_9_qkv_11
	layer_9_qkv_11 -> layer_9_attn_scores_11
	layer_9_attn_scores_11 -> layer_9_attn_softmax_11
	layer_9_attn_softmax_11 -> layer_9_attn_out_11
	layer_9_attn_out_11 -> layer_9_attn_allreduce_2
	layer_9_qkv_12 [label="Layer9_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_13 [label="Layer9_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_14 [label="Layer9_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_15 [label="Layer9_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_12 [label="Layer9_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_13 [label="Layer9_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_14 [label="Layer9_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_15 [label="Layer9_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_12 [label="Layer9_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_13 [label="Layer9_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_14 [label="Layer9_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_15 [label="Layer9_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_12 [label="Layer9_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_13 [label="Layer9_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_14 [label="Layer9_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_15 [label="Layer9_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_3 [label="Layer9_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_12
	layer_9_qkv_12 -> layer_9_attn_scores_12
	layer_9_attn_scores_12 -> layer_9_attn_softmax_12
	layer_9_attn_softmax_12 -> layer_9_attn_out_12
	layer_9_attn_out_12 -> layer_9_attn_allreduce_3
	layer_9_input -> layer_9_qkv_13
	layer_9_qkv_13 -> layer_9_attn_scores_13
	layer_9_attn_scores_13 -> layer_9_attn_softmax_13
	layer_9_attn_softmax_13 -> layer_9_attn_out_13
	layer_9_attn_out_13 -> layer_9_attn_allreduce_3
	layer_9_input -> layer_9_qkv_14
	layer_9_qkv_14 -> layer_9_attn_scores_14
	layer_9_attn_scores_14 -> layer_9_attn_softmax_14
	layer_9_attn_softmax_14 -> layer_9_attn_out_14
	layer_9_attn_out_14 -> layer_9_attn_allreduce_3
	layer_9_input -> layer_9_qkv_15
	layer_9_qkv_15 -> layer_9_attn_scores_15
	layer_9_attn_scores_15 -> layer_9_attn_softmax_15
	layer_9_attn_softmax_15 -> layer_9_attn_out_15
	layer_9_attn_out_15 -> layer_9_attn_allreduce_3
	layer_9_qkv_16 [label="Layer9_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_17 [label="Layer9_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_18 [label="Layer9_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_19 [label="Layer9_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_16 [label="Layer9_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_17 [label="Layer9_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_18 [label="Layer9_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_19 [label="Layer9_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_16 [label="Layer9_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_17 [label="Layer9_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_18 [label="Layer9_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_19 [label="Layer9_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_16 [label="Layer9_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_17 [label="Layer9_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_18 [label="Layer9_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_19 [label="Layer9_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_4 [label="Layer9_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_16
	layer_9_qkv_16 -> layer_9_attn_scores_16
	layer_9_attn_scores_16 -> layer_9_attn_softmax_16
	layer_9_attn_softmax_16 -> layer_9_attn_out_16
	layer_9_attn_out_16 -> layer_9_attn_allreduce_4
	layer_9_input -> layer_9_qkv_17
	layer_9_qkv_17 -> layer_9_attn_scores_17
	layer_9_attn_scores_17 -> layer_9_attn_softmax_17
	layer_9_attn_softmax_17 -> layer_9_attn_out_17
	layer_9_attn_out_17 -> layer_9_attn_allreduce_4
	layer_9_input -> layer_9_qkv_18
	layer_9_qkv_18 -> layer_9_attn_scores_18
	layer_9_attn_scores_18 -> layer_9_attn_softmax_18
	layer_9_attn_softmax_18 -> layer_9_attn_out_18
	layer_9_attn_out_18 -> layer_9_attn_allreduce_4
	layer_9_input -> layer_9_qkv_19
	layer_9_qkv_19 -> layer_9_attn_scores_19
	layer_9_attn_scores_19 -> layer_9_attn_softmax_19
	layer_9_attn_softmax_19 -> layer_9_attn_out_19
	layer_9_attn_out_19 -> layer_9_attn_allreduce_4
	layer_9_qkv_20 [label="Layer9_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_21 [label="Layer9_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_22 [label="Layer9_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_23 [label="Layer9_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_20 [label="Layer9_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_21 [label="Layer9_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_22 [label="Layer9_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_23 [label="Layer9_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_20 [label="Layer9_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_21 [label="Layer9_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_22 [label="Layer9_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_23 [label="Layer9_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_20 [label="Layer9_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_21 [label="Layer9_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_22 [label="Layer9_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_23 [label="Layer9_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_5 [label="Layer9_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_20
	layer_9_qkv_20 -> layer_9_attn_scores_20
	layer_9_attn_scores_20 -> layer_9_attn_softmax_20
	layer_9_attn_softmax_20 -> layer_9_attn_out_20
	layer_9_attn_out_20 -> layer_9_attn_allreduce_5
	layer_9_input -> layer_9_qkv_21
	layer_9_qkv_21 -> layer_9_attn_scores_21
	layer_9_attn_scores_21 -> layer_9_attn_softmax_21
	layer_9_attn_softmax_21 -> layer_9_attn_out_21
	layer_9_attn_out_21 -> layer_9_attn_allreduce_5
	layer_9_input -> layer_9_qkv_22
	layer_9_qkv_22 -> layer_9_attn_scores_22
	layer_9_attn_scores_22 -> layer_9_attn_softmax_22
	layer_9_attn_softmax_22 -> layer_9_attn_out_22
	layer_9_attn_out_22 -> layer_9_attn_allreduce_5
	layer_9_input -> layer_9_qkv_23
	layer_9_qkv_23 -> layer_9_attn_scores_23
	layer_9_attn_scores_23 -> layer_9_attn_softmax_23
	layer_9_attn_softmax_23 -> layer_9_attn_out_23
	layer_9_attn_out_23 -> layer_9_attn_allreduce_5
	layer_9_qkv_24 [label="Layer9_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_25 [label="Layer9_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_26 [label="Layer9_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_27 [label="Layer9_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_24 [label="Layer9_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_25 [label="Layer9_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_26 [label="Layer9_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_27 [label="Layer9_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_24 [label="Layer9_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_25 [label="Layer9_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_26 [label="Layer9_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_27 [label="Layer9_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_24 [label="Layer9_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_25 [label="Layer9_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_26 [label="Layer9_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_27 [label="Layer9_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_6 [label="Layer9_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_24
	layer_9_qkv_24 -> layer_9_attn_scores_24
	layer_9_attn_scores_24 -> layer_9_attn_softmax_24
	layer_9_attn_softmax_24 -> layer_9_attn_out_24
	layer_9_attn_out_24 -> layer_9_attn_allreduce_6
	layer_9_input -> layer_9_qkv_25
	layer_9_qkv_25 -> layer_9_attn_scores_25
	layer_9_attn_scores_25 -> layer_9_attn_softmax_25
	layer_9_attn_softmax_25 -> layer_9_attn_out_25
	layer_9_attn_out_25 -> layer_9_attn_allreduce_6
	layer_9_input -> layer_9_qkv_26
	layer_9_qkv_26 -> layer_9_attn_scores_26
	layer_9_attn_scores_26 -> layer_9_attn_softmax_26
	layer_9_attn_softmax_26 -> layer_9_attn_out_26
	layer_9_attn_out_26 -> layer_9_attn_allreduce_6
	layer_9_input -> layer_9_qkv_27
	layer_9_qkv_27 -> layer_9_attn_scores_27
	layer_9_attn_scores_27 -> layer_9_attn_softmax_27
	layer_9_attn_softmax_27 -> layer_9_attn_out_27
	layer_9_attn_out_27 -> layer_9_attn_allreduce_6
	layer_9_qkv_28 [label="Layer9_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_29 [label="Layer9_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_30 [label="Layer9_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_31 [label="Layer9_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_28 [label="Layer9_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_29 [label="Layer9_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_30 [label="Layer9_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_31 [label="Layer9_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_28 [label="Layer9_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_29 [label="Layer9_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_30 [label="Layer9_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_31 [label="Layer9_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_28 [label="Layer9_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_29 [label="Layer9_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_30 [label="Layer9_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_31 [label="Layer9_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_7 [label="Layer9_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_28
	layer_9_qkv_28 -> layer_9_attn_scores_28
	layer_9_attn_scores_28 -> layer_9_attn_softmax_28
	layer_9_attn_softmax_28 -> layer_9_attn_out_28
	layer_9_attn_out_28 -> layer_9_attn_allreduce_7
	layer_9_input -> layer_9_qkv_29
	layer_9_qkv_29 -> layer_9_attn_scores_29
	layer_9_attn_scores_29 -> layer_9_attn_softmax_29
	layer_9_attn_softmax_29 -> layer_9_attn_out_29
	layer_9_attn_out_29 -> layer_9_attn_allreduce_7
	layer_9_input -> layer_9_qkv_30
	layer_9_qkv_30 -> layer_9_attn_scores_30
	layer_9_attn_scores_30 -> layer_9_attn_softmax_30
	layer_9_attn_softmax_30 -> layer_9_attn_out_30
	layer_9_attn_out_30 -> layer_9_attn_allreduce_7
	layer_9_input -> layer_9_qkv_31
	layer_9_qkv_31 -> layer_9_attn_scores_31
	layer_9_attn_scores_31 -> layer_9_attn_softmax_31
	layer_9_attn_softmax_31 -> layer_9_attn_out_31
	layer_9_attn_out_31 -> layer_9_attn_allreduce_7
	layer_9_qkv_32 [label="Layer9_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_33 [label="Layer9_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_34 [label="Layer9_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_35 [label="Layer9_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_32 [label="Layer9_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_33 [label="Layer9_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_34 [label="Layer9_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_35 [label="Layer9_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_32 [label="Layer9_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_33 [label="Layer9_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_34 [label="Layer9_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_35 [label="Layer9_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_32 [label="Layer9_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_33 [label="Layer9_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_34 [label="Layer9_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_35 [label="Layer9_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_8 [label="Layer9_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_32
	layer_9_qkv_32 -> layer_9_attn_scores_32
	layer_9_attn_scores_32 -> layer_9_attn_softmax_32
	layer_9_attn_softmax_32 -> layer_9_attn_out_32
	layer_9_attn_out_32 -> layer_9_attn_allreduce_8
	layer_9_input -> layer_9_qkv_33
	layer_9_qkv_33 -> layer_9_attn_scores_33
	layer_9_attn_scores_33 -> layer_9_attn_softmax_33
	layer_9_attn_softmax_33 -> layer_9_attn_out_33
	layer_9_attn_out_33 -> layer_9_attn_allreduce_8
	layer_9_input -> layer_9_qkv_34
	layer_9_qkv_34 -> layer_9_attn_scores_34
	layer_9_attn_scores_34 -> layer_9_attn_softmax_34
	layer_9_attn_softmax_34 -> layer_9_attn_out_34
	layer_9_attn_out_34 -> layer_9_attn_allreduce_8
	layer_9_input -> layer_9_qkv_35
	layer_9_qkv_35 -> layer_9_attn_scores_35
	layer_9_attn_scores_35 -> layer_9_attn_softmax_35
	layer_9_attn_softmax_35 -> layer_9_attn_out_35
	layer_9_attn_out_35 -> layer_9_attn_allreduce_8
	layer_9_qkv_36 [label="Layer9_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_37 [label="Layer9_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_38 [label="Layer9_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_39 [label="Layer9_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_36 [label="Layer9_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_37 [label="Layer9_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_38 [label="Layer9_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_39 [label="Layer9_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_36 [label="Layer9_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_37 [label="Layer9_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_38 [label="Layer9_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_39 [label="Layer9_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_36 [label="Layer9_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_37 [label="Layer9_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_38 [label="Layer9_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_39 [label="Layer9_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_9 [label="Layer9_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_36
	layer_9_qkv_36 -> layer_9_attn_scores_36
	layer_9_attn_scores_36 -> layer_9_attn_softmax_36
	layer_9_attn_softmax_36 -> layer_9_attn_out_36
	layer_9_attn_out_36 -> layer_9_attn_allreduce_9
	layer_9_input -> layer_9_qkv_37
	layer_9_qkv_37 -> layer_9_attn_scores_37
	layer_9_attn_scores_37 -> layer_9_attn_softmax_37
	layer_9_attn_softmax_37 -> layer_9_attn_out_37
	layer_9_attn_out_37 -> layer_9_attn_allreduce_9
	layer_9_input -> layer_9_qkv_38
	layer_9_qkv_38 -> layer_9_attn_scores_38
	layer_9_attn_scores_38 -> layer_9_attn_softmax_38
	layer_9_attn_softmax_38 -> layer_9_attn_out_38
	layer_9_attn_out_38 -> layer_9_attn_allreduce_9
	layer_9_input -> layer_9_qkv_39
	layer_9_qkv_39 -> layer_9_attn_scores_39
	layer_9_attn_scores_39 -> layer_9_attn_softmax_39
	layer_9_attn_softmax_39 -> layer_9_attn_out_39
	layer_9_attn_out_39 -> layer_9_attn_allreduce_9
	layer_9_qkv_40 [label="Layer9_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_41 [label="Layer9_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_42 [label="Layer9_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_43 [label="Layer9_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_40 [label="Layer9_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_41 [label="Layer9_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_42 [label="Layer9_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_43 [label="Layer9_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_40 [label="Layer9_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_41 [label="Layer9_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_42 [label="Layer9_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_43 [label="Layer9_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_40 [label="Layer9_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_41 [label="Layer9_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_42 [label="Layer9_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_43 [label="Layer9_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_10 [label="Layer9_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_40
	layer_9_qkv_40 -> layer_9_attn_scores_40
	layer_9_attn_scores_40 -> layer_9_attn_softmax_40
	layer_9_attn_softmax_40 -> layer_9_attn_out_40
	layer_9_attn_out_40 -> layer_9_attn_allreduce_10
	layer_9_input -> layer_9_qkv_41
	layer_9_qkv_41 -> layer_9_attn_scores_41
	layer_9_attn_scores_41 -> layer_9_attn_softmax_41
	layer_9_attn_softmax_41 -> layer_9_attn_out_41
	layer_9_attn_out_41 -> layer_9_attn_allreduce_10
	layer_9_input -> layer_9_qkv_42
	layer_9_qkv_42 -> layer_9_attn_scores_42
	layer_9_attn_scores_42 -> layer_9_attn_softmax_42
	layer_9_attn_softmax_42 -> layer_9_attn_out_42
	layer_9_attn_out_42 -> layer_9_attn_allreduce_10
	layer_9_input -> layer_9_qkv_43
	layer_9_qkv_43 -> layer_9_attn_scores_43
	layer_9_attn_scores_43 -> layer_9_attn_softmax_43
	layer_9_attn_softmax_43 -> layer_9_attn_out_43
	layer_9_attn_out_43 -> layer_9_attn_allreduce_10
	layer_9_qkv_44 [label="Layer9_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_45 [label="Layer9_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_46 [label="Layer9_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_47 [label="Layer9_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_44 [label="Layer9_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_45 [label="Layer9_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_46 [label="Layer9_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_47 [label="Layer9_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_44 [label="Layer9_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_45 [label="Layer9_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_46 [label="Layer9_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_47 [label="Layer9_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_44 [label="Layer9_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_45 [label="Layer9_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_46 [label="Layer9_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_47 [label="Layer9_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_11 [label="Layer9_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_44
	layer_9_qkv_44 -> layer_9_attn_scores_44
	layer_9_attn_scores_44 -> layer_9_attn_softmax_44
	layer_9_attn_softmax_44 -> layer_9_attn_out_44
	layer_9_attn_out_44 -> layer_9_attn_allreduce_11
	layer_9_input -> layer_9_qkv_45
	layer_9_qkv_45 -> layer_9_attn_scores_45
	layer_9_attn_scores_45 -> layer_9_attn_softmax_45
	layer_9_attn_softmax_45 -> layer_9_attn_out_45
	layer_9_attn_out_45 -> layer_9_attn_allreduce_11
	layer_9_input -> layer_9_qkv_46
	layer_9_qkv_46 -> layer_9_attn_scores_46
	layer_9_attn_scores_46 -> layer_9_attn_softmax_46
	layer_9_attn_softmax_46 -> layer_9_attn_out_46
	layer_9_attn_out_46 -> layer_9_attn_allreduce_11
	layer_9_input -> layer_9_qkv_47
	layer_9_qkv_47 -> layer_9_attn_scores_47
	layer_9_attn_scores_47 -> layer_9_attn_softmax_47
	layer_9_attn_softmax_47 -> layer_9_attn_out_47
	layer_9_attn_out_47 -> layer_9_attn_allreduce_11
	layer_9_qkv_48 [label="Layer9_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_49 [label="Layer9_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_50 [label="Layer9_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_51 [label="Layer9_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_48 [label="Layer9_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_49 [label="Layer9_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_50 [label="Layer9_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_51 [label="Layer9_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_48 [label="Layer9_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_49 [label="Layer9_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_50 [label="Layer9_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_51 [label="Layer9_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_48 [label="Layer9_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_49 [label="Layer9_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_50 [label="Layer9_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_51 [label="Layer9_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_12 [label="Layer9_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_48
	layer_9_qkv_48 -> layer_9_attn_scores_48
	layer_9_attn_scores_48 -> layer_9_attn_softmax_48
	layer_9_attn_softmax_48 -> layer_9_attn_out_48
	layer_9_attn_out_48 -> layer_9_attn_allreduce_12
	layer_9_input -> layer_9_qkv_49
	layer_9_qkv_49 -> layer_9_attn_scores_49
	layer_9_attn_scores_49 -> layer_9_attn_softmax_49
	layer_9_attn_softmax_49 -> layer_9_attn_out_49
	layer_9_attn_out_49 -> layer_9_attn_allreduce_12
	layer_9_input -> layer_9_qkv_50
	layer_9_qkv_50 -> layer_9_attn_scores_50
	layer_9_attn_scores_50 -> layer_9_attn_softmax_50
	layer_9_attn_softmax_50 -> layer_9_attn_out_50
	layer_9_attn_out_50 -> layer_9_attn_allreduce_12
	layer_9_input -> layer_9_qkv_51
	layer_9_qkv_51 -> layer_9_attn_scores_51
	layer_9_attn_scores_51 -> layer_9_attn_softmax_51
	layer_9_attn_softmax_51 -> layer_9_attn_out_51
	layer_9_attn_out_51 -> layer_9_attn_allreduce_12
	layer_9_qkv_52 [label="Layer9_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_53 [label="Layer9_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_54 [label="Layer9_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_55 [label="Layer9_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_52 [label="Layer9_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_53 [label="Layer9_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_54 [label="Layer9_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_55 [label="Layer9_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_52 [label="Layer9_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_53 [label="Layer9_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_54 [label="Layer9_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_55 [label="Layer9_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_52 [label="Layer9_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_53 [label="Layer9_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_54 [label="Layer9_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_55 [label="Layer9_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_13 [label="Layer9_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_52
	layer_9_qkv_52 -> layer_9_attn_scores_52
	layer_9_attn_scores_52 -> layer_9_attn_softmax_52
	layer_9_attn_softmax_52 -> layer_9_attn_out_52
	layer_9_attn_out_52 -> layer_9_attn_allreduce_13
	layer_9_input -> layer_9_qkv_53
	layer_9_qkv_53 -> layer_9_attn_scores_53
	layer_9_attn_scores_53 -> layer_9_attn_softmax_53
	layer_9_attn_softmax_53 -> layer_9_attn_out_53
	layer_9_attn_out_53 -> layer_9_attn_allreduce_13
	layer_9_input -> layer_9_qkv_54
	layer_9_qkv_54 -> layer_9_attn_scores_54
	layer_9_attn_scores_54 -> layer_9_attn_softmax_54
	layer_9_attn_softmax_54 -> layer_9_attn_out_54
	layer_9_attn_out_54 -> layer_9_attn_allreduce_13
	layer_9_input -> layer_9_qkv_55
	layer_9_qkv_55 -> layer_9_attn_scores_55
	layer_9_attn_scores_55 -> layer_9_attn_softmax_55
	layer_9_attn_softmax_55 -> layer_9_attn_out_55
	layer_9_attn_out_55 -> layer_9_attn_allreduce_13
	layer_9_qkv_56 [label="Layer9_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_57 [label="Layer9_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_58 [label="Layer9_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_59 [label="Layer9_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_56 [label="Layer9_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_57 [label="Layer9_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_58 [label="Layer9_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_59 [label="Layer9_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_56 [label="Layer9_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_57 [label="Layer9_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_58 [label="Layer9_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_59 [label="Layer9_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_56 [label="Layer9_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_57 [label="Layer9_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_58 [label="Layer9_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_59 [label="Layer9_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_14 [label="Layer9_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_56
	layer_9_qkv_56 -> layer_9_attn_scores_56
	layer_9_attn_scores_56 -> layer_9_attn_softmax_56
	layer_9_attn_softmax_56 -> layer_9_attn_out_56
	layer_9_attn_out_56 -> layer_9_attn_allreduce_14
	layer_9_input -> layer_9_qkv_57
	layer_9_qkv_57 -> layer_9_attn_scores_57
	layer_9_attn_scores_57 -> layer_9_attn_softmax_57
	layer_9_attn_softmax_57 -> layer_9_attn_out_57
	layer_9_attn_out_57 -> layer_9_attn_allreduce_14
	layer_9_input -> layer_9_qkv_58
	layer_9_qkv_58 -> layer_9_attn_scores_58
	layer_9_attn_scores_58 -> layer_9_attn_softmax_58
	layer_9_attn_softmax_58 -> layer_9_attn_out_58
	layer_9_attn_out_58 -> layer_9_attn_allreduce_14
	layer_9_input -> layer_9_qkv_59
	layer_9_qkv_59 -> layer_9_attn_scores_59
	layer_9_attn_scores_59 -> layer_9_attn_softmax_59
	layer_9_attn_softmax_59 -> layer_9_attn_out_59
	layer_9_attn_out_59 -> layer_9_attn_allreduce_14
	layer_9_qkv_60 [label="Layer9_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_61 [label="Layer9_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_62 [label="Layer9_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_qkv_63 [label="Layer9_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_60 [label="Layer9_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_61 [label="Layer9_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_62 [label="Layer9_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_scores_63 [label="Layer9_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_60 [label="Layer9_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_61 [label="Layer9_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_62 [label="Layer9_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_softmax_63 [label="Layer9_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_60 [label="Layer9_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_61 [label="Layer9_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_62 [label="Layer9_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_out_63 [label="Layer9_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_15 [label="Layer9_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_input -> layer_9_qkv_60
	layer_9_qkv_60 -> layer_9_attn_scores_60
	layer_9_attn_scores_60 -> layer_9_attn_softmax_60
	layer_9_attn_softmax_60 -> layer_9_attn_out_60
	layer_9_attn_out_60 -> layer_9_attn_allreduce_15
	layer_9_input -> layer_9_qkv_61
	layer_9_qkv_61 -> layer_9_attn_scores_61
	layer_9_attn_scores_61 -> layer_9_attn_softmax_61
	layer_9_attn_softmax_61 -> layer_9_attn_out_61
	layer_9_attn_out_61 -> layer_9_attn_allreduce_15
	layer_9_input -> layer_9_qkv_62
	layer_9_qkv_62 -> layer_9_attn_scores_62
	layer_9_attn_scores_62 -> layer_9_attn_softmax_62
	layer_9_attn_softmax_62 -> layer_9_attn_out_62
	layer_9_attn_out_62 -> layer_9_attn_allreduce_15
	layer_9_input -> layer_9_qkv_63
	layer_9_qkv_63 -> layer_9_attn_scores_63
	layer_9_attn_scores_63 -> layer_9_attn_softmax_63
	layer_9_attn_softmax_63 -> layer_9_attn_out_63
	layer_9_attn_out_63 -> layer_9_attn_allreduce_15
	layer_9_gate_0 [label="Layer9_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_4 [label="Layer9_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_8 [label="Layer9_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_12 [label="Layer9_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_16 [label="Layer9_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_20 [label="Layer9_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_24 [label="Layer9_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_28 [label="Layer9_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_32 [label="Layer9_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_36 [label="Layer9_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_40 [label="Layer9_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_44 [label="Layer9_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_48 [label="Layer9_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_52 [label="Layer9_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_56 [label="Layer9_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_gate_60 [label="Layer9_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_9_alltoall [label="Layer9_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_0 [label="Layer9_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_1 [label="Layer9_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_2 [label="Layer9_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_3 [label="Layer9_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_0 [label="Layer9_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_1 [label="Layer9_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_2 [label="Layer9_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_3 [label="Layer9_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_0 [label="Layer9_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_1 [label="Layer9_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_2 [label="Layer9_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_3 [label="Layer9_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_0 [label="Layer9_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_4 [label="Layer9_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_5 [label="Layer9_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_6 [label="Layer9_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_7 [label="Layer9_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_4 [label="Layer9_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_5 [label="Layer9_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_6 [label="Layer9_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_7 [label="Layer9_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_4 [label="Layer9_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_5 [label="Layer9_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_6 [label="Layer9_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_7 [label="Layer9_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_1 [label="Layer9_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_8 [label="Layer9_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_9 [label="Layer9_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_10 [label="Layer9_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_11 [label="Layer9_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_8 [label="Layer9_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_9 [label="Layer9_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_10 [label="Layer9_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_11 [label="Layer9_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_8 [label="Layer9_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_9 [label="Layer9_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_10 [label="Layer9_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_11 [label="Layer9_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_2 [label="Layer9_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_12 [label="Layer9_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_13 [label="Layer9_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_14 [label="Layer9_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_15 [label="Layer9_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_12 [label="Layer9_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_13 [label="Layer9_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_14 [label="Layer9_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_15 [label="Layer9_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_12 [label="Layer9_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_13 [label="Layer9_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_14 [label="Layer9_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_15 [label="Layer9_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_3 [label="Layer9_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_16 [label="Layer9_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_17 [label="Layer9_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_18 [label="Layer9_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_19 [label="Layer9_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_16 [label="Layer9_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_17 [label="Layer9_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_18 [label="Layer9_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_19 [label="Layer9_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_16 [label="Layer9_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_17 [label="Layer9_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_18 [label="Layer9_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_19 [label="Layer9_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_4 [label="Layer9_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_20 [label="Layer9_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_21 [label="Layer9_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_22 [label="Layer9_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_23 [label="Layer9_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_20 [label="Layer9_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_21 [label="Layer9_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_22 [label="Layer9_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_23 [label="Layer9_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_20 [label="Layer9_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_21 [label="Layer9_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_22 [label="Layer9_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_23 [label="Layer9_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_5 [label="Layer9_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_24 [label="Layer9_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_25 [label="Layer9_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_26 [label="Layer9_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_27 [label="Layer9_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_24 [label="Layer9_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_25 [label="Layer9_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_26 [label="Layer9_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_27 [label="Layer9_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_24 [label="Layer9_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_25 [label="Layer9_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_26 [label="Layer9_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_27 [label="Layer9_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_6 [label="Layer9_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_28 [label="Layer9_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_29 [label="Layer9_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_30 [label="Layer9_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_31 [label="Layer9_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_28 [label="Layer9_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_29 [label="Layer9_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_30 [label="Layer9_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_31 [label="Layer9_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_28 [label="Layer9_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_29 [label="Layer9_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_30 [label="Layer9_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_31 [label="Layer9_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_7 [label="Layer9_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_32 [label="Layer9_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_33 [label="Layer9_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_34 [label="Layer9_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_35 [label="Layer9_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_32 [label="Layer9_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_33 [label="Layer9_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_34 [label="Layer9_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_35 [label="Layer9_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_32 [label="Layer9_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_33 [label="Layer9_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_34 [label="Layer9_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_35 [label="Layer9_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_8 [label="Layer9_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_36 [label="Layer9_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_37 [label="Layer9_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_38 [label="Layer9_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_39 [label="Layer9_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_36 [label="Layer9_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_37 [label="Layer9_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_38 [label="Layer9_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_39 [label="Layer9_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_36 [label="Layer9_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_37 [label="Layer9_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_38 [label="Layer9_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_39 [label="Layer9_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_9 [label="Layer9_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_40 [label="Layer9_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_41 [label="Layer9_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_42 [label="Layer9_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_43 [label="Layer9_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_40 [label="Layer9_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_41 [label="Layer9_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_42 [label="Layer9_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_43 [label="Layer9_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_40 [label="Layer9_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_41 [label="Layer9_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_42 [label="Layer9_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_43 [label="Layer9_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_10 [label="Layer9_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_44 [label="Layer9_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_45 [label="Layer9_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_46 [label="Layer9_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_47 [label="Layer9_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_44 [label="Layer9_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_45 [label="Layer9_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_46 [label="Layer9_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_47 [label="Layer9_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_44 [label="Layer9_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_45 [label="Layer9_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_46 [label="Layer9_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_47 [label="Layer9_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_11 [label="Layer9_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_48 [label="Layer9_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_49 [label="Layer9_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_50 [label="Layer9_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_51 [label="Layer9_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_48 [label="Layer9_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_49 [label="Layer9_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_50 [label="Layer9_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_51 [label="Layer9_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_48 [label="Layer9_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_49 [label="Layer9_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_50 [label="Layer9_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_51 [label="Layer9_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_12 [label="Layer9_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_52 [label="Layer9_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_53 [label="Layer9_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_54 [label="Layer9_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_55 [label="Layer9_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_52 [label="Layer9_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_53 [label="Layer9_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_54 [label="Layer9_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_55 [label="Layer9_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_52 [label="Layer9_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_53 [label="Layer9_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_54 [label="Layer9_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_55 [label="Layer9_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_13 [label="Layer9_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_56 [label="Layer9_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_57 [label="Layer9_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_58 [label="Layer9_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_59 [label="Layer9_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_56 [label="Layer9_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_57 [label="Layer9_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_58 [label="Layer9_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_59 [label="Layer9_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_56 [label="Layer9_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_57 [label="Layer9_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_58 [label="Layer9_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_59 [label="Layer9_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_14 [label="Layer9_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert1_60 [label="Layer9_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_61 [label="Layer9_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_62 [label="Layer9_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert1_63 [label="Layer9_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_60 [label="Layer9_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_61 [label="Layer9_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_62 [label="Layer9_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert_act_63 [label="Layer9_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_9_expert2_60 [label="Layer9_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_61 [label="Layer9_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_62 [label="Layer9_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert2_63 [label="Layer9_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_9_expert_allreduce_15 [label="Layer9_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_9_expert_agg [label="Layer9_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_9_norm [label="Layer9_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_9_attn_allreduce_0 -> layer_9_gate_0 [style=dashed]
	layer_9_gate_0 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_0
	layer_9_expert1_0 -> layer_9_expert_act_0
	layer_9_expert_act_0 -> layer_9_expert2_0
	layer_9_expert2_0 -> layer_9_expert_allreduce_0
	layer_9_alltoall -> layer_9_expert1_1
	layer_9_expert1_1 -> layer_9_expert_act_1
	layer_9_expert_act_1 -> layer_9_expert2_1
	layer_9_expert2_1 -> layer_9_expert_allreduce_0
	layer_9_alltoall -> layer_9_expert1_2
	layer_9_expert1_2 -> layer_9_expert_act_2
	layer_9_expert_act_2 -> layer_9_expert2_2
	layer_9_expert2_2 -> layer_9_expert_allreduce_0
	layer_9_alltoall -> layer_9_expert1_3
	layer_9_expert1_3 -> layer_9_expert_act_3
	layer_9_expert_act_3 -> layer_9_expert2_3
	layer_9_expert2_3 -> layer_9_expert_allreduce_0
	layer_9_expert_allreduce_0 -> layer_9_expert_agg
	layer_9_attn_allreduce_1 -> layer_9_gate_4 [style=dashed]
	layer_9_gate_4 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_4
	layer_9_expert1_4 -> layer_9_expert_act_4
	layer_9_expert_act_4 -> layer_9_expert2_4
	layer_9_expert2_4 -> layer_9_expert_allreduce_1
	layer_9_alltoall -> layer_9_expert1_5
	layer_9_expert1_5 -> layer_9_expert_act_5
	layer_9_expert_act_5 -> layer_9_expert2_5
	layer_9_expert2_5 -> layer_9_expert_allreduce_1
	layer_9_alltoall -> layer_9_expert1_6
	layer_9_expert1_6 -> layer_9_expert_act_6
	layer_9_expert_act_6 -> layer_9_expert2_6
	layer_9_expert2_6 -> layer_9_expert_allreduce_1
	layer_9_alltoall -> layer_9_expert1_7
	layer_9_expert1_7 -> layer_9_expert_act_7
	layer_9_expert_act_7 -> layer_9_expert2_7
	layer_9_expert2_7 -> layer_9_expert_allreduce_1
	layer_9_expert_allreduce_1 -> layer_9_expert_agg
	layer_9_attn_allreduce_2 -> layer_9_gate_8 [style=dashed]
	layer_9_gate_8 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_8
	layer_9_expert1_8 -> layer_9_expert_act_8
	layer_9_expert_act_8 -> layer_9_expert2_8
	layer_9_expert2_8 -> layer_9_expert_allreduce_2
	layer_9_alltoall -> layer_9_expert1_9
	layer_9_expert1_9 -> layer_9_expert_act_9
	layer_9_expert_act_9 -> layer_9_expert2_9
	layer_9_expert2_9 -> layer_9_expert_allreduce_2
	layer_9_alltoall -> layer_9_expert1_10
	layer_9_expert1_10 -> layer_9_expert_act_10
	layer_9_expert_act_10 -> layer_9_expert2_10
	layer_9_expert2_10 -> layer_9_expert_allreduce_2
	layer_9_alltoall -> layer_9_expert1_11
	layer_9_expert1_11 -> layer_9_expert_act_11
	layer_9_expert_act_11 -> layer_9_expert2_11
	layer_9_expert2_11 -> layer_9_expert_allreduce_2
	layer_9_expert_allreduce_2 -> layer_9_expert_agg
	layer_9_attn_allreduce_3 -> layer_9_gate_12 [style=dashed]
	layer_9_gate_12 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_12
	layer_9_expert1_12 -> layer_9_expert_act_12
	layer_9_expert_act_12 -> layer_9_expert2_12
	layer_9_expert2_12 -> layer_9_expert_allreduce_3
	layer_9_alltoall -> layer_9_expert1_13
	layer_9_expert1_13 -> layer_9_expert_act_13
	layer_9_expert_act_13 -> layer_9_expert2_13
	layer_9_expert2_13 -> layer_9_expert_allreduce_3
	layer_9_alltoall -> layer_9_expert1_14
	layer_9_expert1_14 -> layer_9_expert_act_14
	layer_9_expert_act_14 -> layer_9_expert2_14
	layer_9_expert2_14 -> layer_9_expert_allreduce_3
	layer_9_alltoall -> layer_9_expert1_15
	layer_9_expert1_15 -> layer_9_expert_act_15
	layer_9_expert_act_15 -> layer_9_expert2_15
	layer_9_expert2_15 -> layer_9_expert_allreduce_3
	layer_9_expert_allreduce_3 -> layer_9_expert_agg
	layer_9_attn_allreduce_4 -> layer_9_gate_16 [style=dashed]
	layer_9_gate_16 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_16
	layer_9_expert1_16 -> layer_9_expert_act_16
	layer_9_expert_act_16 -> layer_9_expert2_16
	layer_9_expert2_16 -> layer_9_expert_allreduce_4
	layer_9_alltoall -> layer_9_expert1_17
	layer_9_expert1_17 -> layer_9_expert_act_17
	layer_9_expert_act_17 -> layer_9_expert2_17
	layer_9_expert2_17 -> layer_9_expert_allreduce_4
	layer_9_alltoall -> layer_9_expert1_18
	layer_9_expert1_18 -> layer_9_expert_act_18
	layer_9_expert_act_18 -> layer_9_expert2_18
	layer_9_expert2_18 -> layer_9_expert_allreduce_4
	layer_9_alltoall -> layer_9_expert1_19
	layer_9_expert1_19 -> layer_9_expert_act_19
	layer_9_expert_act_19 -> layer_9_expert2_19
	layer_9_expert2_19 -> layer_9_expert_allreduce_4
	layer_9_expert_allreduce_4 -> layer_9_expert_agg
	layer_9_attn_allreduce_5 -> layer_9_gate_20 [style=dashed]
	layer_9_gate_20 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_20
	layer_9_expert1_20 -> layer_9_expert_act_20
	layer_9_expert_act_20 -> layer_9_expert2_20
	layer_9_expert2_20 -> layer_9_expert_allreduce_5
	layer_9_alltoall -> layer_9_expert1_21
	layer_9_expert1_21 -> layer_9_expert_act_21
	layer_9_expert_act_21 -> layer_9_expert2_21
	layer_9_expert2_21 -> layer_9_expert_allreduce_5
	layer_9_alltoall -> layer_9_expert1_22
	layer_9_expert1_22 -> layer_9_expert_act_22
	layer_9_expert_act_22 -> layer_9_expert2_22
	layer_9_expert2_22 -> layer_9_expert_allreduce_5
	layer_9_alltoall -> layer_9_expert1_23
	layer_9_expert1_23 -> layer_9_expert_act_23
	layer_9_expert_act_23 -> layer_9_expert2_23
	layer_9_expert2_23 -> layer_9_expert_allreduce_5
	layer_9_expert_allreduce_5 -> layer_9_expert_agg
	layer_9_attn_allreduce_6 -> layer_9_gate_24 [style=dashed]
	layer_9_gate_24 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_24
	layer_9_expert1_24 -> layer_9_expert_act_24
	layer_9_expert_act_24 -> layer_9_expert2_24
	layer_9_expert2_24 -> layer_9_expert_allreduce_6
	layer_9_alltoall -> layer_9_expert1_25
	layer_9_expert1_25 -> layer_9_expert_act_25
	layer_9_expert_act_25 -> layer_9_expert2_25
	layer_9_expert2_25 -> layer_9_expert_allreduce_6
	layer_9_alltoall -> layer_9_expert1_26
	layer_9_expert1_26 -> layer_9_expert_act_26
	layer_9_expert_act_26 -> layer_9_expert2_26
	layer_9_expert2_26 -> layer_9_expert_allreduce_6
	layer_9_alltoall -> layer_9_expert1_27
	layer_9_expert1_27 -> layer_9_expert_act_27
	layer_9_expert_act_27 -> layer_9_expert2_27
	layer_9_expert2_27 -> layer_9_expert_allreduce_6
	layer_9_expert_allreduce_6 -> layer_9_expert_agg
	layer_9_attn_allreduce_7 -> layer_9_gate_28 [style=dashed]
	layer_9_gate_28 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_28
	layer_9_expert1_28 -> layer_9_expert_act_28
	layer_9_expert_act_28 -> layer_9_expert2_28
	layer_9_expert2_28 -> layer_9_expert_allreduce_7
	layer_9_alltoall -> layer_9_expert1_29
	layer_9_expert1_29 -> layer_9_expert_act_29
	layer_9_expert_act_29 -> layer_9_expert2_29
	layer_9_expert2_29 -> layer_9_expert_allreduce_7
	layer_9_alltoall -> layer_9_expert1_30
	layer_9_expert1_30 -> layer_9_expert_act_30
	layer_9_expert_act_30 -> layer_9_expert2_30
	layer_9_expert2_30 -> layer_9_expert_allreduce_7
	layer_9_alltoall -> layer_9_expert1_31
	layer_9_expert1_31 -> layer_9_expert_act_31
	layer_9_expert_act_31 -> layer_9_expert2_31
	layer_9_expert2_31 -> layer_9_expert_allreduce_7
	layer_9_expert_allreduce_7 -> layer_9_expert_agg
	layer_9_attn_allreduce_8 -> layer_9_gate_32 [style=dashed]
	layer_9_gate_32 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_32
	layer_9_expert1_32 -> layer_9_expert_act_32
	layer_9_expert_act_32 -> layer_9_expert2_32
	layer_9_expert2_32 -> layer_9_expert_allreduce_8
	layer_9_alltoall -> layer_9_expert1_33
	layer_9_expert1_33 -> layer_9_expert_act_33
	layer_9_expert_act_33 -> layer_9_expert2_33
	layer_9_expert2_33 -> layer_9_expert_allreduce_8
	layer_9_alltoall -> layer_9_expert1_34
	layer_9_expert1_34 -> layer_9_expert_act_34
	layer_9_expert_act_34 -> layer_9_expert2_34
	layer_9_expert2_34 -> layer_9_expert_allreduce_8
	layer_9_alltoall -> layer_9_expert1_35
	layer_9_expert1_35 -> layer_9_expert_act_35
	layer_9_expert_act_35 -> layer_9_expert2_35
	layer_9_expert2_35 -> layer_9_expert_allreduce_8
	layer_9_expert_allreduce_8 -> layer_9_expert_agg
	layer_9_attn_allreduce_9 -> layer_9_gate_36 [style=dashed]
	layer_9_gate_36 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_36
	layer_9_expert1_36 -> layer_9_expert_act_36
	layer_9_expert_act_36 -> layer_9_expert2_36
	layer_9_expert2_36 -> layer_9_expert_allreduce_9
	layer_9_alltoall -> layer_9_expert1_37
	layer_9_expert1_37 -> layer_9_expert_act_37
	layer_9_expert_act_37 -> layer_9_expert2_37
	layer_9_expert2_37 -> layer_9_expert_allreduce_9
	layer_9_alltoall -> layer_9_expert1_38
	layer_9_expert1_38 -> layer_9_expert_act_38
	layer_9_expert_act_38 -> layer_9_expert2_38
	layer_9_expert2_38 -> layer_9_expert_allreduce_9
	layer_9_alltoall -> layer_9_expert1_39
	layer_9_expert1_39 -> layer_9_expert_act_39
	layer_9_expert_act_39 -> layer_9_expert2_39
	layer_9_expert2_39 -> layer_9_expert_allreduce_9
	layer_9_expert_allreduce_9 -> layer_9_expert_agg
	layer_9_attn_allreduce_10 -> layer_9_gate_40 [style=dashed]
	layer_9_gate_40 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_40
	layer_9_expert1_40 -> layer_9_expert_act_40
	layer_9_expert_act_40 -> layer_9_expert2_40
	layer_9_expert2_40 -> layer_9_expert_allreduce_10
	layer_9_alltoall -> layer_9_expert1_41
	layer_9_expert1_41 -> layer_9_expert_act_41
	layer_9_expert_act_41 -> layer_9_expert2_41
	layer_9_expert2_41 -> layer_9_expert_allreduce_10
	layer_9_alltoall -> layer_9_expert1_42
	layer_9_expert1_42 -> layer_9_expert_act_42
	layer_9_expert_act_42 -> layer_9_expert2_42
	layer_9_expert2_42 -> layer_9_expert_allreduce_10
	layer_9_alltoall -> layer_9_expert1_43
	layer_9_expert1_43 -> layer_9_expert_act_43
	layer_9_expert_act_43 -> layer_9_expert2_43
	layer_9_expert2_43 -> layer_9_expert_allreduce_10
	layer_9_expert_allreduce_10 -> layer_9_expert_agg
	layer_9_attn_allreduce_11 -> layer_9_gate_44 [style=dashed]
	layer_9_gate_44 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_44
	layer_9_expert1_44 -> layer_9_expert_act_44
	layer_9_expert_act_44 -> layer_9_expert2_44
	layer_9_expert2_44 -> layer_9_expert_allreduce_11
	layer_9_alltoall -> layer_9_expert1_45
	layer_9_expert1_45 -> layer_9_expert_act_45
	layer_9_expert_act_45 -> layer_9_expert2_45
	layer_9_expert2_45 -> layer_9_expert_allreduce_11
	layer_9_alltoall -> layer_9_expert1_46
	layer_9_expert1_46 -> layer_9_expert_act_46
	layer_9_expert_act_46 -> layer_9_expert2_46
	layer_9_expert2_46 -> layer_9_expert_allreduce_11
	layer_9_alltoall -> layer_9_expert1_47
	layer_9_expert1_47 -> layer_9_expert_act_47
	layer_9_expert_act_47 -> layer_9_expert2_47
	layer_9_expert2_47 -> layer_9_expert_allreduce_11
	layer_9_expert_allreduce_11 -> layer_9_expert_agg
	layer_9_attn_allreduce_12 -> layer_9_gate_48 [style=dashed]
	layer_9_gate_48 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_48
	layer_9_expert1_48 -> layer_9_expert_act_48
	layer_9_expert_act_48 -> layer_9_expert2_48
	layer_9_expert2_48 -> layer_9_expert_allreduce_12
	layer_9_alltoall -> layer_9_expert1_49
	layer_9_expert1_49 -> layer_9_expert_act_49
	layer_9_expert_act_49 -> layer_9_expert2_49
	layer_9_expert2_49 -> layer_9_expert_allreduce_12
	layer_9_alltoall -> layer_9_expert1_50
	layer_9_expert1_50 -> layer_9_expert_act_50
	layer_9_expert_act_50 -> layer_9_expert2_50
	layer_9_expert2_50 -> layer_9_expert_allreduce_12
	layer_9_alltoall -> layer_9_expert1_51
	layer_9_expert1_51 -> layer_9_expert_act_51
	layer_9_expert_act_51 -> layer_9_expert2_51
	layer_9_expert2_51 -> layer_9_expert_allreduce_12
	layer_9_expert_allreduce_12 -> layer_9_expert_agg
	layer_9_attn_allreduce_13 -> layer_9_gate_52 [style=dashed]
	layer_9_gate_52 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_52
	layer_9_expert1_52 -> layer_9_expert_act_52
	layer_9_expert_act_52 -> layer_9_expert2_52
	layer_9_expert2_52 -> layer_9_expert_allreduce_13
	layer_9_alltoall -> layer_9_expert1_53
	layer_9_expert1_53 -> layer_9_expert_act_53
	layer_9_expert_act_53 -> layer_9_expert2_53
	layer_9_expert2_53 -> layer_9_expert_allreduce_13
	layer_9_alltoall -> layer_9_expert1_54
	layer_9_expert1_54 -> layer_9_expert_act_54
	layer_9_expert_act_54 -> layer_9_expert2_54
	layer_9_expert2_54 -> layer_9_expert_allreduce_13
	layer_9_alltoall -> layer_9_expert1_55
	layer_9_expert1_55 -> layer_9_expert_act_55
	layer_9_expert_act_55 -> layer_9_expert2_55
	layer_9_expert2_55 -> layer_9_expert_allreduce_13
	layer_9_expert_allreduce_13 -> layer_9_expert_agg
	layer_9_attn_allreduce_14 -> layer_9_gate_56 [style=dashed]
	layer_9_gate_56 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_56
	layer_9_expert1_56 -> layer_9_expert_act_56
	layer_9_expert_act_56 -> layer_9_expert2_56
	layer_9_expert2_56 -> layer_9_expert_allreduce_14
	layer_9_alltoall -> layer_9_expert1_57
	layer_9_expert1_57 -> layer_9_expert_act_57
	layer_9_expert_act_57 -> layer_9_expert2_57
	layer_9_expert2_57 -> layer_9_expert_allreduce_14
	layer_9_alltoall -> layer_9_expert1_58
	layer_9_expert1_58 -> layer_9_expert_act_58
	layer_9_expert_act_58 -> layer_9_expert2_58
	layer_9_expert2_58 -> layer_9_expert_allreduce_14
	layer_9_alltoall -> layer_9_expert1_59
	layer_9_expert1_59 -> layer_9_expert_act_59
	layer_9_expert_act_59 -> layer_9_expert2_59
	layer_9_expert2_59 -> layer_9_expert_allreduce_14
	layer_9_expert_allreduce_14 -> layer_9_expert_agg
	layer_9_attn_allreduce_15 -> layer_9_gate_60 [style=dashed]
	layer_9_gate_60 -> layer_9_alltoall [style=dashed]
	layer_9_alltoall -> layer_9_expert1_60
	layer_9_expert1_60 -> layer_9_expert_act_60
	layer_9_expert_act_60 -> layer_9_expert2_60
	layer_9_expert2_60 -> layer_9_expert_allreduce_15
	layer_9_alltoall -> layer_9_expert1_61
	layer_9_expert1_61 -> layer_9_expert_act_61
	layer_9_expert_act_61 -> layer_9_expert2_61
	layer_9_expert2_61 -> layer_9_expert_allreduce_15
	layer_9_alltoall -> layer_9_expert1_62
	layer_9_expert1_62 -> layer_9_expert_act_62
	layer_9_expert_act_62 -> layer_9_expert2_62
	layer_9_expert2_62 -> layer_9_expert_allreduce_15
	layer_9_alltoall -> layer_9_expert1_63
	layer_9_expert1_63 -> layer_9_expert_act_63
	layer_9_expert_act_63 -> layer_9_expert2_63
	layer_9_expert2_63 -> layer_9_expert_allreduce_15
	layer_9_expert_allreduce_15 -> layer_9_expert_agg
	layer_9_expert_agg -> layer_9_norm
	layer_9_norm -> layer_10_input
	layer_10_input [label="Layer10_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_10_qkv_0 [label="Layer10_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_1 [label="Layer10_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_2 [label="Layer10_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_3 [label="Layer10_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_0 [label="Layer10_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_1 [label="Layer10_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_2 [label="Layer10_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_3 [label="Layer10_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_0 [label="Layer10_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_1 [label="Layer10_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_2 [label="Layer10_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_3 [label="Layer10_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_0 [label="Layer10_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_1 [label="Layer10_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_2 [label="Layer10_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_3 [label="Layer10_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_0 [label="Layer10_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_0
	layer_10_qkv_0 -> layer_10_attn_scores_0
	layer_10_attn_scores_0 -> layer_10_attn_softmax_0
	layer_10_attn_softmax_0 -> layer_10_attn_out_0
	layer_10_attn_out_0 -> layer_10_attn_allreduce_0
	layer_10_input -> layer_10_qkv_1
	layer_10_qkv_1 -> layer_10_attn_scores_1
	layer_10_attn_scores_1 -> layer_10_attn_softmax_1
	layer_10_attn_softmax_1 -> layer_10_attn_out_1
	layer_10_attn_out_1 -> layer_10_attn_allreduce_0
	layer_10_input -> layer_10_qkv_2
	layer_10_qkv_2 -> layer_10_attn_scores_2
	layer_10_attn_scores_2 -> layer_10_attn_softmax_2
	layer_10_attn_softmax_2 -> layer_10_attn_out_2
	layer_10_attn_out_2 -> layer_10_attn_allreduce_0
	layer_10_input -> layer_10_qkv_3
	layer_10_qkv_3 -> layer_10_attn_scores_3
	layer_10_attn_scores_3 -> layer_10_attn_softmax_3
	layer_10_attn_softmax_3 -> layer_10_attn_out_3
	layer_10_attn_out_3 -> layer_10_attn_allreduce_0
	layer_10_qkv_4 [label="Layer10_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_5 [label="Layer10_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_6 [label="Layer10_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_7 [label="Layer10_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_4 [label="Layer10_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_5 [label="Layer10_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_6 [label="Layer10_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_7 [label="Layer10_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_4 [label="Layer10_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_5 [label="Layer10_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_6 [label="Layer10_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_7 [label="Layer10_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_4 [label="Layer10_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_5 [label="Layer10_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_6 [label="Layer10_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_7 [label="Layer10_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_1 [label="Layer10_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_4
	layer_10_qkv_4 -> layer_10_attn_scores_4
	layer_10_attn_scores_4 -> layer_10_attn_softmax_4
	layer_10_attn_softmax_4 -> layer_10_attn_out_4
	layer_10_attn_out_4 -> layer_10_attn_allreduce_1
	layer_10_input -> layer_10_qkv_5
	layer_10_qkv_5 -> layer_10_attn_scores_5
	layer_10_attn_scores_5 -> layer_10_attn_softmax_5
	layer_10_attn_softmax_5 -> layer_10_attn_out_5
	layer_10_attn_out_5 -> layer_10_attn_allreduce_1
	layer_10_input -> layer_10_qkv_6
	layer_10_qkv_6 -> layer_10_attn_scores_6
	layer_10_attn_scores_6 -> layer_10_attn_softmax_6
	layer_10_attn_softmax_6 -> layer_10_attn_out_6
	layer_10_attn_out_6 -> layer_10_attn_allreduce_1
	layer_10_input -> layer_10_qkv_7
	layer_10_qkv_7 -> layer_10_attn_scores_7
	layer_10_attn_scores_7 -> layer_10_attn_softmax_7
	layer_10_attn_softmax_7 -> layer_10_attn_out_7
	layer_10_attn_out_7 -> layer_10_attn_allreduce_1
	layer_10_qkv_8 [label="Layer10_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_9 [label="Layer10_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_10 [label="Layer10_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_11 [label="Layer10_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_8 [label="Layer10_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_9 [label="Layer10_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_10 [label="Layer10_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_11 [label="Layer10_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_8 [label="Layer10_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_9 [label="Layer10_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_10 [label="Layer10_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_11 [label="Layer10_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_8 [label="Layer10_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_9 [label="Layer10_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_10 [label="Layer10_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_11 [label="Layer10_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_2 [label="Layer10_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_8
	layer_10_qkv_8 -> layer_10_attn_scores_8
	layer_10_attn_scores_8 -> layer_10_attn_softmax_8
	layer_10_attn_softmax_8 -> layer_10_attn_out_8
	layer_10_attn_out_8 -> layer_10_attn_allreduce_2
	layer_10_input -> layer_10_qkv_9
	layer_10_qkv_9 -> layer_10_attn_scores_9
	layer_10_attn_scores_9 -> layer_10_attn_softmax_9
	layer_10_attn_softmax_9 -> layer_10_attn_out_9
	layer_10_attn_out_9 -> layer_10_attn_allreduce_2
	layer_10_input -> layer_10_qkv_10
	layer_10_qkv_10 -> layer_10_attn_scores_10
	layer_10_attn_scores_10 -> layer_10_attn_softmax_10
	layer_10_attn_softmax_10 -> layer_10_attn_out_10
	layer_10_attn_out_10 -> layer_10_attn_allreduce_2
	layer_10_input -> layer_10_qkv_11
	layer_10_qkv_11 -> layer_10_attn_scores_11
	layer_10_attn_scores_11 -> layer_10_attn_softmax_11
	layer_10_attn_softmax_11 -> layer_10_attn_out_11
	layer_10_attn_out_11 -> layer_10_attn_allreduce_2
	layer_10_qkv_12 [label="Layer10_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_13 [label="Layer10_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_14 [label="Layer10_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_15 [label="Layer10_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_12 [label="Layer10_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_13 [label="Layer10_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_14 [label="Layer10_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_15 [label="Layer10_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_12 [label="Layer10_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_13 [label="Layer10_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_14 [label="Layer10_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_15 [label="Layer10_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_12 [label="Layer10_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_13 [label="Layer10_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_14 [label="Layer10_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_15 [label="Layer10_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_3 [label="Layer10_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_12
	layer_10_qkv_12 -> layer_10_attn_scores_12
	layer_10_attn_scores_12 -> layer_10_attn_softmax_12
	layer_10_attn_softmax_12 -> layer_10_attn_out_12
	layer_10_attn_out_12 -> layer_10_attn_allreduce_3
	layer_10_input -> layer_10_qkv_13
	layer_10_qkv_13 -> layer_10_attn_scores_13
	layer_10_attn_scores_13 -> layer_10_attn_softmax_13
	layer_10_attn_softmax_13 -> layer_10_attn_out_13
	layer_10_attn_out_13 -> layer_10_attn_allreduce_3
	layer_10_input -> layer_10_qkv_14
	layer_10_qkv_14 -> layer_10_attn_scores_14
	layer_10_attn_scores_14 -> layer_10_attn_softmax_14
	layer_10_attn_softmax_14 -> layer_10_attn_out_14
	layer_10_attn_out_14 -> layer_10_attn_allreduce_3
	layer_10_input -> layer_10_qkv_15
	layer_10_qkv_15 -> layer_10_attn_scores_15
	layer_10_attn_scores_15 -> layer_10_attn_softmax_15
	layer_10_attn_softmax_15 -> layer_10_attn_out_15
	layer_10_attn_out_15 -> layer_10_attn_allreduce_3
	layer_10_qkv_16 [label="Layer10_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_17 [label="Layer10_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_18 [label="Layer10_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_19 [label="Layer10_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_16 [label="Layer10_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_17 [label="Layer10_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_18 [label="Layer10_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_19 [label="Layer10_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_16 [label="Layer10_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_17 [label="Layer10_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_18 [label="Layer10_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_19 [label="Layer10_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_16 [label="Layer10_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_17 [label="Layer10_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_18 [label="Layer10_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_19 [label="Layer10_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_4 [label="Layer10_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_16
	layer_10_qkv_16 -> layer_10_attn_scores_16
	layer_10_attn_scores_16 -> layer_10_attn_softmax_16
	layer_10_attn_softmax_16 -> layer_10_attn_out_16
	layer_10_attn_out_16 -> layer_10_attn_allreduce_4
	layer_10_input -> layer_10_qkv_17
	layer_10_qkv_17 -> layer_10_attn_scores_17
	layer_10_attn_scores_17 -> layer_10_attn_softmax_17
	layer_10_attn_softmax_17 -> layer_10_attn_out_17
	layer_10_attn_out_17 -> layer_10_attn_allreduce_4
	layer_10_input -> layer_10_qkv_18
	layer_10_qkv_18 -> layer_10_attn_scores_18
	layer_10_attn_scores_18 -> layer_10_attn_softmax_18
	layer_10_attn_softmax_18 -> layer_10_attn_out_18
	layer_10_attn_out_18 -> layer_10_attn_allreduce_4
	layer_10_input -> layer_10_qkv_19
	layer_10_qkv_19 -> layer_10_attn_scores_19
	layer_10_attn_scores_19 -> layer_10_attn_softmax_19
	layer_10_attn_softmax_19 -> layer_10_attn_out_19
	layer_10_attn_out_19 -> layer_10_attn_allreduce_4
	layer_10_qkv_20 [label="Layer10_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_21 [label="Layer10_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_22 [label="Layer10_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_23 [label="Layer10_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_20 [label="Layer10_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_21 [label="Layer10_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_22 [label="Layer10_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_23 [label="Layer10_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_20 [label="Layer10_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_21 [label="Layer10_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_22 [label="Layer10_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_23 [label="Layer10_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_20 [label="Layer10_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_21 [label="Layer10_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_22 [label="Layer10_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_23 [label="Layer10_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_5 [label="Layer10_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_20
	layer_10_qkv_20 -> layer_10_attn_scores_20
	layer_10_attn_scores_20 -> layer_10_attn_softmax_20
	layer_10_attn_softmax_20 -> layer_10_attn_out_20
	layer_10_attn_out_20 -> layer_10_attn_allreduce_5
	layer_10_input -> layer_10_qkv_21
	layer_10_qkv_21 -> layer_10_attn_scores_21
	layer_10_attn_scores_21 -> layer_10_attn_softmax_21
	layer_10_attn_softmax_21 -> layer_10_attn_out_21
	layer_10_attn_out_21 -> layer_10_attn_allreduce_5
	layer_10_input -> layer_10_qkv_22
	layer_10_qkv_22 -> layer_10_attn_scores_22
	layer_10_attn_scores_22 -> layer_10_attn_softmax_22
	layer_10_attn_softmax_22 -> layer_10_attn_out_22
	layer_10_attn_out_22 -> layer_10_attn_allreduce_5
	layer_10_input -> layer_10_qkv_23
	layer_10_qkv_23 -> layer_10_attn_scores_23
	layer_10_attn_scores_23 -> layer_10_attn_softmax_23
	layer_10_attn_softmax_23 -> layer_10_attn_out_23
	layer_10_attn_out_23 -> layer_10_attn_allreduce_5
	layer_10_qkv_24 [label="Layer10_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_25 [label="Layer10_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_26 [label="Layer10_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_27 [label="Layer10_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_24 [label="Layer10_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_25 [label="Layer10_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_26 [label="Layer10_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_27 [label="Layer10_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_24 [label="Layer10_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_25 [label="Layer10_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_26 [label="Layer10_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_27 [label="Layer10_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_24 [label="Layer10_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_25 [label="Layer10_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_26 [label="Layer10_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_27 [label="Layer10_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_6 [label="Layer10_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_24
	layer_10_qkv_24 -> layer_10_attn_scores_24
	layer_10_attn_scores_24 -> layer_10_attn_softmax_24
	layer_10_attn_softmax_24 -> layer_10_attn_out_24
	layer_10_attn_out_24 -> layer_10_attn_allreduce_6
	layer_10_input -> layer_10_qkv_25
	layer_10_qkv_25 -> layer_10_attn_scores_25
	layer_10_attn_scores_25 -> layer_10_attn_softmax_25
	layer_10_attn_softmax_25 -> layer_10_attn_out_25
	layer_10_attn_out_25 -> layer_10_attn_allreduce_6
	layer_10_input -> layer_10_qkv_26
	layer_10_qkv_26 -> layer_10_attn_scores_26
	layer_10_attn_scores_26 -> layer_10_attn_softmax_26
	layer_10_attn_softmax_26 -> layer_10_attn_out_26
	layer_10_attn_out_26 -> layer_10_attn_allreduce_6
	layer_10_input -> layer_10_qkv_27
	layer_10_qkv_27 -> layer_10_attn_scores_27
	layer_10_attn_scores_27 -> layer_10_attn_softmax_27
	layer_10_attn_softmax_27 -> layer_10_attn_out_27
	layer_10_attn_out_27 -> layer_10_attn_allreduce_6
	layer_10_qkv_28 [label="Layer10_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_29 [label="Layer10_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_30 [label="Layer10_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_31 [label="Layer10_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_28 [label="Layer10_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_29 [label="Layer10_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_30 [label="Layer10_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_31 [label="Layer10_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_28 [label="Layer10_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_29 [label="Layer10_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_30 [label="Layer10_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_31 [label="Layer10_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_28 [label="Layer10_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_29 [label="Layer10_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_30 [label="Layer10_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_31 [label="Layer10_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_7 [label="Layer10_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_28
	layer_10_qkv_28 -> layer_10_attn_scores_28
	layer_10_attn_scores_28 -> layer_10_attn_softmax_28
	layer_10_attn_softmax_28 -> layer_10_attn_out_28
	layer_10_attn_out_28 -> layer_10_attn_allreduce_7
	layer_10_input -> layer_10_qkv_29
	layer_10_qkv_29 -> layer_10_attn_scores_29
	layer_10_attn_scores_29 -> layer_10_attn_softmax_29
	layer_10_attn_softmax_29 -> layer_10_attn_out_29
	layer_10_attn_out_29 -> layer_10_attn_allreduce_7
	layer_10_input -> layer_10_qkv_30
	layer_10_qkv_30 -> layer_10_attn_scores_30
	layer_10_attn_scores_30 -> layer_10_attn_softmax_30
	layer_10_attn_softmax_30 -> layer_10_attn_out_30
	layer_10_attn_out_30 -> layer_10_attn_allreduce_7
	layer_10_input -> layer_10_qkv_31
	layer_10_qkv_31 -> layer_10_attn_scores_31
	layer_10_attn_scores_31 -> layer_10_attn_softmax_31
	layer_10_attn_softmax_31 -> layer_10_attn_out_31
	layer_10_attn_out_31 -> layer_10_attn_allreduce_7
	layer_10_qkv_32 [label="Layer10_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_33 [label="Layer10_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_34 [label="Layer10_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_35 [label="Layer10_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_32 [label="Layer10_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_33 [label="Layer10_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_34 [label="Layer10_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_35 [label="Layer10_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_32 [label="Layer10_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_33 [label="Layer10_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_34 [label="Layer10_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_35 [label="Layer10_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_32 [label="Layer10_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_33 [label="Layer10_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_34 [label="Layer10_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_35 [label="Layer10_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_8 [label="Layer10_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_32
	layer_10_qkv_32 -> layer_10_attn_scores_32
	layer_10_attn_scores_32 -> layer_10_attn_softmax_32
	layer_10_attn_softmax_32 -> layer_10_attn_out_32
	layer_10_attn_out_32 -> layer_10_attn_allreduce_8
	layer_10_input -> layer_10_qkv_33
	layer_10_qkv_33 -> layer_10_attn_scores_33
	layer_10_attn_scores_33 -> layer_10_attn_softmax_33
	layer_10_attn_softmax_33 -> layer_10_attn_out_33
	layer_10_attn_out_33 -> layer_10_attn_allreduce_8
	layer_10_input -> layer_10_qkv_34
	layer_10_qkv_34 -> layer_10_attn_scores_34
	layer_10_attn_scores_34 -> layer_10_attn_softmax_34
	layer_10_attn_softmax_34 -> layer_10_attn_out_34
	layer_10_attn_out_34 -> layer_10_attn_allreduce_8
	layer_10_input -> layer_10_qkv_35
	layer_10_qkv_35 -> layer_10_attn_scores_35
	layer_10_attn_scores_35 -> layer_10_attn_softmax_35
	layer_10_attn_softmax_35 -> layer_10_attn_out_35
	layer_10_attn_out_35 -> layer_10_attn_allreduce_8
	layer_10_qkv_36 [label="Layer10_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_37 [label="Layer10_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_38 [label="Layer10_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_39 [label="Layer10_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_36 [label="Layer10_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_37 [label="Layer10_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_38 [label="Layer10_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_39 [label="Layer10_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_36 [label="Layer10_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_37 [label="Layer10_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_38 [label="Layer10_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_39 [label="Layer10_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_36 [label="Layer10_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_37 [label="Layer10_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_38 [label="Layer10_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_39 [label="Layer10_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_9 [label="Layer10_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_36
	layer_10_qkv_36 -> layer_10_attn_scores_36
	layer_10_attn_scores_36 -> layer_10_attn_softmax_36
	layer_10_attn_softmax_36 -> layer_10_attn_out_36
	layer_10_attn_out_36 -> layer_10_attn_allreduce_9
	layer_10_input -> layer_10_qkv_37
	layer_10_qkv_37 -> layer_10_attn_scores_37
	layer_10_attn_scores_37 -> layer_10_attn_softmax_37
	layer_10_attn_softmax_37 -> layer_10_attn_out_37
	layer_10_attn_out_37 -> layer_10_attn_allreduce_9
	layer_10_input -> layer_10_qkv_38
	layer_10_qkv_38 -> layer_10_attn_scores_38
	layer_10_attn_scores_38 -> layer_10_attn_softmax_38
	layer_10_attn_softmax_38 -> layer_10_attn_out_38
	layer_10_attn_out_38 -> layer_10_attn_allreduce_9
	layer_10_input -> layer_10_qkv_39
	layer_10_qkv_39 -> layer_10_attn_scores_39
	layer_10_attn_scores_39 -> layer_10_attn_softmax_39
	layer_10_attn_softmax_39 -> layer_10_attn_out_39
	layer_10_attn_out_39 -> layer_10_attn_allreduce_9
	layer_10_qkv_40 [label="Layer10_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_41 [label="Layer10_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_42 [label="Layer10_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_43 [label="Layer10_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_40 [label="Layer10_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_41 [label="Layer10_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_42 [label="Layer10_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_43 [label="Layer10_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_40 [label="Layer10_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_41 [label="Layer10_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_42 [label="Layer10_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_43 [label="Layer10_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_40 [label="Layer10_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_41 [label="Layer10_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_42 [label="Layer10_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_43 [label="Layer10_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_10 [label="Layer10_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_40
	layer_10_qkv_40 -> layer_10_attn_scores_40
	layer_10_attn_scores_40 -> layer_10_attn_softmax_40
	layer_10_attn_softmax_40 -> layer_10_attn_out_40
	layer_10_attn_out_40 -> layer_10_attn_allreduce_10
	layer_10_input -> layer_10_qkv_41
	layer_10_qkv_41 -> layer_10_attn_scores_41
	layer_10_attn_scores_41 -> layer_10_attn_softmax_41
	layer_10_attn_softmax_41 -> layer_10_attn_out_41
	layer_10_attn_out_41 -> layer_10_attn_allreduce_10
	layer_10_input -> layer_10_qkv_42
	layer_10_qkv_42 -> layer_10_attn_scores_42
	layer_10_attn_scores_42 -> layer_10_attn_softmax_42
	layer_10_attn_softmax_42 -> layer_10_attn_out_42
	layer_10_attn_out_42 -> layer_10_attn_allreduce_10
	layer_10_input -> layer_10_qkv_43
	layer_10_qkv_43 -> layer_10_attn_scores_43
	layer_10_attn_scores_43 -> layer_10_attn_softmax_43
	layer_10_attn_softmax_43 -> layer_10_attn_out_43
	layer_10_attn_out_43 -> layer_10_attn_allreduce_10
	layer_10_qkv_44 [label="Layer10_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_45 [label="Layer10_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_46 [label="Layer10_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_47 [label="Layer10_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_44 [label="Layer10_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_45 [label="Layer10_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_46 [label="Layer10_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_47 [label="Layer10_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_44 [label="Layer10_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_45 [label="Layer10_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_46 [label="Layer10_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_47 [label="Layer10_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_44 [label="Layer10_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_45 [label="Layer10_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_46 [label="Layer10_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_47 [label="Layer10_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_11 [label="Layer10_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_44
	layer_10_qkv_44 -> layer_10_attn_scores_44
	layer_10_attn_scores_44 -> layer_10_attn_softmax_44
	layer_10_attn_softmax_44 -> layer_10_attn_out_44
	layer_10_attn_out_44 -> layer_10_attn_allreduce_11
	layer_10_input -> layer_10_qkv_45
	layer_10_qkv_45 -> layer_10_attn_scores_45
	layer_10_attn_scores_45 -> layer_10_attn_softmax_45
	layer_10_attn_softmax_45 -> layer_10_attn_out_45
	layer_10_attn_out_45 -> layer_10_attn_allreduce_11
	layer_10_input -> layer_10_qkv_46
	layer_10_qkv_46 -> layer_10_attn_scores_46
	layer_10_attn_scores_46 -> layer_10_attn_softmax_46
	layer_10_attn_softmax_46 -> layer_10_attn_out_46
	layer_10_attn_out_46 -> layer_10_attn_allreduce_11
	layer_10_input -> layer_10_qkv_47
	layer_10_qkv_47 -> layer_10_attn_scores_47
	layer_10_attn_scores_47 -> layer_10_attn_softmax_47
	layer_10_attn_softmax_47 -> layer_10_attn_out_47
	layer_10_attn_out_47 -> layer_10_attn_allreduce_11
	layer_10_qkv_48 [label="Layer10_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_49 [label="Layer10_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_50 [label="Layer10_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_51 [label="Layer10_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_48 [label="Layer10_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_49 [label="Layer10_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_50 [label="Layer10_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_51 [label="Layer10_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_48 [label="Layer10_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_49 [label="Layer10_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_50 [label="Layer10_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_51 [label="Layer10_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_48 [label="Layer10_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_49 [label="Layer10_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_50 [label="Layer10_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_51 [label="Layer10_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_12 [label="Layer10_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_48
	layer_10_qkv_48 -> layer_10_attn_scores_48
	layer_10_attn_scores_48 -> layer_10_attn_softmax_48
	layer_10_attn_softmax_48 -> layer_10_attn_out_48
	layer_10_attn_out_48 -> layer_10_attn_allreduce_12
	layer_10_input -> layer_10_qkv_49
	layer_10_qkv_49 -> layer_10_attn_scores_49
	layer_10_attn_scores_49 -> layer_10_attn_softmax_49
	layer_10_attn_softmax_49 -> layer_10_attn_out_49
	layer_10_attn_out_49 -> layer_10_attn_allreduce_12
	layer_10_input -> layer_10_qkv_50
	layer_10_qkv_50 -> layer_10_attn_scores_50
	layer_10_attn_scores_50 -> layer_10_attn_softmax_50
	layer_10_attn_softmax_50 -> layer_10_attn_out_50
	layer_10_attn_out_50 -> layer_10_attn_allreduce_12
	layer_10_input -> layer_10_qkv_51
	layer_10_qkv_51 -> layer_10_attn_scores_51
	layer_10_attn_scores_51 -> layer_10_attn_softmax_51
	layer_10_attn_softmax_51 -> layer_10_attn_out_51
	layer_10_attn_out_51 -> layer_10_attn_allreduce_12
	layer_10_qkv_52 [label="Layer10_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_53 [label="Layer10_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_54 [label="Layer10_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_55 [label="Layer10_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_52 [label="Layer10_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_53 [label="Layer10_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_54 [label="Layer10_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_55 [label="Layer10_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_52 [label="Layer10_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_53 [label="Layer10_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_54 [label="Layer10_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_55 [label="Layer10_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_52 [label="Layer10_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_53 [label="Layer10_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_54 [label="Layer10_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_55 [label="Layer10_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_13 [label="Layer10_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_52
	layer_10_qkv_52 -> layer_10_attn_scores_52
	layer_10_attn_scores_52 -> layer_10_attn_softmax_52
	layer_10_attn_softmax_52 -> layer_10_attn_out_52
	layer_10_attn_out_52 -> layer_10_attn_allreduce_13
	layer_10_input -> layer_10_qkv_53
	layer_10_qkv_53 -> layer_10_attn_scores_53
	layer_10_attn_scores_53 -> layer_10_attn_softmax_53
	layer_10_attn_softmax_53 -> layer_10_attn_out_53
	layer_10_attn_out_53 -> layer_10_attn_allreduce_13
	layer_10_input -> layer_10_qkv_54
	layer_10_qkv_54 -> layer_10_attn_scores_54
	layer_10_attn_scores_54 -> layer_10_attn_softmax_54
	layer_10_attn_softmax_54 -> layer_10_attn_out_54
	layer_10_attn_out_54 -> layer_10_attn_allreduce_13
	layer_10_input -> layer_10_qkv_55
	layer_10_qkv_55 -> layer_10_attn_scores_55
	layer_10_attn_scores_55 -> layer_10_attn_softmax_55
	layer_10_attn_softmax_55 -> layer_10_attn_out_55
	layer_10_attn_out_55 -> layer_10_attn_allreduce_13
	layer_10_qkv_56 [label="Layer10_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_57 [label="Layer10_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_58 [label="Layer10_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_59 [label="Layer10_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_56 [label="Layer10_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_57 [label="Layer10_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_58 [label="Layer10_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_59 [label="Layer10_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_56 [label="Layer10_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_57 [label="Layer10_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_58 [label="Layer10_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_59 [label="Layer10_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_56 [label="Layer10_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_57 [label="Layer10_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_58 [label="Layer10_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_59 [label="Layer10_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_14 [label="Layer10_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_56
	layer_10_qkv_56 -> layer_10_attn_scores_56
	layer_10_attn_scores_56 -> layer_10_attn_softmax_56
	layer_10_attn_softmax_56 -> layer_10_attn_out_56
	layer_10_attn_out_56 -> layer_10_attn_allreduce_14
	layer_10_input -> layer_10_qkv_57
	layer_10_qkv_57 -> layer_10_attn_scores_57
	layer_10_attn_scores_57 -> layer_10_attn_softmax_57
	layer_10_attn_softmax_57 -> layer_10_attn_out_57
	layer_10_attn_out_57 -> layer_10_attn_allreduce_14
	layer_10_input -> layer_10_qkv_58
	layer_10_qkv_58 -> layer_10_attn_scores_58
	layer_10_attn_scores_58 -> layer_10_attn_softmax_58
	layer_10_attn_softmax_58 -> layer_10_attn_out_58
	layer_10_attn_out_58 -> layer_10_attn_allreduce_14
	layer_10_input -> layer_10_qkv_59
	layer_10_qkv_59 -> layer_10_attn_scores_59
	layer_10_attn_scores_59 -> layer_10_attn_softmax_59
	layer_10_attn_softmax_59 -> layer_10_attn_out_59
	layer_10_attn_out_59 -> layer_10_attn_allreduce_14
	layer_10_qkv_60 [label="Layer10_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_61 [label="Layer10_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_62 [label="Layer10_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_qkv_63 [label="Layer10_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_60 [label="Layer10_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_61 [label="Layer10_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_62 [label="Layer10_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_scores_63 [label="Layer10_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_60 [label="Layer10_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_61 [label="Layer10_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_62 [label="Layer10_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_softmax_63 [label="Layer10_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_60 [label="Layer10_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_61 [label="Layer10_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_62 [label="Layer10_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_out_63 [label="Layer10_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_15 [label="Layer10_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_input -> layer_10_qkv_60
	layer_10_qkv_60 -> layer_10_attn_scores_60
	layer_10_attn_scores_60 -> layer_10_attn_softmax_60
	layer_10_attn_softmax_60 -> layer_10_attn_out_60
	layer_10_attn_out_60 -> layer_10_attn_allreduce_15
	layer_10_input -> layer_10_qkv_61
	layer_10_qkv_61 -> layer_10_attn_scores_61
	layer_10_attn_scores_61 -> layer_10_attn_softmax_61
	layer_10_attn_softmax_61 -> layer_10_attn_out_61
	layer_10_attn_out_61 -> layer_10_attn_allreduce_15
	layer_10_input -> layer_10_qkv_62
	layer_10_qkv_62 -> layer_10_attn_scores_62
	layer_10_attn_scores_62 -> layer_10_attn_softmax_62
	layer_10_attn_softmax_62 -> layer_10_attn_out_62
	layer_10_attn_out_62 -> layer_10_attn_allreduce_15
	layer_10_input -> layer_10_qkv_63
	layer_10_qkv_63 -> layer_10_attn_scores_63
	layer_10_attn_scores_63 -> layer_10_attn_softmax_63
	layer_10_attn_softmax_63 -> layer_10_attn_out_63
	layer_10_attn_out_63 -> layer_10_attn_allreduce_15
	layer_10_gate_0 [label="Layer10_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_4 [label="Layer10_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_8 [label="Layer10_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_12 [label="Layer10_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_16 [label="Layer10_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_20 [label="Layer10_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_24 [label="Layer10_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_28 [label="Layer10_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_32 [label="Layer10_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_36 [label="Layer10_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_40 [label="Layer10_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_44 [label="Layer10_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_48 [label="Layer10_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_52 [label="Layer10_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_56 [label="Layer10_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_gate_60 [label="Layer10_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_10_alltoall [label="Layer10_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_0 [label="Layer10_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_1 [label="Layer10_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_2 [label="Layer10_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_3 [label="Layer10_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_0 [label="Layer10_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_1 [label="Layer10_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_2 [label="Layer10_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_3 [label="Layer10_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_0 [label="Layer10_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_1 [label="Layer10_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_2 [label="Layer10_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_3 [label="Layer10_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_0 [label="Layer10_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_4 [label="Layer10_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_5 [label="Layer10_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_6 [label="Layer10_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_7 [label="Layer10_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_4 [label="Layer10_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_5 [label="Layer10_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_6 [label="Layer10_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_7 [label="Layer10_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_4 [label="Layer10_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_5 [label="Layer10_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_6 [label="Layer10_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_7 [label="Layer10_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_1 [label="Layer10_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_8 [label="Layer10_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_9 [label="Layer10_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_10 [label="Layer10_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_11 [label="Layer10_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_8 [label="Layer10_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_9 [label="Layer10_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_10 [label="Layer10_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_11 [label="Layer10_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_8 [label="Layer10_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_9 [label="Layer10_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_10 [label="Layer10_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_11 [label="Layer10_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_2 [label="Layer10_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_12 [label="Layer10_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_13 [label="Layer10_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_14 [label="Layer10_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_15 [label="Layer10_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_12 [label="Layer10_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_13 [label="Layer10_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_14 [label="Layer10_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_15 [label="Layer10_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_12 [label="Layer10_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_13 [label="Layer10_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_14 [label="Layer10_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_15 [label="Layer10_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_3 [label="Layer10_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_16 [label="Layer10_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_17 [label="Layer10_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_18 [label="Layer10_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_19 [label="Layer10_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_16 [label="Layer10_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_17 [label="Layer10_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_18 [label="Layer10_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_19 [label="Layer10_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_16 [label="Layer10_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_17 [label="Layer10_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_18 [label="Layer10_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_19 [label="Layer10_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_4 [label="Layer10_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_20 [label="Layer10_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_21 [label="Layer10_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_22 [label="Layer10_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_23 [label="Layer10_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_20 [label="Layer10_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_21 [label="Layer10_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_22 [label="Layer10_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_23 [label="Layer10_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_20 [label="Layer10_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_21 [label="Layer10_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_22 [label="Layer10_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_23 [label="Layer10_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_5 [label="Layer10_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_24 [label="Layer10_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_25 [label="Layer10_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_26 [label="Layer10_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_27 [label="Layer10_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_24 [label="Layer10_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_25 [label="Layer10_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_26 [label="Layer10_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_27 [label="Layer10_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_24 [label="Layer10_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_25 [label="Layer10_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_26 [label="Layer10_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_27 [label="Layer10_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_6 [label="Layer10_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_28 [label="Layer10_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_29 [label="Layer10_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_30 [label="Layer10_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_31 [label="Layer10_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_28 [label="Layer10_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_29 [label="Layer10_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_30 [label="Layer10_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_31 [label="Layer10_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_28 [label="Layer10_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_29 [label="Layer10_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_30 [label="Layer10_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_31 [label="Layer10_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_7 [label="Layer10_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_32 [label="Layer10_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_33 [label="Layer10_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_34 [label="Layer10_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_35 [label="Layer10_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_32 [label="Layer10_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_33 [label="Layer10_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_34 [label="Layer10_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_35 [label="Layer10_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_32 [label="Layer10_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_33 [label="Layer10_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_34 [label="Layer10_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_35 [label="Layer10_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_8 [label="Layer10_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_36 [label="Layer10_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_37 [label="Layer10_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_38 [label="Layer10_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_39 [label="Layer10_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_36 [label="Layer10_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_37 [label="Layer10_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_38 [label="Layer10_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_39 [label="Layer10_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_36 [label="Layer10_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_37 [label="Layer10_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_38 [label="Layer10_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_39 [label="Layer10_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_9 [label="Layer10_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_40 [label="Layer10_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_41 [label="Layer10_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_42 [label="Layer10_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_43 [label="Layer10_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_40 [label="Layer10_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_41 [label="Layer10_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_42 [label="Layer10_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_43 [label="Layer10_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_40 [label="Layer10_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_41 [label="Layer10_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_42 [label="Layer10_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_43 [label="Layer10_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_10 [label="Layer10_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_44 [label="Layer10_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_45 [label="Layer10_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_46 [label="Layer10_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_47 [label="Layer10_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_44 [label="Layer10_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_45 [label="Layer10_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_46 [label="Layer10_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_47 [label="Layer10_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_44 [label="Layer10_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_45 [label="Layer10_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_46 [label="Layer10_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_47 [label="Layer10_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_11 [label="Layer10_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_48 [label="Layer10_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_49 [label="Layer10_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_50 [label="Layer10_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_51 [label="Layer10_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_48 [label="Layer10_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_49 [label="Layer10_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_50 [label="Layer10_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_51 [label="Layer10_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_48 [label="Layer10_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_49 [label="Layer10_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_50 [label="Layer10_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_51 [label="Layer10_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_12 [label="Layer10_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_52 [label="Layer10_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_53 [label="Layer10_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_54 [label="Layer10_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_55 [label="Layer10_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_52 [label="Layer10_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_53 [label="Layer10_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_54 [label="Layer10_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_55 [label="Layer10_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_52 [label="Layer10_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_53 [label="Layer10_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_54 [label="Layer10_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_55 [label="Layer10_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_13 [label="Layer10_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_56 [label="Layer10_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_57 [label="Layer10_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_58 [label="Layer10_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_59 [label="Layer10_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_56 [label="Layer10_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_57 [label="Layer10_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_58 [label="Layer10_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_59 [label="Layer10_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_56 [label="Layer10_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_57 [label="Layer10_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_58 [label="Layer10_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_59 [label="Layer10_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_14 [label="Layer10_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert1_60 [label="Layer10_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_61 [label="Layer10_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_62 [label="Layer10_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert1_63 [label="Layer10_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_60 [label="Layer10_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_61 [label="Layer10_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_62 [label="Layer10_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert_act_63 [label="Layer10_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_10_expert2_60 [label="Layer10_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_61 [label="Layer10_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_62 [label="Layer10_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert2_63 [label="Layer10_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_10_expert_allreduce_15 [label="Layer10_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_10_expert_agg [label="Layer10_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_10_norm [label="Layer10_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_10_attn_allreduce_0 -> layer_10_gate_0 [style=dashed]
	layer_10_gate_0 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_0
	layer_10_expert1_0 -> layer_10_expert_act_0
	layer_10_expert_act_0 -> layer_10_expert2_0
	layer_10_expert2_0 -> layer_10_expert_allreduce_0
	layer_10_alltoall -> layer_10_expert1_1
	layer_10_expert1_1 -> layer_10_expert_act_1
	layer_10_expert_act_1 -> layer_10_expert2_1
	layer_10_expert2_1 -> layer_10_expert_allreduce_0
	layer_10_alltoall -> layer_10_expert1_2
	layer_10_expert1_2 -> layer_10_expert_act_2
	layer_10_expert_act_2 -> layer_10_expert2_2
	layer_10_expert2_2 -> layer_10_expert_allreduce_0
	layer_10_alltoall -> layer_10_expert1_3
	layer_10_expert1_3 -> layer_10_expert_act_3
	layer_10_expert_act_3 -> layer_10_expert2_3
	layer_10_expert2_3 -> layer_10_expert_allreduce_0
	layer_10_expert_allreduce_0 -> layer_10_expert_agg
	layer_10_attn_allreduce_1 -> layer_10_gate_4 [style=dashed]
	layer_10_gate_4 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_4
	layer_10_expert1_4 -> layer_10_expert_act_4
	layer_10_expert_act_4 -> layer_10_expert2_4
	layer_10_expert2_4 -> layer_10_expert_allreduce_1
	layer_10_alltoall -> layer_10_expert1_5
	layer_10_expert1_5 -> layer_10_expert_act_5
	layer_10_expert_act_5 -> layer_10_expert2_5
	layer_10_expert2_5 -> layer_10_expert_allreduce_1
	layer_10_alltoall -> layer_10_expert1_6
	layer_10_expert1_6 -> layer_10_expert_act_6
	layer_10_expert_act_6 -> layer_10_expert2_6
	layer_10_expert2_6 -> layer_10_expert_allreduce_1
	layer_10_alltoall -> layer_10_expert1_7
	layer_10_expert1_7 -> layer_10_expert_act_7
	layer_10_expert_act_7 -> layer_10_expert2_7
	layer_10_expert2_7 -> layer_10_expert_allreduce_1
	layer_10_expert_allreduce_1 -> layer_10_expert_agg
	layer_10_attn_allreduce_2 -> layer_10_gate_8 [style=dashed]
	layer_10_gate_8 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_8
	layer_10_expert1_8 -> layer_10_expert_act_8
	layer_10_expert_act_8 -> layer_10_expert2_8
	layer_10_expert2_8 -> layer_10_expert_allreduce_2
	layer_10_alltoall -> layer_10_expert1_9
	layer_10_expert1_9 -> layer_10_expert_act_9
	layer_10_expert_act_9 -> layer_10_expert2_9
	layer_10_expert2_9 -> layer_10_expert_allreduce_2
	layer_10_alltoall -> layer_10_expert1_10
	layer_10_expert1_10 -> layer_10_expert_act_10
	layer_10_expert_act_10 -> layer_10_expert2_10
	layer_10_expert2_10 -> layer_10_expert_allreduce_2
	layer_10_alltoall -> layer_10_expert1_11
	layer_10_expert1_11 -> layer_10_expert_act_11
	layer_10_expert_act_11 -> layer_10_expert2_11
	layer_10_expert2_11 -> layer_10_expert_allreduce_2
	layer_10_expert_allreduce_2 -> layer_10_expert_agg
	layer_10_attn_allreduce_3 -> layer_10_gate_12 [style=dashed]
	layer_10_gate_12 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_12
	layer_10_expert1_12 -> layer_10_expert_act_12
	layer_10_expert_act_12 -> layer_10_expert2_12
	layer_10_expert2_12 -> layer_10_expert_allreduce_3
	layer_10_alltoall -> layer_10_expert1_13
	layer_10_expert1_13 -> layer_10_expert_act_13
	layer_10_expert_act_13 -> layer_10_expert2_13
	layer_10_expert2_13 -> layer_10_expert_allreduce_3
	layer_10_alltoall -> layer_10_expert1_14
	layer_10_expert1_14 -> layer_10_expert_act_14
	layer_10_expert_act_14 -> layer_10_expert2_14
	layer_10_expert2_14 -> layer_10_expert_allreduce_3
	layer_10_alltoall -> layer_10_expert1_15
	layer_10_expert1_15 -> layer_10_expert_act_15
	layer_10_expert_act_15 -> layer_10_expert2_15
	layer_10_expert2_15 -> layer_10_expert_allreduce_3
	layer_10_expert_allreduce_3 -> layer_10_expert_agg
	layer_10_attn_allreduce_4 -> layer_10_gate_16 [style=dashed]
	layer_10_gate_16 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_16
	layer_10_expert1_16 -> layer_10_expert_act_16
	layer_10_expert_act_16 -> layer_10_expert2_16
	layer_10_expert2_16 -> layer_10_expert_allreduce_4
	layer_10_alltoall -> layer_10_expert1_17
	layer_10_expert1_17 -> layer_10_expert_act_17
	layer_10_expert_act_17 -> layer_10_expert2_17
	layer_10_expert2_17 -> layer_10_expert_allreduce_4
	layer_10_alltoall -> layer_10_expert1_18
	layer_10_expert1_18 -> layer_10_expert_act_18
	layer_10_expert_act_18 -> layer_10_expert2_18
	layer_10_expert2_18 -> layer_10_expert_allreduce_4
	layer_10_alltoall -> layer_10_expert1_19
	layer_10_expert1_19 -> layer_10_expert_act_19
	layer_10_expert_act_19 -> layer_10_expert2_19
	layer_10_expert2_19 -> layer_10_expert_allreduce_4
	layer_10_expert_allreduce_4 -> layer_10_expert_agg
	layer_10_attn_allreduce_5 -> layer_10_gate_20 [style=dashed]
	layer_10_gate_20 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_20
	layer_10_expert1_20 -> layer_10_expert_act_20
	layer_10_expert_act_20 -> layer_10_expert2_20
	layer_10_expert2_20 -> layer_10_expert_allreduce_5
	layer_10_alltoall -> layer_10_expert1_21
	layer_10_expert1_21 -> layer_10_expert_act_21
	layer_10_expert_act_21 -> layer_10_expert2_21
	layer_10_expert2_21 -> layer_10_expert_allreduce_5
	layer_10_alltoall -> layer_10_expert1_22
	layer_10_expert1_22 -> layer_10_expert_act_22
	layer_10_expert_act_22 -> layer_10_expert2_22
	layer_10_expert2_22 -> layer_10_expert_allreduce_5
	layer_10_alltoall -> layer_10_expert1_23
	layer_10_expert1_23 -> layer_10_expert_act_23
	layer_10_expert_act_23 -> layer_10_expert2_23
	layer_10_expert2_23 -> layer_10_expert_allreduce_5
	layer_10_expert_allreduce_5 -> layer_10_expert_agg
	layer_10_attn_allreduce_6 -> layer_10_gate_24 [style=dashed]
	layer_10_gate_24 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_24
	layer_10_expert1_24 -> layer_10_expert_act_24
	layer_10_expert_act_24 -> layer_10_expert2_24
	layer_10_expert2_24 -> layer_10_expert_allreduce_6
	layer_10_alltoall -> layer_10_expert1_25
	layer_10_expert1_25 -> layer_10_expert_act_25
	layer_10_expert_act_25 -> layer_10_expert2_25
	layer_10_expert2_25 -> layer_10_expert_allreduce_6
	layer_10_alltoall -> layer_10_expert1_26
	layer_10_expert1_26 -> layer_10_expert_act_26
	layer_10_expert_act_26 -> layer_10_expert2_26
	layer_10_expert2_26 -> layer_10_expert_allreduce_6
	layer_10_alltoall -> layer_10_expert1_27
	layer_10_expert1_27 -> layer_10_expert_act_27
	layer_10_expert_act_27 -> layer_10_expert2_27
	layer_10_expert2_27 -> layer_10_expert_allreduce_6
	layer_10_expert_allreduce_6 -> layer_10_expert_agg
	layer_10_attn_allreduce_7 -> layer_10_gate_28 [style=dashed]
	layer_10_gate_28 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_28
	layer_10_expert1_28 -> layer_10_expert_act_28
	layer_10_expert_act_28 -> layer_10_expert2_28
	layer_10_expert2_28 -> layer_10_expert_allreduce_7
	layer_10_alltoall -> layer_10_expert1_29
	layer_10_expert1_29 -> layer_10_expert_act_29
	layer_10_expert_act_29 -> layer_10_expert2_29
	layer_10_expert2_29 -> layer_10_expert_allreduce_7
	layer_10_alltoall -> layer_10_expert1_30
	layer_10_expert1_30 -> layer_10_expert_act_30
	layer_10_expert_act_30 -> layer_10_expert2_30
	layer_10_expert2_30 -> layer_10_expert_allreduce_7
	layer_10_alltoall -> layer_10_expert1_31
	layer_10_expert1_31 -> layer_10_expert_act_31
	layer_10_expert_act_31 -> layer_10_expert2_31
	layer_10_expert2_31 -> layer_10_expert_allreduce_7
	layer_10_expert_allreduce_7 -> layer_10_expert_agg
	layer_10_attn_allreduce_8 -> layer_10_gate_32 [style=dashed]
	layer_10_gate_32 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_32
	layer_10_expert1_32 -> layer_10_expert_act_32
	layer_10_expert_act_32 -> layer_10_expert2_32
	layer_10_expert2_32 -> layer_10_expert_allreduce_8
	layer_10_alltoall -> layer_10_expert1_33
	layer_10_expert1_33 -> layer_10_expert_act_33
	layer_10_expert_act_33 -> layer_10_expert2_33
	layer_10_expert2_33 -> layer_10_expert_allreduce_8
	layer_10_alltoall -> layer_10_expert1_34
	layer_10_expert1_34 -> layer_10_expert_act_34
	layer_10_expert_act_34 -> layer_10_expert2_34
	layer_10_expert2_34 -> layer_10_expert_allreduce_8
	layer_10_alltoall -> layer_10_expert1_35
	layer_10_expert1_35 -> layer_10_expert_act_35
	layer_10_expert_act_35 -> layer_10_expert2_35
	layer_10_expert2_35 -> layer_10_expert_allreduce_8
	layer_10_expert_allreduce_8 -> layer_10_expert_agg
	layer_10_attn_allreduce_9 -> layer_10_gate_36 [style=dashed]
	layer_10_gate_36 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_36
	layer_10_expert1_36 -> layer_10_expert_act_36
	layer_10_expert_act_36 -> layer_10_expert2_36
	layer_10_expert2_36 -> layer_10_expert_allreduce_9
	layer_10_alltoall -> layer_10_expert1_37
	layer_10_expert1_37 -> layer_10_expert_act_37
	layer_10_expert_act_37 -> layer_10_expert2_37
	layer_10_expert2_37 -> layer_10_expert_allreduce_9
	layer_10_alltoall -> layer_10_expert1_38
	layer_10_expert1_38 -> layer_10_expert_act_38
	layer_10_expert_act_38 -> layer_10_expert2_38
	layer_10_expert2_38 -> layer_10_expert_allreduce_9
	layer_10_alltoall -> layer_10_expert1_39
	layer_10_expert1_39 -> layer_10_expert_act_39
	layer_10_expert_act_39 -> layer_10_expert2_39
	layer_10_expert2_39 -> layer_10_expert_allreduce_9
	layer_10_expert_allreduce_9 -> layer_10_expert_agg
	layer_10_attn_allreduce_10 -> layer_10_gate_40 [style=dashed]
	layer_10_gate_40 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_40
	layer_10_expert1_40 -> layer_10_expert_act_40
	layer_10_expert_act_40 -> layer_10_expert2_40
	layer_10_expert2_40 -> layer_10_expert_allreduce_10
	layer_10_alltoall -> layer_10_expert1_41
	layer_10_expert1_41 -> layer_10_expert_act_41
	layer_10_expert_act_41 -> layer_10_expert2_41
	layer_10_expert2_41 -> layer_10_expert_allreduce_10
	layer_10_alltoall -> layer_10_expert1_42
	layer_10_expert1_42 -> layer_10_expert_act_42
	layer_10_expert_act_42 -> layer_10_expert2_42
	layer_10_expert2_42 -> layer_10_expert_allreduce_10
	layer_10_alltoall -> layer_10_expert1_43
	layer_10_expert1_43 -> layer_10_expert_act_43
	layer_10_expert_act_43 -> layer_10_expert2_43
	layer_10_expert2_43 -> layer_10_expert_allreduce_10
	layer_10_expert_allreduce_10 -> layer_10_expert_agg
	layer_10_attn_allreduce_11 -> layer_10_gate_44 [style=dashed]
	layer_10_gate_44 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_44
	layer_10_expert1_44 -> layer_10_expert_act_44
	layer_10_expert_act_44 -> layer_10_expert2_44
	layer_10_expert2_44 -> layer_10_expert_allreduce_11
	layer_10_alltoall -> layer_10_expert1_45
	layer_10_expert1_45 -> layer_10_expert_act_45
	layer_10_expert_act_45 -> layer_10_expert2_45
	layer_10_expert2_45 -> layer_10_expert_allreduce_11
	layer_10_alltoall -> layer_10_expert1_46
	layer_10_expert1_46 -> layer_10_expert_act_46
	layer_10_expert_act_46 -> layer_10_expert2_46
	layer_10_expert2_46 -> layer_10_expert_allreduce_11
	layer_10_alltoall -> layer_10_expert1_47
	layer_10_expert1_47 -> layer_10_expert_act_47
	layer_10_expert_act_47 -> layer_10_expert2_47
	layer_10_expert2_47 -> layer_10_expert_allreduce_11
	layer_10_expert_allreduce_11 -> layer_10_expert_agg
	layer_10_attn_allreduce_12 -> layer_10_gate_48 [style=dashed]
	layer_10_gate_48 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_48
	layer_10_expert1_48 -> layer_10_expert_act_48
	layer_10_expert_act_48 -> layer_10_expert2_48
	layer_10_expert2_48 -> layer_10_expert_allreduce_12
	layer_10_alltoall -> layer_10_expert1_49
	layer_10_expert1_49 -> layer_10_expert_act_49
	layer_10_expert_act_49 -> layer_10_expert2_49
	layer_10_expert2_49 -> layer_10_expert_allreduce_12
	layer_10_alltoall -> layer_10_expert1_50
	layer_10_expert1_50 -> layer_10_expert_act_50
	layer_10_expert_act_50 -> layer_10_expert2_50
	layer_10_expert2_50 -> layer_10_expert_allreduce_12
	layer_10_alltoall -> layer_10_expert1_51
	layer_10_expert1_51 -> layer_10_expert_act_51
	layer_10_expert_act_51 -> layer_10_expert2_51
	layer_10_expert2_51 -> layer_10_expert_allreduce_12
	layer_10_expert_allreduce_12 -> layer_10_expert_agg
	layer_10_attn_allreduce_13 -> layer_10_gate_52 [style=dashed]
	layer_10_gate_52 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_52
	layer_10_expert1_52 -> layer_10_expert_act_52
	layer_10_expert_act_52 -> layer_10_expert2_52
	layer_10_expert2_52 -> layer_10_expert_allreduce_13
	layer_10_alltoall -> layer_10_expert1_53
	layer_10_expert1_53 -> layer_10_expert_act_53
	layer_10_expert_act_53 -> layer_10_expert2_53
	layer_10_expert2_53 -> layer_10_expert_allreduce_13
	layer_10_alltoall -> layer_10_expert1_54
	layer_10_expert1_54 -> layer_10_expert_act_54
	layer_10_expert_act_54 -> layer_10_expert2_54
	layer_10_expert2_54 -> layer_10_expert_allreduce_13
	layer_10_alltoall -> layer_10_expert1_55
	layer_10_expert1_55 -> layer_10_expert_act_55
	layer_10_expert_act_55 -> layer_10_expert2_55
	layer_10_expert2_55 -> layer_10_expert_allreduce_13
	layer_10_expert_allreduce_13 -> layer_10_expert_agg
	layer_10_attn_allreduce_14 -> layer_10_gate_56 [style=dashed]
	layer_10_gate_56 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_56
	layer_10_expert1_56 -> layer_10_expert_act_56
	layer_10_expert_act_56 -> layer_10_expert2_56
	layer_10_expert2_56 -> layer_10_expert_allreduce_14
	layer_10_alltoall -> layer_10_expert1_57
	layer_10_expert1_57 -> layer_10_expert_act_57
	layer_10_expert_act_57 -> layer_10_expert2_57
	layer_10_expert2_57 -> layer_10_expert_allreduce_14
	layer_10_alltoall -> layer_10_expert1_58
	layer_10_expert1_58 -> layer_10_expert_act_58
	layer_10_expert_act_58 -> layer_10_expert2_58
	layer_10_expert2_58 -> layer_10_expert_allreduce_14
	layer_10_alltoall -> layer_10_expert1_59
	layer_10_expert1_59 -> layer_10_expert_act_59
	layer_10_expert_act_59 -> layer_10_expert2_59
	layer_10_expert2_59 -> layer_10_expert_allreduce_14
	layer_10_expert_allreduce_14 -> layer_10_expert_agg
	layer_10_attn_allreduce_15 -> layer_10_gate_60 [style=dashed]
	layer_10_gate_60 -> layer_10_alltoall [style=dashed]
	layer_10_alltoall -> layer_10_expert1_60
	layer_10_expert1_60 -> layer_10_expert_act_60
	layer_10_expert_act_60 -> layer_10_expert2_60
	layer_10_expert2_60 -> layer_10_expert_allreduce_15
	layer_10_alltoall -> layer_10_expert1_61
	layer_10_expert1_61 -> layer_10_expert_act_61
	layer_10_expert_act_61 -> layer_10_expert2_61
	layer_10_expert2_61 -> layer_10_expert_allreduce_15
	layer_10_alltoall -> layer_10_expert1_62
	layer_10_expert1_62 -> layer_10_expert_act_62
	layer_10_expert_act_62 -> layer_10_expert2_62
	layer_10_expert2_62 -> layer_10_expert_allreduce_15
	layer_10_alltoall -> layer_10_expert1_63
	layer_10_expert1_63 -> layer_10_expert_act_63
	layer_10_expert_act_63 -> layer_10_expert2_63
	layer_10_expert2_63 -> layer_10_expert_allreduce_15
	layer_10_expert_allreduce_15 -> layer_10_expert_agg
	layer_10_expert_agg -> layer_10_norm
	layer_10_norm -> layer_11_input
	layer_11_input [label="Layer11_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_11_qkv_0 [label="Layer11_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_1 [label="Layer11_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_2 [label="Layer11_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_3 [label="Layer11_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_0 [label="Layer11_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_1 [label="Layer11_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_2 [label="Layer11_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_3 [label="Layer11_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_0 [label="Layer11_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_1 [label="Layer11_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_2 [label="Layer11_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_3 [label="Layer11_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_0 [label="Layer11_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_1 [label="Layer11_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_2 [label="Layer11_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_3 [label="Layer11_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_0 [label="Layer11_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_0
	layer_11_qkv_0 -> layer_11_attn_scores_0
	layer_11_attn_scores_0 -> layer_11_attn_softmax_0
	layer_11_attn_softmax_0 -> layer_11_attn_out_0
	layer_11_attn_out_0 -> layer_11_attn_allreduce_0
	layer_11_input -> layer_11_qkv_1
	layer_11_qkv_1 -> layer_11_attn_scores_1
	layer_11_attn_scores_1 -> layer_11_attn_softmax_1
	layer_11_attn_softmax_1 -> layer_11_attn_out_1
	layer_11_attn_out_1 -> layer_11_attn_allreduce_0
	layer_11_input -> layer_11_qkv_2
	layer_11_qkv_2 -> layer_11_attn_scores_2
	layer_11_attn_scores_2 -> layer_11_attn_softmax_2
	layer_11_attn_softmax_2 -> layer_11_attn_out_2
	layer_11_attn_out_2 -> layer_11_attn_allreduce_0
	layer_11_input -> layer_11_qkv_3
	layer_11_qkv_3 -> layer_11_attn_scores_3
	layer_11_attn_scores_3 -> layer_11_attn_softmax_3
	layer_11_attn_softmax_3 -> layer_11_attn_out_3
	layer_11_attn_out_3 -> layer_11_attn_allreduce_0
	layer_11_qkv_4 [label="Layer11_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_5 [label="Layer11_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_6 [label="Layer11_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_7 [label="Layer11_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_4 [label="Layer11_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_5 [label="Layer11_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_6 [label="Layer11_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_7 [label="Layer11_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_4 [label="Layer11_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_5 [label="Layer11_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_6 [label="Layer11_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_7 [label="Layer11_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_4 [label="Layer11_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_5 [label="Layer11_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_6 [label="Layer11_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_7 [label="Layer11_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_1 [label="Layer11_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_4
	layer_11_qkv_4 -> layer_11_attn_scores_4
	layer_11_attn_scores_4 -> layer_11_attn_softmax_4
	layer_11_attn_softmax_4 -> layer_11_attn_out_4
	layer_11_attn_out_4 -> layer_11_attn_allreduce_1
	layer_11_input -> layer_11_qkv_5
	layer_11_qkv_5 -> layer_11_attn_scores_5
	layer_11_attn_scores_5 -> layer_11_attn_softmax_5
	layer_11_attn_softmax_5 -> layer_11_attn_out_5
	layer_11_attn_out_5 -> layer_11_attn_allreduce_1
	layer_11_input -> layer_11_qkv_6
	layer_11_qkv_6 -> layer_11_attn_scores_6
	layer_11_attn_scores_6 -> layer_11_attn_softmax_6
	layer_11_attn_softmax_6 -> layer_11_attn_out_6
	layer_11_attn_out_6 -> layer_11_attn_allreduce_1
	layer_11_input -> layer_11_qkv_7
	layer_11_qkv_7 -> layer_11_attn_scores_7
	layer_11_attn_scores_7 -> layer_11_attn_softmax_7
	layer_11_attn_softmax_7 -> layer_11_attn_out_7
	layer_11_attn_out_7 -> layer_11_attn_allreduce_1
	layer_11_qkv_8 [label="Layer11_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_9 [label="Layer11_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_10 [label="Layer11_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_11 [label="Layer11_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_8 [label="Layer11_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_9 [label="Layer11_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_10 [label="Layer11_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_11 [label="Layer11_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_8 [label="Layer11_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_9 [label="Layer11_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_10 [label="Layer11_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_11 [label="Layer11_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_8 [label="Layer11_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_9 [label="Layer11_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_10 [label="Layer11_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_11 [label="Layer11_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_2 [label="Layer11_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_8
	layer_11_qkv_8 -> layer_11_attn_scores_8
	layer_11_attn_scores_8 -> layer_11_attn_softmax_8
	layer_11_attn_softmax_8 -> layer_11_attn_out_8
	layer_11_attn_out_8 -> layer_11_attn_allreduce_2
	layer_11_input -> layer_11_qkv_9
	layer_11_qkv_9 -> layer_11_attn_scores_9
	layer_11_attn_scores_9 -> layer_11_attn_softmax_9
	layer_11_attn_softmax_9 -> layer_11_attn_out_9
	layer_11_attn_out_9 -> layer_11_attn_allreduce_2
	layer_11_input -> layer_11_qkv_10
	layer_11_qkv_10 -> layer_11_attn_scores_10
	layer_11_attn_scores_10 -> layer_11_attn_softmax_10
	layer_11_attn_softmax_10 -> layer_11_attn_out_10
	layer_11_attn_out_10 -> layer_11_attn_allreduce_2
	layer_11_input -> layer_11_qkv_11
	layer_11_qkv_11 -> layer_11_attn_scores_11
	layer_11_attn_scores_11 -> layer_11_attn_softmax_11
	layer_11_attn_softmax_11 -> layer_11_attn_out_11
	layer_11_attn_out_11 -> layer_11_attn_allreduce_2
	layer_11_qkv_12 [label="Layer11_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_13 [label="Layer11_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_14 [label="Layer11_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_15 [label="Layer11_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_12 [label="Layer11_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_13 [label="Layer11_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_14 [label="Layer11_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_15 [label="Layer11_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_12 [label="Layer11_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_13 [label="Layer11_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_14 [label="Layer11_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_15 [label="Layer11_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_12 [label="Layer11_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_13 [label="Layer11_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_14 [label="Layer11_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_15 [label="Layer11_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_3 [label="Layer11_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_12
	layer_11_qkv_12 -> layer_11_attn_scores_12
	layer_11_attn_scores_12 -> layer_11_attn_softmax_12
	layer_11_attn_softmax_12 -> layer_11_attn_out_12
	layer_11_attn_out_12 -> layer_11_attn_allreduce_3
	layer_11_input -> layer_11_qkv_13
	layer_11_qkv_13 -> layer_11_attn_scores_13
	layer_11_attn_scores_13 -> layer_11_attn_softmax_13
	layer_11_attn_softmax_13 -> layer_11_attn_out_13
	layer_11_attn_out_13 -> layer_11_attn_allreduce_3
	layer_11_input -> layer_11_qkv_14
	layer_11_qkv_14 -> layer_11_attn_scores_14
	layer_11_attn_scores_14 -> layer_11_attn_softmax_14
	layer_11_attn_softmax_14 -> layer_11_attn_out_14
	layer_11_attn_out_14 -> layer_11_attn_allreduce_3
	layer_11_input -> layer_11_qkv_15
	layer_11_qkv_15 -> layer_11_attn_scores_15
	layer_11_attn_scores_15 -> layer_11_attn_softmax_15
	layer_11_attn_softmax_15 -> layer_11_attn_out_15
	layer_11_attn_out_15 -> layer_11_attn_allreduce_3
	layer_11_qkv_16 [label="Layer11_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_17 [label="Layer11_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_18 [label="Layer11_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_19 [label="Layer11_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_16 [label="Layer11_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_17 [label="Layer11_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_18 [label="Layer11_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_19 [label="Layer11_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_16 [label="Layer11_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_17 [label="Layer11_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_18 [label="Layer11_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_19 [label="Layer11_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_16 [label="Layer11_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_17 [label="Layer11_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_18 [label="Layer11_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_19 [label="Layer11_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_4 [label="Layer11_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_16
	layer_11_qkv_16 -> layer_11_attn_scores_16
	layer_11_attn_scores_16 -> layer_11_attn_softmax_16
	layer_11_attn_softmax_16 -> layer_11_attn_out_16
	layer_11_attn_out_16 -> layer_11_attn_allreduce_4
	layer_11_input -> layer_11_qkv_17
	layer_11_qkv_17 -> layer_11_attn_scores_17
	layer_11_attn_scores_17 -> layer_11_attn_softmax_17
	layer_11_attn_softmax_17 -> layer_11_attn_out_17
	layer_11_attn_out_17 -> layer_11_attn_allreduce_4
	layer_11_input -> layer_11_qkv_18
	layer_11_qkv_18 -> layer_11_attn_scores_18
	layer_11_attn_scores_18 -> layer_11_attn_softmax_18
	layer_11_attn_softmax_18 -> layer_11_attn_out_18
	layer_11_attn_out_18 -> layer_11_attn_allreduce_4
	layer_11_input -> layer_11_qkv_19
	layer_11_qkv_19 -> layer_11_attn_scores_19
	layer_11_attn_scores_19 -> layer_11_attn_softmax_19
	layer_11_attn_softmax_19 -> layer_11_attn_out_19
	layer_11_attn_out_19 -> layer_11_attn_allreduce_4
	layer_11_qkv_20 [label="Layer11_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_21 [label="Layer11_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_22 [label="Layer11_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_23 [label="Layer11_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_20 [label="Layer11_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_21 [label="Layer11_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_22 [label="Layer11_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_23 [label="Layer11_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_20 [label="Layer11_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_21 [label="Layer11_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_22 [label="Layer11_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_23 [label="Layer11_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_20 [label="Layer11_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_21 [label="Layer11_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_22 [label="Layer11_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_23 [label="Layer11_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_5 [label="Layer11_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_20
	layer_11_qkv_20 -> layer_11_attn_scores_20
	layer_11_attn_scores_20 -> layer_11_attn_softmax_20
	layer_11_attn_softmax_20 -> layer_11_attn_out_20
	layer_11_attn_out_20 -> layer_11_attn_allreduce_5
	layer_11_input -> layer_11_qkv_21
	layer_11_qkv_21 -> layer_11_attn_scores_21
	layer_11_attn_scores_21 -> layer_11_attn_softmax_21
	layer_11_attn_softmax_21 -> layer_11_attn_out_21
	layer_11_attn_out_21 -> layer_11_attn_allreduce_5
	layer_11_input -> layer_11_qkv_22
	layer_11_qkv_22 -> layer_11_attn_scores_22
	layer_11_attn_scores_22 -> layer_11_attn_softmax_22
	layer_11_attn_softmax_22 -> layer_11_attn_out_22
	layer_11_attn_out_22 -> layer_11_attn_allreduce_5
	layer_11_input -> layer_11_qkv_23
	layer_11_qkv_23 -> layer_11_attn_scores_23
	layer_11_attn_scores_23 -> layer_11_attn_softmax_23
	layer_11_attn_softmax_23 -> layer_11_attn_out_23
	layer_11_attn_out_23 -> layer_11_attn_allreduce_5
	layer_11_qkv_24 [label="Layer11_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_25 [label="Layer11_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_26 [label="Layer11_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_27 [label="Layer11_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_24 [label="Layer11_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_25 [label="Layer11_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_26 [label="Layer11_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_27 [label="Layer11_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_24 [label="Layer11_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_25 [label="Layer11_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_26 [label="Layer11_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_27 [label="Layer11_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_24 [label="Layer11_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_25 [label="Layer11_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_26 [label="Layer11_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_27 [label="Layer11_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_6 [label="Layer11_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_24
	layer_11_qkv_24 -> layer_11_attn_scores_24
	layer_11_attn_scores_24 -> layer_11_attn_softmax_24
	layer_11_attn_softmax_24 -> layer_11_attn_out_24
	layer_11_attn_out_24 -> layer_11_attn_allreduce_6
	layer_11_input -> layer_11_qkv_25
	layer_11_qkv_25 -> layer_11_attn_scores_25
	layer_11_attn_scores_25 -> layer_11_attn_softmax_25
	layer_11_attn_softmax_25 -> layer_11_attn_out_25
	layer_11_attn_out_25 -> layer_11_attn_allreduce_6
	layer_11_input -> layer_11_qkv_26
	layer_11_qkv_26 -> layer_11_attn_scores_26
	layer_11_attn_scores_26 -> layer_11_attn_softmax_26
	layer_11_attn_softmax_26 -> layer_11_attn_out_26
	layer_11_attn_out_26 -> layer_11_attn_allreduce_6
	layer_11_input -> layer_11_qkv_27
	layer_11_qkv_27 -> layer_11_attn_scores_27
	layer_11_attn_scores_27 -> layer_11_attn_softmax_27
	layer_11_attn_softmax_27 -> layer_11_attn_out_27
	layer_11_attn_out_27 -> layer_11_attn_allreduce_6
	layer_11_qkv_28 [label="Layer11_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_29 [label="Layer11_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_30 [label="Layer11_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_31 [label="Layer11_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_28 [label="Layer11_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_29 [label="Layer11_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_30 [label="Layer11_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_31 [label="Layer11_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_28 [label="Layer11_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_29 [label="Layer11_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_30 [label="Layer11_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_31 [label="Layer11_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_28 [label="Layer11_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_29 [label="Layer11_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_30 [label="Layer11_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_31 [label="Layer11_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_7 [label="Layer11_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_28
	layer_11_qkv_28 -> layer_11_attn_scores_28
	layer_11_attn_scores_28 -> layer_11_attn_softmax_28
	layer_11_attn_softmax_28 -> layer_11_attn_out_28
	layer_11_attn_out_28 -> layer_11_attn_allreduce_7
	layer_11_input -> layer_11_qkv_29
	layer_11_qkv_29 -> layer_11_attn_scores_29
	layer_11_attn_scores_29 -> layer_11_attn_softmax_29
	layer_11_attn_softmax_29 -> layer_11_attn_out_29
	layer_11_attn_out_29 -> layer_11_attn_allreduce_7
	layer_11_input -> layer_11_qkv_30
	layer_11_qkv_30 -> layer_11_attn_scores_30
	layer_11_attn_scores_30 -> layer_11_attn_softmax_30
	layer_11_attn_softmax_30 -> layer_11_attn_out_30
	layer_11_attn_out_30 -> layer_11_attn_allreduce_7
	layer_11_input -> layer_11_qkv_31
	layer_11_qkv_31 -> layer_11_attn_scores_31
	layer_11_attn_scores_31 -> layer_11_attn_softmax_31
	layer_11_attn_softmax_31 -> layer_11_attn_out_31
	layer_11_attn_out_31 -> layer_11_attn_allreduce_7
	layer_11_qkv_32 [label="Layer11_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_33 [label="Layer11_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_34 [label="Layer11_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_35 [label="Layer11_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_32 [label="Layer11_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_33 [label="Layer11_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_34 [label="Layer11_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_35 [label="Layer11_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_32 [label="Layer11_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_33 [label="Layer11_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_34 [label="Layer11_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_35 [label="Layer11_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_32 [label="Layer11_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_33 [label="Layer11_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_34 [label="Layer11_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_35 [label="Layer11_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_8 [label="Layer11_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_32
	layer_11_qkv_32 -> layer_11_attn_scores_32
	layer_11_attn_scores_32 -> layer_11_attn_softmax_32
	layer_11_attn_softmax_32 -> layer_11_attn_out_32
	layer_11_attn_out_32 -> layer_11_attn_allreduce_8
	layer_11_input -> layer_11_qkv_33
	layer_11_qkv_33 -> layer_11_attn_scores_33
	layer_11_attn_scores_33 -> layer_11_attn_softmax_33
	layer_11_attn_softmax_33 -> layer_11_attn_out_33
	layer_11_attn_out_33 -> layer_11_attn_allreduce_8
	layer_11_input -> layer_11_qkv_34
	layer_11_qkv_34 -> layer_11_attn_scores_34
	layer_11_attn_scores_34 -> layer_11_attn_softmax_34
	layer_11_attn_softmax_34 -> layer_11_attn_out_34
	layer_11_attn_out_34 -> layer_11_attn_allreduce_8
	layer_11_input -> layer_11_qkv_35
	layer_11_qkv_35 -> layer_11_attn_scores_35
	layer_11_attn_scores_35 -> layer_11_attn_softmax_35
	layer_11_attn_softmax_35 -> layer_11_attn_out_35
	layer_11_attn_out_35 -> layer_11_attn_allreduce_8
	layer_11_qkv_36 [label="Layer11_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_37 [label="Layer11_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_38 [label="Layer11_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_39 [label="Layer11_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_36 [label="Layer11_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_37 [label="Layer11_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_38 [label="Layer11_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_39 [label="Layer11_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_36 [label="Layer11_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_37 [label="Layer11_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_38 [label="Layer11_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_39 [label="Layer11_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_36 [label="Layer11_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_37 [label="Layer11_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_38 [label="Layer11_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_39 [label="Layer11_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_9 [label="Layer11_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_36
	layer_11_qkv_36 -> layer_11_attn_scores_36
	layer_11_attn_scores_36 -> layer_11_attn_softmax_36
	layer_11_attn_softmax_36 -> layer_11_attn_out_36
	layer_11_attn_out_36 -> layer_11_attn_allreduce_9
	layer_11_input -> layer_11_qkv_37
	layer_11_qkv_37 -> layer_11_attn_scores_37
	layer_11_attn_scores_37 -> layer_11_attn_softmax_37
	layer_11_attn_softmax_37 -> layer_11_attn_out_37
	layer_11_attn_out_37 -> layer_11_attn_allreduce_9
	layer_11_input -> layer_11_qkv_38
	layer_11_qkv_38 -> layer_11_attn_scores_38
	layer_11_attn_scores_38 -> layer_11_attn_softmax_38
	layer_11_attn_softmax_38 -> layer_11_attn_out_38
	layer_11_attn_out_38 -> layer_11_attn_allreduce_9
	layer_11_input -> layer_11_qkv_39
	layer_11_qkv_39 -> layer_11_attn_scores_39
	layer_11_attn_scores_39 -> layer_11_attn_softmax_39
	layer_11_attn_softmax_39 -> layer_11_attn_out_39
	layer_11_attn_out_39 -> layer_11_attn_allreduce_9
	layer_11_qkv_40 [label="Layer11_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_41 [label="Layer11_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_42 [label="Layer11_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_43 [label="Layer11_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_40 [label="Layer11_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_41 [label="Layer11_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_42 [label="Layer11_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_43 [label="Layer11_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_40 [label="Layer11_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_41 [label="Layer11_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_42 [label="Layer11_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_43 [label="Layer11_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_40 [label="Layer11_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_41 [label="Layer11_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_42 [label="Layer11_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_43 [label="Layer11_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_10 [label="Layer11_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_40
	layer_11_qkv_40 -> layer_11_attn_scores_40
	layer_11_attn_scores_40 -> layer_11_attn_softmax_40
	layer_11_attn_softmax_40 -> layer_11_attn_out_40
	layer_11_attn_out_40 -> layer_11_attn_allreduce_10
	layer_11_input -> layer_11_qkv_41
	layer_11_qkv_41 -> layer_11_attn_scores_41
	layer_11_attn_scores_41 -> layer_11_attn_softmax_41
	layer_11_attn_softmax_41 -> layer_11_attn_out_41
	layer_11_attn_out_41 -> layer_11_attn_allreduce_10
	layer_11_input -> layer_11_qkv_42
	layer_11_qkv_42 -> layer_11_attn_scores_42
	layer_11_attn_scores_42 -> layer_11_attn_softmax_42
	layer_11_attn_softmax_42 -> layer_11_attn_out_42
	layer_11_attn_out_42 -> layer_11_attn_allreduce_10
	layer_11_input -> layer_11_qkv_43
	layer_11_qkv_43 -> layer_11_attn_scores_43
	layer_11_attn_scores_43 -> layer_11_attn_softmax_43
	layer_11_attn_softmax_43 -> layer_11_attn_out_43
	layer_11_attn_out_43 -> layer_11_attn_allreduce_10
	layer_11_qkv_44 [label="Layer11_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_45 [label="Layer11_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_46 [label="Layer11_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_47 [label="Layer11_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_44 [label="Layer11_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_45 [label="Layer11_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_46 [label="Layer11_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_47 [label="Layer11_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_44 [label="Layer11_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_45 [label="Layer11_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_46 [label="Layer11_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_47 [label="Layer11_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_44 [label="Layer11_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_45 [label="Layer11_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_46 [label="Layer11_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_47 [label="Layer11_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_11 [label="Layer11_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_44
	layer_11_qkv_44 -> layer_11_attn_scores_44
	layer_11_attn_scores_44 -> layer_11_attn_softmax_44
	layer_11_attn_softmax_44 -> layer_11_attn_out_44
	layer_11_attn_out_44 -> layer_11_attn_allreduce_11
	layer_11_input -> layer_11_qkv_45
	layer_11_qkv_45 -> layer_11_attn_scores_45
	layer_11_attn_scores_45 -> layer_11_attn_softmax_45
	layer_11_attn_softmax_45 -> layer_11_attn_out_45
	layer_11_attn_out_45 -> layer_11_attn_allreduce_11
	layer_11_input -> layer_11_qkv_46
	layer_11_qkv_46 -> layer_11_attn_scores_46
	layer_11_attn_scores_46 -> layer_11_attn_softmax_46
	layer_11_attn_softmax_46 -> layer_11_attn_out_46
	layer_11_attn_out_46 -> layer_11_attn_allreduce_11
	layer_11_input -> layer_11_qkv_47
	layer_11_qkv_47 -> layer_11_attn_scores_47
	layer_11_attn_scores_47 -> layer_11_attn_softmax_47
	layer_11_attn_softmax_47 -> layer_11_attn_out_47
	layer_11_attn_out_47 -> layer_11_attn_allreduce_11
	layer_11_qkv_48 [label="Layer11_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_49 [label="Layer11_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_50 [label="Layer11_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_51 [label="Layer11_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_48 [label="Layer11_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_49 [label="Layer11_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_50 [label="Layer11_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_51 [label="Layer11_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_48 [label="Layer11_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_49 [label="Layer11_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_50 [label="Layer11_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_51 [label="Layer11_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_48 [label="Layer11_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_49 [label="Layer11_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_50 [label="Layer11_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_51 [label="Layer11_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_12 [label="Layer11_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_48
	layer_11_qkv_48 -> layer_11_attn_scores_48
	layer_11_attn_scores_48 -> layer_11_attn_softmax_48
	layer_11_attn_softmax_48 -> layer_11_attn_out_48
	layer_11_attn_out_48 -> layer_11_attn_allreduce_12
	layer_11_input -> layer_11_qkv_49
	layer_11_qkv_49 -> layer_11_attn_scores_49
	layer_11_attn_scores_49 -> layer_11_attn_softmax_49
	layer_11_attn_softmax_49 -> layer_11_attn_out_49
	layer_11_attn_out_49 -> layer_11_attn_allreduce_12
	layer_11_input -> layer_11_qkv_50
	layer_11_qkv_50 -> layer_11_attn_scores_50
	layer_11_attn_scores_50 -> layer_11_attn_softmax_50
	layer_11_attn_softmax_50 -> layer_11_attn_out_50
	layer_11_attn_out_50 -> layer_11_attn_allreduce_12
	layer_11_input -> layer_11_qkv_51
	layer_11_qkv_51 -> layer_11_attn_scores_51
	layer_11_attn_scores_51 -> layer_11_attn_softmax_51
	layer_11_attn_softmax_51 -> layer_11_attn_out_51
	layer_11_attn_out_51 -> layer_11_attn_allreduce_12
	layer_11_qkv_52 [label="Layer11_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_53 [label="Layer11_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_54 [label="Layer11_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_55 [label="Layer11_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_52 [label="Layer11_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_53 [label="Layer11_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_54 [label="Layer11_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_55 [label="Layer11_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_52 [label="Layer11_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_53 [label="Layer11_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_54 [label="Layer11_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_55 [label="Layer11_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_52 [label="Layer11_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_53 [label="Layer11_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_54 [label="Layer11_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_55 [label="Layer11_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_13 [label="Layer11_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_52
	layer_11_qkv_52 -> layer_11_attn_scores_52
	layer_11_attn_scores_52 -> layer_11_attn_softmax_52
	layer_11_attn_softmax_52 -> layer_11_attn_out_52
	layer_11_attn_out_52 -> layer_11_attn_allreduce_13
	layer_11_input -> layer_11_qkv_53
	layer_11_qkv_53 -> layer_11_attn_scores_53
	layer_11_attn_scores_53 -> layer_11_attn_softmax_53
	layer_11_attn_softmax_53 -> layer_11_attn_out_53
	layer_11_attn_out_53 -> layer_11_attn_allreduce_13
	layer_11_input -> layer_11_qkv_54
	layer_11_qkv_54 -> layer_11_attn_scores_54
	layer_11_attn_scores_54 -> layer_11_attn_softmax_54
	layer_11_attn_softmax_54 -> layer_11_attn_out_54
	layer_11_attn_out_54 -> layer_11_attn_allreduce_13
	layer_11_input -> layer_11_qkv_55
	layer_11_qkv_55 -> layer_11_attn_scores_55
	layer_11_attn_scores_55 -> layer_11_attn_softmax_55
	layer_11_attn_softmax_55 -> layer_11_attn_out_55
	layer_11_attn_out_55 -> layer_11_attn_allreduce_13
	layer_11_qkv_56 [label="Layer11_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_57 [label="Layer11_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_58 [label="Layer11_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_59 [label="Layer11_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_56 [label="Layer11_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_57 [label="Layer11_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_58 [label="Layer11_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_59 [label="Layer11_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_56 [label="Layer11_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_57 [label="Layer11_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_58 [label="Layer11_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_59 [label="Layer11_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_56 [label="Layer11_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_57 [label="Layer11_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_58 [label="Layer11_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_59 [label="Layer11_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_14 [label="Layer11_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_56
	layer_11_qkv_56 -> layer_11_attn_scores_56
	layer_11_attn_scores_56 -> layer_11_attn_softmax_56
	layer_11_attn_softmax_56 -> layer_11_attn_out_56
	layer_11_attn_out_56 -> layer_11_attn_allreduce_14
	layer_11_input -> layer_11_qkv_57
	layer_11_qkv_57 -> layer_11_attn_scores_57
	layer_11_attn_scores_57 -> layer_11_attn_softmax_57
	layer_11_attn_softmax_57 -> layer_11_attn_out_57
	layer_11_attn_out_57 -> layer_11_attn_allreduce_14
	layer_11_input -> layer_11_qkv_58
	layer_11_qkv_58 -> layer_11_attn_scores_58
	layer_11_attn_scores_58 -> layer_11_attn_softmax_58
	layer_11_attn_softmax_58 -> layer_11_attn_out_58
	layer_11_attn_out_58 -> layer_11_attn_allreduce_14
	layer_11_input -> layer_11_qkv_59
	layer_11_qkv_59 -> layer_11_attn_scores_59
	layer_11_attn_scores_59 -> layer_11_attn_softmax_59
	layer_11_attn_softmax_59 -> layer_11_attn_out_59
	layer_11_attn_out_59 -> layer_11_attn_allreduce_14
	layer_11_qkv_60 [label="Layer11_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_61 [label="Layer11_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_62 [label="Layer11_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_qkv_63 [label="Layer11_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_60 [label="Layer11_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_61 [label="Layer11_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_62 [label="Layer11_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_scores_63 [label="Layer11_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_60 [label="Layer11_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_61 [label="Layer11_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_62 [label="Layer11_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_softmax_63 [label="Layer11_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_60 [label="Layer11_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_61 [label="Layer11_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_62 [label="Layer11_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_out_63 [label="Layer11_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_15 [label="Layer11_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_input -> layer_11_qkv_60
	layer_11_qkv_60 -> layer_11_attn_scores_60
	layer_11_attn_scores_60 -> layer_11_attn_softmax_60
	layer_11_attn_softmax_60 -> layer_11_attn_out_60
	layer_11_attn_out_60 -> layer_11_attn_allreduce_15
	layer_11_input -> layer_11_qkv_61
	layer_11_qkv_61 -> layer_11_attn_scores_61
	layer_11_attn_scores_61 -> layer_11_attn_softmax_61
	layer_11_attn_softmax_61 -> layer_11_attn_out_61
	layer_11_attn_out_61 -> layer_11_attn_allreduce_15
	layer_11_input -> layer_11_qkv_62
	layer_11_qkv_62 -> layer_11_attn_scores_62
	layer_11_attn_scores_62 -> layer_11_attn_softmax_62
	layer_11_attn_softmax_62 -> layer_11_attn_out_62
	layer_11_attn_out_62 -> layer_11_attn_allreduce_15
	layer_11_input -> layer_11_qkv_63
	layer_11_qkv_63 -> layer_11_attn_scores_63
	layer_11_attn_scores_63 -> layer_11_attn_softmax_63
	layer_11_attn_softmax_63 -> layer_11_attn_out_63
	layer_11_attn_out_63 -> layer_11_attn_allreduce_15
	layer_11_gate_0 [label="Layer11_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_4 [label="Layer11_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_8 [label="Layer11_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_12 [label="Layer11_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_16 [label="Layer11_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_20 [label="Layer11_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_24 [label="Layer11_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_28 [label="Layer11_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_32 [label="Layer11_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_36 [label="Layer11_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_40 [label="Layer11_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_44 [label="Layer11_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_48 [label="Layer11_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_52 [label="Layer11_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_56 [label="Layer11_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_gate_60 [label="Layer11_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_11_alltoall [label="Layer11_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_0 [label="Layer11_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_1 [label="Layer11_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_2 [label="Layer11_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_3 [label="Layer11_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_0 [label="Layer11_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_1 [label="Layer11_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_2 [label="Layer11_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_3 [label="Layer11_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_0 [label="Layer11_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_1 [label="Layer11_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_2 [label="Layer11_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_3 [label="Layer11_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_0 [label="Layer11_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_4 [label="Layer11_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_5 [label="Layer11_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_6 [label="Layer11_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_7 [label="Layer11_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_4 [label="Layer11_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_5 [label="Layer11_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_6 [label="Layer11_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_7 [label="Layer11_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_4 [label="Layer11_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_5 [label="Layer11_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_6 [label="Layer11_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_7 [label="Layer11_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_1 [label="Layer11_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_8 [label="Layer11_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_9 [label="Layer11_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_10 [label="Layer11_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_11 [label="Layer11_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_8 [label="Layer11_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_9 [label="Layer11_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_10 [label="Layer11_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_11 [label="Layer11_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_8 [label="Layer11_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_9 [label="Layer11_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_10 [label="Layer11_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_11 [label="Layer11_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_2 [label="Layer11_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_12 [label="Layer11_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_13 [label="Layer11_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_14 [label="Layer11_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_15 [label="Layer11_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_12 [label="Layer11_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_13 [label="Layer11_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_14 [label="Layer11_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_15 [label="Layer11_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_12 [label="Layer11_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_13 [label="Layer11_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_14 [label="Layer11_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_15 [label="Layer11_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_3 [label="Layer11_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_16 [label="Layer11_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_17 [label="Layer11_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_18 [label="Layer11_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_19 [label="Layer11_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_16 [label="Layer11_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_17 [label="Layer11_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_18 [label="Layer11_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_19 [label="Layer11_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_16 [label="Layer11_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_17 [label="Layer11_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_18 [label="Layer11_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_19 [label="Layer11_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_4 [label="Layer11_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_20 [label="Layer11_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_21 [label="Layer11_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_22 [label="Layer11_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_23 [label="Layer11_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_20 [label="Layer11_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_21 [label="Layer11_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_22 [label="Layer11_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_23 [label="Layer11_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_20 [label="Layer11_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_21 [label="Layer11_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_22 [label="Layer11_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_23 [label="Layer11_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_5 [label="Layer11_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_24 [label="Layer11_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_25 [label="Layer11_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_26 [label="Layer11_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_27 [label="Layer11_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_24 [label="Layer11_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_25 [label="Layer11_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_26 [label="Layer11_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_27 [label="Layer11_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_24 [label="Layer11_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_25 [label="Layer11_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_26 [label="Layer11_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_27 [label="Layer11_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_6 [label="Layer11_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_28 [label="Layer11_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_29 [label="Layer11_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_30 [label="Layer11_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_31 [label="Layer11_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_28 [label="Layer11_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_29 [label="Layer11_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_30 [label="Layer11_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_31 [label="Layer11_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_28 [label="Layer11_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_29 [label="Layer11_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_30 [label="Layer11_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_31 [label="Layer11_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_7 [label="Layer11_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_32 [label="Layer11_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_33 [label="Layer11_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_34 [label="Layer11_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_35 [label="Layer11_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_32 [label="Layer11_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_33 [label="Layer11_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_34 [label="Layer11_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_35 [label="Layer11_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_32 [label="Layer11_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_33 [label="Layer11_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_34 [label="Layer11_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_35 [label="Layer11_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_8 [label="Layer11_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_36 [label="Layer11_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_37 [label="Layer11_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_38 [label="Layer11_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_39 [label="Layer11_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_36 [label="Layer11_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_37 [label="Layer11_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_38 [label="Layer11_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_39 [label="Layer11_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_36 [label="Layer11_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_37 [label="Layer11_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_38 [label="Layer11_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_39 [label="Layer11_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_9 [label="Layer11_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_40 [label="Layer11_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_41 [label="Layer11_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_42 [label="Layer11_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_43 [label="Layer11_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_40 [label="Layer11_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_41 [label="Layer11_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_42 [label="Layer11_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_43 [label="Layer11_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_40 [label="Layer11_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_41 [label="Layer11_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_42 [label="Layer11_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_43 [label="Layer11_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_10 [label="Layer11_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_44 [label="Layer11_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_45 [label="Layer11_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_46 [label="Layer11_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_47 [label="Layer11_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_44 [label="Layer11_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_45 [label="Layer11_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_46 [label="Layer11_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_47 [label="Layer11_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_44 [label="Layer11_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_45 [label="Layer11_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_46 [label="Layer11_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_47 [label="Layer11_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_11 [label="Layer11_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_48 [label="Layer11_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_49 [label="Layer11_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_50 [label="Layer11_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_51 [label="Layer11_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_48 [label="Layer11_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_49 [label="Layer11_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_50 [label="Layer11_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_51 [label="Layer11_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_48 [label="Layer11_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_49 [label="Layer11_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_50 [label="Layer11_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_51 [label="Layer11_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_12 [label="Layer11_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_52 [label="Layer11_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_53 [label="Layer11_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_54 [label="Layer11_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_55 [label="Layer11_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_52 [label="Layer11_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_53 [label="Layer11_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_54 [label="Layer11_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_55 [label="Layer11_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_52 [label="Layer11_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_53 [label="Layer11_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_54 [label="Layer11_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_55 [label="Layer11_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_13 [label="Layer11_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_56 [label="Layer11_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_57 [label="Layer11_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_58 [label="Layer11_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_59 [label="Layer11_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_56 [label="Layer11_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_57 [label="Layer11_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_58 [label="Layer11_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_59 [label="Layer11_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_56 [label="Layer11_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_57 [label="Layer11_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_58 [label="Layer11_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_59 [label="Layer11_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_14 [label="Layer11_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert1_60 [label="Layer11_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_61 [label="Layer11_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_62 [label="Layer11_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert1_63 [label="Layer11_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_60 [label="Layer11_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_61 [label="Layer11_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_62 [label="Layer11_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert_act_63 [label="Layer11_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_11_expert2_60 [label="Layer11_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_61 [label="Layer11_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_62 [label="Layer11_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert2_63 [label="Layer11_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_11_expert_allreduce_15 [label="Layer11_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_11_expert_agg [label="Layer11_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_11_norm [label="Layer11_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_11_attn_allreduce_0 -> layer_11_gate_0 [style=dashed]
	layer_11_gate_0 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_0
	layer_11_expert1_0 -> layer_11_expert_act_0
	layer_11_expert_act_0 -> layer_11_expert2_0
	layer_11_expert2_0 -> layer_11_expert_allreduce_0
	layer_11_alltoall -> layer_11_expert1_1
	layer_11_expert1_1 -> layer_11_expert_act_1
	layer_11_expert_act_1 -> layer_11_expert2_1
	layer_11_expert2_1 -> layer_11_expert_allreduce_0
	layer_11_alltoall -> layer_11_expert1_2
	layer_11_expert1_2 -> layer_11_expert_act_2
	layer_11_expert_act_2 -> layer_11_expert2_2
	layer_11_expert2_2 -> layer_11_expert_allreduce_0
	layer_11_alltoall -> layer_11_expert1_3
	layer_11_expert1_3 -> layer_11_expert_act_3
	layer_11_expert_act_3 -> layer_11_expert2_3
	layer_11_expert2_3 -> layer_11_expert_allreduce_0
	layer_11_expert_allreduce_0 -> layer_11_expert_agg
	layer_11_attn_allreduce_1 -> layer_11_gate_4 [style=dashed]
	layer_11_gate_4 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_4
	layer_11_expert1_4 -> layer_11_expert_act_4
	layer_11_expert_act_4 -> layer_11_expert2_4
	layer_11_expert2_4 -> layer_11_expert_allreduce_1
	layer_11_alltoall -> layer_11_expert1_5
	layer_11_expert1_5 -> layer_11_expert_act_5
	layer_11_expert_act_5 -> layer_11_expert2_5
	layer_11_expert2_5 -> layer_11_expert_allreduce_1
	layer_11_alltoall -> layer_11_expert1_6
	layer_11_expert1_6 -> layer_11_expert_act_6
	layer_11_expert_act_6 -> layer_11_expert2_6
	layer_11_expert2_6 -> layer_11_expert_allreduce_1
	layer_11_alltoall -> layer_11_expert1_7
	layer_11_expert1_7 -> layer_11_expert_act_7
	layer_11_expert_act_7 -> layer_11_expert2_7
	layer_11_expert2_7 -> layer_11_expert_allreduce_1
	layer_11_expert_allreduce_1 -> layer_11_expert_agg
	layer_11_attn_allreduce_2 -> layer_11_gate_8 [style=dashed]
	layer_11_gate_8 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_8
	layer_11_expert1_8 -> layer_11_expert_act_8
	layer_11_expert_act_8 -> layer_11_expert2_8
	layer_11_expert2_8 -> layer_11_expert_allreduce_2
	layer_11_alltoall -> layer_11_expert1_9
	layer_11_expert1_9 -> layer_11_expert_act_9
	layer_11_expert_act_9 -> layer_11_expert2_9
	layer_11_expert2_9 -> layer_11_expert_allreduce_2
	layer_11_alltoall -> layer_11_expert1_10
	layer_11_expert1_10 -> layer_11_expert_act_10
	layer_11_expert_act_10 -> layer_11_expert2_10
	layer_11_expert2_10 -> layer_11_expert_allreduce_2
	layer_11_alltoall -> layer_11_expert1_11
	layer_11_expert1_11 -> layer_11_expert_act_11
	layer_11_expert_act_11 -> layer_11_expert2_11
	layer_11_expert2_11 -> layer_11_expert_allreduce_2
	layer_11_expert_allreduce_2 -> layer_11_expert_agg
	layer_11_attn_allreduce_3 -> layer_11_gate_12 [style=dashed]
	layer_11_gate_12 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_12
	layer_11_expert1_12 -> layer_11_expert_act_12
	layer_11_expert_act_12 -> layer_11_expert2_12
	layer_11_expert2_12 -> layer_11_expert_allreduce_3
	layer_11_alltoall -> layer_11_expert1_13
	layer_11_expert1_13 -> layer_11_expert_act_13
	layer_11_expert_act_13 -> layer_11_expert2_13
	layer_11_expert2_13 -> layer_11_expert_allreduce_3
	layer_11_alltoall -> layer_11_expert1_14
	layer_11_expert1_14 -> layer_11_expert_act_14
	layer_11_expert_act_14 -> layer_11_expert2_14
	layer_11_expert2_14 -> layer_11_expert_allreduce_3
	layer_11_alltoall -> layer_11_expert1_15
	layer_11_expert1_15 -> layer_11_expert_act_15
	layer_11_expert_act_15 -> layer_11_expert2_15
	layer_11_expert2_15 -> layer_11_expert_allreduce_3
	layer_11_expert_allreduce_3 -> layer_11_expert_agg
	layer_11_attn_allreduce_4 -> layer_11_gate_16 [style=dashed]
	layer_11_gate_16 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_16
	layer_11_expert1_16 -> layer_11_expert_act_16
	layer_11_expert_act_16 -> layer_11_expert2_16
	layer_11_expert2_16 -> layer_11_expert_allreduce_4
	layer_11_alltoall -> layer_11_expert1_17
	layer_11_expert1_17 -> layer_11_expert_act_17
	layer_11_expert_act_17 -> layer_11_expert2_17
	layer_11_expert2_17 -> layer_11_expert_allreduce_4
	layer_11_alltoall -> layer_11_expert1_18
	layer_11_expert1_18 -> layer_11_expert_act_18
	layer_11_expert_act_18 -> layer_11_expert2_18
	layer_11_expert2_18 -> layer_11_expert_allreduce_4
	layer_11_alltoall -> layer_11_expert1_19
	layer_11_expert1_19 -> layer_11_expert_act_19
	layer_11_expert_act_19 -> layer_11_expert2_19
	layer_11_expert2_19 -> layer_11_expert_allreduce_4
	layer_11_expert_allreduce_4 -> layer_11_expert_agg
	layer_11_attn_allreduce_5 -> layer_11_gate_20 [style=dashed]
	layer_11_gate_20 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_20
	layer_11_expert1_20 -> layer_11_expert_act_20
	layer_11_expert_act_20 -> layer_11_expert2_20
	layer_11_expert2_20 -> layer_11_expert_allreduce_5
	layer_11_alltoall -> layer_11_expert1_21
	layer_11_expert1_21 -> layer_11_expert_act_21
	layer_11_expert_act_21 -> layer_11_expert2_21
	layer_11_expert2_21 -> layer_11_expert_allreduce_5
	layer_11_alltoall -> layer_11_expert1_22
	layer_11_expert1_22 -> layer_11_expert_act_22
	layer_11_expert_act_22 -> layer_11_expert2_22
	layer_11_expert2_22 -> layer_11_expert_allreduce_5
	layer_11_alltoall -> layer_11_expert1_23
	layer_11_expert1_23 -> layer_11_expert_act_23
	layer_11_expert_act_23 -> layer_11_expert2_23
	layer_11_expert2_23 -> layer_11_expert_allreduce_5
	layer_11_expert_allreduce_5 -> layer_11_expert_agg
	layer_11_attn_allreduce_6 -> layer_11_gate_24 [style=dashed]
	layer_11_gate_24 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_24
	layer_11_expert1_24 -> layer_11_expert_act_24
	layer_11_expert_act_24 -> layer_11_expert2_24
	layer_11_expert2_24 -> layer_11_expert_allreduce_6
	layer_11_alltoall -> layer_11_expert1_25
	layer_11_expert1_25 -> layer_11_expert_act_25
	layer_11_expert_act_25 -> layer_11_expert2_25
	layer_11_expert2_25 -> layer_11_expert_allreduce_6
	layer_11_alltoall -> layer_11_expert1_26
	layer_11_expert1_26 -> layer_11_expert_act_26
	layer_11_expert_act_26 -> layer_11_expert2_26
	layer_11_expert2_26 -> layer_11_expert_allreduce_6
	layer_11_alltoall -> layer_11_expert1_27
	layer_11_expert1_27 -> layer_11_expert_act_27
	layer_11_expert_act_27 -> layer_11_expert2_27
	layer_11_expert2_27 -> layer_11_expert_allreduce_6
	layer_11_expert_allreduce_6 -> layer_11_expert_agg
	layer_11_attn_allreduce_7 -> layer_11_gate_28 [style=dashed]
	layer_11_gate_28 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_28
	layer_11_expert1_28 -> layer_11_expert_act_28
	layer_11_expert_act_28 -> layer_11_expert2_28
	layer_11_expert2_28 -> layer_11_expert_allreduce_7
	layer_11_alltoall -> layer_11_expert1_29
	layer_11_expert1_29 -> layer_11_expert_act_29
	layer_11_expert_act_29 -> layer_11_expert2_29
	layer_11_expert2_29 -> layer_11_expert_allreduce_7
	layer_11_alltoall -> layer_11_expert1_30
	layer_11_expert1_30 -> layer_11_expert_act_30
	layer_11_expert_act_30 -> layer_11_expert2_30
	layer_11_expert2_30 -> layer_11_expert_allreduce_7
	layer_11_alltoall -> layer_11_expert1_31
	layer_11_expert1_31 -> layer_11_expert_act_31
	layer_11_expert_act_31 -> layer_11_expert2_31
	layer_11_expert2_31 -> layer_11_expert_allreduce_7
	layer_11_expert_allreduce_7 -> layer_11_expert_agg
	layer_11_attn_allreduce_8 -> layer_11_gate_32 [style=dashed]
	layer_11_gate_32 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_32
	layer_11_expert1_32 -> layer_11_expert_act_32
	layer_11_expert_act_32 -> layer_11_expert2_32
	layer_11_expert2_32 -> layer_11_expert_allreduce_8
	layer_11_alltoall -> layer_11_expert1_33
	layer_11_expert1_33 -> layer_11_expert_act_33
	layer_11_expert_act_33 -> layer_11_expert2_33
	layer_11_expert2_33 -> layer_11_expert_allreduce_8
	layer_11_alltoall -> layer_11_expert1_34
	layer_11_expert1_34 -> layer_11_expert_act_34
	layer_11_expert_act_34 -> layer_11_expert2_34
	layer_11_expert2_34 -> layer_11_expert_allreduce_8
	layer_11_alltoall -> layer_11_expert1_35
	layer_11_expert1_35 -> layer_11_expert_act_35
	layer_11_expert_act_35 -> layer_11_expert2_35
	layer_11_expert2_35 -> layer_11_expert_allreduce_8
	layer_11_expert_allreduce_8 -> layer_11_expert_agg
	layer_11_attn_allreduce_9 -> layer_11_gate_36 [style=dashed]
	layer_11_gate_36 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_36
	layer_11_expert1_36 -> layer_11_expert_act_36
	layer_11_expert_act_36 -> layer_11_expert2_36
	layer_11_expert2_36 -> layer_11_expert_allreduce_9
	layer_11_alltoall -> layer_11_expert1_37
	layer_11_expert1_37 -> layer_11_expert_act_37
	layer_11_expert_act_37 -> layer_11_expert2_37
	layer_11_expert2_37 -> layer_11_expert_allreduce_9
	layer_11_alltoall -> layer_11_expert1_38
	layer_11_expert1_38 -> layer_11_expert_act_38
	layer_11_expert_act_38 -> layer_11_expert2_38
	layer_11_expert2_38 -> layer_11_expert_allreduce_9
	layer_11_alltoall -> layer_11_expert1_39
	layer_11_expert1_39 -> layer_11_expert_act_39
	layer_11_expert_act_39 -> layer_11_expert2_39
	layer_11_expert2_39 -> layer_11_expert_allreduce_9
	layer_11_expert_allreduce_9 -> layer_11_expert_agg
	layer_11_attn_allreduce_10 -> layer_11_gate_40 [style=dashed]
	layer_11_gate_40 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_40
	layer_11_expert1_40 -> layer_11_expert_act_40
	layer_11_expert_act_40 -> layer_11_expert2_40
	layer_11_expert2_40 -> layer_11_expert_allreduce_10
	layer_11_alltoall -> layer_11_expert1_41
	layer_11_expert1_41 -> layer_11_expert_act_41
	layer_11_expert_act_41 -> layer_11_expert2_41
	layer_11_expert2_41 -> layer_11_expert_allreduce_10
	layer_11_alltoall -> layer_11_expert1_42
	layer_11_expert1_42 -> layer_11_expert_act_42
	layer_11_expert_act_42 -> layer_11_expert2_42
	layer_11_expert2_42 -> layer_11_expert_allreduce_10
	layer_11_alltoall -> layer_11_expert1_43
	layer_11_expert1_43 -> layer_11_expert_act_43
	layer_11_expert_act_43 -> layer_11_expert2_43
	layer_11_expert2_43 -> layer_11_expert_allreduce_10
	layer_11_expert_allreduce_10 -> layer_11_expert_agg
	layer_11_attn_allreduce_11 -> layer_11_gate_44 [style=dashed]
	layer_11_gate_44 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_44
	layer_11_expert1_44 -> layer_11_expert_act_44
	layer_11_expert_act_44 -> layer_11_expert2_44
	layer_11_expert2_44 -> layer_11_expert_allreduce_11
	layer_11_alltoall -> layer_11_expert1_45
	layer_11_expert1_45 -> layer_11_expert_act_45
	layer_11_expert_act_45 -> layer_11_expert2_45
	layer_11_expert2_45 -> layer_11_expert_allreduce_11
	layer_11_alltoall -> layer_11_expert1_46
	layer_11_expert1_46 -> layer_11_expert_act_46
	layer_11_expert_act_46 -> layer_11_expert2_46
	layer_11_expert2_46 -> layer_11_expert_allreduce_11
	layer_11_alltoall -> layer_11_expert1_47
	layer_11_expert1_47 -> layer_11_expert_act_47
	layer_11_expert_act_47 -> layer_11_expert2_47
	layer_11_expert2_47 -> layer_11_expert_allreduce_11
	layer_11_expert_allreduce_11 -> layer_11_expert_agg
	layer_11_attn_allreduce_12 -> layer_11_gate_48 [style=dashed]
	layer_11_gate_48 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_48
	layer_11_expert1_48 -> layer_11_expert_act_48
	layer_11_expert_act_48 -> layer_11_expert2_48
	layer_11_expert2_48 -> layer_11_expert_allreduce_12
	layer_11_alltoall -> layer_11_expert1_49
	layer_11_expert1_49 -> layer_11_expert_act_49
	layer_11_expert_act_49 -> layer_11_expert2_49
	layer_11_expert2_49 -> layer_11_expert_allreduce_12
	layer_11_alltoall -> layer_11_expert1_50
	layer_11_expert1_50 -> layer_11_expert_act_50
	layer_11_expert_act_50 -> layer_11_expert2_50
	layer_11_expert2_50 -> layer_11_expert_allreduce_12
	layer_11_alltoall -> layer_11_expert1_51
	layer_11_expert1_51 -> layer_11_expert_act_51
	layer_11_expert_act_51 -> layer_11_expert2_51
	layer_11_expert2_51 -> layer_11_expert_allreduce_12
	layer_11_expert_allreduce_12 -> layer_11_expert_agg
	layer_11_attn_allreduce_13 -> layer_11_gate_52 [style=dashed]
	layer_11_gate_52 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_52
	layer_11_expert1_52 -> layer_11_expert_act_52
	layer_11_expert_act_52 -> layer_11_expert2_52
	layer_11_expert2_52 -> layer_11_expert_allreduce_13
	layer_11_alltoall -> layer_11_expert1_53
	layer_11_expert1_53 -> layer_11_expert_act_53
	layer_11_expert_act_53 -> layer_11_expert2_53
	layer_11_expert2_53 -> layer_11_expert_allreduce_13
	layer_11_alltoall -> layer_11_expert1_54
	layer_11_expert1_54 -> layer_11_expert_act_54
	layer_11_expert_act_54 -> layer_11_expert2_54
	layer_11_expert2_54 -> layer_11_expert_allreduce_13
	layer_11_alltoall -> layer_11_expert1_55
	layer_11_expert1_55 -> layer_11_expert_act_55
	layer_11_expert_act_55 -> layer_11_expert2_55
	layer_11_expert2_55 -> layer_11_expert_allreduce_13
	layer_11_expert_allreduce_13 -> layer_11_expert_agg
	layer_11_attn_allreduce_14 -> layer_11_gate_56 [style=dashed]
	layer_11_gate_56 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_56
	layer_11_expert1_56 -> layer_11_expert_act_56
	layer_11_expert_act_56 -> layer_11_expert2_56
	layer_11_expert2_56 -> layer_11_expert_allreduce_14
	layer_11_alltoall -> layer_11_expert1_57
	layer_11_expert1_57 -> layer_11_expert_act_57
	layer_11_expert_act_57 -> layer_11_expert2_57
	layer_11_expert2_57 -> layer_11_expert_allreduce_14
	layer_11_alltoall -> layer_11_expert1_58
	layer_11_expert1_58 -> layer_11_expert_act_58
	layer_11_expert_act_58 -> layer_11_expert2_58
	layer_11_expert2_58 -> layer_11_expert_allreduce_14
	layer_11_alltoall -> layer_11_expert1_59
	layer_11_expert1_59 -> layer_11_expert_act_59
	layer_11_expert_act_59 -> layer_11_expert2_59
	layer_11_expert2_59 -> layer_11_expert_allreduce_14
	layer_11_expert_allreduce_14 -> layer_11_expert_agg
	layer_11_attn_allreduce_15 -> layer_11_gate_60 [style=dashed]
	layer_11_gate_60 -> layer_11_alltoall [style=dashed]
	layer_11_alltoall -> layer_11_expert1_60
	layer_11_expert1_60 -> layer_11_expert_act_60
	layer_11_expert_act_60 -> layer_11_expert2_60
	layer_11_expert2_60 -> layer_11_expert_allreduce_15
	layer_11_alltoall -> layer_11_expert1_61
	layer_11_expert1_61 -> layer_11_expert_act_61
	layer_11_expert_act_61 -> layer_11_expert2_61
	layer_11_expert2_61 -> layer_11_expert_allreduce_15
	layer_11_alltoall -> layer_11_expert1_62
	layer_11_expert1_62 -> layer_11_expert_act_62
	layer_11_expert_act_62 -> layer_11_expert2_62
	layer_11_expert2_62 -> layer_11_expert_allreduce_15
	layer_11_alltoall -> layer_11_expert1_63
	layer_11_expert1_63 -> layer_11_expert_act_63
	layer_11_expert_act_63 -> layer_11_expert2_63
	layer_11_expert2_63 -> layer_11_expert_allreduce_15
	layer_11_expert_allreduce_15 -> layer_11_expert_agg
	layer_11_expert_agg -> layer_11_norm
	layer_11_norm -> layer_12_input
	layer_12_input [label="Layer12_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_12_qkv_0 [label="Layer12_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_1 [label="Layer12_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_2 [label="Layer12_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_3 [label="Layer12_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_0 [label="Layer12_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_1 [label="Layer12_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_2 [label="Layer12_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_3 [label="Layer12_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_0 [label="Layer12_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_1 [label="Layer12_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_2 [label="Layer12_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_3 [label="Layer12_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_0 [label="Layer12_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_1 [label="Layer12_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_2 [label="Layer12_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_3 [label="Layer12_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_0 [label="Layer12_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_0
	layer_12_qkv_0 -> layer_12_attn_scores_0
	layer_12_attn_scores_0 -> layer_12_attn_softmax_0
	layer_12_attn_softmax_0 -> layer_12_attn_out_0
	layer_12_attn_out_0 -> layer_12_attn_allreduce_0
	layer_12_input -> layer_12_qkv_1
	layer_12_qkv_1 -> layer_12_attn_scores_1
	layer_12_attn_scores_1 -> layer_12_attn_softmax_1
	layer_12_attn_softmax_1 -> layer_12_attn_out_1
	layer_12_attn_out_1 -> layer_12_attn_allreduce_0
	layer_12_input -> layer_12_qkv_2
	layer_12_qkv_2 -> layer_12_attn_scores_2
	layer_12_attn_scores_2 -> layer_12_attn_softmax_2
	layer_12_attn_softmax_2 -> layer_12_attn_out_2
	layer_12_attn_out_2 -> layer_12_attn_allreduce_0
	layer_12_input -> layer_12_qkv_3
	layer_12_qkv_3 -> layer_12_attn_scores_3
	layer_12_attn_scores_3 -> layer_12_attn_softmax_3
	layer_12_attn_softmax_3 -> layer_12_attn_out_3
	layer_12_attn_out_3 -> layer_12_attn_allreduce_0
	layer_12_qkv_4 [label="Layer12_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_5 [label="Layer12_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_6 [label="Layer12_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_7 [label="Layer12_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_4 [label="Layer12_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_5 [label="Layer12_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_6 [label="Layer12_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_7 [label="Layer12_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_4 [label="Layer12_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_5 [label="Layer12_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_6 [label="Layer12_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_7 [label="Layer12_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_4 [label="Layer12_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_5 [label="Layer12_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_6 [label="Layer12_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_7 [label="Layer12_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_1 [label="Layer12_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_4
	layer_12_qkv_4 -> layer_12_attn_scores_4
	layer_12_attn_scores_4 -> layer_12_attn_softmax_4
	layer_12_attn_softmax_4 -> layer_12_attn_out_4
	layer_12_attn_out_4 -> layer_12_attn_allreduce_1
	layer_12_input -> layer_12_qkv_5
	layer_12_qkv_5 -> layer_12_attn_scores_5
	layer_12_attn_scores_5 -> layer_12_attn_softmax_5
	layer_12_attn_softmax_5 -> layer_12_attn_out_5
	layer_12_attn_out_5 -> layer_12_attn_allreduce_1
	layer_12_input -> layer_12_qkv_6
	layer_12_qkv_6 -> layer_12_attn_scores_6
	layer_12_attn_scores_6 -> layer_12_attn_softmax_6
	layer_12_attn_softmax_6 -> layer_12_attn_out_6
	layer_12_attn_out_6 -> layer_12_attn_allreduce_1
	layer_12_input -> layer_12_qkv_7
	layer_12_qkv_7 -> layer_12_attn_scores_7
	layer_12_attn_scores_7 -> layer_12_attn_softmax_7
	layer_12_attn_softmax_7 -> layer_12_attn_out_7
	layer_12_attn_out_7 -> layer_12_attn_allreduce_1
	layer_12_qkv_8 [label="Layer12_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_9 [label="Layer12_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_10 [label="Layer12_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_11 [label="Layer12_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_8 [label="Layer12_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_9 [label="Layer12_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_10 [label="Layer12_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_11 [label="Layer12_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_8 [label="Layer12_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_9 [label="Layer12_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_10 [label="Layer12_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_11 [label="Layer12_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_8 [label="Layer12_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_9 [label="Layer12_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_10 [label="Layer12_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_11 [label="Layer12_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_2 [label="Layer12_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_8
	layer_12_qkv_8 -> layer_12_attn_scores_8
	layer_12_attn_scores_8 -> layer_12_attn_softmax_8
	layer_12_attn_softmax_8 -> layer_12_attn_out_8
	layer_12_attn_out_8 -> layer_12_attn_allreduce_2
	layer_12_input -> layer_12_qkv_9
	layer_12_qkv_9 -> layer_12_attn_scores_9
	layer_12_attn_scores_9 -> layer_12_attn_softmax_9
	layer_12_attn_softmax_9 -> layer_12_attn_out_9
	layer_12_attn_out_9 -> layer_12_attn_allreduce_2
	layer_12_input -> layer_12_qkv_10
	layer_12_qkv_10 -> layer_12_attn_scores_10
	layer_12_attn_scores_10 -> layer_12_attn_softmax_10
	layer_12_attn_softmax_10 -> layer_12_attn_out_10
	layer_12_attn_out_10 -> layer_12_attn_allreduce_2
	layer_12_input -> layer_12_qkv_11
	layer_12_qkv_11 -> layer_12_attn_scores_11
	layer_12_attn_scores_11 -> layer_12_attn_softmax_11
	layer_12_attn_softmax_11 -> layer_12_attn_out_11
	layer_12_attn_out_11 -> layer_12_attn_allreduce_2
	layer_12_qkv_12 [label="Layer12_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_13 [label="Layer12_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_14 [label="Layer12_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_15 [label="Layer12_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_12 [label="Layer12_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_13 [label="Layer12_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_14 [label="Layer12_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_15 [label="Layer12_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_12 [label="Layer12_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_13 [label="Layer12_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_14 [label="Layer12_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_15 [label="Layer12_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_12 [label="Layer12_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_13 [label="Layer12_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_14 [label="Layer12_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_15 [label="Layer12_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_3 [label="Layer12_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_12
	layer_12_qkv_12 -> layer_12_attn_scores_12
	layer_12_attn_scores_12 -> layer_12_attn_softmax_12
	layer_12_attn_softmax_12 -> layer_12_attn_out_12
	layer_12_attn_out_12 -> layer_12_attn_allreduce_3
	layer_12_input -> layer_12_qkv_13
	layer_12_qkv_13 -> layer_12_attn_scores_13
	layer_12_attn_scores_13 -> layer_12_attn_softmax_13
	layer_12_attn_softmax_13 -> layer_12_attn_out_13
	layer_12_attn_out_13 -> layer_12_attn_allreduce_3
	layer_12_input -> layer_12_qkv_14
	layer_12_qkv_14 -> layer_12_attn_scores_14
	layer_12_attn_scores_14 -> layer_12_attn_softmax_14
	layer_12_attn_softmax_14 -> layer_12_attn_out_14
	layer_12_attn_out_14 -> layer_12_attn_allreduce_3
	layer_12_input -> layer_12_qkv_15
	layer_12_qkv_15 -> layer_12_attn_scores_15
	layer_12_attn_scores_15 -> layer_12_attn_softmax_15
	layer_12_attn_softmax_15 -> layer_12_attn_out_15
	layer_12_attn_out_15 -> layer_12_attn_allreduce_3
	layer_12_qkv_16 [label="Layer12_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_17 [label="Layer12_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_18 [label="Layer12_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_19 [label="Layer12_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_16 [label="Layer12_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_17 [label="Layer12_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_18 [label="Layer12_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_19 [label="Layer12_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_16 [label="Layer12_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_17 [label="Layer12_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_18 [label="Layer12_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_19 [label="Layer12_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_16 [label="Layer12_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_17 [label="Layer12_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_18 [label="Layer12_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_19 [label="Layer12_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_4 [label="Layer12_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_16
	layer_12_qkv_16 -> layer_12_attn_scores_16
	layer_12_attn_scores_16 -> layer_12_attn_softmax_16
	layer_12_attn_softmax_16 -> layer_12_attn_out_16
	layer_12_attn_out_16 -> layer_12_attn_allreduce_4
	layer_12_input -> layer_12_qkv_17
	layer_12_qkv_17 -> layer_12_attn_scores_17
	layer_12_attn_scores_17 -> layer_12_attn_softmax_17
	layer_12_attn_softmax_17 -> layer_12_attn_out_17
	layer_12_attn_out_17 -> layer_12_attn_allreduce_4
	layer_12_input -> layer_12_qkv_18
	layer_12_qkv_18 -> layer_12_attn_scores_18
	layer_12_attn_scores_18 -> layer_12_attn_softmax_18
	layer_12_attn_softmax_18 -> layer_12_attn_out_18
	layer_12_attn_out_18 -> layer_12_attn_allreduce_4
	layer_12_input -> layer_12_qkv_19
	layer_12_qkv_19 -> layer_12_attn_scores_19
	layer_12_attn_scores_19 -> layer_12_attn_softmax_19
	layer_12_attn_softmax_19 -> layer_12_attn_out_19
	layer_12_attn_out_19 -> layer_12_attn_allreduce_4
	layer_12_qkv_20 [label="Layer12_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_21 [label="Layer12_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_22 [label="Layer12_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_23 [label="Layer12_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_20 [label="Layer12_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_21 [label="Layer12_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_22 [label="Layer12_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_23 [label="Layer12_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_20 [label="Layer12_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_21 [label="Layer12_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_22 [label="Layer12_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_23 [label="Layer12_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_20 [label="Layer12_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_21 [label="Layer12_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_22 [label="Layer12_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_23 [label="Layer12_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_5 [label="Layer12_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_20
	layer_12_qkv_20 -> layer_12_attn_scores_20
	layer_12_attn_scores_20 -> layer_12_attn_softmax_20
	layer_12_attn_softmax_20 -> layer_12_attn_out_20
	layer_12_attn_out_20 -> layer_12_attn_allreduce_5
	layer_12_input -> layer_12_qkv_21
	layer_12_qkv_21 -> layer_12_attn_scores_21
	layer_12_attn_scores_21 -> layer_12_attn_softmax_21
	layer_12_attn_softmax_21 -> layer_12_attn_out_21
	layer_12_attn_out_21 -> layer_12_attn_allreduce_5
	layer_12_input -> layer_12_qkv_22
	layer_12_qkv_22 -> layer_12_attn_scores_22
	layer_12_attn_scores_22 -> layer_12_attn_softmax_22
	layer_12_attn_softmax_22 -> layer_12_attn_out_22
	layer_12_attn_out_22 -> layer_12_attn_allreduce_5
	layer_12_input -> layer_12_qkv_23
	layer_12_qkv_23 -> layer_12_attn_scores_23
	layer_12_attn_scores_23 -> layer_12_attn_softmax_23
	layer_12_attn_softmax_23 -> layer_12_attn_out_23
	layer_12_attn_out_23 -> layer_12_attn_allreduce_5
	layer_12_qkv_24 [label="Layer12_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_25 [label="Layer12_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_26 [label="Layer12_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_27 [label="Layer12_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_24 [label="Layer12_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_25 [label="Layer12_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_26 [label="Layer12_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_27 [label="Layer12_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_24 [label="Layer12_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_25 [label="Layer12_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_26 [label="Layer12_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_27 [label="Layer12_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_24 [label="Layer12_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_25 [label="Layer12_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_26 [label="Layer12_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_27 [label="Layer12_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_6 [label="Layer12_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_24
	layer_12_qkv_24 -> layer_12_attn_scores_24
	layer_12_attn_scores_24 -> layer_12_attn_softmax_24
	layer_12_attn_softmax_24 -> layer_12_attn_out_24
	layer_12_attn_out_24 -> layer_12_attn_allreduce_6
	layer_12_input -> layer_12_qkv_25
	layer_12_qkv_25 -> layer_12_attn_scores_25
	layer_12_attn_scores_25 -> layer_12_attn_softmax_25
	layer_12_attn_softmax_25 -> layer_12_attn_out_25
	layer_12_attn_out_25 -> layer_12_attn_allreduce_6
	layer_12_input -> layer_12_qkv_26
	layer_12_qkv_26 -> layer_12_attn_scores_26
	layer_12_attn_scores_26 -> layer_12_attn_softmax_26
	layer_12_attn_softmax_26 -> layer_12_attn_out_26
	layer_12_attn_out_26 -> layer_12_attn_allreduce_6
	layer_12_input -> layer_12_qkv_27
	layer_12_qkv_27 -> layer_12_attn_scores_27
	layer_12_attn_scores_27 -> layer_12_attn_softmax_27
	layer_12_attn_softmax_27 -> layer_12_attn_out_27
	layer_12_attn_out_27 -> layer_12_attn_allreduce_6
	layer_12_qkv_28 [label="Layer12_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_29 [label="Layer12_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_30 [label="Layer12_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_31 [label="Layer12_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_28 [label="Layer12_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_29 [label="Layer12_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_30 [label="Layer12_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_31 [label="Layer12_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_28 [label="Layer12_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_29 [label="Layer12_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_30 [label="Layer12_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_31 [label="Layer12_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_28 [label="Layer12_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_29 [label="Layer12_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_30 [label="Layer12_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_31 [label="Layer12_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_7 [label="Layer12_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_28
	layer_12_qkv_28 -> layer_12_attn_scores_28
	layer_12_attn_scores_28 -> layer_12_attn_softmax_28
	layer_12_attn_softmax_28 -> layer_12_attn_out_28
	layer_12_attn_out_28 -> layer_12_attn_allreduce_7
	layer_12_input -> layer_12_qkv_29
	layer_12_qkv_29 -> layer_12_attn_scores_29
	layer_12_attn_scores_29 -> layer_12_attn_softmax_29
	layer_12_attn_softmax_29 -> layer_12_attn_out_29
	layer_12_attn_out_29 -> layer_12_attn_allreduce_7
	layer_12_input -> layer_12_qkv_30
	layer_12_qkv_30 -> layer_12_attn_scores_30
	layer_12_attn_scores_30 -> layer_12_attn_softmax_30
	layer_12_attn_softmax_30 -> layer_12_attn_out_30
	layer_12_attn_out_30 -> layer_12_attn_allreduce_7
	layer_12_input -> layer_12_qkv_31
	layer_12_qkv_31 -> layer_12_attn_scores_31
	layer_12_attn_scores_31 -> layer_12_attn_softmax_31
	layer_12_attn_softmax_31 -> layer_12_attn_out_31
	layer_12_attn_out_31 -> layer_12_attn_allreduce_7
	layer_12_qkv_32 [label="Layer12_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_33 [label="Layer12_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_34 [label="Layer12_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_35 [label="Layer12_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_32 [label="Layer12_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_33 [label="Layer12_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_34 [label="Layer12_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_35 [label="Layer12_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_32 [label="Layer12_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_33 [label="Layer12_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_34 [label="Layer12_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_35 [label="Layer12_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_32 [label="Layer12_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_33 [label="Layer12_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_34 [label="Layer12_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_35 [label="Layer12_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_8 [label="Layer12_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_32
	layer_12_qkv_32 -> layer_12_attn_scores_32
	layer_12_attn_scores_32 -> layer_12_attn_softmax_32
	layer_12_attn_softmax_32 -> layer_12_attn_out_32
	layer_12_attn_out_32 -> layer_12_attn_allreduce_8
	layer_12_input -> layer_12_qkv_33
	layer_12_qkv_33 -> layer_12_attn_scores_33
	layer_12_attn_scores_33 -> layer_12_attn_softmax_33
	layer_12_attn_softmax_33 -> layer_12_attn_out_33
	layer_12_attn_out_33 -> layer_12_attn_allreduce_8
	layer_12_input -> layer_12_qkv_34
	layer_12_qkv_34 -> layer_12_attn_scores_34
	layer_12_attn_scores_34 -> layer_12_attn_softmax_34
	layer_12_attn_softmax_34 -> layer_12_attn_out_34
	layer_12_attn_out_34 -> layer_12_attn_allreduce_8
	layer_12_input -> layer_12_qkv_35
	layer_12_qkv_35 -> layer_12_attn_scores_35
	layer_12_attn_scores_35 -> layer_12_attn_softmax_35
	layer_12_attn_softmax_35 -> layer_12_attn_out_35
	layer_12_attn_out_35 -> layer_12_attn_allreduce_8
	layer_12_qkv_36 [label="Layer12_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_37 [label="Layer12_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_38 [label="Layer12_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_39 [label="Layer12_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_36 [label="Layer12_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_37 [label="Layer12_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_38 [label="Layer12_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_39 [label="Layer12_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_36 [label="Layer12_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_37 [label="Layer12_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_38 [label="Layer12_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_39 [label="Layer12_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_36 [label="Layer12_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_37 [label="Layer12_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_38 [label="Layer12_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_39 [label="Layer12_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_9 [label="Layer12_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_36
	layer_12_qkv_36 -> layer_12_attn_scores_36
	layer_12_attn_scores_36 -> layer_12_attn_softmax_36
	layer_12_attn_softmax_36 -> layer_12_attn_out_36
	layer_12_attn_out_36 -> layer_12_attn_allreduce_9
	layer_12_input -> layer_12_qkv_37
	layer_12_qkv_37 -> layer_12_attn_scores_37
	layer_12_attn_scores_37 -> layer_12_attn_softmax_37
	layer_12_attn_softmax_37 -> layer_12_attn_out_37
	layer_12_attn_out_37 -> layer_12_attn_allreduce_9
	layer_12_input -> layer_12_qkv_38
	layer_12_qkv_38 -> layer_12_attn_scores_38
	layer_12_attn_scores_38 -> layer_12_attn_softmax_38
	layer_12_attn_softmax_38 -> layer_12_attn_out_38
	layer_12_attn_out_38 -> layer_12_attn_allreduce_9
	layer_12_input -> layer_12_qkv_39
	layer_12_qkv_39 -> layer_12_attn_scores_39
	layer_12_attn_scores_39 -> layer_12_attn_softmax_39
	layer_12_attn_softmax_39 -> layer_12_attn_out_39
	layer_12_attn_out_39 -> layer_12_attn_allreduce_9
	layer_12_qkv_40 [label="Layer12_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_41 [label="Layer12_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_42 [label="Layer12_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_43 [label="Layer12_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_40 [label="Layer12_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_41 [label="Layer12_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_42 [label="Layer12_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_43 [label="Layer12_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_40 [label="Layer12_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_41 [label="Layer12_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_42 [label="Layer12_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_43 [label="Layer12_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_40 [label="Layer12_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_41 [label="Layer12_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_42 [label="Layer12_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_43 [label="Layer12_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_10 [label="Layer12_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_40
	layer_12_qkv_40 -> layer_12_attn_scores_40
	layer_12_attn_scores_40 -> layer_12_attn_softmax_40
	layer_12_attn_softmax_40 -> layer_12_attn_out_40
	layer_12_attn_out_40 -> layer_12_attn_allreduce_10
	layer_12_input -> layer_12_qkv_41
	layer_12_qkv_41 -> layer_12_attn_scores_41
	layer_12_attn_scores_41 -> layer_12_attn_softmax_41
	layer_12_attn_softmax_41 -> layer_12_attn_out_41
	layer_12_attn_out_41 -> layer_12_attn_allreduce_10
	layer_12_input -> layer_12_qkv_42
	layer_12_qkv_42 -> layer_12_attn_scores_42
	layer_12_attn_scores_42 -> layer_12_attn_softmax_42
	layer_12_attn_softmax_42 -> layer_12_attn_out_42
	layer_12_attn_out_42 -> layer_12_attn_allreduce_10
	layer_12_input -> layer_12_qkv_43
	layer_12_qkv_43 -> layer_12_attn_scores_43
	layer_12_attn_scores_43 -> layer_12_attn_softmax_43
	layer_12_attn_softmax_43 -> layer_12_attn_out_43
	layer_12_attn_out_43 -> layer_12_attn_allreduce_10
	layer_12_qkv_44 [label="Layer12_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_45 [label="Layer12_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_46 [label="Layer12_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_47 [label="Layer12_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_44 [label="Layer12_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_45 [label="Layer12_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_46 [label="Layer12_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_47 [label="Layer12_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_44 [label="Layer12_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_45 [label="Layer12_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_46 [label="Layer12_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_47 [label="Layer12_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_44 [label="Layer12_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_45 [label="Layer12_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_46 [label="Layer12_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_47 [label="Layer12_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_11 [label="Layer12_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_44
	layer_12_qkv_44 -> layer_12_attn_scores_44
	layer_12_attn_scores_44 -> layer_12_attn_softmax_44
	layer_12_attn_softmax_44 -> layer_12_attn_out_44
	layer_12_attn_out_44 -> layer_12_attn_allreduce_11
	layer_12_input -> layer_12_qkv_45
	layer_12_qkv_45 -> layer_12_attn_scores_45
	layer_12_attn_scores_45 -> layer_12_attn_softmax_45
	layer_12_attn_softmax_45 -> layer_12_attn_out_45
	layer_12_attn_out_45 -> layer_12_attn_allreduce_11
	layer_12_input -> layer_12_qkv_46
	layer_12_qkv_46 -> layer_12_attn_scores_46
	layer_12_attn_scores_46 -> layer_12_attn_softmax_46
	layer_12_attn_softmax_46 -> layer_12_attn_out_46
	layer_12_attn_out_46 -> layer_12_attn_allreduce_11
	layer_12_input -> layer_12_qkv_47
	layer_12_qkv_47 -> layer_12_attn_scores_47
	layer_12_attn_scores_47 -> layer_12_attn_softmax_47
	layer_12_attn_softmax_47 -> layer_12_attn_out_47
	layer_12_attn_out_47 -> layer_12_attn_allreduce_11
	layer_12_qkv_48 [label="Layer12_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_49 [label="Layer12_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_50 [label="Layer12_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_51 [label="Layer12_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_48 [label="Layer12_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_49 [label="Layer12_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_50 [label="Layer12_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_51 [label="Layer12_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_48 [label="Layer12_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_49 [label="Layer12_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_50 [label="Layer12_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_51 [label="Layer12_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_48 [label="Layer12_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_49 [label="Layer12_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_50 [label="Layer12_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_51 [label="Layer12_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_12 [label="Layer12_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_48
	layer_12_qkv_48 -> layer_12_attn_scores_48
	layer_12_attn_scores_48 -> layer_12_attn_softmax_48
	layer_12_attn_softmax_48 -> layer_12_attn_out_48
	layer_12_attn_out_48 -> layer_12_attn_allreduce_12
	layer_12_input -> layer_12_qkv_49
	layer_12_qkv_49 -> layer_12_attn_scores_49
	layer_12_attn_scores_49 -> layer_12_attn_softmax_49
	layer_12_attn_softmax_49 -> layer_12_attn_out_49
	layer_12_attn_out_49 -> layer_12_attn_allreduce_12
	layer_12_input -> layer_12_qkv_50
	layer_12_qkv_50 -> layer_12_attn_scores_50
	layer_12_attn_scores_50 -> layer_12_attn_softmax_50
	layer_12_attn_softmax_50 -> layer_12_attn_out_50
	layer_12_attn_out_50 -> layer_12_attn_allreduce_12
	layer_12_input -> layer_12_qkv_51
	layer_12_qkv_51 -> layer_12_attn_scores_51
	layer_12_attn_scores_51 -> layer_12_attn_softmax_51
	layer_12_attn_softmax_51 -> layer_12_attn_out_51
	layer_12_attn_out_51 -> layer_12_attn_allreduce_12
	layer_12_qkv_52 [label="Layer12_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_53 [label="Layer12_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_54 [label="Layer12_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_55 [label="Layer12_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_52 [label="Layer12_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_53 [label="Layer12_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_54 [label="Layer12_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_55 [label="Layer12_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_52 [label="Layer12_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_53 [label="Layer12_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_54 [label="Layer12_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_55 [label="Layer12_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_52 [label="Layer12_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_53 [label="Layer12_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_54 [label="Layer12_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_55 [label="Layer12_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_13 [label="Layer12_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_52
	layer_12_qkv_52 -> layer_12_attn_scores_52
	layer_12_attn_scores_52 -> layer_12_attn_softmax_52
	layer_12_attn_softmax_52 -> layer_12_attn_out_52
	layer_12_attn_out_52 -> layer_12_attn_allreduce_13
	layer_12_input -> layer_12_qkv_53
	layer_12_qkv_53 -> layer_12_attn_scores_53
	layer_12_attn_scores_53 -> layer_12_attn_softmax_53
	layer_12_attn_softmax_53 -> layer_12_attn_out_53
	layer_12_attn_out_53 -> layer_12_attn_allreduce_13
	layer_12_input -> layer_12_qkv_54
	layer_12_qkv_54 -> layer_12_attn_scores_54
	layer_12_attn_scores_54 -> layer_12_attn_softmax_54
	layer_12_attn_softmax_54 -> layer_12_attn_out_54
	layer_12_attn_out_54 -> layer_12_attn_allreduce_13
	layer_12_input -> layer_12_qkv_55
	layer_12_qkv_55 -> layer_12_attn_scores_55
	layer_12_attn_scores_55 -> layer_12_attn_softmax_55
	layer_12_attn_softmax_55 -> layer_12_attn_out_55
	layer_12_attn_out_55 -> layer_12_attn_allreduce_13
	layer_12_qkv_56 [label="Layer12_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_57 [label="Layer12_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_58 [label="Layer12_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_59 [label="Layer12_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_56 [label="Layer12_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_57 [label="Layer12_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_58 [label="Layer12_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_59 [label="Layer12_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_56 [label="Layer12_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_57 [label="Layer12_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_58 [label="Layer12_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_59 [label="Layer12_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_56 [label="Layer12_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_57 [label="Layer12_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_58 [label="Layer12_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_59 [label="Layer12_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_14 [label="Layer12_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_56
	layer_12_qkv_56 -> layer_12_attn_scores_56
	layer_12_attn_scores_56 -> layer_12_attn_softmax_56
	layer_12_attn_softmax_56 -> layer_12_attn_out_56
	layer_12_attn_out_56 -> layer_12_attn_allreduce_14
	layer_12_input -> layer_12_qkv_57
	layer_12_qkv_57 -> layer_12_attn_scores_57
	layer_12_attn_scores_57 -> layer_12_attn_softmax_57
	layer_12_attn_softmax_57 -> layer_12_attn_out_57
	layer_12_attn_out_57 -> layer_12_attn_allreduce_14
	layer_12_input -> layer_12_qkv_58
	layer_12_qkv_58 -> layer_12_attn_scores_58
	layer_12_attn_scores_58 -> layer_12_attn_softmax_58
	layer_12_attn_softmax_58 -> layer_12_attn_out_58
	layer_12_attn_out_58 -> layer_12_attn_allreduce_14
	layer_12_input -> layer_12_qkv_59
	layer_12_qkv_59 -> layer_12_attn_scores_59
	layer_12_attn_scores_59 -> layer_12_attn_softmax_59
	layer_12_attn_softmax_59 -> layer_12_attn_out_59
	layer_12_attn_out_59 -> layer_12_attn_allreduce_14
	layer_12_qkv_60 [label="Layer12_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_61 [label="Layer12_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_62 [label="Layer12_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_qkv_63 [label="Layer12_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_60 [label="Layer12_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_61 [label="Layer12_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_62 [label="Layer12_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_scores_63 [label="Layer12_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_60 [label="Layer12_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_61 [label="Layer12_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_62 [label="Layer12_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_softmax_63 [label="Layer12_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_60 [label="Layer12_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_61 [label="Layer12_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_62 [label="Layer12_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_out_63 [label="Layer12_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_15 [label="Layer12_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_input -> layer_12_qkv_60
	layer_12_qkv_60 -> layer_12_attn_scores_60
	layer_12_attn_scores_60 -> layer_12_attn_softmax_60
	layer_12_attn_softmax_60 -> layer_12_attn_out_60
	layer_12_attn_out_60 -> layer_12_attn_allreduce_15
	layer_12_input -> layer_12_qkv_61
	layer_12_qkv_61 -> layer_12_attn_scores_61
	layer_12_attn_scores_61 -> layer_12_attn_softmax_61
	layer_12_attn_softmax_61 -> layer_12_attn_out_61
	layer_12_attn_out_61 -> layer_12_attn_allreduce_15
	layer_12_input -> layer_12_qkv_62
	layer_12_qkv_62 -> layer_12_attn_scores_62
	layer_12_attn_scores_62 -> layer_12_attn_softmax_62
	layer_12_attn_softmax_62 -> layer_12_attn_out_62
	layer_12_attn_out_62 -> layer_12_attn_allreduce_15
	layer_12_input -> layer_12_qkv_63
	layer_12_qkv_63 -> layer_12_attn_scores_63
	layer_12_attn_scores_63 -> layer_12_attn_softmax_63
	layer_12_attn_softmax_63 -> layer_12_attn_out_63
	layer_12_attn_out_63 -> layer_12_attn_allreduce_15
	layer_12_gate_0 [label="Layer12_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_4 [label="Layer12_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_8 [label="Layer12_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_12 [label="Layer12_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_16 [label="Layer12_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_20 [label="Layer12_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_24 [label="Layer12_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_28 [label="Layer12_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_32 [label="Layer12_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_36 [label="Layer12_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_40 [label="Layer12_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_44 [label="Layer12_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_48 [label="Layer12_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_52 [label="Layer12_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_56 [label="Layer12_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_gate_60 [label="Layer12_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_12_alltoall [label="Layer12_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_0 [label="Layer12_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_1 [label="Layer12_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_2 [label="Layer12_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_3 [label="Layer12_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_0 [label="Layer12_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_1 [label="Layer12_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_2 [label="Layer12_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_3 [label="Layer12_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_0 [label="Layer12_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_1 [label="Layer12_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_2 [label="Layer12_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_3 [label="Layer12_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_0 [label="Layer12_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_4 [label="Layer12_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_5 [label="Layer12_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_6 [label="Layer12_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_7 [label="Layer12_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_4 [label="Layer12_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_5 [label="Layer12_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_6 [label="Layer12_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_7 [label="Layer12_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_4 [label="Layer12_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_5 [label="Layer12_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_6 [label="Layer12_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_7 [label="Layer12_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_1 [label="Layer12_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_8 [label="Layer12_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_9 [label="Layer12_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_10 [label="Layer12_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_11 [label="Layer12_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_8 [label="Layer12_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_9 [label="Layer12_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_10 [label="Layer12_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_11 [label="Layer12_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_8 [label="Layer12_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_9 [label="Layer12_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_10 [label="Layer12_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_11 [label="Layer12_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_2 [label="Layer12_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_12 [label="Layer12_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_13 [label="Layer12_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_14 [label="Layer12_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_15 [label="Layer12_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_12 [label="Layer12_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_13 [label="Layer12_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_14 [label="Layer12_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_15 [label="Layer12_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_12 [label="Layer12_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_13 [label="Layer12_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_14 [label="Layer12_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_15 [label="Layer12_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_3 [label="Layer12_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_16 [label="Layer12_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_17 [label="Layer12_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_18 [label="Layer12_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_19 [label="Layer12_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_16 [label="Layer12_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_17 [label="Layer12_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_18 [label="Layer12_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_19 [label="Layer12_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_16 [label="Layer12_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_17 [label="Layer12_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_18 [label="Layer12_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_19 [label="Layer12_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_4 [label="Layer12_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_20 [label="Layer12_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_21 [label="Layer12_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_22 [label="Layer12_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_23 [label="Layer12_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_20 [label="Layer12_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_21 [label="Layer12_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_22 [label="Layer12_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_23 [label="Layer12_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_20 [label="Layer12_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_21 [label="Layer12_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_22 [label="Layer12_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_23 [label="Layer12_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_5 [label="Layer12_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_24 [label="Layer12_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_25 [label="Layer12_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_26 [label="Layer12_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_27 [label="Layer12_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_24 [label="Layer12_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_25 [label="Layer12_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_26 [label="Layer12_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_27 [label="Layer12_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_24 [label="Layer12_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_25 [label="Layer12_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_26 [label="Layer12_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_27 [label="Layer12_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_6 [label="Layer12_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_28 [label="Layer12_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_29 [label="Layer12_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_30 [label="Layer12_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_31 [label="Layer12_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_28 [label="Layer12_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_29 [label="Layer12_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_30 [label="Layer12_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_31 [label="Layer12_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_28 [label="Layer12_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_29 [label="Layer12_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_30 [label="Layer12_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_31 [label="Layer12_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_7 [label="Layer12_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_32 [label="Layer12_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_33 [label="Layer12_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_34 [label="Layer12_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_35 [label="Layer12_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_32 [label="Layer12_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_33 [label="Layer12_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_34 [label="Layer12_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_35 [label="Layer12_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_32 [label="Layer12_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_33 [label="Layer12_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_34 [label="Layer12_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_35 [label="Layer12_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_8 [label="Layer12_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_36 [label="Layer12_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_37 [label="Layer12_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_38 [label="Layer12_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_39 [label="Layer12_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_36 [label="Layer12_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_37 [label="Layer12_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_38 [label="Layer12_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_39 [label="Layer12_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_36 [label="Layer12_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_37 [label="Layer12_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_38 [label="Layer12_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_39 [label="Layer12_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_9 [label="Layer12_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_40 [label="Layer12_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_41 [label="Layer12_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_42 [label="Layer12_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_43 [label="Layer12_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_40 [label="Layer12_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_41 [label="Layer12_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_42 [label="Layer12_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_43 [label="Layer12_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_40 [label="Layer12_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_41 [label="Layer12_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_42 [label="Layer12_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_43 [label="Layer12_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_10 [label="Layer12_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_44 [label="Layer12_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_45 [label="Layer12_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_46 [label="Layer12_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_47 [label="Layer12_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_44 [label="Layer12_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_45 [label="Layer12_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_46 [label="Layer12_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_47 [label="Layer12_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_44 [label="Layer12_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_45 [label="Layer12_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_46 [label="Layer12_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_47 [label="Layer12_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_11 [label="Layer12_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_48 [label="Layer12_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_49 [label="Layer12_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_50 [label="Layer12_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_51 [label="Layer12_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_48 [label="Layer12_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_49 [label="Layer12_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_50 [label="Layer12_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_51 [label="Layer12_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_48 [label="Layer12_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_49 [label="Layer12_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_50 [label="Layer12_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_51 [label="Layer12_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_12 [label="Layer12_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_52 [label="Layer12_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_53 [label="Layer12_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_54 [label="Layer12_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_55 [label="Layer12_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_52 [label="Layer12_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_53 [label="Layer12_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_54 [label="Layer12_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_55 [label="Layer12_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_52 [label="Layer12_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_53 [label="Layer12_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_54 [label="Layer12_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_55 [label="Layer12_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_13 [label="Layer12_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_56 [label="Layer12_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_57 [label="Layer12_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_58 [label="Layer12_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_59 [label="Layer12_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_56 [label="Layer12_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_57 [label="Layer12_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_58 [label="Layer12_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_59 [label="Layer12_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_56 [label="Layer12_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_57 [label="Layer12_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_58 [label="Layer12_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_59 [label="Layer12_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_14 [label="Layer12_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert1_60 [label="Layer12_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_61 [label="Layer12_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_62 [label="Layer12_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert1_63 [label="Layer12_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_60 [label="Layer12_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_61 [label="Layer12_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_62 [label="Layer12_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert_act_63 [label="Layer12_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_12_expert2_60 [label="Layer12_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_61 [label="Layer12_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_62 [label="Layer12_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert2_63 [label="Layer12_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_12_expert_allreduce_15 [label="Layer12_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_12_expert_agg [label="Layer12_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_12_norm [label="Layer12_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_12_attn_allreduce_0 -> layer_12_gate_0 [style=dashed]
	layer_12_gate_0 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_0
	layer_12_expert1_0 -> layer_12_expert_act_0
	layer_12_expert_act_0 -> layer_12_expert2_0
	layer_12_expert2_0 -> layer_12_expert_allreduce_0
	layer_12_alltoall -> layer_12_expert1_1
	layer_12_expert1_1 -> layer_12_expert_act_1
	layer_12_expert_act_1 -> layer_12_expert2_1
	layer_12_expert2_1 -> layer_12_expert_allreduce_0
	layer_12_alltoall -> layer_12_expert1_2
	layer_12_expert1_2 -> layer_12_expert_act_2
	layer_12_expert_act_2 -> layer_12_expert2_2
	layer_12_expert2_2 -> layer_12_expert_allreduce_0
	layer_12_alltoall -> layer_12_expert1_3
	layer_12_expert1_3 -> layer_12_expert_act_3
	layer_12_expert_act_3 -> layer_12_expert2_3
	layer_12_expert2_3 -> layer_12_expert_allreduce_0
	layer_12_expert_allreduce_0 -> layer_12_expert_agg
	layer_12_attn_allreduce_1 -> layer_12_gate_4 [style=dashed]
	layer_12_gate_4 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_4
	layer_12_expert1_4 -> layer_12_expert_act_4
	layer_12_expert_act_4 -> layer_12_expert2_4
	layer_12_expert2_4 -> layer_12_expert_allreduce_1
	layer_12_alltoall -> layer_12_expert1_5
	layer_12_expert1_5 -> layer_12_expert_act_5
	layer_12_expert_act_5 -> layer_12_expert2_5
	layer_12_expert2_5 -> layer_12_expert_allreduce_1
	layer_12_alltoall -> layer_12_expert1_6
	layer_12_expert1_6 -> layer_12_expert_act_6
	layer_12_expert_act_6 -> layer_12_expert2_6
	layer_12_expert2_6 -> layer_12_expert_allreduce_1
	layer_12_alltoall -> layer_12_expert1_7
	layer_12_expert1_7 -> layer_12_expert_act_7
	layer_12_expert_act_7 -> layer_12_expert2_7
	layer_12_expert2_7 -> layer_12_expert_allreduce_1
	layer_12_expert_allreduce_1 -> layer_12_expert_agg
	layer_12_attn_allreduce_2 -> layer_12_gate_8 [style=dashed]
	layer_12_gate_8 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_8
	layer_12_expert1_8 -> layer_12_expert_act_8
	layer_12_expert_act_8 -> layer_12_expert2_8
	layer_12_expert2_8 -> layer_12_expert_allreduce_2
	layer_12_alltoall -> layer_12_expert1_9
	layer_12_expert1_9 -> layer_12_expert_act_9
	layer_12_expert_act_9 -> layer_12_expert2_9
	layer_12_expert2_9 -> layer_12_expert_allreduce_2
	layer_12_alltoall -> layer_12_expert1_10
	layer_12_expert1_10 -> layer_12_expert_act_10
	layer_12_expert_act_10 -> layer_12_expert2_10
	layer_12_expert2_10 -> layer_12_expert_allreduce_2
	layer_12_alltoall -> layer_12_expert1_11
	layer_12_expert1_11 -> layer_12_expert_act_11
	layer_12_expert_act_11 -> layer_12_expert2_11
	layer_12_expert2_11 -> layer_12_expert_allreduce_2
	layer_12_expert_allreduce_2 -> layer_12_expert_agg
	layer_12_attn_allreduce_3 -> layer_12_gate_12 [style=dashed]
	layer_12_gate_12 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_12
	layer_12_expert1_12 -> layer_12_expert_act_12
	layer_12_expert_act_12 -> layer_12_expert2_12
	layer_12_expert2_12 -> layer_12_expert_allreduce_3
	layer_12_alltoall -> layer_12_expert1_13
	layer_12_expert1_13 -> layer_12_expert_act_13
	layer_12_expert_act_13 -> layer_12_expert2_13
	layer_12_expert2_13 -> layer_12_expert_allreduce_3
	layer_12_alltoall -> layer_12_expert1_14
	layer_12_expert1_14 -> layer_12_expert_act_14
	layer_12_expert_act_14 -> layer_12_expert2_14
	layer_12_expert2_14 -> layer_12_expert_allreduce_3
	layer_12_alltoall -> layer_12_expert1_15
	layer_12_expert1_15 -> layer_12_expert_act_15
	layer_12_expert_act_15 -> layer_12_expert2_15
	layer_12_expert2_15 -> layer_12_expert_allreduce_3
	layer_12_expert_allreduce_3 -> layer_12_expert_agg
	layer_12_attn_allreduce_4 -> layer_12_gate_16 [style=dashed]
	layer_12_gate_16 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_16
	layer_12_expert1_16 -> layer_12_expert_act_16
	layer_12_expert_act_16 -> layer_12_expert2_16
	layer_12_expert2_16 -> layer_12_expert_allreduce_4
	layer_12_alltoall -> layer_12_expert1_17
	layer_12_expert1_17 -> layer_12_expert_act_17
	layer_12_expert_act_17 -> layer_12_expert2_17
	layer_12_expert2_17 -> layer_12_expert_allreduce_4
	layer_12_alltoall -> layer_12_expert1_18
	layer_12_expert1_18 -> layer_12_expert_act_18
	layer_12_expert_act_18 -> layer_12_expert2_18
	layer_12_expert2_18 -> layer_12_expert_allreduce_4
	layer_12_alltoall -> layer_12_expert1_19
	layer_12_expert1_19 -> layer_12_expert_act_19
	layer_12_expert_act_19 -> layer_12_expert2_19
	layer_12_expert2_19 -> layer_12_expert_allreduce_4
	layer_12_expert_allreduce_4 -> layer_12_expert_agg
	layer_12_attn_allreduce_5 -> layer_12_gate_20 [style=dashed]
	layer_12_gate_20 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_20
	layer_12_expert1_20 -> layer_12_expert_act_20
	layer_12_expert_act_20 -> layer_12_expert2_20
	layer_12_expert2_20 -> layer_12_expert_allreduce_5
	layer_12_alltoall -> layer_12_expert1_21
	layer_12_expert1_21 -> layer_12_expert_act_21
	layer_12_expert_act_21 -> layer_12_expert2_21
	layer_12_expert2_21 -> layer_12_expert_allreduce_5
	layer_12_alltoall -> layer_12_expert1_22
	layer_12_expert1_22 -> layer_12_expert_act_22
	layer_12_expert_act_22 -> layer_12_expert2_22
	layer_12_expert2_22 -> layer_12_expert_allreduce_5
	layer_12_alltoall -> layer_12_expert1_23
	layer_12_expert1_23 -> layer_12_expert_act_23
	layer_12_expert_act_23 -> layer_12_expert2_23
	layer_12_expert2_23 -> layer_12_expert_allreduce_5
	layer_12_expert_allreduce_5 -> layer_12_expert_agg
	layer_12_attn_allreduce_6 -> layer_12_gate_24 [style=dashed]
	layer_12_gate_24 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_24
	layer_12_expert1_24 -> layer_12_expert_act_24
	layer_12_expert_act_24 -> layer_12_expert2_24
	layer_12_expert2_24 -> layer_12_expert_allreduce_6
	layer_12_alltoall -> layer_12_expert1_25
	layer_12_expert1_25 -> layer_12_expert_act_25
	layer_12_expert_act_25 -> layer_12_expert2_25
	layer_12_expert2_25 -> layer_12_expert_allreduce_6
	layer_12_alltoall -> layer_12_expert1_26
	layer_12_expert1_26 -> layer_12_expert_act_26
	layer_12_expert_act_26 -> layer_12_expert2_26
	layer_12_expert2_26 -> layer_12_expert_allreduce_6
	layer_12_alltoall -> layer_12_expert1_27
	layer_12_expert1_27 -> layer_12_expert_act_27
	layer_12_expert_act_27 -> layer_12_expert2_27
	layer_12_expert2_27 -> layer_12_expert_allreduce_6
	layer_12_expert_allreduce_6 -> layer_12_expert_agg
	layer_12_attn_allreduce_7 -> layer_12_gate_28 [style=dashed]
	layer_12_gate_28 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_28
	layer_12_expert1_28 -> layer_12_expert_act_28
	layer_12_expert_act_28 -> layer_12_expert2_28
	layer_12_expert2_28 -> layer_12_expert_allreduce_7
	layer_12_alltoall -> layer_12_expert1_29
	layer_12_expert1_29 -> layer_12_expert_act_29
	layer_12_expert_act_29 -> layer_12_expert2_29
	layer_12_expert2_29 -> layer_12_expert_allreduce_7
	layer_12_alltoall -> layer_12_expert1_30
	layer_12_expert1_30 -> layer_12_expert_act_30
	layer_12_expert_act_30 -> layer_12_expert2_30
	layer_12_expert2_30 -> layer_12_expert_allreduce_7
	layer_12_alltoall -> layer_12_expert1_31
	layer_12_expert1_31 -> layer_12_expert_act_31
	layer_12_expert_act_31 -> layer_12_expert2_31
	layer_12_expert2_31 -> layer_12_expert_allreduce_7
	layer_12_expert_allreduce_7 -> layer_12_expert_agg
	layer_12_attn_allreduce_8 -> layer_12_gate_32 [style=dashed]
	layer_12_gate_32 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_32
	layer_12_expert1_32 -> layer_12_expert_act_32
	layer_12_expert_act_32 -> layer_12_expert2_32
	layer_12_expert2_32 -> layer_12_expert_allreduce_8
	layer_12_alltoall -> layer_12_expert1_33
	layer_12_expert1_33 -> layer_12_expert_act_33
	layer_12_expert_act_33 -> layer_12_expert2_33
	layer_12_expert2_33 -> layer_12_expert_allreduce_8
	layer_12_alltoall -> layer_12_expert1_34
	layer_12_expert1_34 -> layer_12_expert_act_34
	layer_12_expert_act_34 -> layer_12_expert2_34
	layer_12_expert2_34 -> layer_12_expert_allreduce_8
	layer_12_alltoall -> layer_12_expert1_35
	layer_12_expert1_35 -> layer_12_expert_act_35
	layer_12_expert_act_35 -> layer_12_expert2_35
	layer_12_expert2_35 -> layer_12_expert_allreduce_8
	layer_12_expert_allreduce_8 -> layer_12_expert_agg
	layer_12_attn_allreduce_9 -> layer_12_gate_36 [style=dashed]
	layer_12_gate_36 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_36
	layer_12_expert1_36 -> layer_12_expert_act_36
	layer_12_expert_act_36 -> layer_12_expert2_36
	layer_12_expert2_36 -> layer_12_expert_allreduce_9
	layer_12_alltoall -> layer_12_expert1_37
	layer_12_expert1_37 -> layer_12_expert_act_37
	layer_12_expert_act_37 -> layer_12_expert2_37
	layer_12_expert2_37 -> layer_12_expert_allreduce_9
	layer_12_alltoall -> layer_12_expert1_38
	layer_12_expert1_38 -> layer_12_expert_act_38
	layer_12_expert_act_38 -> layer_12_expert2_38
	layer_12_expert2_38 -> layer_12_expert_allreduce_9
	layer_12_alltoall -> layer_12_expert1_39
	layer_12_expert1_39 -> layer_12_expert_act_39
	layer_12_expert_act_39 -> layer_12_expert2_39
	layer_12_expert2_39 -> layer_12_expert_allreduce_9
	layer_12_expert_allreduce_9 -> layer_12_expert_agg
	layer_12_attn_allreduce_10 -> layer_12_gate_40 [style=dashed]
	layer_12_gate_40 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_40
	layer_12_expert1_40 -> layer_12_expert_act_40
	layer_12_expert_act_40 -> layer_12_expert2_40
	layer_12_expert2_40 -> layer_12_expert_allreduce_10
	layer_12_alltoall -> layer_12_expert1_41
	layer_12_expert1_41 -> layer_12_expert_act_41
	layer_12_expert_act_41 -> layer_12_expert2_41
	layer_12_expert2_41 -> layer_12_expert_allreduce_10
	layer_12_alltoall -> layer_12_expert1_42
	layer_12_expert1_42 -> layer_12_expert_act_42
	layer_12_expert_act_42 -> layer_12_expert2_42
	layer_12_expert2_42 -> layer_12_expert_allreduce_10
	layer_12_alltoall -> layer_12_expert1_43
	layer_12_expert1_43 -> layer_12_expert_act_43
	layer_12_expert_act_43 -> layer_12_expert2_43
	layer_12_expert2_43 -> layer_12_expert_allreduce_10
	layer_12_expert_allreduce_10 -> layer_12_expert_agg
	layer_12_attn_allreduce_11 -> layer_12_gate_44 [style=dashed]
	layer_12_gate_44 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_44
	layer_12_expert1_44 -> layer_12_expert_act_44
	layer_12_expert_act_44 -> layer_12_expert2_44
	layer_12_expert2_44 -> layer_12_expert_allreduce_11
	layer_12_alltoall -> layer_12_expert1_45
	layer_12_expert1_45 -> layer_12_expert_act_45
	layer_12_expert_act_45 -> layer_12_expert2_45
	layer_12_expert2_45 -> layer_12_expert_allreduce_11
	layer_12_alltoall -> layer_12_expert1_46
	layer_12_expert1_46 -> layer_12_expert_act_46
	layer_12_expert_act_46 -> layer_12_expert2_46
	layer_12_expert2_46 -> layer_12_expert_allreduce_11
	layer_12_alltoall -> layer_12_expert1_47
	layer_12_expert1_47 -> layer_12_expert_act_47
	layer_12_expert_act_47 -> layer_12_expert2_47
	layer_12_expert2_47 -> layer_12_expert_allreduce_11
	layer_12_expert_allreduce_11 -> layer_12_expert_agg
	layer_12_attn_allreduce_12 -> layer_12_gate_48 [style=dashed]
	layer_12_gate_48 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_48
	layer_12_expert1_48 -> layer_12_expert_act_48
	layer_12_expert_act_48 -> layer_12_expert2_48
	layer_12_expert2_48 -> layer_12_expert_allreduce_12
	layer_12_alltoall -> layer_12_expert1_49
	layer_12_expert1_49 -> layer_12_expert_act_49
	layer_12_expert_act_49 -> layer_12_expert2_49
	layer_12_expert2_49 -> layer_12_expert_allreduce_12
	layer_12_alltoall -> layer_12_expert1_50
	layer_12_expert1_50 -> layer_12_expert_act_50
	layer_12_expert_act_50 -> layer_12_expert2_50
	layer_12_expert2_50 -> layer_12_expert_allreduce_12
	layer_12_alltoall -> layer_12_expert1_51
	layer_12_expert1_51 -> layer_12_expert_act_51
	layer_12_expert_act_51 -> layer_12_expert2_51
	layer_12_expert2_51 -> layer_12_expert_allreduce_12
	layer_12_expert_allreduce_12 -> layer_12_expert_agg
	layer_12_attn_allreduce_13 -> layer_12_gate_52 [style=dashed]
	layer_12_gate_52 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_52
	layer_12_expert1_52 -> layer_12_expert_act_52
	layer_12_expert_act_52 -> layer_12_expert2_52
	layer_12_expert2_52 -> layer_12_expert_allreduce_13
	layer_12_alltoall -> layer_12_expert1_53
	layer_12_expert1_53 -> layer_12_expert_act_53
	layer_12_expert_act_53 -> layer_12_expert2_53
	layer_12_expert2_53 -> layer_12_expert_allreduce_13
	layer_12_alltoall -> layer_12_expert1_54
	layer_12_expert1_54 -> layer_12_expert_act_54
	layer_12_expert_act_54 -> layer_12_expert2_54
	layer_12_expert2_54 -> layer_12_expert_allreduce_13
	layer_12_alltoall -> layer_12_expert1_55
	layer_12_expert1_55 -> layer_12_expert_act_55
	layer_12_expert_act_55 -> layer_12_expert2_55
	layer_12_expert2_55 -> layer_12_expert_allreduce_13
	layer_12_expert_allreduce_13 -> layer_12_expert_agg
	layer_12_attn_allreduce_14 -> layer_12_gate_56 [style=dashed]
	layer_12_gate_56 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_56
	layer_12_expert1_56 -> layer_12_expert_act_56
	layer_12_expert_act_56 -> layer_12_expert2_56
	layer_12_expert2_56 -> layer_12_expert_allreduce_14
	layer_12_alltoall -> layer_12_expert1_57
	layer_12_expert1_57 -> layer_12_expert_act_57
	layer_12_expert_act_57 -> layer_12_expert2_57
	layer_12_expert2_57 -> layer_12_expert_allreduce_14
	layer_12_alltoall -> layer_12_expert1_58
	layer_12_expert1_58 -> layer_12_expert_act_58
	layer_12_expert_act_58 -> layer_12_expert2_58
	layer_12_expert2_58 -> layer_12_expert_allreduce_14
	layer_12_alltoall -> layer_12_expert1_59
	layer_12_expert1_59 -> layer_12_expert_act_59
	layer_12_expert_act_59 -> layer_12_expert2_59
	layer_12_expert2_59 -> layer_12_expert_allreduce_14
	layer_12_expert_allreduce_14 -> layer_12_expert_agg
	layer_12_attn_allreduce_15 -> layer_12_gate_60 [style=dashed]
	layer_12_gate_60 -> layer_12_alltoall [style=dashed]
	layer_12_alltoall -> layer_12_expert1_60
	layer_12_expert1_60 -> layer_12_expert_act_60
	layer_12_expert_act_60 -> layer_12_expert2_60
	layer_12_expert2_60 -> layer_12_expert_allreduce_15
	layer_12_alltoall -> layer_12_expert1_61
	layer_12_expert1_61 -> layer_12_expert_act_61
	layer_12_expert_act_61 -> layer_12_expert2_61
	layer_12_expert2_61 -> layer_12_expert_allreduce_15
	layer_12_alltoall -> layer_12_expert1_62
	layer_12_expert1_62 -> layer_12_expert_act_62
	layer_12_expert_act_62 -> layer_12_expert2_62
	layer_12_expert2_62 -> layer_12_expert_allreduce_15
	layer_12_alltoall -> layer_12_expert1_63
	layer_12_expert1_63 -> layer_12_expert_act_63
	layer_12_expert_act_63 -> layer_12_expert2_63
	layer_12_expert2_63 -> layer_12_expert_allreduce_15
	layer_12_expert_allreduce_15 -> layer_12_expert_agg
	layer_12_expert_agg -> layer_12_norm
	layer_12_norm -> layer_13_input
	layer_13_input [label="Layer13_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_13_qkv_0 [label="Layer13_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_1 [label="Layer13_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_2 [label="Layer13_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_3 [label="Layer13_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_0 [label="Layer13_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_1 [label="Layer13_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_2 [label="Layer13_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_3 [label="Layer13_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_0 [label="Layer13_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_1 [label="Layer13_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_2 [label="Layer13_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_3 [label="Layer13_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_0 [label="Layer13_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_1 [label="Layer13_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_2 [label="Layer13_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_3 [label="Layer13_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_0 [label="Layer13_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_0
	layer_13_qkv_0 -> layer_13_attn_scores_0
	layer_13_attn_scores_0 -> layer_13_attn_softmax_0
	layer_13_attn_softmax_0 -> layer_13_attn_out_0
	layer_13_attn_out_0 -> layer_13_attn_allreduce_0
	layer_13_input -> layer_13_qkv_1
	layer_13_qkv_1 -> layer_13_attn_scores_1
	layer_13_attn_scores_1 -> layer_13_attn_softmax_1
	layer_13_attn_softmax_1 -> layer_13_attn_out_1
	layer_13_attn_out_1 -> layer_13_attn_allreduce_0
	layer_13_input -> layer_13_qkv_2
	layer_13_qkv_2 -> layer_13_attn_scores_2
	layer_13_attn_scores_2 -> layer_13_attn_softmax_2
	layer_13_attn_softmax_2 -> layer_13_attn_out_2
	layer_13_attn_out_2 -> layer_13_attn_allreduce_0
	layer_13_input -> layer_13_qkv_3
	layer_13_qkv_3 -> layer_13_attn_scores_3
	layer_13_attn_scores_3 -> layer_13_attn_softmax_3
	layer_13_attn_softmax_3 -> layer_13_attn_out_3
	layer_13_attn_out_3 -> layer_13_attn_allreduce_0
	layer_13_qkv_4 [label="Layer13_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_5 [label="Layer13_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_6 [label="Layer13_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_7 [label="Layer13_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_4 [label="Layer13_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_5 [label="Layer13_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_6 [label="Layer13_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_7 [label="Layer13_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_4 [label="Layer13_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_5 [label="Layer13_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_6 [label="Layer13_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_7 [label="Layer13_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_4 [label="Layer13_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_5 [label="Layer13_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_6 [label="Layer13_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_7 [label="Layer13_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_1 [label="Layer13_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_4
	layer_13_qkv_4 -> layer_13_attn_scores_4
	layer_13_attn_scores_4 -> layer_13_attn_softmax_4
	layer_13_attn_softmax_4 -> layer_13_attn_out_4
	layer_13_attn_out_4 -> layer_13_attn_allreduce_1
	layer_13_input -> layer_13_qkv_5
	layer_13_qkv_5 -> layer_13_attn_scores_5
	layer_13_attn_scores_5 -> layer_13_attn_softmax_5
	layer_13_attn_softmax_5 -> layer_13_attn_out_5
	layer_13_attn_out_5 -> layer_13_attn_allreduce_1
	layer_13_input -> layer_13_qkv_6
	layer_13_qkv_6 -> layer_13_attn_scores_6
	layer_13_attn_scores_6 -> layer_13_attn_softmax_6
	layer_13_attn_softmax_6 -> layer_13_attn_out_6
	layer_13_attn_out_6 -> layer_13_attn_allreduce_1
	layer_13_input -> layer_13_qkv_7
	layer_13_qkv_7 -> layer_13_attn_scores_7
	layer_13_attn_scores_7 -> layer_13_attn_softmax_7
	layer_13_attn_softmax_7 -> layer_13_attn_out_7
	layer_13_attn_out_7 -> layer_13_attn_allreduce_1
	layer_13_qkv_8 [label="Layer13_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_9 [label="Layer13_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_10 [label="Layer13_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_11 [label="Layer13_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_8 [label="Layer13_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_9 [label="Layer13_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_10 [label="Layer13_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_11 [label="Layer13_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_8 [label="Layer13_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_9 [label="Layer13_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_10 [label="Layer13_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_11 [label="Layer13_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_8 [label="Layer13_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_9 [label="Layer13_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_10 [label="Layer13_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_11 [label="Layer13_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_2 [label="Layer13_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_8
	layer_13_qkv_8 -> layer_13_attn_scores_8
	layer_13_attn_scores_8 -> layer_13_attn_softmax_8
	layer_13_attn_softmax_8 -> layer_13_attn_out_8
	layer_13_attn_out_8 -> layer_13_attn_allreduce_2
	layer_13_input -> layer_13_qkv_9
	layer_13_qkv_9 -> layer_13_attn_scores_9
	layer_13_attn_scores_9 -> layer_13_attn_softmax_9
	layer_13_attn_softmax_9 -> layer_13_attn_out_9
	layer_13_attn_out_9 -> layer_13_attn_allreduce_2
	layer_13_input -> layer_13_qkv_10
	layer_13_qkv_10 -> layer_13_attn_scores_10
	layer_13_attn_scores_10 -> layer_13_attn_softmax_10
	layer_13_attn_softmax_10 -> layer_13_attn_out_10
	layer_13_attn_out_10 -> layer_13_attn_allreduce_2
	layer_13_input -> layer_13_qkv_11
	layer_13_qkv_11 -> layer_13_attn_scores_11
	layer_13_attn_scores_11 -> layer_13_attn_softmax_11
	layer_13_attn_softmax_11 -> layer_13_attn_out_11
	layer_13_attn_out_11 -> layer_13_attn_allreduce_2
	layer_13_qkv_12 [label="Layer13_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_13 [label="Layer13_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_14 [label="Layer13_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_15 [label="Layer13_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_12 [label="Layer13_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_13 [label="Layer13_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_14 [label="Layer13_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_15 [label="Layer13_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_12 [label="Layer13_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_13 [label="Layer13_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_14 [label="Layer13_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_15 [label="Layer13_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_12 [label="Layer13_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_13 [label="Layer13_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_14 [label="Layer13_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_15 [label="Layer13_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_3 [label="Layer13_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_12
	layer_13_qkv_12 -> layer_13_attn_scores_12
	layer_13_attn_scores_12 -> layer_13_attn_softmax_12
	layer_13_attn_softmax_12 -> layer_13_attn_out_12
	layer_13_attn_out_12 -> layer_13_attn_allreduce_3
	layer_13_input -> layer_13_qkv_13
	layer_13_qkv_13 -> layer_13_attn_scores_13
	layer_13_attn_scores_13 -> layer_13_attn_softmax_13
	layer_13_attn_softmax_13 -> layer_13_attn_out_13
	layer_13_attn_out_13 -> layer_13_attn_allreduce_3
	layer_13_input -> layer_13_qkv_14
	layer_13_qkv_14 -> layer_13_attn_scores_14
	layer_13_attn_scores_14 -> layer_13_attn_softmax_14
	layer_13_attn_softmax_14 -> layer_13_attn_out_14
	layer_13_attn_out_14 -> layer_13_attn_allreduce_3
	layer_13_input -> layer_13_qkv_15
	layer_13_qkv_15 -> layer_13_attn_scores_15
	layer_13_attn_scores_15 -> layer_13_attn_softmax_15
	layer_13_attn_softmax_15 -> layer_13_attn_out_15
	layer_13_attn_out_15 -> layer_13_attn_allreduce_3
	layer_13_qkv_16 [label="Layer13_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_17 [label="Layer13_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_18 [label="Layer13_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_19 [label="Layer13_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_16 [label="Layer13_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_17 [label="Layer13_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_18 [label="Layer13_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_19 [label="Layer13_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_16 [label="Layer13_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_17 [label="Layer13_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_18 [label="Layer13_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_19 [label="Layer13_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_16 [label="Layer13_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_17 [label="Layer13_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_18 [label="Layer13_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_19 [label="Layer13_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_4 [label="Layer13_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_16
	layer_13_qkv_16 -> layer_13_attn_scores_16
	layer_13_attn_scores_16 -> layer_13_attn_softmax_16
	layer_13_attn_softmax_16 -> layer_13_attn_out_16
	layer_13_attn_out_16 -> layer_13_attn_allreduce_4
	layer_13_input -> layer_13_qkv_17
	layer_13_qkv_17 -> layer_13_attn_scores_17
	layer_13_attn_scores_17 -> layer_13_attn_softmax_17
	layer_13_attn_softmax_17 -> layer_13_attn_out_17
	layer_13_attn_out_17 -> layer_13_attn_allreduce_4
	layer_13_input -> layer_13_qkv_18
	layer_13_qkv_18 -> layer_13_attn_scores_18
	layer_13_attn_scores_18 -> layer_13_attn_softmax_18
	layer_13_attn_softmax_18 -> layer_13_attn_out_18
	layer_13_attn_out_18 -> layer_13_attn_allreduce_4
	layer_13_input -> layer_13_qkv_19
	layer_13_qkv_19 -> layer_13_attn_scores_19
	layer_13_attn_scores_19 -> layer_13_attn_softmax_19
	layer_13_attn_softmax_19 -> layer_13_attn_out_19
	layer_13_attn_out_19 -> layer_13_attn_allreduce_4
	layer_13_qkv_20 [label="Layer13_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_21 [label="Layer13_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_22 [label="Layer13_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_23 [label="Layer13_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_20 [label="Layer13_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_21 [label="Layer13_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_22 [label="Layer13_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_23 [label="Layer13_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_20 [label="Layer13_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_21 [label="Layer13_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_22 [label="Layer13_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_23 [label="Layer13_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_20 [label="Layer13_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_21 [label="Layer13_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_22 [label="Layer13_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_23 [label="Layer13_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_5 [label="Layer13_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_20
	layer_13_qkv_20 -> layer_13_attn_scores_20
	layer_13_attn_scores_20 -> layer_13_attn_softmax_20
	layer_13_attn_softmax_20 -> layer_13_attn_out_20
	layer_13_attn_out_20 -> layer_13_attn_allreduce_5
	layer_13_input -> layer_13_qkv_21
	layer_13_qkv_21 -> layer_13_attn_scores_21
	layer_13_attn_scores_21 -> layer_13_attn_softmax_21
	layer_13_attn_softmax_21 -> layer_13_attn_out_21
	layer_13_attn_out_21 -> layer_13_attn_allreduce_5
	layer_13_input -> layer_13_qkv_22
	layer_13_qkv_22 -> layer_13_attn_scores_22
	layer_13_attn_scores_22 -> layer_13_attn_softmax_22
	layer_13_attn_softmax_22 -> layer_13_attn_out_22
	layer_13_attn_out_22 -> layer_13_attn_allreduce_5
	layer_13_input -> layer_13_qkv_23
	layer_13_qkv_23 -> layer_13_attn_scores_23
	layer_13_attn_scores_23 -> layer_13_attn_softmax_23
	layer_13_attn_softmax_23 -> layer_13_attn_out_23
	layer_13_attn_out_23 -> layer_13_attn_allreduce_5
	layer_13_qkv_24 [label="Layer13_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_25 [label="Layer13_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_26 [label="Layer13_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_27 [label="Layer13_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_24 [label="Layer13_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_25 [label="Layer13_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_26 [label="Layer13_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_27 [label="Layer13_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_24 [label="Layer13_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_25 [label="Layer13_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_26 [label="Layer13_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_27 [label="Layer13_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_24 [label="Layer13_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_25 [label="Layer13_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_26 [label="Layer13_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_27 [label="Layer13_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_6 [label="Layer13_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_24
	layer_13_qkv_24 -> layer_13_attn_scores_24
	layer_13_attn_scores_24 -> layer_13_attn_softmax_24
	layer_13_attn_softmax_24 -> layer_13_attn_out_24
	layer_13_attn_out_24 -> layer_13_attn_allreduce_6
	layer_13_input -> layer_13_qkv_25
	layer_13_qkv_25 -> layer_13_attn_scores_25
	layer_13_attn_scores_25 -> layer_13_attn_softmax_25
	layer_13_attn_softmax_25 -> layer_13_attn_out_25
	layer_13_attn_out_25 -> layer_13_attn_allreduce_6
	layer_13_input -> layer_13_qkv_26
	layer_13_qkv_26 -> layer_13_attn_scores_26
	layer_13_attn_scores_26 -> layer_13_attn_softmax_26
	layer_13_attn_softmax_26 -> layer_13_attn_out_26
	layer_13_attn_out_26 -> layer_13_attn_allreduce_6
	layer_13_input -> layer_13_qkv_27
	layer_13_qkv_27 -> layer_13_attn_scores_27
	layer_13_attn_scores_27 -> layer_13_attn_softmax_27
	layer_13_attn_softmax_27 -> layer_13_attn_out_27
	layer_13_attn_out_27 -> layer_13_attn_allreduce_6
	layer_13_qkv_28 [label="Layer13_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_29 [label="Layer13_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_30 [label="Layer13_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_31 [label="Layer13_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_28 [label="Layer13_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_29 [label="Layer13_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_30 [label="Layer13_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_31 [label="Layer13_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_28 [label="Layer13_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_29 [label="Layer13_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_30 [label="Layer13_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_31 [label="Layer13_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_28 [label="Layer13_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_29 [label="Layer13_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_30 [label="Layer13_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_31 [label="Layer13_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_7 [label="Layer13_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_28
	layer_13_qkv_28 -> layer_13_attn_scores_28
	layer_13_attn_scores_28 -> layer_13_attn_softmax_28
	layer_13_attn_softmax_28 -> layer_13_attn_out_28
	layer_13_attn_out_28 -> layer_13_attn_allreduce_7
	layer_13_input -> layer_13_qkv_29
	layer_13_qkv_29 -> layer_13_attn_scores_29
	layer_13_attn_scores_29 -> layer_13_attn_softmax_29
	layer_13_attn_softmax_29 -> layer_13_attn_out_29
	layer_13_attn_out_29 -> layer_13_attn_allreduce_7
	layer_13_input -> layer_13_qkv_30
	layer_13_qkv_30 -> layer_13_attn_scores_30
	layer_13_attn_scores_30 -> layer_13_attn_softmax_30
	layer_13_attn_softmax_30 -> layer_13_attn_out_30
	layer_13_attn_out_30 -> layer_13_attn_allreduce_7
	layer_13_input -> layer_13_qkv_31
	layer_13_qkv_31 -> layer_13_attn_scores_31
	layer_13_attn_scores_31 -> layer_13_attn_softmax_31
	layer_13_attn_softmax_31 -> layer_13_attn_out_31
	layer_13_attn_out_31 -> layer_13_attn_allreduce_7
	layer_13_qkv_32 [label="Layer13_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_33 [label="Layer13_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_34 [label="Layer13_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_35 [label="Layer13_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_32 [label="Layer13_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_33 [label="Layer13_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_34 [label="Layer13_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_35 [label="Layer13_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_32 [label="Layer13_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_33 [label="Layer13_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_34 [label="Layer13_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_35 [label="Layer13_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_32 [label="Layer13_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_33 [label="Layer13_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_34 [label="Layer13_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_35 [label="Layer13_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_8 [label="Layer13_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_32
	layer_13_qkv_32 -> layer_13_attn_scores_32
	layer_13_attn_scores_32 -> layer_13_attn_softmax_32
	layer_13_attn_softmax_32 -> layer_13_attn_out_32
	layer_13_attn_out_32 -> layer_13_attn_allreduce_8
	layer_13_input -> layer_13_qkv_33
	layer_13_qkv_33 -> layer_13_attn_scores_33
	layer_13_attn_scores_33 -> layer_13_attn_softmax_33
	layer_13_attn_softmax_33 -> layer_13_attn_out_33
	layer_13_attn_out_33 -> layer_13_attn_allreduce_8
	layer_13_input -> layer_13_qkv_34
	layer_13_qkv_34 -> layer_13_attn_scores_34
	layer_13_attn_scores_34 -> layer_13_attn_softmax_34
	layer_13_attn_softmax_34 -> layer_13_attn_out_34
	layer_13_attn_out_34 -> layer_13_attn_allreduce_8
	layer_13_input -> layer_13_qkv_35
	layer_13_qkv_35 -> layer_13_attn_scores_35
	layer_13_attn_scores_35 -> layer_13_attn_softmax_35
	layer_13_attn_softmax_35 -> layer_13_attn_out_35
	layer_13_attn_out_35 -> layer_13_attn_allreduce_8
	layer_13_qkv_36 [label="Layer13_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_37 [label="Layer13_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_38 [label="Layer13_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_39 [label="Layer13_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_36 [label="Layer13_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_37 [label="Layer13_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_38 [label="Layer13_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_39 [label="Layer13_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_36 [label="Layer13_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_37 [label="Layer13_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_38 [label="Layer13_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_39 [label="Layer13_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_36 [label="Layer13_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_37 [label="Layer13_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_38 [label="Layer13_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_39 [label="Layer13_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_9 [label="Layer13_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_36
	layer_13_qkv_36 -> layer_13_attn_scores_36
	layer_13_attn_scores_36 -> layer_13_attn_softmax_36
	layer_13_attn_softmax_36 -> layer_13_attn_out_36
	layer_13_attn_out_36 -> layer_13_attn_allreduce_9
	layer_13_input -> layer_13_qkv_37
	layer_13_qkv_37 -> layer_13_attn_scores_37
	layer_13_attn_scores_37 -> layer_13_attn_softmax_37
	layer_13_attn_softmax_37 -> layer_13_attn_out_37
	layer_13_attn_out_37 -> layer_13_attn_allreduce_9
	layer_13_input -> layer_13_qkv_38
	layer_13_qkv_38 -> layer_13_attn_scores_38
	layer_13_attn_scores_38 -> layer_13_attn_softmax_38
	layer_13_attn_softmax_38 -> layer_13_attn_out_38
	layer_13_attn_out_38 -> layer_13_attn_allreduce_9
	layer_13_input -> layer_13_qkv_39
	layer_13_qkv_39 -> layer_13_attn_scores_39
	layer_13_attn_scores_39 -> layer_13_attn_softmax_39
	layer_13_attn_softmax_39 -> layer_13_attn_out_39
	layer_13_attn_out_39 -> layer_13_attn_allreduce_9
	layer_13_qkv_40 [label="Layer13_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_41 [label="Layer13_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_42 [label="Layer13_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_43 [label="Layer13_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_40 [label="Layer13_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_41 [label="Layer13_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_42 [label="Layer13_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_43 [label="Layer13_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_40 [label="Layer13_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_41 [label="Layer13_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_42 [label="Layer13_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_43 [label="Layer13_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_40 [label="Layer13_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_41 [label="Layer13_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_42 [label="Layer13_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_43 [label="Layer13_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_10 [label="Layer13_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_40
	layer_13_qkv_40 -> layer_13_attn_scores_40
	layer_13_attn_scores_40 -> layer_13_attn_softmax_40
	layer_13_attn_softmax_40 -> layer_13_attn_out_40
	layer_13_attn_out_40 -> layer_13_attn_allreduce_10
	layer_13_input -> layer_13_qkv_41
	layer_13_qkv_41 -> layer_13_attn_scores_41
	layer_13_attn_scores_41 -> layer_13_attn_softmax_41
	layer_13_attn_softmax_41 -> layer_13_attn_out_41
	layer_13_attn_out_41 -> layer_13_attn_allreduce_10
	layer_13_input -> layer_13_qkv_42
	layer_13_qkv_42 -> layer_13_attn_scores_42
	layer_13_attn_scores_42 -> layer_13_attn_softmax_42
	layer_13_attn_softmax_42 -> layer_13_attn_out_42
	layer_13_attn_out_42 -> layer_13_attn_allreduce_10
	layer_13_input -> layer_13_qkv_43
	layer_13_qkv_43 -> layer_13_attn_scores_43
	layer_13_attn_scores_43 -> layer_13_attn_softmax_43
	layer_13_attn_softmax_43 -> layer_13_attn_out_43
	layer_13_attn_out_43 -> layer_13_attn_allreduce_10
	layer_13_qkv_44 [label="Layer13_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_45 [label="Layer13_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_46 [label="Layer13_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_47 [label="Layer13_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_44 [label="Layer13_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_45 [label="Layer13_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_46 [label="Layer13_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_47 [label="Layer13_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_44 [label="Layer13_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_45 [label="Layer13_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_46 [label="Layer13_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_47 [label="Layer13_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_44 [label="Layer13_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_45 [label="Layer13_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_46 [label="Layer13_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_47 [label="Layer13_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_11 [label="Layer13_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_44
	layer_13_qkv_44 -> layer_13_attn_scores_44
	layer_13_attn_scores_44 -> layer_13_attn_softmax_44
	layer_13_attn_softmax_44 -> layer_13_attn_out_44
	layer_13_attn_out_44 -> layer_13_attn_allreduce_11
	layer_13_input -> layer_13_qkv_45
	layer_13_qkv_45 -> layer_13_attn_scores_45
	layer_13_attn_scores_45 -> layer_13_attn_softmax_45
	layer_13_attn_softmax_45 -> layer_13_attn_out_45
	layer_13_attn_out_45 -> layer_13_attn_allreduce_11
	layer_13_input -> layer_13_qkv_46
	layer_13_qkv_46 -> layer_13_attn_scores_46
	layer_13_attn_scores_46 -> layer_13_attn_softmax_46
	layer_13_attn_softmax_46 -> layer_13_attn_out_46
	layer_13_attn_out_46 -> layer_13_attn_allreduce_11
	layer_13_input -> layer_13_qkv_47
	layer_13_qkv_47 -> layer_13_attn_scores_47
	layer_13_attn_scores_47 -> layer_13_attn_softmax_47
	layer_13_attn_softmax_47 -> layer_13_attn_out_47
	layer_13_attn_out_47 -> layer_13_attn_allreduce_11
	layer_13_qkv_48 [label="Layer13_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_49 [label="Layer13_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_50 [label="Layer13_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_51 [label="Layer13_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_48 [label="Layer13_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_49 [label="Layer13_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_50 [label="Layer13_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_51 [label="Layer13_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_48 [label="Layer13_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_49 [label="Layer13_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_50 [label="Layer13_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_51 [label="Layer13_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_48 [label="Layer13_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_49 [label="Layer13_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_50 [label="Layer13_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_51 [label="Layer13_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_12 [label="Layer13_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_48
	layer_13_qkv_48 -> layer_13_attn_scores_48
	layer_13_attn_scores_48 -> layer_13_attn_softmax_48
	layer_13_attn_softmax_48 -> layer_13_attn_out_48
	layer_13_attn_out_48 -> layer_13_attn_allreduce_12
	layer_13_input -> layer_13_qkv_49
	layer_13_qkv_49 -> layer_13_attn_scores_49
	layer_13_attn_scores_49 -> layer_13_attn_softmax_49
	layer_13_attn_softmax_49 -> layer_13_attn_out_49
	layer_13_attn_out_49 -> layer_13_attn_allreduce_12
	layer_13_input -> layer_13_qkv_50
	layer_13_qkv_50 -> layer_13_attn_scores_50
	layer_13_attn_scores_50 -> layer_13_attn_softmax_50
	layer_13_attn_softmax_50 -> layer_13_attn_out_50
	layer_13_attn_out_50 -> layer_13_attn_allreduce_12
	layer_13_input -> layer_13_qkv_51
	layer_13_qkv_51 -> layer_13_attn_scores_51
	layer_13_attn_scores_51 -> layer_13_attn_softmax_51
	layer_13_attn_softmax_51 -> layer_13_attn_out_51
	layer_13_attn_out_51 -> layer_13_attn_allreduce_12
	layer_13_qkv_52 [label="Layer13_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_53 [label="Layer13_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_54 [label="Layer13_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_55 [label="Layer13_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_52 [label="Layer13_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_53 [label="Layer13_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_54 [label="Layer13_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_55 [label="Layer13_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_52 [label="Layer13_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_53 [label="Layer13_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_54 [label="Layer13_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_55 [label="Layer13_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_52 [label="Layer13_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_53 [label="Layer13_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_54 [label="Layer13_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_55 [label="Layer13_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_13 [label="Layer13_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_52
	layer_13_qkv_52 -> layer_13_attn_scores_52
	layer_13_attn_scores_52 -> layer_13_attn_softmax_52
	layer_13_attn_softmax_52 -> layer_13_attn_out_52
	layer_13_attn_out_52 -> layer_13_attn_allreduce_13
	layer_13_input -> layer_13_qkv_53
	layer_13_qkv_53 -> layer_13_attn_scores_53
	layer_13_attn_scores_53 -> layer_13_attn_softmax_53
	layer_13_attn_softmax_53 -> layer_13_attn_out_53
	layer_13_attn_out_53 -> layer_13_attn_allreduce_13
	layer_13_input -> layer_13_qkv_54
	layer_13_qkv_54 -> layer_13_attn_scores_54
	layer_13_attn_scores_54 -> layer_13_attn_softmax_54
	layer_13_attn_softmax_54 -> layer_13_attn_out_54
	layer_13_attn_out_54 -> layer_13_attn_allreduce_13
	layer_13_input -> layer_13_qkv_55
	layer_13_qkv_55 -> layer_13_attn_scores_55
	layer_13_attn_scores_55 -> layer_13_attn_softmax_55
	layer_13_attn_softmax_55 -> layer_13_attn_out_55
	layer_13_attn_out_55 -> layer_13_attn_allreduce_13
	layer_13_qkv_56 [label="Layer13_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_57 [label="Layer13_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_58 [label="Layer13_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_59 [label="Layer13_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_56 [label="Layer13_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_57 [label="Layer13_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_58 [label="Layer13_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_59 [label="Layer13_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_56 [label="Layer13_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_57 [label="Layer13_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_58 [label="Layer13_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_59 [label="Layer13_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_56 [label="Layer13_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_57 [label="Layer13_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_58 [label="Layer13_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_59 [label="Layer13_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_14 [label="Layer13_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_56
	layer_13_qkv_56 -> layer_13_attn_scores_56
	layer_13_attn_scores_56 -> layer_13_attn_softmax_56
	layer_13_attn_softmax_56 -> layer_13_attn_out_56
	layer_13_attn_out_56 -> layer_13_attn_allreduce_14
	layer_13_input -> layer_13_qkv_57
	layer_13_qkv_57 -> layer_13_attn_scores_57
	layer_13_attn_scores_57 -> layer_13_attn_softmax_57
	layer_13_attn_softmax_57 -> layer_13_attn_out_57
	layer_13_attn_out_57 -> layer_13_attn_allreduce_14
	layer_13_input -> layer_13_qkv_58
	layer_13_qkv_58 -> layer_13_attn_scores_58
	layer_13_attn_scores_58 -> layer_13_attn_softmax_58
	layer_13_attn_softmax_58 -> layer_13_attn_out_58
	layer_13_attn_out_58 -> layer_13_attn_allreduce_14
	layer_13_input -> layer_13_qkv_59
	layer_13_qkv_59 -> layer_13_attn_scores_59
	layer_13_attn_scores_59 -> layer_13_attn_softmax_59
	layer_13_attn_softmax_59 -> layer_13_attn_out_59
	layer_13_attn_out_59 -> layer_13_attn_allreduce_14
	layer_13_qkv_60 [label="Layer13_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_61 [label="Layer13_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_62 [label="Layer13_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_qkv_63 [label="Layer13_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_60 [label="Layer13_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_61 [label="Layer13_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_62 [label="Layer13_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_scores_63 [label="Layer13_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_60 [label="Layer13_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_61 [label="Layer13_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_62 [label="Layer13_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_softmax_63 [label="Layer13_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_60 [label="Layer13_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_61 [label="Layer13_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_62 [label="Layer13_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_out_63 [label="Layer13_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_15 [label="Layer13_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_input -> layer_13_qkv_60
	layer_13_qkv_60 -> layer_13_attn_scores_60
	layer_13_attn_scores_60 -> layer_13_attn_softmax_60
	layer_13_attn_softmax_60 -> layer_13_attn_out_60
	layer_13_attn_out_60 -> layer_13_attn_allreduce_15
	layer_13_input -> layer_13_qkv_61
	layer_13_qkv_61 -> layer_13_attn_scores_61
	layer_13_attn_scores_61 -> layer_13_attn_softmax_61
	layer_13_attn_softmax_61 -> layer_13_attn_out_61
	layer_13_attn_out_61 -> layer_13_attn_allreduce_15
	layer_13_input -> layer_13_qkv_62
	layer_13_qkv_62 -> layer_13_attn_scores_62
	layer_13_attn_scores_62 -> layer_13_attn_softmax_62
	layer_13_attn_softmax_62 -> layer_13_attn_out_62
	layer_13_attn_out_62 -> layer_13_attn_allreduce_15
	layer_13_input -> layer_13_qkv_63
	layer_13_qkv_63 -> layer_13_attn_scores_63
	layer_13_attn_scores_63 -> layer_13_attn_softmax_63
	layer_13_attn_softmax_63 -> layer_13_attn_out_63
	layer_13_attn_out_63 -> layer_13_attn_allreduce_15
	layer_13_gate_0 [label="Layer13_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_4 [label="Layer13_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_8 [label="Layer13_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_12 [label="Layer13_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_16 [label="Layer13_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_20 [label="Layer13_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_24 [label="Layer13_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_28 [label="Layer13_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_32 [label="Layer13_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_36 [label="Layer13_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_40 [label="Layer13_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_44 [label="Layer13_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_48 [label="Layer13_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_52 [label="Layer13_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_56 [label="Layer13_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_gate_60 [label="Layer13_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_13_alltoall [label="Layer13_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_0 [label="Layer13_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_1 [label="Layer13_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_2 [label="Layer13_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_3 [label="Layer13_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_0 [label="Layer13_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_1 [label="Layer13_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_2 [label="Layer13_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_3 [label="Layer13_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_0 [label="Layer13_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_1 [label="Layer13_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_2 [label="Layer13_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_3 [label="Layer13_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_0 [label="Layer13_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_4 [label="Layer13_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_5 [label="Layer13_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_6 [label="Layer13_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_7 [label="Layer13_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_4 [label="Layer13_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_5 [label="Layer13_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_6 [label="Layer13_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_7 [label="Layer13_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_4 [label="Layer13_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_5 [label="Layer13_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_6 [label="Layer13_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_7 [label="Layer13_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_1 [label="Layer13_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_8 [label="Layer13_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_9 [label="Layer13_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_10 [label="Layer13_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_11 [label="Layer13_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_8 [label="Layer13_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_9 [label="Layer13_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_10 [label="Layer13_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_11 [label="Layer13_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_8 [label="Layer13_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_9 [label="Layer13_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_10 [label="Layer13_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_11 [label="Layer13_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_2 [label="Layer13_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_12 [label="Layer13_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_13 [label="Layer13_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_14 [label="Layer13_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_15 [label="Layer13_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_12 [label="Layer13_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_13 [label="Layer13_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_14 [label="Layer13_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_15 [label="Layer13_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_12 [label="Layer13_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_13 [label="Layer13_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_14 [label="Layer13_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_15 [label="Layer13_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_3 [label="Layer13_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_16 [label="Layer13_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_17 [label="Layer13_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_18 [label="Layer13_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_19 [label="Layer13_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_16 [label="Layer13_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_17 [label="Layer13_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_18 [label="Layer13_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_19 [label="Layer13_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_16 [label="Layer13_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_17 [label="Layer13_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_18 [label="Layer13_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_19 [label="Layer13_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_4 [label="Layer13_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_20 [label="Layer13_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_21 [label="Layer13_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_22 [label="Layer13_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_23 [label="Layer13_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_20 [label="Layer13_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_21 [label="Layer13_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_22 [label="Layer13_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_23 [label="Layer13_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_20 [label="Layer13_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_21 [label="Layer13_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_22 [label="Layer13_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_23 [label="Layer13_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_5 [label="Layer13_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_24 [label="Layer13_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_25 [label="Layer13_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_26 [label="Layer13_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_27 [label="Layer13_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_24 [label="Layer13_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_25 [label="Layer13_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_26 [label="Layer13_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_27 [label="Layer13_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_24 [label="Layer13_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_25 [label="Layer13_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_26 [label="Layer13_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_27 [label="Layer13_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_6 [label="Layer13_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_28 [label="Layer13_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_29 [label="Layer13_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_30 [label="Layer13_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_31 [label="Layer13_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_28 [label="Layer13_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_29 [label="Layer13_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_30 [label="Layer13_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_31 [label="Layer13_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_28 [label="Layer13_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_29 [label="Layer13_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_30 [label="Layer13_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_31 [label="Layer13_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_7 [label="Layer13_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_32 [label="Layer13_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_33 [label="Layer13_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_34 [label="Layer13_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_35 [label="Layer13_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_32 [label="Layer13_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_33 [label="Layer13_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_34 [label="Layer13_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_35 [label="Layer13_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_32 [label="Layer13_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_33 [label="Layer13_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_34 [label="Layer13_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_35 [label="Layer13_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_8 [label="Layer13_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_36 [label="Layer13_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_37 [label="Layer13_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_38 [label="Layer13_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_39 [label="Layer13_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_36 [label="Layer13_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_37 [label="Layer13_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_38 [label="Layer13_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_39 [label="Layer13_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_36 [label="Layer13_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_37 [label="Layer13_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_38 [label="Layer13_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_39 [label="Layer13_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_9 [label="Layer13_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_40 [label="Layer13_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_41 [label="Layer13_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_42 [label="Layer13_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_43 [label="Layer13_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_40 [label="Layer13_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_41 [label="Layer13_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_42 [label="Layer13_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_43 [label="Layer13_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_40 [label="Layer13_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_41 [label="Layer13_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_42 [label="Layer13_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_43 [label="Layer13_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_10 [label="Layer13_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_44 [label="Layer13_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_45 [label="Layer13_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_46 [label="Layer13_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_47 [label="Layer13_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_44 [label="Layer13_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_45 [label="Layer13_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_46 [label="Layer13_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_47 [label="Layer13_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_44 [label="Layer13_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_45 [label="Layer13_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_46 [label="Layer13_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_47 [label="Layer13_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_11 [label="Layer13_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_48 [label="Layer13_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_49 [label="Layer13_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_50 [label="Layer13_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_51 [label="Layer13_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_48 [label="Layer13_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_49 [label="Layer13_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_50 [label="Layer13_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_51 [label="Layer13_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_48 [label="Layer13_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_49 [label="Layer13_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_50 [label="Layer13_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_51 [label="Layer13_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_12 [label="Layer13_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_52 [label="Layer13_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_53 [label="Layer13_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_54 [label="Layer13_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_55 [label="Layer13_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_52 [label="Layer13_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_53 [label="Layer13_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_54 [label="Layer13_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_55 [label="Layer13_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_52 [label="Layer13_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_53 [label="Layer13_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_54 [label="Layer13_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_55 [label="Layer13_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_13 [label="Layer13_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_56 [label="Layer13_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_57 [label="Layer13_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_58 [label="Layer13_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_59 [label="Layer13_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_56 [label="Layer13_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_57 [label="Layer13_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_58 [label="Layer13_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_59 [label="Layer13_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_56 [label="Layer13_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_57 [label="Layer13_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_58 [label="Layer13_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_59 [label="Layer13_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_14 [label="Layer13_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert1_60 [label="Layer13_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_61 [label="Layer13_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_62 [label="Layer13_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert1_63 [label="Layer13_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_60 [label="Layer13_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_61 [label="Layer13_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_62 [label="Layer13_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert_act_63 [label="Layer13_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_13_expert2_60 [label="Layer13_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_61 [label="Layer13_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_62 [label="Layer13_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert2_63 [label="Layer13_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_13_expert_allreduce_15 [label="Layer13_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_13_expert_agg [label="Layer13_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_13_norm [label="Layer13_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_13_attn_allreduce_0 -> layer_13_gate_0 [style=dashed]
	layer_13_gate_0 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_0
	layer_13_expert1_0 -> layer_13_expert_act_0
	layer_13_expert_act_0 -> layer_13_expert2_0
	layer_13_expert2_0 -> layer_13_expert_allreduce_0
	layer_13_alltoall -> layer_13_expert1_1
	layer_13_expert1_1 -> layer_13_expert_act_1
	layer_13_expert_act_1 -> layer_13_expert2_1
	layer_13_expert2_1 -> layer_13_expert_allreduce_0
	layer_13_alltoall -> layer_13_expert1_2
	layer_13_expert1_2 -> layer_13_expert_act_2
	layer_13_expert_act_2 -> layer_13_expert2_2
	layer_13_expert2_2 -> layer_13_expert_allreduce_0
	layer_13_alltoall -> layer_13_expert1_3
	layer_13_expert1_3 -> layer_13_expert_act_3
	layer_13_expert_act_3 -> layer_13_expert2_3
	layer_13_expert2_3 -> layer_13_expert_allreduce_0
	layer_13_expert_allreduce_0 -> layer_13_expert_agg
	layer_13_attn_allreduce_1 -> layer_13_gate_4 [style=dashed]
	layer_13_gate_4 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_4
	layer_13_expert1_4 -> layer_13_expert_act_4
	layer_13_expert_act_4 -> layer_13_expert2_4
	layer_13_expert2_4 -> layer_13_expert_allreduce_1
	layer_13_alltoall -> layer_13_expert1_5
	layer_13_expert1_5 -> layer_13_expert_act_5
	layer_13_expert_act_5 -> layer_13_expert2_5
	layer_13_expert2_5 -> layer_13_expert_allreduce_1
	layer_13_alltoall -> layer_13_expert1_6
	layer_13_expert1_6 -> layer_13_expert_act_6
	layer_13_expert_act_6 -> layer_13_expert2_6
	layer_13_expert2_6 -> layer_13_expert_allreduce_1
	layer_13_alltoall -> layer_13_expert1_7
	layer_13_expert1_7 -> layer_13_expert_act_7
	layer_13_expert_act_7 -> layer_13_expert2_7
	layer_13_expert2_7 -> layer_13_expert_allreduce_1
	layer_13_expert_allreduce_1 -> layer_13_expert_agg
	layer_13_attn_allreduce_2 -> layer_13_gate_8 [style=dashed]
	layer_13_gate_8 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_8
	layer_13_expert1_8 -> layer_13_expert_act_8
	layer_13_expert_act_8 -> layer_13_expert2_8
	layer_13_expert2_8 -> layer_13_expert_allreduce_2
	layer_13_alltoall -> layer_13_expert1_9
	layer_13_expert1_9 -> layer_13_expert_act_9
	layer_13_expert_act_9 -> layer_13_expert2_9
	layer_13_expert2_9 -> layer_13_expert_allreduce_2
	layer_13_alltoall -> layer_13_expert1_10
	layer_13_expert1_10 -> layer_13_expert_act_10
	layer_13_expert_act_10 -> layer_13_expert2_10
	layer_13_expert2_10 -> layer_13_expert_allreduce_2
	layer_13_alltoall -> layer_13_expert1_11
	layer_13_expert1_11 -> layer_13_expert_act_11
	layer_13_expert_act_11 -> layer_13_expert2_11
	layer_13_expert2_11 -> layer_13_expert_allreduce_2
	layer_13_expert_allreduce_2 -> layer_13_expert_agg
	layer_13_attn_allreduce_3 -> layer_13_gate_12 [style=dashed]
	layer_13_gate_12 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_12
	layer_13_expert1_12 -> layer_13_expert_act_12
	layer_13_expert_act_12 -> layer_13_expert2_12
	layer_13_expert2_12 -> layer_13_expert_allreduce_3
	layer_13_alltoall -> layer_13_expert1_13
	layer_13_expert1_13 -> layer_13_expert_act_13
	layer_13_expert_act_13 -> layer_13_expert2_13
	layer_13_expert2_13 -> layer_13_expert_allreduce_3
	layer_13_alltoall -> layer_13_expert1_14
	layer_13_expert1_14 -> layer_13_expert_act_14
	layer_13_expert_act_14 -> layer_13_expert2_14
	layer_13_expert2_14 -> layer_13_expert_allreduce_3
	layer_13_alltoall -> layer_13_expert1_15
	layer_13_expert1_15 -> layer_13_expert_act_15
	layer_13_expert_act_15 -> layer_13_expert2_15
	layer_13_expert2_15 -> layer_13_expert_allreduce_3
	layer_13_expert_allreduce_3 -> layer_13_expert_agg
	layer_13_attn_allreduce_4 -> layer_13_gate_16 [style=dashed]
	layer_13_gate_16 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_16
	layer_13_expert1_16 -> layer_13_expert_act_16
	layer_13_expert_act_16 -> layer_13_expert2_16
	layer_13_expert2_16 -> layer_13_expert_allreduce_4
	layer_13_alltoall -> layer_13_expert1_17
	layer_13_expert1_17 -> layer_13_expert_act_17
	layer_13_expert_act_17 -> layer_13_expert2_17
	layer_13_expert2_17 -> layer_13_expert_allreduce_4
	layer_13_alltoall -> layer_13_expert1_18
	layer_13_expert1_18 -> layer_13_expert_act_18
	layer_13_expert_act_18 -> layer_13_expert2_18
	layer_13_expert2_18 -> layer_13_expert_allreduce_4
	layer_13_alltoall -> layer_13_expert1_19
	layer_13_expert1_19 -> layer_13_expert_act_19
	layer_13_expert_act_19 -> layer_13_expert2_19
	layer_13_expert2_19 -> layer_13_expert_allreduce_4
	layer_13_expert_allreduce_4 -> layer_13_expert_agg
	layer_13_attn_allreduce_5 -> layer_13_gate_20 [style=dashed]
	layer_13_gate_20 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_20
	layer_13_expert1_20 -> layer_13_expert_act_20
	layer_13_expert_act_20 -> layer_13_expert2_20
	layer_13_expert2_20 -> layer_13_expert_allreduce_5
	layer_13_alltoall -> layer_13_expert1_21
	layer_13_expert1_21 -> layer_13_expert_act_21
	layer_13_expert_act_21 -> layer_13_expert2_21
	layer_13_expert2_21 -> layer_13_expert_allreduce_5
	layer_13_alltoall -> layer_13_expert1_22
	layer_13_expert1_22 -> layer_13_expert_act_22
	layer_13_expert_act_22 -> layer_13_expert2_22
	layer_13_expert2_22 -> layer_13_expert_allreduce_5
	layer_13_alltoall -> layer_13_expert1_23
	layer_13_expert1_23 -> layer_13_expert_act_23
	layer_13_expert_act_23 -> layer_13_expert2_23
	layer_13_expert2_23 -> layer_13_expert_allreduce_5
	layer_13_expert_allreduce_5 -> layer_13_expert_agg
	layer_13_attn_allreduce_6 -> layer_13_gate_24 [style=dashed]
	layer_13_gate_24 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_24
	layer_13_expert1_24 -> layer_13_expert_act_24
	layer_13_expert_act_24 -> layer_13_expert2_24
	layer_13_expert2_24 -> layer_13_expert_allreduce_6
	layer_13_alltoall -> layer_13_expert1_25
	layer_13_expert1_25 -> layer_13_expert_act_25
	layer_13_expert_act_25 -> layer_13_expert2_25
	layer_13_expert2_25 -> layer_13_expert_allreduce_6
	layer_13_alltoall -> layer_13_expert1_26
	layer_13_expert1_26 -> layer_13_expert_act_26
	layer_13_expert_act_26 -> layer_13_expert2_26
	layer_13_expert2_26 -> layer_13_expert_allreduce_6
	layer_13_alltoall -> layer_13_expert1_27
	layer_13_expert1_27 -> layer_13_expert_act_27
	layer_13_expert_act_27 -> layer_13_expert2_27
	layer_13_expert2_27 -> layer_13_expert_allreduce_6
	layer_13_expert_allreduce_6 -> layer_13_expert_agg
	layer_13_attn_allreduce_7 -> layer_13_gate_28 [style=dashed]
	layer_13_gate_28 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_28
	layer_13_expert1_28 -> layer_13_expert_act_28
	layer_13_expert_act_28 -> layer_13_expert2_28
	layer_13_expert2_28 -> layer_13_expert_allreduce_7
	layer_13_alltoall -> layer_13_expert1_29
	layer_13_expert1_29 -> layer_13_expert_act_29
	layer_13_expert_act_29 -> layer_13_expert2_29
	layer_13_expert2_29 -> layer_13_expert_allreduce_7
	layer_13_alltoall -> layer_13_expert1_30
	layer_13_expert1_30 -> layer_13_expert_act_30
	layer_13_expert_act_30 -> layer_13_expert2_30
	layer_13_expert2_30 -> layer_13_expert_allreduce_7
	layer_13_alltoall -> layer_13_expert1_31
	layer_13_expert1_31 -> layer_13_expert_act_31
	layer_13_expert_act_31 -> layer_13_expert2_31
	layer_13_expert2_31 -> layer_13_expert_allreduce_7
	layer_13_expert_allreduce_7 -> layer_13_expert_agg
	layer_13_attn_allreduce_8 -> layer_13_gate_32 [style=dashed]
	layer_13_gate_32 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_32
	layer_13_expert1_32 -> layer_13_expert_act_32
	layer_13_expert_act_32 -> layer_13_expert2_32
	layer_13_expert2_32 -> layer_13_expert_allreduce_8
	layer_13_alltoall -> layer_13_expert1_33
	layer_13_expert1_33 -> layer_13_expert_act_33
	layer_13_expert_act_33 -> layer_13_expert2_33
	layer_13_expert2_33 -> layer_13_expert_allreduce_8
	layer_13_alltoall -> layer_13_expert1_34
	layer_13_expert1_34 -> layer_13_expert_act_34
	layer_13_expert_act_34 -> layer_13_expert2_34
	layer_13_expert2_34 -> layer_13_expert_allreduce_8
	layer_13_alltoall -> layer_13_expert1_35
	layer_13_expert1_35 -> layer_13_expert_act_35
	layer_13_expert_act_35 -> layer_13_expert2_35
	layer_13_expert2_35 -> layer_13_expert_allreduce_8
	layer_13_expert_allreduce_8 -> layer_13_expert_agg
	layer_13_attn_allreduce_9 -> layer_13_gate_36 [style=dashed]
	layer_13_gate_36 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_36
	layer_13_expert1_36 -> layer_13_expert_act_36
	layer_13_expert_act_36 -> layer_13_expert2_36
	layer_13_expert2_36 -> layer_13_expert_allreduce_9
	layer_13_alltoall -> layer_13_expert1_37
	layer_13_expert1_37 -> layer_13_expert_act_37
	layer_13_expert_act_37 -> layer_13_expert2_37
	layer_13_expert2_37 -> layer_13_expert_allreduce_9
	layer_13_alltoall -> layer_13_expert1_38
	layer_13_expert1_38 -> layer_13_expert_act_38
	layer_13_expert_act_38 -> layer_13_expert2_38
	layer_13_expert2_38 -> layer_13_expert_allreduce_9
	layer_13_alltoall -> layer_13_expert1_39
	layer_13_expert1_39 -> layer_13_expert_act_39
	layer_13_expert_act_39 -> layer_13_expert2_39
	layer_13_expert2_39 -> layer_13_expert_allreduce_9
	layer_13_expert_allreduce_9 -> layer_13_expert_agg
	layer_13_attn_allreduce_10 -> layer_13_gate_40 [style=dashed]
	layer_13_gate_40 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_40
	layer_13_expert1_40 -> layer_13_expert_act_40
	layer_13_expert_act_40 -> layer_13_expert2_40
	layer_13_expert2_40 -> layer_13_expert_allreduce_10
	layer_13_alltoall -> layer_13_expert1_41
	layer_13_expert1_41 -> layer_13_expert_act_41
	layer_13_expert_act_41 -> layer_13_expert2_41
	layer_13_expert2_41 -> layer_13_expert_allreduce_10
	layer_13_alltoall -> layer_13_expert1_42
	layer_13_expert1_42 -> layer_13_expert_act_42
	layer_13_expert_act_42 -> layer_13_expert2_42
	layer_13_expert2_42 -> layer_13_expert_allreduce_10
	layer_13_alltoall -> layer_13_expert1_43
	layer_13_expert1_43 -> layer_13_expert_act_43
	layer_13_expert_act_43 -> layer_13_expert2_43
	layer_13_expert2_43 -> layer_13_expert_allreduce_10
	layer_13_expert_allreduce_10 -> layer_13_expert_agg
	layer_13_attn_allreduce_11 -> layer_13_gate_44 [style=dashed]
	layer_13_gate_44 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_44
	layer_13_expert1_44 -> layer_13_expert_act_44
	layer_13_expert_act_44 -> layer_13_expert2_44
	layer_13_expert2_44 -> layer_13_expert_allreduce_11
	layer_13_alltoall -> layer_13_expert1_45
	layer_13_expert1_45 -> layer_13_expert_act_45
	layer_13_expert_act_45 -> layer_13_expert2_45
	layer_13_expert2_45 -> layer_13_expert_allreduce_11
	layer_13_alltoall -> layer_13_expert1_46
	layer_13_expert1_46 -> layer_13_expert_act_46
	layer_13_expert_act_46 -> layer_13_expert2_46
	layer_13_expert2_46 -> layer_13_expert_allreduce_11
	layer_13_alltoall -> layer_13_expert1_47
	layer_13_expert1_47 -> layer_13_expert_act_47
	layer_13_expert_act_47 -> layer_13_expert2_47
	layer_13_expert2_47 -> layer_13_expert_allreduce_11
	layer_13_expert_allreduce_11 -> layer_13_expert_agg
	layer_13_attn_allreduce_12 -> layer_13_gate_48 [style=dashed]
	layer_13_gate_48 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_48
	layer_13_expert1_48 -> layer_13_expert_act_48
	layer_13_expert_act_48 -> layer_13_expert2_48
	layer_13_expert2_48 -> layer_13_expert_allreduce_12
	layer_13_alltoall -> layer_13_expert1_49
	layer_13_expert1_49 -> layer_13_expert_act_49
	layer_13_expert_act_49 -> layer_13_expert2_49
	layer_13_expert2_49 -> layer_13_expert_allreduce_12
	layer_13_alltoall -> layer_13_expert1_50
	layer_13_expert1_50 -> layer_13_expert_act_50
	layer_13_expert_act_50 -> layer_13_expert2_50
	layer_13_expert2_50 -> layer_13_expert_allreduce_12
	layer_13_alltoall -> layer_13_expert1_51
	layer_13_expert1_51 -> layer_13_expert_act_51
	layer_13_expert_act_51 -> layer_13_expert2_51
	layer_13_expert2_51 -> layer_13_expert_allreduce_12
	layer_13_expert_allreduce_12 -> layer_13_expert_agg
	layer_13_attn_allreduce_13 -> layer_13_gate_52 [style=dashed]
	layer_13_gate_52 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_52
	layer_13_expert1_52 -> layer_13_expert_act_52
	layer_13_expert_act_52 -> layer_13_expert2_52
	layer_13_expert2_52 -> layer_13_expert_allreduce_13
	layer_13_alltoall -> layer_13_expert1_53
	layer_13_expert1_53 -> layer_13_expert_act_53
	layer_13_expert_act_53 -> layer_13_expert2_53
	layer_13_expert2_53 -> layer_13_expert_allreduce_13
	layer_13_alltoall -> layer_13_expert1_54
	layer_13_expert1_54 -> layer_13_expert_act_54
	layer_13_expert_act_54 -> layer_13_expert2_54
	layer_13_expert2_54 -> layer_13_expert_allreduce_13
	layer_13_alltoall -> layer_13_expert1_55
	layer_13_expert1_55 -> layer_13_expert_act_55
	layer_13_expert_act_55 -> layer_13_expert2_55
	layer_13_expert2_55 -> layer_13_expert_allreduce_13
	layer_13_expert_allreduce_13 -> layer_13_expert_agg
	layer_13_attn_allreduce_14 -> layer_13_gate_56 [style=dashed]
	layer_13_gate_56 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_56
	layer_13_expert1_56 -> layer_13_expert_act_56
	layer_13_expert_act_56 -> layer_13_expert2_56
	layer_13_expert2_56 -> layer_13_expert_allreduce_14
	layer_13_alltoall -> layer_13_expert1_57
	layer_13_expert1_57 -> layer_13_expert_act_57
	layer_13_expert_act_57 -> layer_13_expert2_57
	layer_13_expert2_57 -> layer_13_expert_allreduce_14
	layer_13_alltoall -> layer_13_expert1_58
	layer_13_expert1_58 -> layer_13_expert_act_58
	layer_13_expert_act_58 -> layer_13_expert2_58
	layer_13_expert2_58 -> layer_13_expert_allreduce_14
	layer_13_alltoall -> layer_13_expert1_59
	layer_13_expert1_59 -> layer_13_expert_act_59
	layer_13_expert_act_59 -> layer_13_expert2_59
	layer_13_expert2_59 -> layer_13_expert_allreduce_14
	layer_13_expert_allreduce_14 -> layer_13_expert_agg
	layer_13_attn_allreduce_15 -> layer_13_gate_60 [style=dashed]
	layer_13_gate_60 -> layer_13_alltoall [style=dashed]
	layer_13_alltoall -> layer_13_expert1_60
	layer_13_expert1_60 -> layer_13_expert_act_60
	layer_13_expert_act_60 -> layer_13_expert2_60
	layer_13_expert2_60 -> layer_13_expert_allreduce_15
	layer_13_alltoall -> layer_13_expert1_61
	layer_13_expert1_61 -> layer_13_expert_act_61
	layer_13_expert_act_61 -> layer_13_expert2_61
	layer_13_expert2_61 -> layer_13_expert_allreduce_15
	layer_13_alltoall -> layer_13_expert1_62
	layer_13_expert1_62 -> layer_13_expert_act_62
	layer_13_expert_act_62 -> layer_13_expert2_62
	layer_13_expert2_62 -> layer_13_expert_allreduce_15
	layer_13_alltoall -> layer_13_expert1_63
	layer_13_expert1_63 -> layer_13_expert_act_63
	layer_13_expert_act_63 -> layer_13_expert2_63
	layer_13_expert2_63 -> layer_13_expert_allreduce_15
	layer_13_expert_allreduce_15 -> layer_13_expert_agg
	layer_13_expert_agg -> layer_13_norm
	layer_13_norm -> layer_14_input
	layer_14_input [label="Layer14_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_14_qkv_0 [label="Layer14_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_1 [label="Layer14_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_2 [label="Layer14_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_3 [label="Layer14_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_0 [label="Layer14_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_1 [label="Layer14_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_2 [label="Layer14_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_3 [label="Layer14_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_0 [label="Layer14_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_1 [label="Layer14_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_2 [label="Layer14_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_3 [label="Layer14_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_0 [label="Layer14_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_1 [label="Layer14_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_2 [label="Layer14_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_3 [label="Layer14_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_0 [label="Layer14_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_0
	layer_14_qkv_0 -> layer_14_attn_scores_0
	layer_14_attn_scores_0 -> layer_14_attn_softmax_0
	layer_14_attn_softmax_0 -> layer_14_attn_out_0
	layer_14_attn_out_0 -> layer_14_attn_allreduce_0
	layer_14_input -> layer_14_qkv_1
	layer_14_qkv_1 -> layer_14_attn_scores_1
	layer_14_attn_scores_1 -> layer_14_attn_softmax_1
	layer_14_attn_softmax_1 -> layer_14_attn_out_1
	layer_14_attn_out_1 -> layer_14_attn_allreduce_0
	layer_14_input -> layer_14_qkv_2
	layer_14_qkv_2 -> layer_14_attn_scores_2
	layer_14_attn_scores_2 -> layer_14_attn_softmax_2
	layer_14_attn_softmax_2 -> layer_14_attn_out_2
	layer_14_attn_out_2 -> layer_14_attn_allreduce_0
	layer_14_input -> layer_14_qkv_3
	layer_14_qkv_3 -> layer_14_attn_scores_3
	layer_14_attn_scores_3 -> layer_14_attn_softmax_3
	layer_14_attn_softmax_3 -> layer_14_attn_out_3
	layer_14_attn_out_3 -> layer_14_attn_allreduce_0
	layer_14_qkv_4 [label="Layer14_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_5 [label="Layer14_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_6 [label="Layer14_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_7 [label="Layer14_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_4 [label="Layer14_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_5 [label="Layer14_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_6 [label="Layer14_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_7 [label="Layer14_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_4 [label="Layer14_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_5 [label="Layer14_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_6 [label="Layer14_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_7 [label="Layer14_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_4 [label="Layer14_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_5 [label="Layer14_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_6 [label="Layer14_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_7 [label="Layer14_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_1 [label="Layer14_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_4
	layer_14_qkv_4 -> layer_14_attn_scores_4
	layer_14_attn_scores_4 -> layer_14_attn_softmax_4
	layer_14_attn_softmax_4 -> layer_14_attn_out_4
	layer_14_attn_out_4 -> layer_14_attn_allreduce_1
	layer_14_input -> layer_14_qkv_5
	layer_14_qkv_5 -> layer_14_attn_scores_5
	layer_14_attn_scores_5 -> layer_14_attn_softmax_5
	layer_14_attn_softmax_5 -> layer_14_attn_out_5
	layer_14_attn_out_5 -> layer_14_attn_allreduce_1
	layer_14_input -> layer_14_qkv_6
	layer_14_qkv_6 -> layer_14_attn_scores_6
	layer_14_attn_scores_6 -> layer_14_attn_softmax_6
	layer_14_attn_softmax_6 -> layer_14_attn_out_6
	layer_14_attn_out_6 -> layer_14_attn_allreduce_1
	layer_14_input -> layer_14_qkv_7
	layer_14_qkv_7 -> layer_14_attn_scores_7
	layer_14_attn_scores_7 -> layer_14_attn_softmax_7
	layer_14_attn_softmax_7 -> layer_14_attn_out_7
	layer_14_attn_out_7 -> layer_14_attn_allreduce_1
	layer_14_qkv_8 [label="Layer14_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_9 [label="Layer14_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_10 [label="Layer14_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_11 [label="Layer14_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_8 [label="Layer14_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_9 [label="Layer14_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_10 [label="Layer14_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_11 [label="Layer14_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_8 [label="Layer14_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_9 [label="Layer14_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_10 [label="Layer14_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_11 [label="Layer14_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_8 [label="Layer14_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_9 [label="Layer14_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_10 [label="Layer14_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_11 [label="Layer14_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_2 [label="Layer14_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_8
	layer_14_qkv_8 -> layer_14_attn_scores_8
	layer_14_attn_scores_8 -> layer_14_attn_softmax_8
	layer_14_attn_softmax_8 -> layer_14_attn_out_8
	layer_14_attn_out_8 -> layer_14_attn_allreduce_2
	layer_14_input -> layer_14_qkv_9
	layer_14_qkv_9 -> layer_14_attn_scores_9
	layer_14_attn_scores_9 -> layer_14_attn_softmax_9
	layer_14_attn_softmax_9 -> layer_14_attn_out_9
	layer_14_attn_out_9 -> layer_14_attn_allreduce_2
	layer_14_input -> layer_14_qkv_10
	layer_14_qkv_10 -> layer_14_attn_scores_10
	layer_14_attn_scores_10 -> layer_14_attn_softmax_10
	layer_14_attn_softmax_10 -> layer_14_attn_out_10
	layer_14_attn_out_10 -> layer_14_attn_allreduce_2
	layer_14_input -> layer_14_qkv_11
	layer_14_qkv_11 -> layer_14_attn_scores_11
	layer_14_attn_scores_11 -> layer_14_attn_softmax_11
	layer_14_attn_softmax_11 -> layer_14_attn_out_11
	layer_14_attn_out_11 -> layer_14_attn_allreduce_2
	layer_14_qkv_12 [label="Layer14_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_13 [label="Layer14_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_14 [label="Layer14_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_15 [label="Layer14_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_12 [label="Layer14_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_13 [label="Layer14_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_14 [label="Layer14_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_15 [label="Layer14_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_12 [label="Layer14_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_13 [label="Layer14_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_14 [label="Layer14_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_15 [label="Layer14_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_12 [label="Layer14_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_13 [label="Layer14_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_14 [label="Layer14_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_15 [label="Layer14_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_3 [label="Layer14_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_12
	layer_14_qkv_12 -> layer_14_attn_scores_12
	layer_14_attn_scores_12 -> layer_14_attn_softmax_12
	layer_14_attn_softmax_12 -> layer_14_attn_out_12
	layer_14_attn_out_12 -> layer_14_attn_allreduce_3
	layer_14_input -> layer_14_qkv_13
	layer_14_qkv_13 -> layer_14_attn_scores_13
	layer_14_attn_scores_13 -> layer_14_attn_softmax_13
	layer_14_attn_softmax_13 -> layer_14_attn_out_13
	layer_14_attn_out_13 -> layer_14_attn_allreduce_3
	layer_14_input -> layer_14_qkv_14
	layer_14_qkv_14 -> layer_14_attn_scores_14
	layer_14_attn_scores_14 -> layer_14_attn_softmax_14
	layer_14_attn_softmax_14 -> layer_14_attn_out_14
	layer_14_attn_out_14 -> layer_14_attn_allreduce_3
	layer_14_input -> layer_14_qkv_15
	layer_14_qkv_15 -> layer_14_attn_scores_15
	layer_14_attn_scores_15 -> layer_14_attn_softmax_15
	layer_14_attn_softmax_15 -> layer_14_attn_out_15
	layer_14_attn_out_15 -> layer_14_attn_allreduce_3
	layer_14_qkv_16 [label="Layer14_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_17 [label="Layer14_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_18 [label="Layer14_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_19 [label="Layer14_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_16 [label="Layer14_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_17 [label="Layer14_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_18 [label="Layer14_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_19 [label="Layer14_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_16 [label="Layer14_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_17 [label="Layer14_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_18 [label="Layer14_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_19 [label="Layer14_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_16 [label="Layer14_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_17 [label="Layer14_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_18 [label="Layer14_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_19 [label="Layer14_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_4 [label="Layer14_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_16
	layer_14_qkv_16 -> layer_14_attn_scores_16
	layer_14_attn_scores_16 -> layer_14_attn_softmax_16
	layer_14_attn_softmax_16 -> layer_14_attn_out_16
	layer_14_attn_out_16 -> layer_14_attn_allreduce_4
	layer_14_input -> layer_14_qkv_17
	layer_14_qkv_17 -> layer_14_attn_scores_17
	layer_14_attn_scores_17 -> layer_14_attn_softmax_17
	layer_14_attn_softmax_17 -> layer_14_attn_out_17
	layer_14_attn_out_17 -> layer_14_attn_allreduce_4
	layer_14_input -> layer_14_qkv_18
	layer_14_qkv_18 -> layer_14_attn_scores_18
	layer_14_attn_scores_18 -> layer_14_attn_softmax_18
	layer_14_attn_softmax_18 -> layer_14_attn_out_18
	layer_14_attn_out_18 -> layer_14_attn_allreduce_4
	layer_14_input -> layer_14_qkv_19
	layer_14_qkv_19 -> layer_14_attn_scores_19
	layer_14_attn_scores_19 -> layer_14_attn_softmax_19
	layer_14_attn_softmax_19 -> layer_14_attn_out_19
	layer_14_attn_out_19 -> layer_14_attn_allreduce_4
	layer_14_qkv_20 [label="Layer14_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_21 [label="Layer14_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_22 [label="Layer14_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_23 [label="Layer14_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_20 [label="Layer14_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_21 [label="Layer14_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_22 [label="Layer14_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_23 [label="Layer14_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_20 [label="Layer14_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_21 [label="Layer14_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_22 [label="Layer14_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_23 [label="Layer14_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_20 [label="Layer14_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_21 [label="Layer14_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_22 [label="Layer14_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_23 [label="Layer14_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_5 [label="Layer14_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_20
	layer_14_qkv_20 -> layer_14_attn_scores_20
	layer_14_attn_scores_20 -> layer_14_attn_softmax_20
	layer_14_attn_softmax_20 -> layer_14_attn_out_20
	layer_14_attn_out_20 -> layer_14_attn_allreduce_5
	layer_14_input -> layer_14_qkv_21
	layer_14_qkv_21 -> layer_14_attn_scores_21
	layer_14_attn_scores_21 -> layer_14_attn_softmax_21
	layer_14_attn_softmax_21 -> layer_14_attn_out_21
	layer_14_attn_out_21 -> layer_14_attn_allreduce_5
	layer_14_input -> layer_14_qkv_22
	layer_14_qkv_22 -> layer_14_attn_scores_22
	layer_14_attn_scores_22 -> layer_14_attn_softmax_22
	layer_14_attn_softmax_22 -> layer_14_attn_out_22
	layer_14_attn_out_22 -> layer_14_attn_allreduce_5
	layer_14_input -> layer_14_qkv_23
	layer_14_qkv_23 -> layer_14_attn_scores_23
	layer_14_attn_scores_23 -> layer_14_attn_softmax_23
	layer_14_attn_softmax_23 -> layer_14_attn_out_23
	layer_14_attn_out_23 -> layer_14_attn_allreduce_5
	layer_14_qkv_24 [label="Layer14_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_25 [label="Layer14_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_26 [label="Layer14_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_27 [label="Layer14_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_24 [label="Layer14_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_25 [label="Layer14_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_26 [label="Layer14_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_27 [label="Layer14_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_24 [label="Layer14_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_25 [label="Layer14_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_26 [label="Layer14_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_27 [label="Layer14_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_24 [label="Layer14_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_25 [label="Layer14_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_26 [label="Layer14_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_27 [label="Layer14_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_6 [label="Layer14_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_24
	layer_14_qkv_24 -> layer_14_attn_scores_24
	layer_14_attn_scores_24 -> layer_14_attn_softmax_24
	layer_14_attn_softmax_24 -> layer_14_attn_out_24
	layer_14_attn_out_24 -> layer_14_attn_allreduce_6
	layer_14_input -> layer_14_qkv_25
	layer_14_qkv_25 -> layer_14_attn_scores_25
	layer_14_attn_scores_25 -> layer_14_attn_softmax_25
	layer_14_attn_softmax_25 -> layer_14_attn_out_25
	layer_14_attn_out_25 -> layer_14_attn_allreduce_6
	layer_14_input -> layer_14_qkv_26
	layer_14_qkv_26 -> layer_14_attn_scores_26
	layer_14_attn_scores_26 -> layer_14_attn_softmax_26
	layer_14_attn_softmax_26 -> layer_14_attn_out_26
	layer_14_attn_out_26 -> layer_14_attn_allreduce_6
	layer_14_input -> layer_14_qkv_27
	layer_14_qkv_27 -> layer_14_attn_scores_27
	layer_14_attn_scores_27 -> layer_14_attn_softmax_27
	layer_14_attn_softmax_27 -> layer_14_attn_out_27
	layer_14_attn_out_27 -> layer_14_attn_allreduce_6
	layer_14_qkv_28 [label="Layer14_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_29 [label="Layer14_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_30 [label="Layer14_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_31 [label="Layer14_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_28 [label="Layer14_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_29 [label="Layer14_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_30 [label="Layer14_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_31 [label="Layer14_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_28 [label="Layer14_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_29 [label="Layer14_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_30 [label="Layer14_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_31 [label="Layer14_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_28 [label="Layer14_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_29 [label="Layer14_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_30 [label="Layer14_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_31 [label="Layer14_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_7 [label="Layer14_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_28
	layer_14_qkv_28 -> layer_14_attn_scores_28
	layer_14_attn_scores_28 -> layer_14_attn_softmax_28
	layer_14_attn_softmax_28 -> layer_14_attn_out_28
	layer_14_attn_out_28 -> layer_14_attn_allreduce_7
	layer_14_input -> layer_14_qkv_29
	layer_14_qkv_29 -> layer_14_attn_scores_29
	layer_14_attn_scores_29 -> layer_14_attn_softmax_29
	layer_14_attn_softmax_29 -> layer_14_attn_out_29
	layer_14_attn_out_29 -> layer_14_attn_allreduce_7
	layer_14_input -> layer_14_qkv_30
	layer_14_qkv_30 -> layer_14_attn_scores_30
	layer_14_attn_scores_30 -> layer_14_attn_softmax_30
	layer_14_attn_softmax_30 -> layer_14_attn_out_30
	layer_14_attn_out_30 -> layer_14_attn_allreduce_7
	layer_14_input -> layer_14_qkv_31
	layer_14_qkv_31 -> layer_14_attn_scores_31
	layer_14_attn_scores_31 -> layer_14_attn_softmax_31
	layer_14_attn_softmax_31 -> layer_14_attn_out_31
	layer_14_attn_out_31 -> layer_14_attn_allreduce_7
	layer_14_qkv_32 [label="Layer14_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_33 [label="Layer14_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_34 [label="Layer14_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_35 [label="Layer14_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_32 [label="Layer14_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_33 [label="Layer14_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_34 [label="Layer14_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_35 [label="Layer14_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_32 [label="Layer14_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_33 [label="Layer14_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_34 [label="Layer14_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_35 [label="Layer14_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_32 [label="Layer14_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_33 [label="Layer14_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_34 [label="Layer14_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_35 [label="Layer14_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_8 [label="Layer14_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_32
	layer_14_qkv_32 -> layer_14_attn_scores_32
	layer_14_attn_scores_32 -> layer_14_attn_softmax_32
	layer_14_attn_softmax_32 -> layer_14_attn_out_32
	layer_14_attn_out_32 -> layer_14_attn_allreduce_8
	layer_14_input -> layer_14_qkv_33
	layer_14_qkv_33 -> layer_14_attn_scores_33
	layer_14_attn_scores_33 -> layer_14_attn_softmax_33
	layer_14_attn_softmax_33 -> layer_14_attn_out_33
	layer_14_attn_out_33 -> layer_14_attn_allreduce_8
	layer_14_input -> layer_14_qkv_34
	layer_14_qkv_34 -> layer_14_attn_scores_34
	layer_14_attn_scores_34 -> layer_14_attn_softmax_34
	layer_14_attn_softmax_34 -> layer_14_attn_out_34
	layer_14_attn_out_34 -> layer_14_attn_allreduce_8
	layer_14_input -> layer_14_qkv_35
	layer_14_qkv_35 -> layer_14_attn_scores_35
	layer_14_attn_scores_35 -> layer_14_attn_softmax_35
	layer_14_attn_softmax_35 -> layer_14_attn_out_35
	layer_14_attn_out_35 -> layer_14_attn_allreduce_8
	layer_14_qkv_36 [label="Layer14_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_37 [label="Layer14_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_38 [label="Layer14_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_39 [label="Layer14_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_36 [label="Layer14_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_37 [label="Layer14_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_38 [label="Layer14_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_39 [label="Layer14_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_36 [label="Layer14_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_37 [label="Layer14_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_38 [label="Layer14_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_39 [label="Layer14_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_36 [label="Layer14_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_37 [label="Layer14_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_38 [label="Layer14_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_39 [label="Layer14_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_9 [label="Layer14_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_36
	layer_14_qkv_36 -> layer_14_attn_scores_36
	layer_14_attn_scores_36 -> layer_14_attn_softmax_36
	layer_14_attn_softmax_36 -> layer_14_attn_out_36
	layer_14_attn_out_36 -> layer_14_attn_allreduce_9
	layer_14_input -> layer_14_qkv_37
	layer_14_qkv_37 -> layer_14_attn_scores_37
	layer_14_attn_scores_37 -> layer_14_attn_softmax_37
	layer_14_attn_softmax_37 -> layer_14_attn_out_37
	layer_14_attn_out_37 -> layer_14_attn_allreduce_9
	layer_14_input -> layer_14_qkv_38
	layer_14_qkv_38 -> layer_14_attn_scores_38
	layer_14_attn_scores_38 -> layer_14_attn_softmax_38
	layer_14_attn_softmax_38 -> layer_14_attn_out_38
	layer_14_attn_out_38 -> layer_14_attn_allreduce_9
	layer_14_input -> layer_14_qkv_39
	layer_14_qkv_39 -> layer_14_attn_scores_39
	layer_14_attn_scores_39 -> layer_14_attn_softmax_39
	layer_14_attn_softmax_39 -> layer_14_attn_out_39
	layer_14_attn_out_39 -> layer_14_attn_allreduce_9
	layer_14_qkv_40 [label="Layer14_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_41 [label="Layer14_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_42 [label="Layer14_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_43 [label="Layer14_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_40 [label="Layer14_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_41 [label="Layer14_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_42 [label="Layer14_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_43 [label="Layer14_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_40 [label="Layer14_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_41 [label="Layer14_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_42 [label="Layer14_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_43 [label="Layer14_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_40 [label="Layer14_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_41 [label="Layer14_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_42 [label="Layer14_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_43 [label="Layer14_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_10 [label="Layer14_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_40
	layer_14_qkv_40 -> layer_14_attn_scores_40
	layer_14_attn_scores_40 -> layer_14_attn_softmax_40
	layer_14_attn_softmax_40 -> layer_14_attn_out_40
	layer_14_attn_out_40 -> layer_14_attn_allreduce_10
	layer_14_input -> layer_14_qkv_41
	layer_14_qkv_41 -> layer_14_attn_scores_41
	layer_14_attn_scores_41 -> layer_14_attn_softmax_41
	layer_14_attn_softmax_41 -> layer_14_attn_out_41
	layer_14_attn_out_41 -> layer_14_attn_allreduce_10
	layer_14_input -> layer_14_qkv_42
	layer_14_qkv_42 -> layer_14_attn_scores_42
	layer_14_attn_scores_42 -> layer_14_attn_softmax_42
	layer_14_attn_softmax_42 -> layer_14_attn_out_42
	layer_14_attn_out_42 -> layer_14_attn_allreduce_10
	layer_14_input -> layer_14_qkv_43
	layer_14_qkv_43 -> layer_14_attn_scores_43
	layer_14_attn_scores_43 -> layer_14_attn_softmax_43
	layer_14_attn_softmax_43 -> layer_14_attn_out_43
	layer_14_attn_out_43 -> layer_14_attn_allreduce_10
	layer_14_qkv_44 [label="Layer14_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_45 [label="Layer14_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_46 [label="Layer14_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_47 [label="Layer14_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_44 [label="Layer14_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_45 [label="Layer14_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_46 [label="Layer14_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_47 [label="Layer14_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_44 [label="Layer14_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_45 [label="Layer14_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_46 [label="Layer14_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_47 [label="Layer14_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_44 [label="Layer14_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_45 [label="Layer14_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_46 [label="Layer14_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_47 [label="Layer14_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_11 [label="Layer14_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_44
	layer_14_qkv_44 -> layer_14_attn_scores_44
	layer_14_attn_scores_44 -> layer_14_attn_softmax_44
	layer_14_attn_softmax_44 -> layer_14_attn_out_44
	layer_14_attn_out_44 -> layer_14_attn_allreduce_11
	layer_14_input -> layer_14_qkv_45
	layer_14_qkv_45 -> layer_14_attn_scores_45
	layer_14_attn_scores_45 -> layer_14_attn_softmax_45
	layer_14_attn_softmax_45 -> layer_14_attn_out_45
	layer_14_attn_out_45 -> layer_14_attn_allreduce_11
	layer_14_input -> layer_14_qkv_46
	layer_14_qkv_46 -> layer_14_attn_scores_46
	layer_14_attn_scores_46 -> layer_14_attn_softmax_46
	layer_14_attn_softmax_46 -> layer_14_attn_out_46
	layer_14_attn_out_46 -> layer_14_attn_allreduce_11
	layer_14_input -> layer_14_qkv_47
	layer_14_qkv_47 -> layer_14_attn_scores_47
	layer_14_attn_scores_47 -> layer_14_attn_softmax_47
	layer_14_attn_softmax_47 -> layer_14_attn_out_47
	layer_14_attn_out_47 -> layer_14_attn_allreduce_11
	layer_14_qkv_48 [label="Layer14_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_49 [label="Layer14_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_50 [label="Layer14_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_51 [label="Layer14_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_48 [label="Layer14_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_49 [label="Layer14_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_50 [label="Layer14_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_51 [label="Layer14_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_48 [label="Layer14_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_49 [label="Layer14_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_50 [label="Layer14_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_51 [label="Layer14_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_48 [label="Layer14_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_49 [label="Layer14_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_50 [label="Layer14_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_51 [label="Layer14_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_12 [label="Layer14_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_48
	layer_14_qkv_48 -> layer_14_attn_scores_48
	layer_14_attn_scores_48 -> layer_14_attn_softmax_48
	layer_14_attn_softmax_48 -> layer_14_attn_out_48
	layer_14_attn_out_48 -> layer_14_attn_allreduce_12
	layer_14_input -> layer_14_qkv_49
	layer_14_qkv_49 -> layer_14_attn_scores_49
	layer_14_attn_scores_49 -> layer_14_attn_softmax_49
	layer_14_attn_softmax_49 -> layer_14_attn_out_49
	layer_14_attn_out_49 -> layer_14_attn_allreduce_12
	layer_14_input -> layer_14_qkv_50
	layer_14_qkv_50 -> layer_14_attn_scores_50
	layer_14_attn_scores_50 -> layer_14_attn_softmax_50
	layer_14_attn_softmax_50 -> layer_14_attn_out_50
	layer_14_attn_out_50 -> layer_14_attn_allreduce_12
	layer_14_input -> layer_14_qkv_51
	layer_14_qkv_51 -> layer_14_attn_scores_51
	layer_14_attn_scores_51 -> layer_14_attn_softmax_51
	layer_14_attn_softmax_51 -> layer_14_attn_out_51
	layer_14_attn_out_51 -> layer_14_attn_allreduce_12
	layer_14_qkv_52 [label="Layer14_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_53 [label="Layer14_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_54 [label="Layer14_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_55 [label="Layer14_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_52 [label="Layer14_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_53 [label="Layer14_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_54 [label="Layer14_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_55 [label="Layer14_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_52 [label="Layer14_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_53 [label="Layer14_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_54 [label="Layer14_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_55 [label="Layer14_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_52 [label="Layer14_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_53 [label="Layer14_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_54 [label="Layer14_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_55 [label="Layer14_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_13 [label="Layer14_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_52
	layer_14_qkv_52 -> layer_14_attn_scores_52
	layer_14_attn_scores_52 -> layer_14_attn_softmax_52
	layer_14_attn_softmax_52 -> layer_14_attn_out_52
	layer_14_attn_out_52 -> layer_14_attn_allreduce_13
	layer_14_input -> layer_14_qkv_53
	layer_14_qkv_53 -> layer_14_attn_scores_53
	layer_14_attn_scores_53 -> layer_14_attn_softmax_53
	layer_14_attn_softmax_53 -> layer_14_attn_out_53
	layer_14_attn_out_53 -> layer_14_attn_allreduce_13
	layer_14_input -> layer_14_qkv_54
	layer_14_qkv_54 -> layer_14_attn_scores_54
	layer_14_attn_scores_54 -> layer_14_attn_softmax_54
	layer_14_attn_softmax_54 -> layer_14_attn_out_54
	layer_14_attn_out_54 -> layer_14_attn_allreduce_13
	layer_14_input -> layer_14_qkv_55
	layer_14_qkv_55 -> layer_14_attn_scores_55
	layer_14_attn_scores_55 -> layer_14_attn_softmax_55
	layer_14_attn_softmax_55 -> layer_14_attn_out_55
	layer_14_attn_out_55 -> layer_14_attn_allreduce_13
	layer_14_qkv_56 [label="Layer14_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_57 [label="Layer14_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_58 [label="Layer14_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_59 [label="Layer14_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_56 [label="Layer14_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_57 [label="Layer14_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_58 [label="Layer14_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_59 [label="Layer14_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_56 [label="Layer14_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_57 [label="Layer14_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_58 [label="Layer14_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_59 [label="Layer14_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_56 [label="Layer14_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_57 [label="Layer14_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_58 [label="Layer14_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_59 [label="Layer14_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_14 [label="Layer14_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_56
	layer_14_qkv_56 -> layer_14_attn_scores_56
	layer_14_attn_scores_56 -> layer_14_attn_softmax_56
	layer_14_attn_softmax_56 -> layer_14_attn_out_56
	layer_14_attn_out_56 -> layer_14_attn_allreduce_14
	layer_14_input -> layer_14_qkv_57
	layer_14_qkv_57 -> layer_14_attn_scores_57
	layer_14_attn_scores_57 -> layer_14_attn_softmax_57
	layer_14_attn_softmax_57 -> layer_14_attn_out_57
	layer_14_attn_out_57 -> layer_14_attn_allreduce_14
	layer_14_input -> layer_14_qkv_58
	layer_14_qkv_58 -> layer_14_attn_scores_58
	layer_14_attn_scores_58 -> layer_14_attn_softmax_58
	layer_14_attn_softmax_58 -> layer_14_attn_out_58
	layer_14_attn_out_58 -> layer_14_attn_allreduce_14
	layer_14_input -> layer_14_qkv_59
	layer_14_qkv_59 -> layer_14_attn_scores_59
	layer_14_attn_scores_59 -> layer_14_attn_softmax_59
	layer_14_attn_softmax_59 -> layer_14_attn_out_59
	layer_14_attn_out_59 -> layer_14_attn_allreduce_14
	layer_14_qkv_60 [label="Layer14_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_61 [label="Layer14_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_62 [label="Layer14_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_qkv_63 [label="Layer14_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_60 [label="Layer14_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_61 [label="Layer14_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_62 [label="Layer14_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_scores_63 [label="Layer14_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_60 [label="Layer14_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_61 [label="Layer14_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_62 [label="Layer14_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_softmax_63 [label="Layer14_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_60 [label="Layer14_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_61 [label="Layer14_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_62 [label="Layer14_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_out_63 [label="Layer14_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_15 [label="Layer14_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_input -> layer_14_qkv_60
	layer_14_qkv_60 -> layer_14_attn_scores_60
	layer_14_attn_scores_60 -> layer_14_attn_softmax_60
	layer_14_attn_softmax_60 -> layer_14_attn_out_60
	layer_14_attn_out_60 -> layer_14_attn_allreduce_15
	layer_14_input -> layer_14_qkv_61
	layer_14_qkv_61 -> layer_14_attn_scores_61
	layer_14_attn_scores_61 -> layer_14_attn_softmax_61
	layer_14_attn_softmax_61 -> layer_14_attn_out_61
	layer_14_attn_out_61 -> layer_14_attn_allreduce_15
	layer_14_input -> layer_14_qkv_62
	layer_14_qkv_62 -> layer_14_attn_scores_62
	layer_14_attn_scores_62 -> layer_14_attn_softmax_62
	layer_14_attn_softmax_62 -> layer_14_attn_out_62
	layer_14_attn_out_62 -> layer_14_attn_allreduce_15
	layer_14_input -> layer_14_qkv_63
	layer_14_qkv_63 -> layer_14_attn_scores_63
	layer_14_attn_scores_63 -> layer_14_attn_softmax_63
	layer_14_attn_softmax_63 -> layer_14_attn_out_63
	layer_14_attn_out_63 -> layer_14_attn_allreduce_15
	layer_14_gate_0 [label="Layer14_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_4 [label="Layer14_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_8 [label="Layer14_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_12 [label="Layer14_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_16 [label="Layer14_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_20 [label="Layer14_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_24 [label="Layer14_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_28 [label="Layer14_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_32 [label="Layer14_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_36 [label="Layer14_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_40 [label="Layer14_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_44 [label="Layer14_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_48 [label="Layer14_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_52 [label="Layer14_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_56 [label="Layer14_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_gate_60 [label="Layer14_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_14_alltoall [label="Layer14_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_0 [label="Layer14_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_1 [label="Layer14_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_2 [label="Layer14_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_3 [label="Layer14_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_0 [label="Layer14_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_1 [label="Layer14_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_2 [label="Layer14_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_3 [label="Layer14_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_0 [label="Layer14_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_1 [label="Layer14_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_2 [label="Layer14_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_3 [label="Layer14_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_0 [label="Layer14_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_4 [label="Layer14_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_5 [label="Layer14_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_6 [label="Layer14_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_7 [label="Layer14_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_4 [label="Layer14_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_5 [label="Layer14_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_6 [label="Layer14_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_7 [label="Layer14_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_4 [label="Layer14_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_5 [label="Layer14_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_6 [label="Layer14_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_7 [label="Layer14_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_1 [label="Layer14_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_8 [label="Layer14_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_9 [label="Layer14_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_10 [label="Layer14_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_11 [label="Layer14_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_8 [label="Layer14_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_9 [label="Layer14_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_10 [label="Layer14_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_11 [label="Layer14_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_8 [label="Layer14_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_9 [label="Layer14_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_10 [label="Layer14_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_11 [label="Layer14_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_2 [label="Layer14_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_12 [label="Layer14_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_13 [label="Layer14_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_14 [label="Layer14_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_15 [label="Layer14_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_12 [label="Layer14_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_13 [label="Layer14_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_14 [label="Layer14_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_15 [label="Layer14_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_12 [label="Layer14_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_13 [label="Layer14_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_14 [label="Layer14_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_15 [label="Layer14_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_3 [label="Layer14_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_16 [label="Layer14_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_17 [label="Layer14_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_18 [label="Layer14_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_19 [label="Layer14_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_16 [label="Layer14_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_17 [label="Layer14_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_18 [label="Layer14_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_19 [label="Layer14_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_16 [label="Layer14_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_17 [label="Layer14_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_18 [label="Layer14_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_19 [label="Layer14_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_4 [label="Layer14_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_20 [label="Layer14_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_21 [label="Layer14_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_22 [label="Layer14_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_23 [label="Layer14_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_20 [label="Layer14_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_21 [label="Layer14_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_22 [label="Layer14_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_23 [label="Layer14_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_20 [label="Layer14_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_21 [label="Layer14_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_22 [label="Layer14_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_23 [label="Layer14_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_5 [label="Layer14_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_24 [label="Layer14_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_25 [label="Layer14_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_26 [label="Layer14_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_27 [label="Layer14_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_24 [label="Layer14_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_25 [label="Layer14_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_26 [label="Layer14_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_27 [label="Layer14_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_24 [label="Layer14_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_25 [label="Layer14_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_26 [label="Layer14_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_27 [label="Layer14_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_6 [label="Layer14_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_28 [label="Layer14_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_29 [label="Layer14_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_30 [label="Layer14_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_31 [label="Layer14_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_28 [label="Layer14_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_29 [label="Layer14_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_30 [label="Layer14_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_31 [label="Layer14_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_28 [label="Layer14_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_29 [label="Layer14_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_30 [label="Layer14_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_31 [label="Layer14_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_7 [label="Layer14_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_32 [label="Layer14_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_33 [label="Layer14_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_34 [label="Layer14_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_35 [label="Layer14_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_32 [label="Layer14_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_33 [label="Layer14_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_34 [label="Layer14_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_35 [label="Layer14_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_32 [label="Layer14_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_33 [label="Layer14_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_34 [label="Layer14_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_35 [label="Layer14_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_8 [label="Layer14_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_36 [label="Layer14_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_37 [label="Layer14_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_38 [label="Layer14_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_39 [label="Layer14_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_36 [label="Layer14_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_37 [label="Layer14_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_38 [label="Layer14_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_39 [label="Layer14_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_36 [label="Layer14_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_37 [label="Layer14_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_38 [label="Layer14_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_39 [label="Layer14_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_9 [label="Layer14_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_40 [label="Layer14_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_41 [label="Layer14_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_42 [label="Layer14_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_43 [label="Layer14_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_40 [label="Layer14_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_41 [label="Layer14_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_42 [label="Layer14_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_43 [label="Layer14_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_40 [label="Layer14_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_41 [label="Layer14_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_42 [label="Layer14_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_43 [label="Layer14_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_10 [label="Layer14_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_44 [label="Layer14_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_45 [label="Layer14_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_46 [label="Layer14_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_47 [label="Layer14_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_44 [label="Layer14_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_45 [label="Layer14_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_46 [label="Layer14_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_47 [label="Layer14_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_44 [label="Layer14_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_45 [label="Layer14_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_46 [label="Layer14_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_47 [label="Layer14_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_11 [label="Layer14_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_48 [label="Layer14_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_49 [label="Layer14_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_50 [label="Layer14_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_51 [label="Layer14_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_48 [label="Layer14_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_49 [label="Layer14_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_50 [label="Layer14_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_51 [label="Layer14_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_48 [label="Layer14_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_49 [label="Layer14_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_50 [label="Layer14_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_51 [label="Layer14_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_12 [label="Layer14_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_52 [label="Layer14_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_53 [label="Layer14_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_54 [label="Layer14_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_55 [label="Layer14_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_52 [label="Layer14_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_53 [label="Layer14_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_54 [label="Layer14_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_55 [label="Layer14_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_52 [label="Layer14_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_53 [label="Layer14_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_54 [label="Layer14_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_55 [label="Layer14_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_13 [label="Layer14_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_56 [label="Layer14_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_57 [label="Layer14_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_58 [label="Layer14_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_59 [label="Layer14_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_56 [label="Layer14_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_57 [label="Layer14_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_58 [label="Layer14_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_59 [label="Layer14_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_56 [label="Layer14_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_57 [label="Layer14_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_58 [label="Layer14_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_59 [label="Layer14_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_14 [label="Layer14_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert1_60 [label="Layer14_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_61 [label="Layer14_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_62 [label="Layer14_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert1_63 [label="Layer14_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_60 [label="Layer14_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_61 [label="Layer14_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_62 [label="Layer14_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert_act_63 [label="Layer14_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_14_expert2_60 [label="Layer14_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_61 [label="Layer14_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_62 [label="Layer14_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert2_63 [label="Layer14_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_14_expert_allreduce_15 [label="Layer14_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_14_expert_agg [label="Layer14_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_14_norm [label="Layer14_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_14_attn_allreduce_0 -> layer_14_gate_0 [style=dashed]
	layer_14_gate_0 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_0
	layer_14_expert1_0 -> layer_14_expert_act_0
	layer_14_expert_act_0 -> layer_14_expert2_0
	layer_14_expert2_0 -> layer_14_expert_allreduce_0
	layer_14_alltoall -> layer_14_expert1_1
	layer_14_expert1_1 -> layer_14_expert_act_1
	layer_14_expert_act_1 -> layer_14_expert2_1
	layer_14_expert2_1 -> layer_14_expert_allreduce_0
	layer_14_alltoall -> layer_14_expert1_2
	layer_14_expert1_2 -> layer_14_expert_act_2
	layer_14_expert_act_2 -> layer_14_expert2_2
	layer_14_expert2_2 -> layer_14_expert_allreduce_0
	layer_14_alltoall -> layer_14_expert1_3
	layer_14_expert1_3 -> layer_14_expert_act_3
	layer_14_expert_act_3 -> layer_14_expert2_3
	layer_14_expert2_3 -> layer_14_expert_allreduce_0
	layer_14_expert_allreduce_0 -> layer_14_expert_agg
	layer_14_attn_allreduce_1 -> layer_14_gate_4 [style=dashed]
	layer_14_gate_4 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_4
	layer_14_expert1_4 -> layer_14_expert_act_4
	layer_14_expert_act_4 -> layer_14_expert2_4
	layer_14_expert2_4 -> layer_14_expert_allreduce_1
	layer_14_alltoall -> layer_14_expert1_5
	layer_14_expert1_5 -> layer_14_expert_act_5
	layer_14_expert_act_5 -> layer_14_expert2_5
	layer_14_expert2_5 -> layer_14_expert_allreduce_1
	layer_14_alltoall -> layer_14_expert1_6
	layer_14_expert1_6 -> layer_14_expert_act_6
	layer_14_expert_act_6 -> layer_14_expert2_6
	layer_14_expert2_6 -> layer_14_expert_allreduce_1
	layer_14_alltoall -> layer_14_expert1_7
	layer_14_expert1_7 -> layer_14_expert_act_7
	layer_14_expert_act_7 -> layer_14_expert2_7
	layer_14_expert2_7 -> layer_14_expert_allreduce_1
	layer_14_expert_allreduce_1 -> layer_14_expert_agg
	layer_14_attn_allreduce_2 -> layer_14_gate_8 [style=dashed]
	layer_14_gate_8 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_8
	layer_14_expert1_8 -> layer_14_expert_act_8
	layer_14_expert_act_8 -> layer_14_expert2_8
	layer_14_expert2_8 -> layer_14_expert_allreduce_2
	layer_14_alltoall -> layer_14_expert1_9
	layer_14_expert1_9 -> layer_14_expert_act_9
	layer_14_expert_act_9 -> layer_14_expert2_9
	layer_14_expert2_9 -> layer_14_expert_allreduce_2
	layer_14_alltoall -> layer_14_expert1_10
	layer_14_expert1_10 -> layer_14_expert_act_10
	layer_14_expert_act_10 -> layer_14_expert2_10
	layer_14_expert2_10 -> layer_14_expert_allreduce_2
	layer_14_alltoall -> layer_14_expert1_11
	layer_14_expert1_11 -> layer_14_expert_act_11
	layer_14_expert_act_11 -> layer_14_expert2_11
	layer_14_expert2_11 -> layer_14_expert_allreduce_2
	layer_14_expert_allreduce_2 -> layer_14_expert_agg
	layer_14_attn_allreduce_3 -> layer_14_gate_12 [style=dashed]
	layer_14_gate_12 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_12
	layer_14_expert1_12 -> layer_14_expert_act_12
	layer_14_expert_act_12 -> layer_14_expert2_12
	layer_14_expert2_12 -> layer_14_expert_allreduce_3
	layer_14_alltoall -> layer_14_expert1_13
	layer_14_expert1_13 -> layer_14_expert_act_13
	layer_14_expert_act_13 -> layer_14_expert2_13
	layer_14_expert2_13 -> layer_14_expert_allreduce_3
	layer_14_alltoall -> layer_14_expert1_14
	layer_14_expert1_14 -> layer_14_expert_act_14
	layer_14_expert_act_14 -> layer_14_expert2_14
	layer_14_expert2_14 -> layer_14_expert_allreduce_3
	layer_14_alltoall -> layer_14_expert1_15
	layer_14_expert1_15 -> layer_14_expert_act_15
	layer_14_expert_act_15 -> layer_14_expert2_15
	layer_14_expert2_15 -> layer_14_expert_allreduce_3
	layer_14_expert_allreduce_3 -> layer_14_expert_agg
	layer_14_attn_allreduce_4 -> layer_14_gate_16 [style=dashed]
	layer_14_gate_16 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_16
	layer_14_expert1_16 -> layer_14_expert_act_16
	layer_14_expert_act_16 -> layer_14_expert2_16
	layer_14_expert2_16 -> layer_14_expert_allreduce_4
	layer_14_alltoall -> layer_14_expert1_17
	layer_14_expert1_17 -> layer_14_expert_act_17
	layer_14_expert_act_17 -> layer_14_expert2_17
	layer_14_expert2_17 -> layer_14_expert_allreduce_4
	layer_14_alltoall -> layer_14_expert1_18
	layer_14_expert1_18 -> layer_14_expert_act_18
	layer_14_expert_act_18 -> layer_14_expert2_18
	layer_14_expert2_18 -> layer_14_expert_allreduce_4
	layer_14_alltoall -> layer_14_expert1_19
	layer_14_expert1_19 -> layer_14_expert_act_19
	layer_14_expert_act_19 -> layer_14_expert2_19
	layer_14_expert2_19 -> layer_14_expert_allreduce_4
	layer_14_expert_allreduce_4 -> layer_14_expert_agg
	layer_14_attn_allreduce_5 -> layer_14_gate_20 [style=dashed]
	layer_14_gate_20 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_20
	layer_14_expert1_20 -> layer_14_expert_act_20
	layer_14_expert_act_20 -> layer_14_expert2_20
	layer_14_expert2_20 -> layer_14_expert_allreduce_5
	layer_14_alltoall -> layer_14_expert1_21
	layer_14_expert1_21 -> layer_14_expert_act_21
	layer_14_expert_act_21 -> layer_14_expert2_21
	layer_14_expert2_21 -> layer_14_expert_allreduce_5
	layer_14_alltoall -> layer_14_expert1_22
	layer_14_expert1_22 -> layer_14_expert_act_22
	layer_14_expert_act_22 -> layer_14_expert2_22
	layer_14_expert2_22 -> layer_14_expert_allreduce_5
	layer_14_alltoall -> layer_14_expert1_23
	layer_14_expert1_23 -> layer_14_expert_act_23
	layer_14_expert_act_23 -> layer_14_expert2_23
	layer_14_expert2_23 -> layer_14_expert_allreduce_5
	layer_14_expert_allreduce_5 -> layer_14_expert_agg
	layer_14_attn_allreduce_6 -> layer_14_gate_24 [style=dashed]
	layer_14_gate_24 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_24
	layer_14_expert1_24 -> layer_14_expert_act_24
	layer_14_expert_act_24 -> layer_14_expert2_24
	layer_14_expert2_24 -> layer_14_expert_allreduce_6
	layer_14_alltoall -> layer_14_expert1_25
	layer_14_expert1_25 -> layer_14_expert_act_25
	layer_14_expert_act_25 -> layer_14_expert2_25
	layer_14_expert2_25 -> layer_14_expert_allreduce_6
	layer_14_alltoall -> layer_14_expert1_26
	layer_14_expert1_26 -> layer_14_expert_act_26
	layer_14_expert_act_26 -> layer_14_expert2_26
	layer_14_expert2_26 -> layer_14_expert_allreduce_6
	layer_14_alltoall -> layer_14_expert1_27
	layer_14_expert1_27 -> layer_14_expert_act_27
	layer_14_expert_act_27 -> layer_14_expert2_27
	layer_14_expert2_27 -> layer_14_expert_allreduce_6
	layer_14_expert_allreduce_6 -> layer_14_expert_agg
	layer_14_attn_allreduce_7 -> layer_14_gate_28 [style=dashed]
	layer_14_gate_28 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_28
	layer_14_expert1_28 -> layer_14_expert_act_28
	layer_14_expert_act_28 -> layer_14_expert2_28
	layer_14_expert2_28 -> layer_14_expert_allreduce_7
	layer_14_alltoall -> layer_14_expert1_29
	layer_14_expert1_29 -> layer_14_expert_act_29
	layer_14_expert_act_29 -> layer_14_expert2_29
	layer_14_expert2_29 -> layer_14_expert_allreduce_7
	layer_14_alltoall -> layer_14_expert1_30
	layer_14_expert1_30 -> layer_14_expert_act_30
	layer_14_expert_act_30 -> layer_14_expert2_30
	layer_14_expert2_30 -> layer_14_expert_allreduce_7
	layer_14_alltoall -> layer_14_expert1_31
	layer_14_expert1_31 -> layer_14_expert_act_31
	layer_14_expert_act_31 -> layer_14_expert2_31
	layer_14_expert2_31 -> layer_14_expert_allreduce_7
	layer_14_expert_allreduce_7 -> layer_14_expert_agg
	layer_14_attn_allreduce_8 -> layer_14_gate_32 [style=dashed]
	layer_14_gate_32 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_32
	layer_14_expert1_32 -> layer_14_expert_act_32
	layer_14_expert_act_32 -> layer_14_expert2_32
	layer_14_expert2_32 -> layer_14_expert_allreduce_8
	layer_14_alltoall -> layer_14_expert1_33
	layer_14_expert1_33 -> layer_14_expert_act_33
	layer_14_expert_act_33 -> layer_14_expert2_33
	layer_14_expert2_33 -> layer_14_expert_allreduce_8
	layer_14_alltoall -> layer_14_expert1_34
	layer_14_expert1_34 -> layer_14_expert_act_34
	layer_14_expert_act_34 -> layer_14_expert2_34
	layer_14_expert2_34 -> layer_14_expert_allreduce_8
	layer_14_alltoall -> layer_14_expert1_35
	layer_14_expert1_35 -> layer_14_expert_act_35
	layer_14_expert_act_35 -> layer_14_expert2_35
	layer_14_expert2_35 -> layer_14_expert_allreduce_8
	layer_14_expert_allreduce_8 -> layer_14_expert_agg
	layer_14_attn_allreduce_9 -> layer_14_gate_36 [style=dashed]
	layer_14_gate_36 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_36
	layer_14_expert1_36 -> layer_14_expert_act_36
	layer_14_expert_act_36 -> layer_14_expert2_36
	layer_14_expert2_36 -> layer_14_expert_allreduce_9
	layer_14_alltoall -> layer_14_expert1_37
	layer_14_expert1_37 -> layer_14_expert_act_37
	layer_14_expert_act_37 -> layer_14_expert2_37
	layer_14_expert2_37 -> layer_14_expert_allreduce_9
	layer_14_alltoall -> layer_14_expert1_38
	layer_14_expert1_38 -> layer_14_expert_act_38
	layer_14_expert_act_38 -> layer_14_expert2_38
	layer_14_expert2_38 -> layer_14_expert_allreduce_9
	layer_14_alltoall -> layer_14_expert1_39
	layer_14_expert1_39 -> layer_14_expert_act_39
	layer_14_expert_act_39 -> layer_14_expert2_39
	layer_14_expert2_39 -> layer_14_expert_allreduce_9
	layer_14_expert_allreduce_9 -> layer_14_expert_agg
	layer_14_attn_allreduce_10 -> layer_14_gate_40 [style=dashed]
	layer_14_gate_40 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_40
	layer_14_expert1_40 -> layer_14_expert_act_40
	layer_14_expert_act_40 -> layer_14_expert2_40
	layer_14_expert2_40 -> layer_14_expert_allreduce_10
	layer_14_alltoall -> layer_14_expert1_41
	layer_14_expert1_41 -> layer_14_expert_act_41
	layer_14_expert_act_41 -> layer_14_expert2_41
	layer_14_expert2_41 -> layer_14_expert_allreduce_10
	layer_14_alltoall -> layer_14_expert1_42
	layer_14_expert1_42 -> layer_14_expert_act_42
	layer_14_expert_act_42 -> layer_14_expert2_42
	layer_14_expert2_42 -> layer_14_expert_allreduce_10
	layer_14_alltoall -> layer_14_expert1_43
	layer_14_expert1_43 -> layer_14_expert_act_43
	layer_14_expert_act_43 -> layer_14_expert2_43
	layer_14_expert2_43 -> layer_14_expert_allreduce_10
	layer_14_expert_allreduce_10 -> layer_14_expert_agg
	layer_14_attn_allreduce_11 -> layer_14_gate_44 [style=dashed]
	layer_14_gate_44 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_44
	layer_14_expert1_44 -> layer_14_expert_act_44
	layer_14_expert_act_44 -> layer_14_expert2_44
	layer_14_expert2_44 -> layer_14_expert_allreduce_11
	layer_14_alltoall -> layer_14_expert1_45
	layer_14_expert1_45 -> layer_14_expert_act_45
	layer_14_expert_act_45 -> layer_14_expert2_45
	layer_14_expert2_45 -> layer_14_expert_allreduce_11
	layer_14_alltoall -> layer_14_expert1_46
	layer_14_expert1_46 -> layer_14_expert_act_46
	layer_14_expert_act_46 -> layer_14_expert2_46
	layer_14_expert2_46 -> layer_14_expert_allreduce_11
	layer_14_alltoall -> layer_14_expert1_47
	layer_14_expert1_47 -> layer_14_expert_act_47
	layer_14_expert_act_47 -> layer_14_expert2_47
	layer_14_expert2_47 -> layer_14_expert_allreduce_11
	layer_14_expert_allreduce_11 -> layer_14_expert_agg
	layer_14_attn_allreduce_12 -> layer_14_gate_48 [style=dashed]
	layer_14_gate_48 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_48
	layer_14_expert1_48 -> layer_14_expert_act_48
	layer_14_expert_act_48 -> layer_14_expert2_48
	layer_14_expert2_48 -> layer_14_expert_allreduce_12
	layer_14_alltoall -> layer_14_expert1_49
	layer_14_expert1_49 -> layer_14_expert_act_49
	layer_14_expert_act_49 -> layer_14_expert2_49
	layer_14_expert2_49 -> layer_14_expert_allreduce_12
	layer_14_alltoall -> layer_14_expert1_50
	layer_14_expert1_50 -> layer_14_expert_act_50
	layer_14_expert_act_50 -> layer_14_expert2_50
	layer_14_expert2_50 -> layer_14_expert_allreduce_12
	layer_14_alltoall -> layer_14_expert1_51
	layer_14_expert1_51 -> layer_14_expert_act_51
	layer_14_expert_act_51 -> layer_14_expert2_51
	layer_14_expert2_51 -> layer_14_expert_allreduce_12
	layer_14_expert_allreduce_12 -> layer_14_expert_agg
	layer_14_attn_allreduce_13 -> layer_14_gate_52 [style=dashed]
	layer_14_gate_52 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_52
	layer_14_expert1_52 -> layer_14_expert_act_52
	layer_14_expert_act_52 -> layer_14_expert2_52
	layer_14_expert2_52 -> layer_14_expert_allreduce_13
	layer_14_alltoall -> layer_14_expert1_53
	layer_14_expert1_53 -> layer_14_expert_act_53
	layer_14_expert_act_53 -> layer_14_expert2_53
	layer_14_expert2_53 -> layer_14_expert_allreduce_13
	layer_14_alltoall -> layer_14_expert1_54
	layer_14_expert1_54 -> layer_14_expert_act_54
	layer_14_expert_act_54 -> layer_14_expert2_54
	layer_14_expert2_54 -> layer_14_expert_allreduce_13
	layer_14_alltoall -> layer_14_expert1_55
	layer_14_expert1_55 -> layer_14_expert_act_55
	layer_14_expert_act_55 -> layer_14_expert2_55
	layer_14_expert2_55 -> layer_14_expert_allreduce_13
	layer_14_expert_allreduce_13 -> layer_14_expert_agg
	layer_14_attn_allreduce_14 -> layer_14_gate_56 [style=dashed]
	layer_14_gate_56 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_56
	layer_14_expert1_56 -> layer_14_expert_act_56
	layer_14_expert_act_56 -> layer_14_expert2_56
	layer_14_expert2_56 -> layer_14_expert_allreduce_14
	layer_14_alltoall -> layer_14_expert1_57
	layer_14_expert1_57 -> layer_14_expert_act_57
	layer_14_expert_act_57 -> layer_14_expert2_57
	layer_14_expert2_57 -> layer_14_expert_allreduce_14
	layer_14_alltoall -> layer_14_expert1_58
	layer_14_expert1_58 -> layer_14_expert_act_58
	layer_14_expert_act_58 -> layer_14_expert2_58
	layer_14_expert2_58 -> layer_14_expert_allreduce_14
	layer_14_alltoall -> layer_14_expert1_59
	layer_14_expert1_59 -> layer_14_expert_act_59
	layer_14_expert_act_59 -> layer_14_expert2_59
	layer_14_expert2_59 -> layer_14_expert_allreduce_14
	layer_14_expert_allreduce_14 -> layer_14_expert_agg
	layer_14_attn_allreduce_15 -> layer_14_gate_60 [style=dashed]
	layer_14_gate_60 -> layer_14_alltoall [style=dashed]
	layer_14_alltoall -> layer_14_expert1_60
	layer_14_expert1_60 -> layer_14_expert_act_60
	layer_14_expert_act_60 -> layer_14_expert2_60
	layer_14_expert2_60 -> layer_14_expert_allreduce_15
	layer_14_alltoall -> layer_14_expert1_61
	layer_14_expert1_61 -> layer_14_expert_act_61
	layer_14_expert_act_61 -> layer_14_expert2_61
	layer_14_expert2_61 -> layer_14_expert_allreduce_15
	layer_14_alltoall -> layer_14_expert1_62
	layer_14_expert1_62 -> layer_14_expert_act_62
	layer_14_expert_act_62 -> layer_14_expert2_62
	layer_14_expert2_62 -> layer_14_expert_allreduce_15
	layer_14_alltoall -> layer_14_expert1_63
	layer_14_expert1_63 -> layer_14_expert_act_63
	layer_14_expert_act_63 -> layer_14_expert2_63
	layer_14_expert2_63 -> layer_14_expert_allreduce_15
	layer_14_expert_allreduce_15 -> layer_14_expert_agg
	layer_14_expert_agg -> layer_14_norm
	layer_14_norm -> layer_15_input
	layer_15_input [label="Layer15_Input_Aggregate\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_15_qkv_0 [label="Layer15_QKV_Proj_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_1 [label="Layer15_QKV_Proj_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_2 [label="Layer15_QKV_Proj_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_3 [label="Layer15_QKV_Proj_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_0 [label="Layer15_Attention_Scores_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_1 [label="Layer15_Attention_Scores_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_2 [label="Layer15_Attention_Scores_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_3 [label="Layer15_Attention_Scores_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_0 [label="Layer15_Attention_Softmax_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_1 [label="Layer15_Attention_Softmax_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_2 [label="Layer15_Attention_Softmax_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_3 [label="Layer15_Attention_Softmax_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_0 [label="Layer15_Attention_Output_GPU0\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_1 [label="Layer15_Attention_Output_GPU1\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_2 [label="Layer15_Attention_Output_GPU2\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_3 [label="Layer15_Attention_Output_GPU3\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_0 [label="Layer15_Attention_AllReduce_ExpertGroup0\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_0
	layer_15_qkv_0 -> layer_15_attn_scores_0
	layer_15_attn_scores_0 -> layer_15_attn_softmax_0
	layer_15_attn_softmax_0 -> layer_15_attn_out_0
	layer_15_attn_out_0 -> layer_15_attn_allreduce_0
	layer_15_input -> layer_15_qkv_1
	layer_15_qkv_1 -> layer_15_attn_scores_1
	layer_15_attn_scores_1 -> layer_15_attn_softmax_1
	layer_15_attn_softmax_1 -> layer_15_attn_out_1
	layer_15_attn_out_1 -> layer_15_attn_allreduce_0
	layer_15_input -> layer_15_qkv_2
	layer_15_qkv_2 -> layer_15_attn_scores_2
	layer_15_attn_scores_2 -> layer_15_attn_softmax_2
	layer_15_attn_softmax_2 -> layer_15_attn_out_2
	layer_15_attn_out_2 -> layer_15_attn_allreduce_0
	layer_15_input -> layer_15_qkv_3
	layer_15_qkv_3 -> layer_15_attn_scores_3
	layer_15_attn_scores_3 -> layer_15_attn_softmax_3
	layer_15_attn_softmax_3 -> layer_15_attn_out_3
	layer_15_attn_out_3 -> layer_15_attn_allreduce_0
	layer_15_qkv_4 [label="Layer15_QKV_Proj_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_5 [label="Layer15_QKV_Proj_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_6 [label="Layer15_QKV_Proj_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_7 [label="Layer15_QKV_Proj_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_4 [label="Layer15_Attention_Scores_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_5 [label="Layer15_Attention_Scores_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_6 [label="Layer15_Attention_Scores_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_7 [label="Layer15_Attention_Scores_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_4 [label="Layer15_Attention_Softmax_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_5 [label="Layer15_Attention_Softmax_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_6 [label="Layer15_Attention_Softmax_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_7 [label="Layer15_Attention_Softmax_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_4 [label="Layer15_Attention_Output_GPU4\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_5 [label="Layer15_Attention_Output_GPU5\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_6 [label="Layer15_Attention_Output_GPU6\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_7 [label="Layer15_Attention_Output_GPU7\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_1 [label="Layer15_Attention_AllReduce_ExpertGroup1\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_4
	layer_15_qkv_4 -> layer_15_attn_scores_4
	layer_15_attn_scores_4 -> layer_15_attn_softmax_4
	layer_15_attn_softmax_4 -> layer_15_attn_out_4
	layer_15_attn_out_4 -> layer_15_attn_allreduce_1
	layer_15_input -> layer_15_qkv_5
	layer_15_qkv_5 -> layer_15_attn_scores_5
	layer_15_attn_scores_5 -> layer_15_attn_softmax_5
	layer_15_attn_softmax_5 -> layer_15_attn_out_5
	layer_15_attn_out_5 -> layer_15_attn_allreduce_1
	layer_15_input -> layer_15_qkv_6
	layer_15_qkv_6 -> layer_15_attn_scores_6
	layer_15_attn_scores_6 -> layer_15_attn_softmax_6
	layer_15_attn_softmax_6 -> layer_15_attn_out_6
	layer_15_attn_out_6 -> layer_15_attn_allreduce_1
	layer_15_input -> layer_15_qkv_7
	layer_15_qkv_7 -> layer_15_attn_scores_7
	layer_15_attn_scores_7 -> layer_15_attn_softmax_7
	layer_15_attn_softmax_7 -> layer_15_attn_out_7
	layer_15_attn_out_7 -> layer_15_attn_allreduce_1
	layer_15_qkv_8 [label="Layer15_QKV_Proj_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_9 [label="Layer15_QKV_Proj_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_10 [label="Layer15_QKV_Proj_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_11 [label="Layer15_QKV_Proj_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_8 [label="Layer15_Attention_Scores_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_9 [label="Layer15_Attention_Scores_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_10 [label="Layer15_Attention_Scores_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_11 [label="Layer15_Attention_Scores_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_8 [label="Layer15_Attention_Softmax_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_9 [label="Layer15_Attention_Softmax_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_10 [label="Layer15_Attention_Softmax_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_11 [label="Layer15_Attention_Softmax_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_8 [label="Layer15_Attention_Output_GPU8\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_9 [label="Layer15_Attention_Output_GPU9\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_10 [label="Layer15_Attention_Output_GPU10\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_11 [label="Layer15_Attention_Output_GPU11\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_2 [label="Layer15_Attention_AllReduce_ExpertGroup2\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_8
	layer_15_qkv_8 -> layer_15_attn_scores_8
	layer_15_attn_scores_8 -> layer_15_attn_softmax_8
	layer_15_attn_softmax_8 -> layer_15_attn_out_8
	layer_15_attn_out_8 -> layer_15_attn_allreduce_2
	layer_15_input -> layer_15_qkv_9
	layer_15_qkv_9 -> layer_15_attn_scores_9
	layer_15_attn_scores_9 -> layer_15_attn_softmax_9
	layer_15_attn_softmax_9 -> layer_15_attn_out_9
	layer_15_attn_out_9 -> layer_15_attn_allreduce_2
	layer_15_input -> layer_15_qkv_10
	layer_15_qkv_10 -> layer_15_attn_scores_10
	layer_15_attn_scores_10 -> layer_15_attn_softmax_10
	layer_15_attn_softmax_10 -> layer_15_attn_out_10
	layer_15_attn_out_10 -> layer_15_attn_allreduce_2
	layer_15_input -> layer_15_qkv_11
	layer_15_qkv_11 -> layer_15_attn_scores_11
	layer_15_attn_scores_11 -> layer_15_attn_softmax_11
	layer_15_attn_softmax_11 -> layer_15_attn_out_11
	layer_15_attn_out_11 -> layer_15_attn_allreduce_2
	layer_15_qkv_12 [label="Layer15_QKV_Proj_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_13 [label="Layer15_QKV_Proj_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_14 [label="Layer15_QKV_Proj_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_15 [label="Layer15_QKV_Proj_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_12 [label="Layer15_Attention_Scores_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_13 [label="Layer15_Attention_Scores_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_14 [label="Layer15_Attention_Scores_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_15 [label="Layer15_Attention_Scores_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_12 [label="Layer15_Attention_Softmax_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_13 [label="Layer15_Attention_Softmax_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_14 [label="Layer15_Attention_Softmax_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_15 [label="Layer15_Attention_Softmax_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_12 [label="Layer15_Attention_Output_GPU12\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_13 [label="Layer15_Attention_Output_GPU13\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_14 [label="Layer15_Attention_Output_GPU14\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_15 [label="Layer15_Attention_Output_GPU15\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_3 [label="Layer15_Attention_AllReduce_ExpertGroup3\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_12
	layer_15_qkv_12 -> layer_15_attn_scores_12
	layer_15_attn_scores_12 -> layer_15_attn_softmax_12
	layer_15_attn_softmax_12 -> layer_15_attn_out_12
	layer_15_attn_out_12 -> layer_15_attn_allreduce_3
	layer_15_input -> layer_15_qkv_13
	layer_15_qkv_13 -> layer_15_attn_scores_13
	layer_15_attn_scores_13 -> layer_15_attn_softmax_13
	layer_15_attn_softmax_13 -> layer_15_attn_out_13
	layer_15_attn_out_13 -> layer_15_attn_allreduce_3
	layer_15_input -> layer_15_qkv_14
	layer_15_qkv_14 -> layer_15_attn_scores_14
	layer_15_attn_scores_14 -> layer_15_attn_softmax_14
	layer_15_attn_softmax_14 -> layer_15_attn_out_14
	layer_15_attn_out_14 -> layer_15_attn_allreduce_3
	layer_15_input -> layer_15_qkv_15
	layer_15_qkv_15 -> layer_15_attn_scores_15
	layer_15_attn_scores_15 -> layer_15_attn_softmax_15
	layer_15_attn_softmax_15 -> layer_15_attn_out_15
	layer_15_attn_out_15 -> layer_15_attn_allreduce_3
	layer_15_qkv_16 [label="Layer15_QKV_Proj_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_17 [label="Layer15_QKV_Proj_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_18 [label="Layer15_QKV_Proj_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_19 [label="Layer15_QKV_Proj_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_16 [label="Layer15_Attention_Scores_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_17 [label="Layer15_Attention_Scores_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_18 [label="Layer15_Attention_Scores_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_19 [label="Layer15_Attention_Scores_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_16 [label="Layer15_Attention_Softmax_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_17 [label="Layer15_Attention_Softmax_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_18 [label="Layer15_Attention_Softmax_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_19 [label="Layer15_Attention_Softmax_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_16 [label="Layer15_Attention_Output_GPU16\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_17 [label="Layer15_Attention_Output_GPU17\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_18 [label="Layer15_Attention_Output_GPU18\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_19 [label="Layer15_Attention_Output_GPU19\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_4 [label="Layer15_Attention_AllReduce_ExpertGroup4\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_16
	layer_15_qkv_16 -> layer_15_attn_scores_16
	layer_15_attn_scores_16 -> layer_15_attn_softmax_16
	layer_15_attn_softmax_16 -> layer_15_attn_out_16
	layer_15_attn_out_16 -> layer_15_attn_allreduce_4
	layer_15_input -> layer_15_qkv_17
	layer_15_qkv_17 -> layer_15_attn_scores_17
	layer_15_attn_scores_17 -> layer_15_attn_softmax_17
	layer_15_attn_softmax_17 -> layer_15_attn_out_17
	layer_15_attn_out_17 -> layer_15_attn_allreduce_4
	layer_15_input -> layer_15_qkv_18
	layer_15_qkv_18 -> layer_15_attn_scores_18
	layer_15_attn_scores_18 -> layer_15_attn_softmax_18
	layer_15_attn_softmax_18 -> layer_15_attn_out_18
	layer_15_attn_out_18 -> layer_15_attn_allreduce_4
	layer_15_input -> layer_15_qkv_19
	layer_15_qkv_19 -> layer_15_attn_scores_19
	layer_15_attn_scores_19 -> layer_15_attn_softmax_19
	layer_15_attn_softmax_19 -> layer_15_attn_out_19
	layer_15_attn_out_19 -> layer_15_attn_allreduce_4
	layer_15_qkv_20 [label="Layer15_QKV_Proj_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_21 [label="Layer15_QKV_Proj_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_22 [label="Layer15_QKV_Proj_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_23 [label="Layer15_QKV_Proj_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_20 [label="Layer15_Attention_Scores_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_21 [label="Layer15_Attention_Scores_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_22 [label="Layer15_Attention_Scores_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_23 [label="Layer15_Attention_Scores_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_20 [label="Layer15_Attention_Softmax_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_21 [label="Layer15_Attention_Softmax_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_22 [label="Layer15_Attention_Softmax_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_23 [label="Layer15_Attention_Softmax_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_20 [label="Layer15_Attention_Output_GPU20\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_21 [label="Layer15_Attention_Output_GPU21\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_22 [label="Layer15_Attention_Output_GPU22\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_23 [label="Layer15_Attention_Output_GPU23\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_5 [label="Layer15_Attention_AllReduce_ExpertGroup5\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_20
	layer_15_qkv_20 -> layer_15_attn_scores_20
	layer_15_attn_scores_20 -> layer_15_attn_softmax_20
	layer_15_attn_softmax_20 -> layer_15_attn_out_20
	layer_15_attn_out_20 -> layer_15_attn_allreduce_5
	layer_15_input -> layer_15_qkv_21
	layer_15_qkv_21 -> layer_15_attn_scores_21
	layer_15_attn_scores_21 -> layer_15_attn_softmax_21
	layer_15_attn_softmax_21 -> layer_15_attn_out_21
	layer_15_attn_out_21 -> layer_15_attn_allreduce_5
	layer_15_input -> layer_15_qkv_22
	layer_15_qkv_22 -> layer_15_attn_scores_22
	layer_15_attn_scores_22 -> layer_15_attn_softmax_22
	layer_15_attn_softmax_22 -> layer_15_attn_out_22
	layer_15_attn_out_22 -> layer_15_attn_allreduce_5
	layer_15_input -> layer_15_qkv_23
	layer_15_qkv_23 -> layer_15_attn_scores_23
	layer_15_attn_scores_23 -> layer_15_attn_softmax_23
	layer_15_attn_softmax_23 -> layer_15_attn_out_23
	layer_15_attn_out_23 -> layer_15_attn_allreduce_5
	layer_15_qkv_24 [label="Layer15_QKV_Proj_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_25 [label="Layer15_QKV_Proj_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_26 [label="Layer15_QKV_Proj_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_27 [label="Layer15_QKV_Proj_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_24 [label="Layer15_Attention_Scores_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_25 [label="Layer15_Attention_Scores_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_26 [label="Layer15_Attention_Scores_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_27 [label="Layer15_Attention_Scores_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_24 [label="Layer15_Attention_Softmax_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_25 [label="Layer15_Attention_Softmax_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_26 [label="Layer15_Attention_Softmax_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_27 [label="Layer15_Attention_Softmax_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_24 [label="Layer15_Attention_Output_GPU24\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_25 [label="Layer15_Attention_Output_GPU25\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_26 [label="Layer15_Attention_Output_GPU26\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_27 [label="Layer15_Attention_Output_GPU27\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_6 [label="Layer15_Attention_AllReduce_ExpertGroup6\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_24
	layer_15_qkv_24 -> layer_15_attn_scores_24
	layer_15_attn_scores_24 -> layer_15_attn_softmax_24
	layer_15_attn_softmax_24 -> layer_15_attn_out_24
	layer_15_attn_out_24 -> layer_15_attn_allreduce_6
	layer_15_input -> layer_15_qkv_25
	layer_15_qkv_25 -> layer_15_attn_scores_25
	layer_15_attn_scores_25 -> layer_15_attn_softmax_25
	layer_15_attn_softmax_25 -> layer_15_attn_out_25
	layer_15_attn_out_25 -> layer_15_attn_allreduce_6
	layer_15_input -> layer_15_qkv_26
	layer_15_qkv_26 -> layer_15_attn_scores_26
	layer_15_attn_scores_26 -> layer_15_attn_softmax_26
	layer_15_attn_softmax_26 -> layer_15_attn_out_26
	layer_15_attn_out_26 -> layer_15_attn_allreduce_6
	layer_15_input -> layer_15_qkv_27
	layer_15_qkv_27 -> layer_15_attn_scores_27
	layer_15_attn_scores_27 -> layer_15_attn_softmax_27
	layer_15_attn_softmax_27 -> layer_15_attn_out_27
	layer_15_attn_out_27 -> layer_15_attn_allreduce_6
	layer_15_qkv_28 [label="Layer15_QKV_Proj_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_29 [label="Layer15_QKV_Proj_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_30 [label="Layer15_QKV_Proj_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_31 [label="Layer15_QKV_Proj_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_28 [label="Layer15_Attention_Scores_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_29 [label="Layer15_Attention_Scores_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_30 [label="Layer15_Attention_Scores_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_31 [label="Layer15_Attention_Scores_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_28 [label="Layer15_Attention_Softmax_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_29 [label="Layer15_Attention_Softmax_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_30 [label="Layer15_Attention_Softmax_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_31 [label="Layer15_Attention_Softmax_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_28 [label="Layer15_Attention_Output_GPU28\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_29 [label="Layer15_Attention_Output_GPU29\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_30 [label="Layer15_Attention_Output_GPU30\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_31 [label="Layer15_Attention_Output_GPU31\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_7 [label="Layer15_Attention_AllReduce_ExpertGroup7\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_28
	layer_15_qkv_28 -> layer_15_attn_scores_28
	layer_15_attn_scores_28 -> layer_15_attn_softmax_28
	layer_15_attn_softmax_28 -> layer_15_attn_out_28
	layer_15_attn_out_28 -> layer_15_attn_allreduce_7
	layer_15_input -> layer_15_qkv_29
	layer_15_qkv_29 -> layer_15_attn_scores_29
	layer_15_attn_scores_29 -> layer_15_attn_softmax_29
	layer_15_attn_softmax_29 -> layer_15_attn_out_29
	layer_15_attn_out_29 -> layer_15_attn_allreduce_7
	layer_15_input -> layer_15_qkv_30
	layer_15_qkv_30 -> layer_15_attn_scores_30
	layer_15_attn_scores_30 -> layer_15_attn_softmax_30
	layer_15_attn_softmax_30 -> layer_15_attn_out_30
	layer_15_attn_out_30 -> layer_15_attn_allreduce_7
	layer_15_input -> layer_15_qkv_31
	layer_15_qkv_31 -> layer_15_attn_scores_31
	layer_15_attn_scores_31 -> layer_15_attn_softmax_31
	layer_15_attn_softmax_31 -> layer_15_attn_out_31
	layer_15_attn_out_31 -> layer_15_attn_allreduce_7
	layer_15_qkv_32 [label="Layer15_QKV_Proj_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_33 [label="Layer15_QKV_Proj_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_34 [label="Layer15_QKV_Proj_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_35 [label="Layer15_QKV_Proj_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_32 [label="Layer15_Attention_Scores_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_33 [label="Layer15_Attention_Scores_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_34 [label="Layer15_Attention_Scores_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_35 [label="Layer15_Attention_Scores_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_32 [label="Layer15_Attention_Softmax_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_33 [label="Layer15_Attention_Softmax_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_34 [label="Layer15_Attention_Softmax_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_35 [label="Layer15_Attention_Softmax_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_32 [label="Layer15_Attention_Output_GPU32\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_33 [label="Layer15_Attention_Output_GPU33\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_34 [label="Layer15_Attention_Output_GPU34\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_35 [label="Layer15_Attention_Output_GPU35\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_8 [label="Layer15_Attention_AllReduce_ExpertGroup8\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_32
	layer_15_qkv_32 -> layer_15_attn_scores_32
	layer_15_attn_scores_32 -> layer_15_attn_softmax_32
	layer_15_attn_softmax_32 -> layer_15_attn_out_32
	layer_15_attn_out_32 -> layer_15_attn_allreduce_8
	layer_15_input -> layer_15_qkv_33
	layer_15_qkv_33 -> layer_15_attn_scores_33
	layer_15_attn_scores_33 -> layer_15_attn_softmax_33
	layer_15_attn_softmax_33 -> layer_15_attn_out_33
	layer_15_attn_out_33 -> layer_15_attn_allreduce_8
	layer_15_input -> layer_15_qkv_34
	layer_15_qkv_34 -> layer_15_attn_scores_34
	layer_15_attn_scores_34 -> layer_15_attn_softmax_34
	layer_15_attn_softmax_34 -> layer_15_attn_out_34
	layer_15_attn_out_34 -> layer_15_attn_allreduce_8
	layer_15_input -> layer_15_qkv_35
	layer_15_qkv_35 -> layer_15_attn_scores_35
	layer_15_attn_scores_35 -> layer_15_attn_softmax_35
	layer_15_attn_softmax_35 -> layer_15_attn_out_35
	layer_15_attn_out_35 -> layer_15_attn_allreduce_8
	layer_15_qkv_36 [label="Layer15_QKV_Proj_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_37 [label="Layer15_QKV_Proj_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_38 [label="Layer15_QKV_Proj_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_39 [label="Layer15_QKV_Proj_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_36 [label="Layer15_Attention_Scores_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_37 [label="Layer15_Attention_Scores_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_38 [label="Layer15_Attention_Scores_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_39 [label="Layer15_Attention_Scores_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_36 [label="Layer15_Attention_Softmax_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_37 [label="Layer15_Attention_Softmax_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_38 [label="Layer15_Attention_Softmax_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_39 [label="Layer15_Attention_Softmax_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_36 [label="Layer15_Attention_Output_GPU36\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_37 [label="Layer15_Attention_Output_GPU37\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_38 [label="Layer15_Attention_Output_GPU38\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_39 [label="Layer15_Attention_Output_GPU39\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_9 [label="Layer15_Attention_AllReduce_ExpertGroup9\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_36
	layer_15_qkv_36 -> layer_15_attn_scores_36
	layer_15_attn_scores_36 -> layer_15_attn_softmax_36
	layer_15_attn_softmax_36 -> layer_15_attn_out_36
	layer_15_attn_out_36 -> layer_15_attn_allreduce_9
	layer_15_input -> layer_15_qkv_37
	layer_15_qkv_37 -> layer_15_attn_scores_37
	layer_15_attn_scores_37 -> layer_15_attn_softmax_37
	layer_15_attn_softmax_37 -> layer_15_attn_out_37
	layer_15_attn_out_37 -> layer_15_attn_allreduce_9
	layer_15_input -> layer_15_qkv_38
	layer_15_qkv_38 -> layer_15_attn_scores_38
	layer_15_attn_scores_38 -> layer_15_attn_softmax_38
	layer_15_attn_softmax_38 -> layer_15_attn_out_38
	layer_15_attn_out_38 -> layer_15_attn_allreduce_9
	layer_15_input -> layer_15_qkv_39
	layer_15_qkv_39 -> layer_15_attn_scores_39
	layer_15_attn_scores_39 -> layer_15_attn_softmax_39
	layer_15_attn_softmax_39 -> layer_15_attn_out_39
	layer_15_attn_out_39 -> layer_15_attn_allreduce_9
	layer_15_qkv_40 [label="Layer15_QKV_Proj_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_41 [label="Layer15_QKV_Proj_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_42 [label="Layer15_QKV_Proj_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_43 [label="Layer15_QKV_Proj_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_40 [label="Layer15_Attention_Scores_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_41 [label="Layer15_Attention_Scores_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_42 [label="Layer15_Attention_Scores_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_43 [label="Layer15_Attention_Scores_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_40 [label="Layer15_Attention_Softmax_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_41 [label="Layer15_Attention_Softmax_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_42 [label="Layer15_Attention_Softmax_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_43 [label="Layer15_Attention_Softmax_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_40 [label="Layer15_Attention_Output_GPU40\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_41 [label="Layer15_Attention_Output_GPU41\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_42 [label="Layer15_Attention_Output_GPU42\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_43 [label="Layer15_Attention_Output_GPU43\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_10 [label="Layer15_Attention_AllReduce_ExpertGroup10\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_40
	layer_15_qkv_40 -> layer_15_attn_scores_40
	layer_15_attn_scores_40 -> layer_15_attn_softmax_40
	layer_15_attn_softmax_40 -> layer_15_attn_out_40
	layer_15_attn_out_40 -> layer_15_attn_allreduce_10
	layer_15_input -> layer_15_qkv_41
	layer_15_qkv_41 -> layer_15_attn_scores_41
	layer_15_attn_scores_41 -> layer_15_attn_softmax_41
	layer_15_attn_softmax_41 -> layer_15_attn_out_41
	layer_15_attn_out_41 -> layer_15_attn_allreduce_10
	layer_15_input -> layer_15_qkv_42
	layer_15_qkv_42 -> layer_15_attn_scores_42
	layer_15_attn_scores_42 -> layer_15_attn_softmax_42
	layer_15_attn_softmax_42 -> layer_15_attn_out_42
	layer_15_attn_out_42 -> layer_15_attn_allreduce_10
	layer_15_input -> layer_15_qkv_43
	layer_15_qkv_43 -> layer_15_attn_scores_43
	layer_15_attn_scores_43 -> layer_15_attn_softmax_43
	layer_15_attn_softmax_43 -> layer_15_attn_out_43
	layer_15_attn_out_43 -> layer_15_attn_allreduce_10
	layer_15_qkv_44 [label="Layer15_QKV_Proj_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_45 [label="Layer15_QKV_Proj_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_46 [label="Layer15_QKV_Proj_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_47 [label="Layer15_QKV_Proj_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_44 [label="Layer15_Attention_Scores_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_45 [label="Layer15_Attention_Scores_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_46 [label="Layer15_Attention_Scores_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_47 [label="Layer15_Attention_Scores_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_44 [label="Layer15_Attention_Softmax_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_45 [label="Layer15_Attention_Softmax_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_46 [label="Layer15_Attention_Softmax_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_47 [label="Layer15_Attention_Softmax_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_44 [label="Layer15_Attention_Output_GPU44\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_45 [label="Layer15_Attention_Output_GPU45\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_46 [label="Layer15_Attention_Output_GPU46\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_47 [label="Layer15_Attention_Output_GPU47\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_11 [label="Layer15_Attention_AllReduce_ExpertGroup11\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_44
	layer_15_qkv_44 -> layer_15_attn_scores_44
	layer_15_attn_scores_44 -> layer_15_attn_softmax_44
	layer_15_attn_softmax_44 -> layer_15_attn_out_44
	layer_15_attn_out_44 -> layer_15_attn_allreduce_11
	layer_15_input -> layer_15_qkv_45
	layer_15_qkv_45 -> layer_15_attn_scores_45
	layer_15_attn_scores_45 -> layer_15_attn_softmax_45
	layer_15_attn_softmax_45 -> layer_15_attn_out_45
	layer_15_attn_out_45 -> layer_15_attn_allreduce_11
	layer_15_input -> layer_15_qkv_46
	layer_15_qkv_46 -> layer_15_attn_scores_46
	layer_15_attn_scores_46 -> layer_15_attn_softmax_46
	layer_15_attn_softmax_46 -> layer_15_attn_out_46
	layer_15_attn_out_46 -> layer_15_attn_allreduce_11
	layer_15_input -> layer_15_qkv_47
	layer_15_qkv_47 -> layer_15_attn_scores_47
	layer_15_attn_scores_47 -> layer_15_attn_softmax_47
	layer_15_attn_softmax_47 -> layer_15_attn_out_47
	layer_15_attn_out_47 -> layer_15_attn_allreduce_11
	layer_15_qkv_48 [label="Layer15_QKV_Proj_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_49 [label="Layer15_QKV_Proj_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_50 [label="Layer15_QKV_Proj_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_51 [label="Layer15_QKV_Proj_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_48 [label="Layer15_Attention_Scores_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_49 [label="Layer15_Attention_Scores_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_50 [label="Layer15_Attention_Scores_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_51 [label="Layer15_Attention_Scores_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_48 [label="Layer15_Attention_Softmax_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_49 [label="Layer15_Attention_Softmax_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_50 [label="Layer15_Attention_Softmax_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_51 [label="Layer15_Attention_Softmax_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_48 [label="Layer15_Attention_Output_GPU48\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_49 [label="Layer15_Attention_Output_GPU49\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_50 [label="Layer15_Attention_Output_GPU50\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_51 [label="Layer15_Attention_Output_GPU51\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_12 [label="Layer15_Attention_AllReduce_ExpertGroup12\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_48
	layer_15_qkv_48 -> layer_15_attn_scores_48
	layer_15_attn_scores_48 -> layer_15_attn_softmax_48
	layer_15_attn_softmax_48 -> layer_15_attn_out_48
	layer_15_attn_out_48 -> layer_15_attn_allreduce_12
	layer_15_input -> layer_15_qkv_49
	layer_15_qkv_49 -> layer_15_attn_scores_49
	layer_15_attn_scores_49 -> layer_15_attn_softmax_49
	layer_15_attn_softmax_49 -> layer_15_attn_out_49
	layer_15_attn_out_49 -> layer_15_attn_allreduce_12
	layer_15_input -> layer_15_qkv_50
	layer_15_qkv_50 -> layer_15_attn_scores_50
	layer_15_attn_scores_50 -> layer_15_attn_softmax_50
	layer_15_attn_softmax_50 -> layer_15_attn_out_50
	layer_15_attn_out_50 -> layer_15_attn_allreduce_12
	layer_15_input -> layer_15_qkv_51
	layer_15_qkv_51 -> layer_15_attn_scores_51
	layer_15_attn_scores_51 -> layer_15_attn_softmax_51
	layer_15_attn_softmax_51 -> layer_15_attn_out_51
	layer_15_attn_out_51 -> layer_15_attn_allreduce_12
	layer_15_qkv_52 [label="Layer15_QKV_Proj_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_53 [label="Layer15_QKV_Proj_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_54 [label="Layer15_QKV_Proj_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_55 [label="Layer15_QKV_Proj_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_52 [label="Layer15_Attention_Scores_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_53 [label="Layer15_Attention_Scores_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_54 [label="Layer15_Attention_Scores_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_55 [label="Layer15_Attention_Scores_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_52 [label="Layer15_Attention_Softmax_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_53 [label="Layer15_Attention_Softmax_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_54 [label="Layer15_Attention_Softmax_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_55 [label="Layer15_Attention_Softmax_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_52 [label="Layer15_Attention_Output_GPU52\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_53 [label="Layer15_Attention_Output_GPU53\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_54 [label="Layer15_Attention_Output_GPU54\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_55 [label="Layer15_Attention_Output_GPU55\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_13 [label="Layer15_Attention_AllReduce_ExpertGroup13\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_52
	layer_15_qkv_52 -> layer_15_attn_scores_52
	layer_15_attn_scores_52 -> layer_15_attn_softmax_52
	layer_15_attn_softmax_52 -> layer_15_attn_out_52
	layer_15_attn_out_52 -> layer_15_attn_allreduce_13
	layer_15_input -> layer_15_qkv_53
	layer_15_qkv_53 -> layer_15_attn_scores_53
	layer_15_attn_scores_53 -> layer_15_attn_softmax_53
	layer_15_attn_softmax_53 -> layer_15_attn_out_53
	layer_15_attn_out_53 -> layer_15_attn_allreduce_13
	layer_15_input -> layer_15_qkv_54
	layer_15_qkv_54 -> layer_15_attn_scores_54
	layer_15_attn_scores_54 -> layer_15_attn_softmax_54
	layer_15_attn_softmax_54 -> layer_15_attn_out_54
	layer_15_attn_out_54 -> layer_15_attn_allreduce_13
	layer_15_input -> layer_15_qkv_55
	layer_15_qkv_55 -> layer_15_attn_scores_55
	layer_15_attn_scores_55 -> layer_15_attn_softmax_55
	layer_15_attn_softmax_55 -> layer_15_attn_out_55
	layer_15_attn_out_55 -> layer_15_attn_allreduce_13
	layer_15_qkv_56 [label="Layer15_QKV_Proj_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_57 [label="Layer15_QKV_Proj_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_58 [label="Layer15_QKV_Proj_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_59 [label="Layer15_QKV_Proj_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_56 [label="Layer15_Attention_Scores_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_57 [label="Layer15_Attention_Scores_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_58 [label="Layer15_Attention_Scores_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_59 [label="Layer15_Attention_Scores_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_56 [label="Layer15_Attention_Softmax_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_57 [label="Layer15_Attention_Softmax_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_58 [label="Layer15_Attention_Softmax_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_59 [label="Layer15_Attention_Softmax_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_56 [label="Layer15_Attention_Output_GPU56\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_57 [label="Layer15_Attention_Output_GPU57\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_58 [label="Layer15_Attention_Output_GPU58\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_59 [label="Layer15_Attention_Output_GPU59\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_14 [label="Layer15_Attention_AllReduce_ExpertGroup14\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_56
	layer_15_qkv_56 -> layer_15_attn_scores_56
	layer_15_attn_scores_56 -> layer_15_attn_softmax_56
	layer_15_attn_softmax_56 -> layer_15_attn_out_56
	layer_15_attn_out_56 -> layer_15_attn_allreduce_14
	layer_15_input -> layer_15_qkv_57
	layer_15_qkv_57 -> layer_15_attn_scores_57
	layer_15_attn_scores_57 -> layer_15_attn_softmax_57
	layer_15_attn_softmax_57 -> layer_15_attn_out_57
	layer_15_attn_out_57 -> layer_15_attn_allreduce_14
	layer_15_input -> layer_15_qkv_58
	layer_15_qkv_58 -> layer_15_attn_scores_58
	layer_15_attn_scores_58 -> layer_15_attn_softmax_58
	layer_15_attn_softmax_58 -> layer_15_attn_out_58
	layer_15_attn_out_58 -> layer_15_attn_allreduce_14
	layer_15_input -> layer_15_qkv_59
	layer_15_qkv_59 -> layer_15_attn_scores_59
	layer_15_attn_scores_59 -> layer_15_attn_softmax_59
	layer_15_attn_softmax_59 -> layer_15_attn_out_59
	layer_15_attn_out_59 -> layer_15_attn_allreduce_14
	layer_15_qkv_60 [label="Layer15_QKV_Proj_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_61 [label="Layer15_QKV_Proj_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_62 [label="Layer15_QKV_Proj_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_qkv_63 [label="Layer15_QKV_Proj_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, heads=4, d_k=32]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_60 [label="Layer15_Attention_Scores_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_61 [label="Layer15_Attention_Scores_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_62 [label="Layer15_Attention_Scores_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_scores_63 [label="Layer15_Attention_Scores_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, heads=4, d_k=32]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_60 [label="Layer15_Attention_Softmax_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_61 [label="Layer15_Attention_Softmax_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_62 [label="Layer15_Attention_Softmax_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_softmax_63 [label="Layer15_Attention_Softmax_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, seq_len=512]\nOutput: [batch_size=128, heads=4, seq_len=512, seq_len=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_60 [label="Layer15_Attention_Output_GPU60\nTP-Rank0\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_61 [label="Layer15_Attention_Output_GPU61\nTP-Rank1\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_62 [label="Layer15_Attention_Output_GPU62\nTP-Rank2\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_out_63 [label="Layer15_Attention_Output_GPU63\nTP-Rank3\nInput: [batch_size=128, heads=4, seq_len=512, d_k=32]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_15 [label="Layer15_Attention_AllReduce_ExpertGroup15\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_input -> layer_15_qkv_60
	layer_15_qkv_60 -> layer_15_attn_scores_60
	layer_15_attn_scores_60 -> layer_15_attn_softmax_60
	layer_15_attn_softmax_60 -> layer_15_attn_out_60
	layer_15_attn_out_60 -> layer_15_attn_allreduce_15
	layer_15_input -> layer_15_qkv_61
	layer_15_qkv_61 -> layer_15_attn_scores_61
	layer_15_attn_scores_61 -> layer_15_attn_softmax_61
	layer_15_attn_softmax_61 -> layer_15_attn_out_61
	layer_15_attn_out_61 -> layer_15_attn_allreduce_15
	layer_15_input -> layer_15_qkv_62
	layer_15_qkv_62 -> layer_15_attn_scores_62
	layer_15_attn_scores_62 -> layer_15_attn_softmax_62
	layer_15_attn_softmax_62 -> layer_15_attn_out_62
	layer_15_attn_out_62 -> layer_15_attn_allreduce_15
	layer_15_input -> layer_15_qkv_63
	layer_15_qkv_63 -> layer_15_attn_scores_63
	layer_15_attn_scores_63 -> layer_15_attn_softmax_63
	layer_15_attn_softmax_63 -> layer_15_attn_out_63
	layer_15_attn_out_63 -> layer_15_attn_allreduce_15
	layer_15_gate_0 [label="Layer15_Expert_Gate_GPU0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_4 [label="Layer15_Expert_Gate_GPU4\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_8 [label="Layer15_Expert_Gate_GPU8\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_12 [label="Layer15_Expert_Gate_GPU12\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_16 [label="Layer15_Expert_Gate_GPU16\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_20 [label="Layer15_Expert_Gate_GPU20\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_24 [label="Layer15_Expert_Gate_GPU24\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_28 [label="Layer15_Expert_Gate_GPU28\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_32 [label="Layer15_Expert_Gate_GPU32\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_36 [label="Layer15_Expert_Gate_GPU36\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_40 [label="Layer15_Expert_Gate_GPU40\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_44 [label="Layer15_Expert_Gate_GPU44\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_48 [label="Layer15_Expert_Gate_GPU48\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_52 [label="Layer15_Expert_Gate_GPU52\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_56 [label="Layer15_Expert_Gate_GPU56\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_gate_60 [label="Layer15_Expert_Gate_GPU60\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, experts=16]" fillcolor=lightyellow shape=parallelogram]
	layer_15_alltoall [label="Layer15_Expert_Routing_AllToAll\nInput: Token assignments\nOutput: Routed tokens to expert groups" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_0 [label="Layer15_Expert0_MLP1_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_1 [label="Layer15_Expert0_MLP1_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_2 [label="Layer15_Expert0_MLP1_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_3 [label="Layer15_Expert0_MLP1_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_0 [label="Layer15_Expert0_GELU_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_1 [label="Layer15_Expert0_GELU_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_2 [label="Layer15_Expert0_GELU_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_3 [label="Layer15_Expert0_GELU_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_0 [label="Layer15_Expert0_MLP2_GPU0\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_1 [label="Layer15_Expert0_MLP2_GPU1\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_2 [label="Layer15_Expert0_MLP2_GPU2\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_3 [label="Layer15_Expert0_MLP2_GPU3\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_0 [label="Layer15_Expert0_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_4 [label="Layer15_Expert1_MLP1_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_5 [label="Layer15_Expert1_MLP1_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_6 [label="Layer15_Expert1_MLP1_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_7 [label="Layer15_Expert1_MLP1_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_4 [label="Layer15_Expert1_GELU_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_5 [label="Layer15_Expert1_GELU_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_6 [label="Layer15_Expert1_GELU_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_7 [label="Layer15_Expert1_GELU_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_4 [label="Layer15_Expert1_MLP2_GPU4\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_5 [label="Layer15_Expert1_MLP2_GPU5\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_6 [label="Layer15_Expert1_MLP2_GPU6\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_7 [label="Layer15_Expert1_MLP2_GPU7\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_1 [label="Layer15_Expert1_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_8 [label="Layer15_Expert2_MLP1_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_9 [label="Layer15_Expert2_MLP1_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_10 [label="Layer15_Expert2_MLP1_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_11 [label="Layer15_Expert2_MLP1_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_8 [label="Layer15_Expert2_GELU_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_9 [label="Layer15_Expert2_GELU_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_10 [label="Layer15_Expert2_GELU_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_11 [label="Layer15_Expert2_GELU_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_8 [label="Layer15_Expert2_MLP2_GPU8\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_9 [label="Layer15_Expert2_MLP2_GPU9\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_10 [label="Layer15_Expert2_MLP2_GPU10\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_11 [label="Layer15_Expert2_MLP2_GPU11\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_2 [label="Layer15_Expert2_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_12 [label="Layer15_Expert3_MLP1_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_13 [label="Layer15_Expert3_MLP1_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_14 [label="Layer15_Expert3_MLP1_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_15 [label="Layer15_Expert3_MLP1_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_12 [label="Layer15_Expert3_GELU_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_13 [label="Layer15_Expert3_GELU_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_14 [label="Layer15_Expert3_GELU_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_15 [label="Layer15_Expert3_GELU_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_12 [label="Layer15_Expert3_MLP2_GPU12\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_13 [label="Layer15_Expert3_MLP2_GPU13\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_14 [label="Layer15_Expert3_MLP2_GPU14\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_15 [label="Layer15_Expert3_MLP2_GPU15\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_3 [label="Layer15_Expert3_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_16 [label="Layer15_Expert4_MLP1_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_17 [label="Layer15_Expert4_MLP1_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_18 [label="Layer15_Expert4_MLP1_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_19 [label="Layer15_Expert4_MLP1_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_16 [label="Layer15_Expert4_GELU_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_17 [label="Layer15_Expert4_GELU_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_18 [label="Layer15_Expert4_GELU_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_19 [label="Layer15_Expert4_GELU_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_16 [label="Layer15_Expert4_MLP2_GPU16\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_17 [label="Layer15_Expert4_MLP2_GPU17\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_18 [label="Layer15_Expert4_MLP2_GPU18\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_19 [label="Layer15_Expert4_MLP2_GPU19\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_4 [label="Layer15_Expert4_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_20 [label="Layer15_Expert5_MLP1_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_21 [label="Layer15_Expert5_MLP1_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_22 [label="Layer15_Expert5_MLP1_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_23 [label="Layer15_Expert5_MLP1_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_20 [label="Layer15_Expert5_GELU_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_21 [label="Layer15_Expert5_GELU_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_22 [label="Layer15_Expert5_GELU_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_23 [label="Layer15_Expert5_GELU_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_20 [label="Layer15_Expert5_MLP2_GPU20\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_21 [label="Layer15_Expert5_MLP2_GPU21\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_22 [label="Layer15_Expert5_MLP2_GPU22\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_23 [label="Layer15_Expert5_MLP2_GPU23\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_5 [label="Layer15_Expert5_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_24 [label="Layer15_Expert6_MLP1_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_25 [label="Layer15_Expert6_MLP1_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_26 [label="Layer15_Expert6_MLP1_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_27 [label="Layer15_Expert6_MLP1_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_24 [label="Layer15_Expert6_GELU_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_25 [label="Layer15_Expert6_GELU_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_26 [label="Layer15_Expert6_GELU_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_27 [label="Layer15_Expert6_GELU_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_24 [label="Layer15_Expert6_MLP2_GPU24\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_25 [label="Layer15_Expert6_MLP2_GPU25\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_26 [label="Layer15_Expert6_MLP2_GPU26\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_27 [label="Layer15_Expert6_MLP2_GPU27\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_6 [label="Layer15_Expert6_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_28 [label="Layer15_Expert7_MLP1_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_29 [label="Layer15_Expert7_MLP1_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_30 [label="Layer15_Expert7_MLP1_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_31 [label="Layer15_Expert7_MLP1_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_28 [label="Layer15_Expert7_GELU_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_29 [label="Layer15_Expert7_GELU_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_30 [label="Layer15_Expert7_GELU_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_31 [label="Layer15_Expert7_GELU_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_28 [label="Layer15_Expert7_MLP2_GPU28\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_29 [label="Layer15_Expert7_MLP2_GPU29\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_30 [label="Layer15_Expert7_MLP2_GPU30\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_31 [label="Layer15_Expert7_MLP2_GPU31\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_7 [label="Layer15_Expert7_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_32 [label="Layer15_Expert8_MLP1_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_33 [label="Layer15_Expert8_MLP1_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_34 [label="Layer15_Expert8_MLP1_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_35 [label="Layer15_Expert8_MLP1_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_32 [label="Layer15_Expert8_GELU_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_33 [label="Layer15_Expert8_GELU_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_34 [label="Layer15_Expert8_GELU_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_35 [label="Layer15_Expert8_GELU_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_32 [label="Layer15_Expert8_MLP2_GPU32\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_33 [label="Layer15_Expert8_MLP2_GPU33\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_34 [label="Layer15_Expert8_MLP2_GPU34\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_35 [label="Layer15_Expert8_MLP2_GPU35\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_8 [label="Layer15_Expert8_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_36 [label="Layer15_Expert9_MLP1_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_37 [label="Layer15_Expert9_MLP1_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_38 [label="Layer15_Expert9_MLP1_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_39 [label="Layer15_Expert9_MLP1_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_36 [label="Layer15_Expert9_GELU_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_37 [label="Layer15_Expert9_GELU_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_38 [label="Layer15_Expert9_GELU_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_39 [label="Layer15_Expert9_GELU_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_36 [label="Layer15_Expert9_MLP2_GPU36\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_37 [label="Layer15_Expert9_MLP2_GPU37\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_38 [label="Layer15_Expert9_MLP2_GPU38\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_39 [label="Layer15_Expert9_MLP2_GPU39\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_9 [label="Layer15_Expert9_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_40 [label="Layer15_Expert10_MLP1_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_41 [label="Layer15_Expert10_MLP1_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_42 [label="Layer15_Expert10_MLP1_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_43 [label="Layer15_Expert10_MLP1_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_40 [label="Layer15_Expert10_GELU_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_41 [label="Layer15_Expert10_GELU_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_42 [label="Layer15_Expert10_GELU_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_43 [label="Layer15_Expert10_GELU_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_40 [label="Layer15_Expert10_MLP2_GPU40\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_41 [label="Layer15_Expert10_MLP2_GPU41\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_42 [label="Layer15_Expert10_MLP2_GPU42\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_43 [label="Layer15_Expert10_MLP2_GPU43\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_10 [label="Layer15_Expert10_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_44 [label="Layer15_Expert11_MLP1_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_45 [label="Layer15_Expert11_MLP1_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_46 [label="Layer15_Expert11_MLP1_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_47 [label="Layer15_Expert11_MLP1_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_44 [label="Layer15_Expert11_GELU_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_45 [label="Layer15_Expert11_GELU_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_46 [label="Layer15_Expert11_GELU_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_47 [label="Layer15_Expert11_GELU_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_44 [label="Layer15_Expert11_MLP2_GPU44\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_45 [label="Layer15_Expert11_MLP2_GPU45\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_46 [label="Layer15_Expert11_MLP2_GPU46\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_47 [label="Layer15_Expert11_MLP2_GPU47\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_11 [label="Layer15_Expert11_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_48 [label="Layer15_Expert12_MLP1_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_49 [label="Layer15_Expert12_MLP1_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_50 [label="Layer15_Expert12_MLP1_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_51 [label="Layer15_Expert12_MLP1_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_48 [label="Layer15_Expert12_GELU_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_49 [label="Layer15_Expert12_GELU_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_50 [label="Layer15_Expert12_GELU_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_51 [label="Layer15_Expert12_GELU_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_48 [label="Layer15_Expert12_MLP2_GPU48\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_49 [label="Layer15_Expert12_MLP2_GPU49\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_50 [label="Layer15_Expert12_MLP2_GPU50\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_51 [label="Layer15_Expert12_MLP2_GPU51\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_12 [label="Layer15_Expert12_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_52 [label="Layer15_Expert13_MLP1_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_53 [label="Layer15_Expert13_MLP1_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_54 [label="Layer15_Expert13_MLP1_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_55 [label="Layer15_Expert13_MLP1_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_52 [label="Layer15_Expert13_GELU_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_53 [label="Layer15_Expert13_GELU_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_54 [label="Layer15_Expert13_GELU_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_55 [label="Layer15_Expert13_GELU_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_52 [label="Layer15_Expert13_MLP2_GPU52\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_53 [label="Layer15_Expert13_MLP2_GPU53\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_54 [label="Layer15_Expert13_MLP2_GPU54\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_55 [label="Layer15_Expert13_MLP2_GPU55\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_13 [label="Layer15_Expert13_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_56 [label="Layer15_Expert14_MLP1_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_57 [label="Layer15_Expert14_MLP1_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_58 [label="Layer15_Expert14_MLP1_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_59 [label="Layer15_Expert14_MLP1_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_56 [label="Layer15_Expert14_GELU_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_57 [label="Layer15_Expert14_GELU_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_58 [label="Layer15_Expert14_GELU_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_59 [label="Layer15_Expert14_GELU_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_56 [label="Layer15_Expert14_MLP2_GPU56\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_57 [label="Layer15_Expert14_MLP2_GPU57\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_58 [label="Layer15_Expert14_MLP2_GPU58\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_59 [label="Layer15_Expert14_MLP2_GPU59\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_14 [label="Layer15_Expert14_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert1_60 [label="Layer15_Expert15_MLP1_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_61 [label="Layer15_Expert15_MLP1_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_62 [label="Layer15_Expert15_MLP1_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert1_63 [label="Layer15_Expert15_MLP1_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_60 [label="Layer15_Expert15_GELU_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_61 [label="Layer15_Expert15_GELU_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_62 [label="Layer15_Expert15_GELU_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert_act_63 [label="Layer15_Expert15_GELU_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, ffn=256]" fillcolor=lightgreen shape=box]
	layer_15_expert2_60 [label="Layer15_Expert15_MLP2_GPU60\nTP-Rank0\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_61 [label="Layer15_Expert15_MLP2_GPU61\nTP-Rank1\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_62 [label="Layer15_Expert15_MLP2_GPU62\nTP-Rank2\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert2_63 [label="Layer15_Expert15_MLP2_GPU63\nTP-Rank3\nInput: [batch_size=128, seq_len=512, ffn=256]\nOutput: [batch_size=128, seq_len=512, hidden=128]" fillcolor=lightgreen shape=box]
	layer_15_expert_allreduce_15 [label="Layer15_Expert15_AllReduce\nInput: Distributed\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightblue shape=ellipse]
	layer_15_expert_agg [label="Layer15_Expert_Aggregation\nInput: Expert outputs from top-2\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightyellow shape=parallelogram]
	layer_15_norm [label="Layer15_LayerNorm\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, hidden=512]" fillcolor=lightgreen shape=box]
	layer_15_attn_allreduce_0 -> layer_15_gate_0 [style=dashed]
	layer_15_gate_0 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_0
	layer_15_expert1_0 -> layer_15_expert_act_0
	layer_15_expert_act_0 -> layer_15_expert2_0
	layer_15_expert2_0 -> layer_15_expert_allreduce_0
	layer_15_alltoall -> layer_15_expert1_1
	layer_15_expert1_1 -> layer_15_expert_act_1
	layer_15_expert_act_1 -> layer_15_expert2_1
	layer_15_expert2_1 -> layer_15_expert_allreduce_0
	layer_15_alltoall -> layer_15_expert1_2
	layer_15_expert1_2 -> layer_15_expert_act_2
	layer_15_expert_act_2 -> layer_15_expert2_2
	layer_15_expert2_2 -> layer_15_expert_allreduce_0
	layer_15_alltoall -> layer_15_expert1_3
	layer_15_expert1_3 -> layer_15_expert_act_3
	layer_15_expert_act_3 -> layer_15_expert2_3
	layer_15_expert2_3 -> layer_15_expert_allreduce_0
	layer_15_expert_allreduce_0 -> layer_15_expert_agg
	layer_15_attn_allreduce_1 -> layer_15_gate_4 [style=dashed]
	layer_15_gate_4 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_4
	layer_15_expert1_4 -> layer_15_expert_act_4
	layer_15_expert_act_4 -> layer_15_expert2_4
	layer_15_expert2_4 -> layer_15_expert_allreduce_1
	layer_15_alltoall -> layer_15_expert1_5
	layer_15_expert1_5 -> layer_15_expert_act_5
	layer_15_expert_act_5 -> layer_15_expert2_5
	layer_15_expert2_5 -> layer_15_expert_allreduce_1
	layer_15_alltoall -> layer_15_expert1_6
	layer_15_expert1_6 -> layer_15_expert_act_6
	layer_15_expert_act_6 -> layer_15_expert2_6
	layer_15_expert2_6 -> layer_15_expert_allreduce_1
	layer_15_alltoall -> layer_15_expert1_7
	layer_15_expert1_7 -> layer_15_expert_act_7
	layer_15_expert_act_7 -> layer_15_expert2_7
	layer_15_expert2_7 -> layer_15_expert_allreduce_1
	layer_15_expert_allreduce_1 -> layer_15_expert_agg
	layer_15_attn_allreduce_2 -> layer_15_gate_8 [style=dashed]
	layer_15_gate_8 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_8
	layer_15_expert1_8 -> layer_15_expert_act_8
	layer_15_expert_act_8 -> layer_15_expert2_8
	layer_15_expert2_8 -> layer_15_expert_allreduce_2
	layer_15_alltoall -> layer_15_expert1_9
	layer_15_expert1_9 -> layer_15_expert_act_9
	layer_15_expert_act_9 -> layer_15_expert2_9
	layer_15_expert2_9 -> layer_15_expert_allreduce_2
	layer_15_alltoall -> layer_15_expert1_10
	layer_15_expert1_10 -> layer_15_expert_act_10
	layer_15_expert_act_10 -> layer_15_expert2_10
	layer_15_expert2_10 -> layer_15_expert_allreduce_2
	layer_15_alltoall -> layer_15_expert1_11
	layer_15_expert1_11 -> layer_15_expert_act_11
	layer_15_expert_act_11 -> layer_15_expert2_11
	layer_15_expert2_11 -> layer_15_expert_allreduce_2
	layer_15_expert_allreduce_2 -> layer_15_expert_agg
	layer_15_attn_allreduce_3 -> layer_15_gate_12 [style=dashed]
	layer_15_gate_12 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_12
	layer_15_expert1_12 -> layer_15_expert_act_12
	layer_15_expert_act_12 -> layer_15_expert2_12
	layer_15_expert2_12 -> layer_15_expert_allreduce_3
	layer_15_alltoall -> layer_15_expert1_13
	layer_15_expert1_13 -> layer_15_expert_act_13
	layer_15_expert_act_13 -> layer_15_expert2_13
	layer_15_expert2_13 -> layer_15_expert_allreduce_3
	layer_15_alltoall -> layer_15_expert1_14
	layer_15_expert1_14 -> layer_15_expert_act_14
	layer_15_expert_act_14 -> layer_15_expert2_14
	layer_15_expert2_14 -> layer_15_expert_allreduce_3
	layer_15_alltoall -> layer_15_expert1_15
	layer_15_expert1_15 -> layer_15_expert_act_15
	layer_15_expert_act_15 -> layer_15_expert2_15
	layer_15_expert2_15 -> layer_15_expert_allreduce_3
	layer_15_expert_allreduce_3 -> layer_15_expert_agg
	layer_15_attn_allreduce_4 -> layer_15_gate_16 [style=dashed]
	layer_15_gate_16 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_16
	layer_15_expert1_16 -> layer_15_expert_act_16
	layer_15_expert_act_16 -> layer_15_expert2_16
	layer_15_expert2_16 -> layer_15_expert_allreduce_4
	layer_15_alltoall -> layer_15_expert1_17
	layer_15_expert1_17 -> layer_15_expert_act_17
	layer_15_expert_act_17 -> layer_15_expert2_17
	layer_15_expert2_17 -> layer_15_expert_allreduce_4
	layer_15_alltoall -> layer_15_expert1_18
	layer_15_expert1_18 -> layer_15_expert_act_18
	layer_15_expert_act_18 -> layer_15_expert2_18
	layer_15_expert2_18 -> layer_15_expert_allreduce_4
	layer_15_alltoall -> layer_15_expert1_19
	layer_15_expert1_19 -> layer_15_expert_act_19
	layer_15_expert_act_19 -> layer_15_expert2_19
	layer_15_expert2_19 -> layer_15_expert_allreduce_4
	layer_15_expert_allreduce_4 -> layer_15_expert_agg
	layer_15_attn_allreduce_5 -> layer_15_gate_20 [style=dashed]
	layer_15_gate_20 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_20
	layer_15_expert1_20 -> layer_15_expert_act_20
	layer_15_expert_act_20 -> layer_15_expert2_20
	layer_15_expert2_20 -> layer_15_expert_allreduce_5
	layer_15_alltoall -> layer_15_expert1_21
	layer_15_expert1_21 -> layer_15_expert_act_21
	layer_15_expert_act_21 -> layer_15_expert2_21
	layer_15_expert2_21 -> layer_15_expert_allreduce_5
	layer_15_alltoall -> layer_15_expert1_22
	layer_15_expert1_22 -> layer_15_expert_act_22
	layer_15_expert_act_22 -> layer_15_expert2_22
	layer_15_expert2_22 -> layer_15_expert_allreduce_5
	layer_15_alltoall -> layer_15_expert1_23
	layer_15_expert1_23 -> layer_15_expert_act_23
	layer_15_expert_act_23 -> layer_15_expert2_23
	layer_15_expert2_23 -> layer_15_expert_allreduce_5
	layer_15_expert_allreduce_5 -> layer_15_expert_agg
	layer_15_attn_allreduce_6 -> layer_15_gate_24 [style=dashed]
	layer_15_gate_24 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_24
	layer_15_expert1_24 -> layer_15_expert_act_24
	layer_15_expert_act_24 -> layer_15_expert2_24
	layer_15_expert2_24 -> layer_15_expert_allreduce_6
	layer_15_alltoall -> layer_15_expert1_25
	layer_15_expert1_25 -> layer_15_expert_act_25
	layer_15_expert_act_25 -> layer_15_expert2_25
	layer_15_expert2_25 -> layer_15_expert_allreduce_6
	layer_15_alltoall -> layer_15_expert1_26
	layer_15_expert1_26 -> layer_15_expert_act_26
	layer_15_expert_act_26 -> layer_15_expert2_26
	layer_15_expert2_26 -> layer_15_expert_allreduce_6
	layer_15_alltoall -> layer_15_expert1_27
	layer_15_expert1_27 -> layer_15_expert_act_27
	layer_15_expert_act_27 -> layer_15_expert2_27
	layer_15_expert2_27 -> layer_15_expert_allreduce_6
	layer_15_expert_allreduce_6 -> layer_15_expert_agg
	layer_15_attn_allreduce_7 -> layer_15_gate_28 [style=dashed]
	layer_15_gate_28 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_28
	layer_15_expert1_28 -> layer_15_expert_act_28
	layer_15_expert_act_28 -> layer_15_expert2_28
	layer_15_expert2_28 -> layer_15_expert_allreduce_7
	layer_15_alltoall -> layer_15_expert1_29
	layer_15_expert1_29 -> layer_15_expert_act_29
	layer_15_expert_act_29 -> layer_15_expert2_29
	layer_15_expert2_29 -> layer_15_expert_allreduce_7
	layer_15_alltoall -> layer_15_expert1_30
	layer_15_expert1_30 -> layer_15_expert_act_30
	layer_15_expert_act_30 -> layer_15_expert2_30
	layer_15_expert2_30 -> layer_15_expert_allreduce_7
	layer_15_alltoall -> layer_15_expert1_31
	layer_15_expert1_31 -> layer_15_expert_act_31
	layer_15_expert_act_31 -> layer_15_expert2_31
	layer_15_expert2_31 -> layer_15_expert_allreduce_7
	layer_15_expert_allreduce_7 -> layer_15_expert_agg
	layer_15_attn_allreduce_8 -> layer_15_gate_32 [style=dashed]
	layer_15_gate_32 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_32
	layer_15_expert1_32 -> layer_15_expert_act_32
	layer_15_expert_act_32 -> layer_15_expert2_32
	layer_15_expert2_32 -> layer_15_expert_allreduce_8
	layer_15_alltoall -> layer_15_expert1_33
	layer_15_expert1_33 -> layer_15_expert_act_33
	layer_15_expert_act_33 -> layer_15_expert2_33
	layer_15_expert2_33 -> layer_15_expert_allreduce_8
	layer_15_alltoall -> layer_15_expert1_34
	layer_15_expert1_34 -> layer_15_expert_act_34
	layer_15_expert_act_34 -> layer_15_expert2_34
	layer_15_expert2_34 -> layer_15_expert_allreduce_8
	layer_15_alltoall -> layer_15_expert1_35
	layer_15_expert1_35 -> layer_15_expert_act_35
	layer_15_expert_act_35 -> layer_15_expert2_35
	layer_15_expert2_35 -> layer_15_expert_allreduce_8
	layer_15_expert_allreduce_8 -> layer_15_expert_agg
	layer_15_attn_allreduce_9 -> layer_15_gate_36 [style=dashed]
	layer_15_gate_36 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_36
	layer_15_expert1_36 -> layer_15_expert_act_36
	layer_15_expert_act_36 -> layer_15_expert2_36
	layer_15_expert2_36 -> layer_15_expert_allreduce_9
	layer_15_alltoall -> layer_15_expert1_37
	layer_15_expert1_37 -> layer_15_expert_act_37
	layer_15_expert_act_37 -> layer_15_expert2_37
	layer_15_expert2_37 -> layer_15_expert_allreduce_9
	layer_15_alltoall -> layer_15_expert1_38
	layer_15_expert1_38 -> layer_15_expert_act_38
	layer_15_expert_act_38 -> layer_15_expert2_38
	layer_15_expert2_38 -> layer_15_expert_allreduce_9
	layer_15_alltoall -> layer_15_expert1_39
	layer_15_expert1_39 -> layer_15_expert_act_39
	layer_15_expert_act_39 -> layer_15_expert2_39
	layer_15_expert2_39 -> layer_15_expert_allreduce_9
	layer_15_expert_allreduce_9 -> layer_15_expert_agg
	layer_15_attn_allreduce_10 -> layer_15_gate_40 [style=dashed]
	layer_15_gate_40 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_40
	layer_15_expert1_40 -> layer_15_expert_act_40
	layer_15_expert_act_40 -> layer_15_expert2_40
	layer_15_expert2_40 -> layer_15_expert_allreduce_10
	layer_15_alltoall -> layer_15_expert1_41
	layer_15_expert1_41 -> layer_15_expert_act_41
	layer_15_expert_act_41 -> layer_15_expert2_41
	layer_15_expert2_41 -> layer_15_expert_allreduce_10
	layer_15_alltoall -> layer_15_expert1_42
	layer_15_expert1_42 -> layer_15_expert_act_42
	layer_15_expert_act_42 -> layer_15_expert2_42
	layer_15_expert2_42 -> layer_15_expert_allreduce_10
	layer_15_alltoall -> layer_15_expert1_43
	layer_15_expert1_43 -> layer_15_expert_act_43
	layer_15_expert_act_43 -> layer_15_expert2_43
	layer_15_expert2_43 -> layer_15_expert_allreduce_10
	layer_15_expert_allreduce_10 -> layer_15_expert_agg
	layer_15_attn_allreduce_11 -> layer_15_gate_44 [style=dashed]
	layer_15_gate_44 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_44
	layer_15_expert1_44 -> layer_15_expert_act_44
	layer_15_expert_act_44 -> layer_15_expert2_44
	layer_15_expert2_44 -> layer_15_expert_allreduce_11
	layer_15_alltoall -> layer_15_expert1_45
	layer_15_expert1_45 -> layer_15_expert_act_45
	layer_15_expert_act_45 -> layer_15_expert2_45
	layer_15_expert2_45 -> layer_15_expert_allreduce_11
	layer_15_alltoall -> layer_15_expert1_46
	layer_15_expert1_46 -> layer_15_expert_act_46
	layer_15_expert_act_46 -> layer_15_expert2_46
	layer_15_expert2_46 -> layer_15_expert_allreduce_11
	layer_15_alltoall -> layer_15_expert1_47
	layer_15_expert1_47 -> layer_15_expert_act_47
	layer_15_expert_act_47 -> layer_15_expert2_47
	layer_15_expert2_47 -> layer_15_expert_allreduce_11
	layer_15_expert_allreduce_11 -> layer_15_expert_agg
	layer_15_attn_allreduce_12 -> layer_15_gate_48 [style=dashed]
	layer_15_gate_48 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_48
	layer_15_expert1_48 -> layer_15_expert_act_48
	layer_15_expert_act_48 -> layer_15_expert2_48
	layer_15_expert2_48 -> layer_15_expert_allreduce_12
	layer_15_alltoall -> layer_15_expert1_49
	layer_15_expert1_49 -> layer_15_expert_act_49
	layer_15_expert_act_49 -> layer_15_expert2_49
	layer_15_expert2_49 -> layer_15_expert_allreduce_12
	layer_15_alltoall -> layer_15_expert1_50
	layer_15_expert1_50 -> layer_15_expert_act_50
	layer_15_expert_act_50 -> layer_15_expert2_50
	layer_15_expert2_50 -> layer_15_expert_allreduce_12
	layer_15_alltoall -> layer_15_expert1_51
	layer_15_expert1_51 -> layer_15_expert_act_51
	layer_15_expert_act_51 -> layer_15_expert2_51
	layer_15_expert2_51 -> layer_15_expert_allreduce_12
	layer_15_expert_allreduce_12 -> layer_15_expert_agg
	layer_15_attn_allreduce_13 -> layer_15_gate_52 [style=dashed]
	layer_15_gate_52 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_52
	layer_15_expert1_52 -> layer_15_expert_act_52
	layer_15_expert_act_52 -> layer_15_expert2_52
	layer_15_expert2_52 -> layer_15_expert_allreduce_13
	layer_15_alltoall -> layer_15_expert1_53
	layer_15_expert1_53 -> layer_15_expert_act_53
	layer_15_expert_act_53 -> layer_15_expert2_53
	layer_15_expert2_53 -> layer_15_expert_allreduce_13
	layer_15_alltoall -> layer_15_expert1_54
	layer_15_expert1_54 -> layer_15_expert_act_54
	layer_15_expert_act_54 -> layer_15_expert2_54
	layer_15_expert2_54 -> layer_15_expert_allreduce_13
	layer_15_alltoall -> layer_15_expert1_55
	layer_15_expert1_55 -> layer_15_expert_act_55
	layer_15_expert_act_55 -> layer_15_expert2_55
	layer_15_expert2_55 -> layer_15_expert_allreduce_13
	layer_15_expert_allreduce_13 -> layer_15_expert_agg
	layer_15_attn_allreduce_14 -> layer_15_gate_56 [style=dashed]
	layer_15_gate_56 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_56
	layer_15_expert1_56 -> layer_15_expert_act_56
	layer_15_expert_act_56 -> layer_15_expert2_56
	layer_15_expert2_56 -> layer_15_expert_allreduce_14
	layer_15_alltoall -> layer_15_expert1_57
	layer_15_expert1_57 -> layer_15_expert_act_57
	layer_15_expert_act_57 -> layer_15_expert2_57
	layer_15_expert2_57 -> layer_15_expert_allreduce_14
	layer_15_alltoall -> layer_15_expert1_58
	layer_15_expert1_58 -> layer_15_expert_act_58
	layer_15_expert_act_58 -> layer_15_expert2_58
	layer_15_expert2_58 -> layer_15_expert_allreduce_14
	layer_15_alltoall -> layer_15_expert1_59
	layer_15_expert1_59 -> layer_15_expert_act_59
	layer_15_expert_act_59 -> layer_15_expert2_59
	layer_15_expert2_59 -> layer_15_expert_allreduce_14
	layer_15_expert_allreduce_14 -> layer_15_expert_agg
	layer_15_attn_allreduce_15 -> layer_15_gate_60 [style=dashed]
	layer_15_gate_60 -> layer_15_alltoall [style=dashed]
	layer_15_alltoall -> layer_15_expert1_60
	layer_15_expert1_60 -> layer_15_expert_act_60
	layer_15_expert_act_60 -> layer_15_expert2_60
	layer_15_expert2_60 -> layer_15_expert_allreduce_15
	layer_15_alltoall -> layer_15_expert1_61
	layer_15_expert1_61 -> layer_15_expert_act_61
	layer_15_expert_act_61 -> layer_15_expert2_61
	layer_15_expert2_61 -> layer_15_expert_allreduce_15
	layer_15_alltoall -> layer_15_expert1_62
	layer_15_expert1_62 -> layer_15_expert_act_62
	layer_15_expert_act_62 -> layer_15_expert2_62
	layer_15_expert2_62 -> layer_15_expert_allreduce_15
	layer_15_alltoall -> layer_15_expert1_63
	layer_15_expert1_63 -> layer_15_expert_act_63
	layer_15_expert_act_63 -> layer_15_expert2_63
	layer_15_expert2_63 -> layer_15_expert_allreduce_15
	layer_15_expert_allreduce_15 -> layer_15_expert_agg
	layer_15_expert_agg -> layer_15_norm
	output [label="Output\nInput: [batch_size=128, seq_len=512, hidden=512]\nOutput: [batch_size=128, seq_len=512, vocab_size=?]" fillcolor=lightcoral shape=ellipse]
	layer_15_norm -> output
}
