digraph {
	graph [bb="0,0,2340,19474",
		fontname=Arial,
		rankdir=TB,
		size="20,15"
	];
	node [label="\N"];
	subgraph cluster_legend {
		graph [bb="8,19341,922,19416",
			color=gray,
			label=Legend,
			lheight=0.21,
			lp="465,19405",
			lwidth=0.64,
			style=dashed
		];
		legend_compute	[color=lightblue,
			height=0.5,
			label=Computation,
			pos="859,19367",
			shape=rectangle,
			style=filled,
			width=1.5139];
		legend_comm	[color=lightgreen,
			height=0.5,
			label=Communication,
			pos="703,19367",
			shape=ellipse,
			style=filled,
			width=2.3109];
		legend_route	[color=lightyellow,
			height=0.5,
			label="Routing/Aggregation",
			pos="445,19367",
			shape=parallelogram,
			style=filled,
			width=4.3676];
		legend_gather	[color=orange,
			height=0.5,
			label="All-Gather",
			pos="212,19367",
			shape=ellipse,
			style=filled,
			width=1.6068];
		legend_reduce	[color=red,
			height=0.5,
			label="All-Reduce",
			pos="76,19367",
			shape=ellipse,
			style=filled,
			width=1.661];
	}
	subgraph cluster_stage0 {
		graph [bb="930,9784.6,2230,19466",
			label="Pipeline Stage 0 (Devices 0-7)",
			lheight=0.21,
			lp="1580,19455",
			lwidth=2.54,
			style=rounded
		];
		subgraph cluster_layer0 {
			graph [label="Layer 0 (TP=8 across devices 0-7)",
				style=dashed
			];
		}
		subgraph cluster_layer1 {
			graph [label="Layer 1 (TP=8 across devices 0-7)",
				style=dashed
			];
		}
		subgraph cluster_layer2 {
			graph [label="Layer 2 (TP=8 across devices 0-7)",
				style=dashed
			];
		}
		subgraph cluster_layer3 {
			graph [label="Layer 3 (TP=8 across devices 0-7)",
				style=dashed
			];
		}
		subgraph cluster_layer4 {
			graph [label="Layer 4 (TP=8 across devices 0-7)",
				style=dashed
			];
		}
		subgraph cluster_layer5 {
			graph [label="Layer 5 (TP=8 across devices 0-7)",
				style=dashed
			];
		}
		subgraph cluster_layer6 {
			graph [label="Layer 6 (TP=8 across devices 0-7)",
				style=dashed
			];
		}
		subgraph cluster_layer7 {
			graph [label="Layer 7 (TP=8 across devices 0-7)",
				style=dashed
			];
		}
		input	[color=lightyellow,
			height=1.8889,
			label="Input Embedding
Input: [batch=128, seq=10000, hidden=4096]
Device: All GPUs
Layer: Embedding",
			pos="1714,19367",
			shape=parallelogram,
			style=filled,
			width=10.08];
		mha_q_0	[color=lightblue,
			height=0.94444,
			label="Layer0_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1143,19229",
			shape=rectangle,
			style=filled,
			width=5.6806];
		input -> mha_q_0	[pos="e,1281.7,19263 1432.7,19299 1383.9,19288 1334.9,19276 1291.5,19266"];
		mha_k_0	[color=lightblue,
			height=0.94444,
			label="Layer0_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1570,19229",
			shape=rectangle,
			style=filled,
			width=5.6806];
		input -> mha_k_0	[pos="e,1605,19263 1642.8,19299 1632.4,19289 1622,19279 1612.4,19270"];
		mha_v_0	[color=lightblue,
			height=0.94444,
			label="Layer0_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1997,19229",
			shape=rectangle,
			style=filled,
			width=5.6806];
		input -> mha_v_0	[pos="e,1928.1,19263 1853.4,19299 1875.9,19288 1898.5,19278 1918.9,19268"];
		attn_residual_0	[color=lightyellow,
			height=2.3056,
			label="Layer0_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1847,18736",
			shape=parallelogram,
			style=filled,
			width=10.425];
		input -> attn_residual_0	[pos="e,2070,18819 1973.9,19341 2076.6,19325 2178,19301 2211,19263 2251.2,19217 2221,19187 2221,19126 2221,19126 2221,19126 2221,19020 \
2221,18933 2155.2,18869 2078.8,18824"];
		attn_score_0	[color=lightblue,
			height=0.94444,
			label="Layer0_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
0-7 (TP)",
			pos="1570,19125",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_0 -> attn_score_0	[pos="e,1431.5,19159 1281.3,19195 1326.2,19185 1376,19173 1421.6,19162"];
		mha_k_0 -> attn_score_0	[pos="e,1570,19160 1570,19195 1570,19187 1570,19178 1570,19170"];
		attn_out_0	[color=lightblue,
			height=0.94444,
			label="Layer0_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1847,19021",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_0 -> attn_out_0	[pos="e,1757,19055 1659.9,19091 1687.9,19081 1718.9,19069 1747.5,19059"];
		attn_allreduce_0	[color=red,
			height=1.3356,
			label="Layer0_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1847,18903",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_0 -> attn_allreduce_0	[pos="e,1847,18951 1847,18987 1847,18979 1847,18970 1847,18962"];
		attn_allreduce_0 -> attn_residual_0	[pos="e,1847,18819 1847,18855 1847,18847 1847,18838 1847,18830"];
		ffn_up_0	[color=lightblue,
			height=0.94444,
			label="Layer0_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1479,18583",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_0 -> ffn_up_0	[pos="e,1559.8,18617 1647.1,18653 1619.8,18642 1593.1,18631 1569.1,18621"];
		ffn_gate_0	[color=lightblue,
			height=0.94444,
			label="Layer0_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1847,18583",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_0 -> ffn_gate_0	[pos="e,1847,18617 1847,18653 1847,18644 1847,18636 1847,18628"];
		ffn_residual_0	[color=lightyellow,
			height=2.3056,
			label="Layer0_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1806,18194",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_0 -> ffn_residual_0	[pos="e,2001.2,18277 2003.8,18653 2014.5,18642 2023.8,18630 2031,18617 2095.8,18498 2101,18429 2031,18313 2024.7,18302 2017.1,18293 2008.6,\
18284"];
		ffn_down_0	[color=lightblue,
			height=0.94444,
			label="Layer0_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1766,18479",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_0 -> ffn_down_0	[pos="e,1672.8,18513 1572.2,18549 1601.3,18539 1633.5,18527 1663.2,18517"];
		ffn_gate_0 -> ffn_down_0	[pos="e,1792.4,18513 1820.7,18549 1813.7,18540 1806,18531 1798.7,18521"];
		ffn_allreduce_0	[color=red,
			height=1.3356,
			label="Layer0_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1766,18361",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_0 -> ffn_allreduce_0	[pos="e,1766,18409 1766,18445 1766,18437 1766,18428 1766,18419"];
		ffn_allreduce_0 -> ffn_residual_0	[pos="e,1786.1,18277 1777.5,18313 1779.4,18305 1781.5,18296 1783.7,18287"];
		mha_q_1	[color=lightblue,
			height=0.94444,
			label="Layer1_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1143,18041",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_0 -> mha_q_1	[pos="e,1288.1,18075 1445.9,18111 1393.9,18099 1342.9,18087 1297.9,18077"];
		mha_k_1	[color=lightblue,
			height=0.94444,
			label="Layer1_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1570,18041",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_0 -> mha_k_1	[pos="e,1621.7,18075 1677.6,18111 1661.1,18100 1644.8,18090 1630.2,18080"];
		mha_v_1	[color=lightblue,
			height=0.94444,
			label="Layer1_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1997,18041",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_0 -> mha_v_1	[pos="e,1954.9,18075 1909.9,18111 1922.7,18101 1935.3,18091 1946.8,18082"];
		attn_residual_1	[color=lightyellow,
			height=2.3056,
			label="Layer1_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1847,17548",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_0 -> attn_residual_1	[pos="e,2070,17631 2058.3,18144 2127.2,18125 2189,18102 2211,18075 2249.4,18027 2221,17999 2221,17938 2221,17938 2221,17938 2221,17832 \
2221,17745 2155.2,17681 2078.8,17636"];
		attn_score_1	[color=lightblue,
			height=0.94444,
			label="Layer1_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
0-7 (TP)",
			pos="1570,17937",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_1 -> attn_score_1	[pos="e,1431.5,17971 1281.3,18007 1326.2,17996 1376,17984 1421.6,17973"];
		mha_k_1 -> attn_score_1	[pos="e,1570,17971 1570,18007 1570,17999 1570,17990 1570,17981"];
		attn_out_1	[color=lightblue,
			height=0.94444,
			label="Layer1_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1847,17833",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_1 -> attn_out_1	[pos="e,1757,17867 1659.9,17903 1687.9,17893 1718.9,17881 1747.5,17871"];
		attn_allreduce_1	[color=red,
			height=1.3356,
			label="Layer1_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1847,17715",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_1 -> attn_allreduce_1	[pos="e,1847,17763 1847,17799 1847,17791 1847,17782 1847,17773"];
		attn_allreduce_1 -> attn_residual_1	[pos="e,1847,17631 1847,17667 1847,17659 1847,17650 1847,17641"];
		ffn_up_1	[color=lightblue,
			height=0.94444,
			label="Layer1_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1479,17395",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_1 -> ffn_up_1	[pos="e,1559.8,17429 1647.1,17465 1619.8,17454 1593.1,17443 1569.1,17433"];
		ffn_gate_1	[color=lightblue,
			height=0.94444,
			label="Layer1_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1847,17395",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_1 -> ffn_gate_1	[pos="e,1847,17429 1847,17465 1847,17456 1847,17447 1847,17439"];
		ffn_residual_1	[color=lightyellow,
			height=2.3056,
			label="Layer1_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1806,17006",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_1 -> ffn_residual_1	[pos="e,2001.2,17089 2003.8,17465 2014.5,17454 2023.8,17442 2031,17429 2095.8,17310 2101,17240 2031,17125 2024.7,17114 2017.1,17105 2008.6,\
17096"];
		ffn_down_1	[color=lightblue,
			height=0.94444,
			label="Layer1_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1766,17291",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_1 -> ffn_down_1	[pos="e,1672.8,17325 1572.2,17361 1601.3,17350 1633.5,17339 1663.2,17328"];
		ffn_gate_1 -> ffn_down_1	[pos="e,1792.4,17325 1820.7,17361 1813.7,17352 1806,17342 1798.7,17333"];
		ffn_allreduce_1	[color=red,
			height=1.3356,
			label="Layer1_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1766,17173",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_1 -> ffn_allreduce_1	[pos="e,1766,17221 1766,17257 1766,17249 1766,17240 1766,17231"];
		ffn_allreduce_1 -> ffn_residual_1	[pos="e,1786.1,17089 1777.5,17124 1779.4,17116 1781.5,17108 1783.7,17099"];
		mha_q_2	[color=lightblue,
			height=0.94444,
			label="Layer2_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1143,16853",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_1 -> mha_q_2	[pos="e,1288.1,16887 1445.9,16923 1393.9,16911 1342.9,16899 1297.9,16889"];
		mha_k_2	[color=lightblue,
			height=0.94444,
			label="Layer2_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1570,16853",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_1 -> mha_k_2	[pos="e,1621.7,16887 1677.6,16922 1661.1,16912 1644.8,16901 1630.2,16892"];
		mha_v_2	[color=lightblue,
			height=0.94444,
			label="Layer2_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1997,16853",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_1 -> mha_v_2	[pos="e,1954.9,16887 1909.9,16922 1922.7,16912 1935.3,16902 1946.8,16893"];
		attn_residual_2	[color=lightyellow,
			height=2.3056,
			label="Layer2_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1847,16359",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_1 -> attn_residual_2	[pos="e,2070,16442 2058.3,16956 2127.2,16937 2189,16914 2211,16887 2249.4,16839 2221,16811 2221,16750 2221,16750 2221,16750 2221,16644 \
2221,16556 2155.2,16492 2078.8,16448"];
		attn_score_2	[color=lightblue,
			height=0.94444,
			label="Layer2_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
0-7 (TP)",
			pos="1570,16749",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_2 -> attn_score_2	[pos="e,1431.5,16783 1281.3,16819 1326.2,16808 1376,16796 1421.6,16785"];
		mha_k_2 -> attn_score_2	[pos="e,1570,16783 1570,16818 1570,16810 1570,16802 1570,16793"];
		attn_out_2	[color=lightblue,
			height=0.94444,
			label="Layer2_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1847,16645",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_2 -> attn_out_2	[pos="e,1757,16679 1659.9,16714 1687.9,16704 1718.9,16693 1747.5,16682"];
		attn_allreduce_2	[color=red,
			height=1.3356,
			label="Layer2_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1847,16527",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_2 -> attn_allreduce_2	[pos="e,1847,16575 1847,16610 1847,16602 1847,16594 1847,16585"];
		attn_allreduce_2 -> attn_residual_2	[pos="e,1847,16443 1847,16478 1847,16470 1847,16462 1847,16453"];
		ffn_up_2	[color=lightblue,
			height=0.94444,
			label="Layer2_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1479,16206",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_2 -> ffn_up_2	[pos="e,1559.8,16241 1647.1,16276 1619.8,16265 1593.1,16254 1569.1,16244"];
		ffn_gate_2	[color=lightblue,
			height=0.94444,
			label="Layer2_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1847,16206",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_2 -> ffn_gate_2	[pos="e,1847,16241 1847,16276 1847,16268 1847,16259 1847,16251"];
		ffn_residual_2	[color=lightyellow,
			height=2.3056,
			label="Layer2_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1806,15817",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_2 -> ffn_residual_2	[pos="e,2001.2,15900 2003.8,16276 2014.5,16266 2023.8,16254 2031,16240 2095.8,16122 2101,16052 2031,15936 2024.7,15926 2017.1,15916 2008.6,\
15907"];
		ffn_down_2	[color=lightblue,
			height=0.94444,
			label="Layer2_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1766,16102",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_2 -> ffn_down_2	[pos="e,1672.8,16137 1572.2,16172 1601.3,16162 1633.5,16151 1663.2,16140"];
		ffn_gate_2 -> ffn_down_2	[pos="e,1792.4,16137 1820.7,16172 1813.7,16164 1806,16154 1798.7,16145"];
		ffn_allreduce_2	[color=red,
			height=1.3356,
			label="Layer2_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1766,15984",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_2 -> ffn_allreduce_2	[pos="e,1766,16032 1766,16068 1766,16060 1766,16051 1766,16043"];
		ffn_allreduce_2 -> ffn_residual_2	[pos="e,1786.1,15901 1777.5,15936 1779.4,15928 1781.5,15919 1783.7,15910"];
		mha_q_3	[color=lightblue,
			height=0.94444,
			label="Layer3_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1143,15664",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_2 -> mha_q_3	[pos="e,1288.1,15698 1445.9,15734 1393.9,15722 1342.9,15711 1297.9,15701"];
		mha_k_3	[color=lightblue,
			height=0.94444,
			label="Layer3_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1570,15664",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_2 -> mha_k_3	[pos="e,1621.7,15698 1677.6,15734 1661.1,15724 1644.8,15713 1630.2,15704"];
		mha_v_3	[color=lightblue,
			height=0.94444,
			label="Layer3_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1997,15664",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_2 -> mha_v_3	[pos="e,1954.9,15699 1909.9,15734 1922.7,15724 1935.3,15714 1946.8,15705"];
		attn_residual_3	[color=lightyellow,
			height=2.3056,
			label="Layer3_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1847,15171",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_2 -> attn_residual_3	[pos="e,2070,15254 2058.3,15767 2127.2,15749 2189,15725 2211,15698 2249.4,15651 2221,15622 2221,15561 2221,15561 2221,15561 2221,15455 \
2221,15368 2155.2,15304 2078.8,15259"];
		attn_score_3	[color=lightblue,
			height=0.94444,
			label="Layer3_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
0-7 (TP)",
			pos="1570,15560",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_3 -> attn_score_3	[pos="e,1431.5,15594 1281.3,15630 1326.2,15620 1376,15608 1421.6,15597"];
		mha_k_3 -> attn_score_3	[pos="e,1570,15595 1570,15630 1570,15622 1570,15613 1570,15605"];
		attn_out_3	[color=lightblue,
			height=0.94444,
			label="Layer3_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1847,15456",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_3 -> attn_out_3	[pos="e,1757,15490 1659.9,15526 1687.9,15516 1718.9,15504 1747.5,15494"];
		attn_allreduce_3	[color=red,
			height=1.3356,
			label="Layer3_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1847,15338",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_3 -> attn_allreduce_3	[pos="e,1847,15386 1847,15422 1847,15414 1847,15405 1847,15397"];
		attn_allreduce_3 -> attn_residual_3	[pos="e,1847,15254 1847,15290 1847,15282 1847,15273 1847,15265"];
		ffn_up_3	[color=lightblue,
			height=0.94444,
			label="Layer3_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1479,15018",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_3 -> ffn_up_3	[pos="e,1559.8,15052 1647.1,15088 1619.8,15077 1593.1,15066 1569.1,15056"];
		ffn_gate_3	[color=lightblue,
			height=0.94444,
			label="Layer3_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1847,15018",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_3 -> ffn_gate_3	[pos="e,1847,15052 1847,15088 1847,15079 1847,15071 1847,15063"];
		ffn_residual_3	[color=lightyellow,
			height=2.3056,
			label="Layer3_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1806,14629",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_3 -> ffn_residual_3	[pos="e,2001.2,14712 2003.8,15088 2014.5,15077 2023.8,15065 2031,15052 2095.8,14933 2101,14864 2031,14748 2024.7,14737 2017.1,14728 2008.6,\
14719"];
		ffn_down_3	[color=lightblue,
			height=0.94444,
			label="Layer3_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1766,14914",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_3 -> ffn_down_3	[pos="e,1672.8,14948 1572.2,14984 1601.3,14974 1633.5,14962 1663.2,14952"];
		ffn_gate_3 -> ffn_down_3	[pos="e,1792.4,14948 1820.7,14984 1813.7,14975 1806,14966 1798.7,14956"];
		ffn_allreduce_3	[color=red,
			height=1.3356,
			label="Layer3_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1766,14796",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_3 -> ffn_allreduce_3	[pos="e,1766,14844 1766,14880 1766,14872 1766,14863 1766,14854"];
		ffn_allreduce_3 -> ffn_residual_3	[pos="e,1786.1,14712 1777.5,14748 1779.4,14740 1781.5,14731 1783.7,14722"];
		mha_q_4	[color=lightblue,
			height=0.94444,
			label="Layer4_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1143,14476",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_3 -> mha_q_4	[pos="e,1288.1,14510 1445.9,14546 1393.9,14534 1342.9,14522 1297.9,14512"];
		mha_k_4	[color=lightblue,
			height=0.94444,
			label="Layer4_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1570,14476",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_3 -> mha_k_4	[pos="e,1621.7,14510 1677.6,14546 1661.1,14535 1644.8,14525 1630.2,14515"];
		mha_v_4	[color=lightblue,
			height=0.94444,
			label="Layer4_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1997,14476",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_3 -> mha_v_4	[pos="e,1954.9,14510 1909.9,14546 1922.7,14536 1935.3,14526 1946.8,14517"];
		attn_residual_4	[color=lightyellow,
			height=2.3056,
			label="Layer4_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1847,13983",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_3 -> attn_residual_4	[pos="e,2070,14066 2058.3,14579 2127.2,14560 2189,14537 2211,14510 2249.4,14462 2221,14434 2221,14373 2221,14373 2221,14373 2221,14267 \
2221,14180 2155.2,14116 2078.8,14071"];
		attn_score_4	[color=lightblue,
			height=0.94444,
			label="Layer4_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
0-7 (TP)",
			pos="1570,14372",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_4 -> attn_score_4	[pos="e,1431.5,14406 1281.3,14442 1326.2,14431 1376,14419 1421.6,14408"];
		mha_k_4 -> attn_score_4	[pos="e,1570,14406 1570,14442 1570,14434 1570,14425 1570,14416"];
		attn_out_4	[color=lightblue,
			height=0.94444,
			label="Layer4_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1847,14268",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_4 -> attn_out_4	[pos="e,1757,14302 1659.9,14338 1687.9,14328 1718.9,14316 1747.5,14306"];
		attn_allreduce_4	[color=red,
			height=1.3356,
			label="Layer4_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1847,14150",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_4 -> attn_allreduce_4	[pos="e,1847,14198 1847,14234 1847,14226 1847,14217 1847,14208"];
		attn_allreduce_4 -> attn_residual_4	[pos="e,1847,14066 1847,14102 1847,14094 1847,14085 1847,14076"];
		ffn_up_4	[color=lightblue,
			height=0.94444,
			label="Layer4_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1479,13830",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_4 -> ffn_up_4	[pos="e,1559.8,13864 1647.1,13900 1619.8,13889 1593.1,13878 1569.1,13868"];
		ffn_gate_4	[color=lightblue,
			height=0.94444,
			label="Layer4_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1847,13830",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_4 -> ffn_gate_4	[pos="e,1847,13864 1847,13900 1847,13891 1847,13882 1847,13874"];
		ffn_residual_4	[color=lightyellow,
			height=2.3056,
			label="Layer4_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1806,13441",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_4 -> ffn_residual_4	[pos="e,2001.2,13524 2003.8,13900 2014.5,13889 2023.8,13877 2031,13864 2095.8,13745 2101,13675 2031,13560 2024.7,13549 2017.1,13540 2008.6,\
13531"];
		ffn_down_4	[color=lightblue,
			height=0.94444,
			label="Layer4_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1766,13726",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_4 -> ffn_down_4	[pos="e,1672.8,13760 1572.2,13796 1601.3,13785 1633.5,13774 1663.2,13763"];
		ffn_gate_4 -> ffn_down_4	[pos="e,1792.4,13760 1820.7,13796 1813.7,13787 1806,13777 1798.7,13768"];
		ffn_allreduce_4	[color=red,
			height=1.3356,
			label="Layer4_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1766,13608",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_4 -> ffn_allreduce_4	[pos="e,1766,13656 1766,13692 1766,13684 1766,13675 1766,13666"];
		ffn_allreduce_4 -> ffn_residual_4	[pos="e,1786.1,13524 1777.5,13559 1779.4,13551 1781.5,13543 1783.7,13534"];
		mha_q_5	[color=lightblue,
			height=0.94444,
			label="Layer5_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1143,13288",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_4 -> mha_q_5	[pos="e,1288.1,13322 1445.9,13358 1393.9,13346 1342.9,13334 1297.9,13324"];
		mha_k_5	[color=lightblue,
			height=0.94444,
			label="Layer5_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1570,13288",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_4 -> mha_k_5	[pos="e,1621.7,13322 1677.6,13357 1661.1,13347 1644.8,13337 1630.2,13327"];
		mha_v_5	[color=lightblue,
			height=0.94444,
			label="Layer5_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1997,13288",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_4 -> mha_v_5	[pos="e,1954.9,13322 1909.9,13357 1922.7,13347 1935.3,13337 1946.8,13328"];
		attn_residual_5	[color=lightyellow,
			height=2.3056,
			label="Layer5_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1847,12794",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_4 -> attn_residual_5	[pos="e,2070,12877 2058.3,13391 2127.2,13372 2189,13349 2211,13322 2249.4,13274 2221,13246 2221,13185 2221,13185 2221,13185 2221,13079 \
2221,12991 2155.2,12927 2078.8,12883"];
		attn_score_5	[color=lightblue,
			height=0.94444,
			label="Layer5_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
0-7 (TP)",
			pos="1570,13184",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_5 -> attn_score_5	[pos="e,1431.5,13218 1281.3,13254 1326.2,13243 1376,13231 1421.6,13220"];
		mha_k_5 -> attn_score_5	[pos="e,1570,13218 1570,13254 1570,13245 1570,13237 1570,13228"];
		attn_out_5	[color=lightblue,
			height=0.94444,
			label="Layer5_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1847,13080",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_5 -> attn_out_5	[pos="e,1757,13114 1659.9,13150 1687.9,13139 1718.9,13128 1747.5,13117"];
		attn_allreduce_5	[color=red,
			height=1.3356,
			label="Layer5_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1847,12962",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_5 -> attn_allreduce_5	[pos="e,1847,13010 1847,13045 1847,13037 1847,13029 1847,13020"];
		attn_allreduce_5 -> attn_residual_5	[pos="e,1847,12878 1847,12913 1847,12905 1847,12897 1847,12888"];
		ffn_up_5	[color=lightblue,
			height=0.94444,
			label="Layer5_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1479,12641",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_5 -> ffn_up_5	[pos="e,1559.8,12676 1647.1,12711 1619.8,12700 1593.1,12689 1569.1,12679"];
		ffn_gate_5	[color=lightblue,
			height=0.94444,
			label="Layer5_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1847,12641",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_5 -> ffn_gate_5	[pos="e,1847,12676 1847,12711 1847,12703 1847,12694 1847,12686"];
		ffn_residual_5	[color=lightyellow,
			height=2.3056,
			label="Layer5_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1806,12252",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_5 -> ffn_residual_5	[pos="e,2001.2,12335 2003.8,12711 2014.5,12701 2023.8,12689 2031,12675 2095.8,12557 2101,12487 2031,12371 2024.7,12361 2017.1,12351 2008.6,\
12342"];
		ffn_down_5	[color=lightblue,
			height=0.94444,
			label="Layer5_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1766,12537",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_5 -> ffn_down_5	[pos="e,1672.8,12572 1572.2,12607 1601.3,12597 1633.5,12586 1663.2,12575"];
		ffn_gate_5 -> ffn_down_5	[pos="e,1792.4,12572 1820.7,12607 1813.7,12599 1806,12589 1798.7,12580"];
		ffn_allreduce_5	[color=red,
			height=1.3356,
			label="Layer5_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1766,12419",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_5 -> ffn_allreduce_5	[pos="e,1766,12467 1766,12503 1766,12495 1766,12486 1766,12478"];
		ffn_allreduce_5 -> ffn_residual_5	[pos="e,1786.1,12336 1777.5,12371 1779.4,12363 1781.5,12354 1783.7,12345"];
		mha_q_6	[color=lightblue,
			height=0.94444,
			label="Layer6_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1143,12099",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_5 -> mha_q_6	[pos="e,1288.1,12133 1445.9,12169 1393.9,12157 1342.9,12146 1297.9,12136"];
		mha_k_6	[color=lightblue,
			height=0.94444,
			label="Layer6_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1570,12099",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_5 -> mha_k_6	[pos="e,1621.7,12133 1677.6,12169 1661.1,12159 1644.8,12148 1630.2,12139"];
		mha_v_6	[color=lightblue,
			height=0.94444,
			label="Layer6_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1997,12099",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_5 -> mha_v_6	[pos="e,1954.9,12134 1909.9,12169 1922.7,12159 1935.3,12149 1946.8,12140"];
		attn_residual_6	[color=lightyellow,
			height=2.3056,
			label="Layer6_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1847,11606",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_5 -> attn_residual_6	[pos="e,2070,11689 2058.3,12202 2127.2,12184 2189,12160 2211,12133 2249.4,12086 2221,12057 2221,11996 2221,11996 2221,11996 2221,11890 \
2221,11803 2155.2,11739 2078.8,11694"];
		attn_score_6	[color=lightblue,
			height=0.94444,
			label="Layer6_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
0-7 (TP)",
			pos="1570,11995",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_6 -> attn_score_6	[pos="e,1431.5,12029 1281.3,12065 1326.2,12055 1376,12043 1421.6,12032"];
		mha_k_6 -> attn_score_6	[pos="e,1570,12030 1570,12065 1570,12057 1570,12048 1570,12040"];
		attn_out_6	[color=lightblue,
			height=0.94444,
			label="Layer6_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1847,11891",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_6 -> attn_out_6	[pos="e,1757,11925 1659.9,11961 1687.9,11951 1718.9,11939 1747.5,11929"];
		attn_allreduce_6	[color=red,
			height=1.3356,
			label="Layer6_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1847,11773",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_6 -> attn_allreduce_6	[pos="e,1847,11821 1847,11857 1847,11849 1847,11840 1847,11832"];
		attn_allreduce_6 -> attn_residual_6	[pos="e,1847,11689 1847,11725 1847,11717 1847,11708 1847,11700"];
		ffn_up_6	[color=lightblue,
			height=0.94444,
			label="Layer6_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1479,11453",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_6 -> ffn_up_6	[pos="e,1559.8,11487 1647.1,11523 1619.8,11512 1593.1,11501 1569.1,11491"];
		ffn_gate_6	[color=lightblue,
			height=0.94444,
			label="Layer6_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1847,11453",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_6 -> ffn_gate_6	[pos="e,1847,11487 1847,11523 1847,11514 1847,11506 1847,11498"];
		ffn_residual_6	[color=lightyellow,
			height=2.3056,
			label="Layer6_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1806,11064",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_6 -> ffn_residual_6	[pos="e,2001.2,11147 2003.8,11523 2014.5,11512 2023.8,11500 2031,11487 2095.8,11368 2101,11299 2031,11183 2024.7,11172 2017.1,11163 2008.6,\
11154"];
		ffn_down_6	[color=lightblue,
			height=0.94444,
			label="Layer6_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1766,11349",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_6 -> ffn_down_6	[pos="e,1672.8,11383 1572.2,11419 1601.3,11409 1633.5,11397 1663.2,11387"];
		ffn_gate_6 -> ffn_down_6	[pos="e,1792.4,11383 1820.7,11419 1813.7,11410 1806,11401 1798.7,11391"];
		ffn_allreduce_6	[color=red,
			height=1.3356,
			label="Layer6_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1766,11231",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_6 -> ffn_allreduce_6	[pos="e,1766,11279 1766,11315 1766,11307 1766,11298 1766,11289"];
		ffn_allreduce_6 -> ffn_residual_6	[pos="e,1786.1,11147 1777.5,11183 1779.4,11175 1781.5,11166 1783.7,11157"];
		mha_q_7	[color=lightblue,
			height=0.94444,
			label="Layer7_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1143,10911",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_6 -> mha_q_7	[pos="e,1288.1,10945 1445.9,10981 1393.9,10969 1342.9,10957 1297.9,10947"];
		mha_k_7	[color=lightblue,
			height=0.94444,
			label="Layer7_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1570,10911",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_6 -> mha_k_7	[pos="e,1621.7,10945 1677.6,10981 1661.1,10970 1644.8,10960 1630.2,10950"];
		mha_v_7	[color=lightblue,
			height=0.94444,
			label="Layer7_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 0-7 (TP)",
			pos="1997,10911",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_6 -> mha_v_7	[pos="e,1954.9,10945 1909.9,10981 1922.7,10971 1935.3,10961 1946.8,10952"];
		attn_residual_7	[color=lightyellow,
			height=2.3056,
			label="Layer7_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1847,10418",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_6 -> attn_residual_7	[pos="e,2070,10501 2058.3,11014 2127.2,10995 2189,10972 2211,10945 2249.4,10897 2221,10869 2221,10808 2221,10808 2221,10808 2221,10702 \
2221,10615 2155.2,10551 2078.8,10506"];
		attn_score_7	[color=lightblue,
			height=0.94444,
			label="Layer7_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
0-7 (TP)",
			pos="1570,10807",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_7 -> attn_score_7	[pos="e,1431.5,10841 1281.3,10877 1326.2,10866 1376,10854 1421.6,10843"];
		mha_k_7 -> attn_score_7	[pos="e,1570,10841 1570,10877 1570,10869 1570,10860 1570,10851"];
		attn_out_7	[color=lightblue,
			height=0.94444,
			label="Layer7_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1847,10703",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_7 -> attn_out_7	[pos="e,1757,10737 1659.9,10773 1687.9,10763 1718.9,10751 1747.5,10741"];
		attn_allreduce_7	[color=red,
			height=1.3356,
			label="Layer7_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1847,10585",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_7 -> attn_allreduce_7	[pos="e,1847,10633 1847,10669 1847,10661 1847,10652 1847,10643"];
		attn_allreduce_7 -> attn_residual_7	[pos="e,1847,10501 1847,10537 1847,10529 1847,10520 1847,10511"];
		ffn_up_7	[color=lightblue,
			height=0.94444,
			label="Layer7_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1479,10265",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_7 -> ffn_up_7	[pos="e,1559.8,10299 1647.1,10335 1619.8,10324 1593.1,10313 1569.1,10303"];
		ffn_gate_7	[color=lightblue,
			height=0.94444,
			label="Layer7_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 0-7 (TP)",
			pos="1847,10265",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_7 -> ffn_gate_7	[pos="e,1847,10299 1847,10335 1847,10326 1847,10317 1847,10309"];
		ffn_residual_7	[color=lightyellow,
			height=2.3056,
			label="Layer7_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 0-7",
			pos="1806,9875.6",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_7 -> ffn_residual_7	[pos="e,2001.2,9958.6 2003.8,10335 2014.5,10324 2023.8,10312 2031,10299 2095.8,10180 2101,10110 2031,9994.6 2024.7,9984.1 2017.1,9974.6 \
2008.6,9965.8"];
		ffn_down_7	[color=lightblue,
			height=0.94444,
			label="Layer7_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (TP)",
			pos="1766,10161",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_7 -> ffn_down_7	[pos="e,1672.8,10195 1572.2,10231 1601.3,10220 1633.5,10209 1663.2,10198"];
		ffn_gate_7 -> ffn_down_7	[pos="e,1792.4,10195 1820.7,10231 1813.7,10222 1806,10212 1798.7,10203"];
		ffn_allreduce_7	[color=red,
			height=1.3356,
			label="Layer7_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 0-7 (All-Reduce)",
			pos="1766,10043",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_7 -> ffn_allreduce_7	[pos="e,1766,10091 1766,10127 1766,10119 1766,10110 1766,10101"];
		ffn_allreduce_7 -> ffn_residual_7	[pos="e,1786.1,9958.9 1777.5,9994.4 1779.4,9986.3 1781.5,9977.6 1783.7,9968.8"];
	}
	subgraph cluster_stage1 {
		graph [bb="824,164,2124,9673.7",
			label="Pipeline Stage 1 (Devices 8-15)",
			lheight=0.21,
			lp="1474,9662.2",
			lwidth=2.64,
			style=rounded
		];
		subgraph cluster_layer8 {
			graph [label="Layer 8 (TP=8 across devices 8-15)",
				style=dashed
			];
		}
		subgraph cluster_layer9 {
			graph [label="Layer 9 (TP=8 across devices 8-15)",
				style=dashed
			];
		}
		subgraph cluster_layer10 {
			graph [label="Layer 10 (TP=8 across devices 8-15)",
				style=dashed
			];
		}
		subgraph cluster_layer11 {
			graph [label="Layer 11 (TP=8 across devices 8-15)",
				style=dashed
			];
		}
		subgraph cluster_layer12 {
			graph [label="Layer 12 (TP=8 across devices 8-15)",
				style=dashed
			];
		}
		subgraph cluster_layer13 {
			graph [label="Layer 13 (TP=8 across devices 8-15)",
				style=dashed
			];
		}
		subgraph cluster_layer14 {
			graph [label="Layer 14 (TP=8 across devices 8-15)",
				style=dashed
			];
		}
		subgraph cluster_layer15 {
			graph [label="Layer 15 (TP=8 across devices 8-15)",
				style=dashed
			];
		}
		mha_q_8	[color=lightblue,
			height=0.94444,
			label="Layer8_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1057,9608.7",
			shape=rectangle,
			style=filled,
			width=5.6806];
		attn_score_8	[color=lightblue,
			height=0.94444,
			label="Layer8_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
8-15 (TP)",
			pos="1821,9504.7",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_8 -> attn_score_8	[pos="e,1607,9531.1 1261.7,9576 1264.8,9575.5 1267.9,9575.1 1271,9574.7 1378,9559.7 1496.9,9544.6 1596.7,9532.3"];
		mha_k_8	[color=lightblue,
			height=0.94444,
			label="Layer8_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1911,9608.7",
			shape=rectangle,
			style=filled,
			width=5.6806];
		mha_k_8 -> attn_score_8	[pos="e,1850.4,9539 1881.8,9574.5 1873.9,9565.6 1865.3,9555.9 1857.1,9546.6"];
		mha_v_8	[color=lightblue,
			height=0.94444,
			label="Layer8_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1484,9608.7",
			shape=rectangle,
			style=filled,
			width=5.6806];
		attn_out_8	[color=lightblue,
			height=0.94444,
			label="Layer8_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1821,9400.7",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_8 -> attn_out_8	[pos="e,1821,9435 1821,9470.5 1821,9462.4 1821,9453.6 1821,9445.1"];
		attn_allreduce_8	[color=red,
			height=1.3356,
			label="Layer8_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1801,9282.6",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_8 -> attn_allreduce_8	[pos="e,1809.1,9330.7 1815.3,9366.4 1813.9,9358.3 1812.3,9349.4 1810.8,9340.6"];
		attn_residual_8	[color=lightyellow,
			height=2.3056,
			label="Layer8_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1741,9115.5",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_allreduce_8 -> attn_residual_8	[pos="e,1770.9,9198.7 1783.8,9234.3 1780.8,9226 1777.6,9217.1 1774.3,9208.1"];
		ffn_up_8	[color=lightblue,
			height=0.94444,
			label="Layer8_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1373,8962.5",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_8 -> ffn_up_8	[pos="e,1453.8,8996.6 1541.1,9032.5 1513.8,9021.3 1487.1,9010.3 1463.1,9000.5"];
		ffn_gate_8	[color=lightblue,
			height=0.94444,
			label="Layer8_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1741,8962.5",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_8 -> ffn_gate_8	[pos="e,1741,8996.8 1741,9032.4 1741,9023.6 1741,9015 1741,9006.9"];
		ffn_residual_8	[color=lightyellow,
			height=2.3056,
			label="Layer8_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1700,8573.3",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_8 -> ffn_residual_8	[pos="e,1895.2,8656.3 1897.8,9032.3 1908.5,9021.6 1917.8,9009.7 1925,8996.5 1989.8,8877.9 1995,8808 1925,8692.3 1918.7,8681.9 1911.1,8672.3 \
1902.6,8663.5"];
		ffn_down_8	[color=lightblue,
			height=0.94444,
			label="Layer8_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1660,8858.5",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_8 -> ffn_down_8	[pos="e,1566.8,8892.6 1466.2,8928.4 1495.3,8918 1527.5,8906.6 1557.2,8896"];
		ffn_gate_8 -> ffn_down_8	[pos="e,1686.4,8892.8 1714.7,8928.4 1707.7,8919.6 1700,8909.9 1692.7,8900.7"];
		ffn_allreduce_8	[color=red,
			height=1.3356,
			label="Layer8_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1660,8740.4",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_8 -> ffn_allreduce_8	[pos="e,1660,8788.5 1660,8824.2 1660,8816.2 1660,8807.5 1660,8798.7"];
		ffn_allreduce_8 -> ffn_residual_8	[pos="e,1680.1,8656.6 1671.5,8692.1 1673.4,8684 1675.5,8675.4 1677.7,8666.5"];
		mha_q_9	[color=lightblue,
			height=0.94444,
			label="Layer9_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1037,8420.3",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_8 -> mha_q_9	[pos="e,1182.1,8454.4 1339.9,8490.3 1287.9,8478.5 1236.9,8466.9 1191.9,8456.6"];
		mha_k_9	[color=lightblue,
			height=0.94444,
			label="Layer9_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1464,8420.3",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_8 -> mha_k_9	[pos="e,1515.7,8454.4 1571.6,8490.2 1555.1,8479.6 1538.8,8469.2 1524.2,8459.8"];
		mha_v_9	[color=lightblue,
			height=0.94444,
			label="Layer9_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1891,8420.3",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_8 -> mha_v_9	[pos="e,1848.9,8454.6 1803.9,8490.2 1816.7,8480.1 1829.3,8470.1 1840.8,8461"];
		attn_residual_9	[color=lightyellow,
			height=2.3056,
			label="Layer9_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1741,7927.2",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_8 -> attn_residual_9	[pos="e,1964,8010.2 1952.3,8523.3 2021.2,8504.8 2083,8481.5 2105,8454.3 2143.4,8406.9 2115,8378.4 2115,8317.3 2115,8317.3 2115,8317.3 \
2115,8211.3 2115,8123.9 2049.2,8060 1972.8,8015.3"];
		attn_score_9	[color=lightblue,
			height=0.94444,
			label="Layer9_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
8-15 (TP)",
			pos="1464,8316.3",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_9 -> attn_score_9	[pos="e,1325.5,8350.4 1175.3,8386.3 1220.2,8375.6 1270,8363.7 1315.6,8352.8"];
		mha_k_9 -> attn_score_9	[pos="e,1464,8350.6 1464,8386.2 1464,8378.1 1464,8369.3 1464,8360.8"];
		attn_out_9	[color=lightblue,
			height=0.94444,
			label="Layer9_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1741,8212.3",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_9 -> attn_out_9	[pos="e,1651,8246.5 1553.9,8282.2 1581.9,8271.9 1612.9,8260.5 1641.5,8250"];
		attn_allreduce_9	[color=red,
			height=1.3356,
			label="Layer9_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1741,8094.2",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_9 -> attn_allreduce_9	[pos="e,1741,8142.4 1741,8178.1 1741,8170.1 1741,8161.3 1741,8152.6"];
		attn_allreduce_9 -> attn_residual_9	[pos="e,1741,8010.4 1741,8045.9 1741,8037.9 1741,8029.4 1741,8020.7"];
		ffn_up_9	[color=lightblue,
			height=0.94444,
			label="Layer9_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1373,7774.2",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_9 -> ffn_up_9	[pos="e,1453.8,7808.3 1541.1,7844.1 1513.8,7833 1487.1,7822 1463.1,7812.2"];
		ffn_gate_9	[color=lightblue,
			height=0.94444,
			label="Layer9_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1741,7774.2",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_9 -> ffn_gate_9	[pos="e,1741,7808.4 1741,7844 1741,7835.3 1741,7826.6 1741,7818.6"];
		ffn_residual_9	[color=lightyellow,
			height=2.3056,
			label="Layer9_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1700,7385",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_9 -> ffn_residual_9	[pos="e,1895.2,7468 1897.8,7843.9 1908.5,7833.2 1917.8,7821.3 1925,7808.2 1989.8,7689.5 1995,7619.7 1925,7504 1918.7,7493.5 1911.1,7483.9 \
1902.6,7475.2"];
		ffn_down_9	[color=lightblue,
			height=0.94444,
			label="Layer9_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1660,7670.2",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_9 -> ffn_down_9	[pos="e,1566.8,7704.3 1466.2,7740 1495.3,7729.7 1527.5,7718.2 1557.2,7707.7"];
		ffn_gate_9 -> ffn_down_9	[pos="e,1686.4,7704.5 1714.7,7740 1707.7,7731.2 1700,7721.6 1692.7,7712.4"];
		ffn_allreduce_9	[color=red,
			height=1.3356,
			label="Layer9_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1660,7552.1",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_9 -> ffn_allreduce_9	[pos="e,1660,7600.2 1660,7635.9 1660,7627.9 1660,7619.1 1660,7610.4"];
		ffn_allreduce_9 -> ffn_residual_9	[pos="e,1680.1,7468.2 1671.5,7503.8 1673.4,7495.7 1675.5,7487 1677.7,7478.2"];
		mha_q_10	[color=lightblue,
			height=0.94444,
			label="Layer10_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1037,7232",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_9 -> mha_q_10	[pos="e,1182.1,7266.1 1339.9,7302 1287.9,7290.1 1236.9,7278.5 1191.9,7268.3"];
		mha_k_10	[color=lightblue,
			height=0.94444,
			label="Layer10_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1464,7232",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_9 -> mha_k_10	[pos="e,1515.7,7266.1 1571.6,7301.9 1555.1,7291.3 1538.8,7280.9 1524.2,7271.5"];
		mha_v_10	[color=lightblue,
			height=0.94444,
			label="Layer10_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1891,7232",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_9 -> mha_v_10	[pos="e,1848.9,7266.3 1803.9,7301.9 1816.7,7291.7 1829.3,7281.8 1840.8,7272.7"];
		attn_residual_10	[color=lightyellow,
			height=2.3056,
			label="Layer10_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1741,6738.8",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_9 -> attn_residual_10	[pos="e,1964,6821.8 1952.3,7335 2021.2,7316.5 2083,7293.2 2105,7266 2143.4,7218.5 2115,7190 2115,7129 2115,7129 2115,7129 2115,7023 2115,\
6935.6 2049.2,6871.7 1972.8,6826.9"];
		attn_score_10	[color=lightblue,
			height=0.94444,
			label="Layer10_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
8-15 (TP)",
			pos="1464,7128",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_10 -> attn_score_10	[pos="e,1325.5,7162.1 1175.3,7198 1220.2,7187.2 1270,7175.3 1315.6,7164.5"];
		mha_k_10 -> attn_score_10	[pos="e,1464,7162.3 1464,7197.9 1464,7189.8 1464,7181 1464,7172.5"];
		attn_out_10	[color=lightblue,
			height=0.94444,
			label="Layer10_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1741,7024",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_10 -> attn_out_10	[pos="e,1651,7058.1 1553.9,7093.9 1581.9,7083.6 1612.9,7072.2 1641.5,7061.7"];
		attn_allreduce_10	[color=red,
			height=1.3356,
			label="Layer10_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1741,6905.9",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_10 -> attn_allreduce_10	[pos="e,1741,6954 1741,6989.7 1741,6981.7 1741,6973 1741,6964.2"];
		attn_allreduce_10 -> attn_residual_10	[pos="e,1741,6822.1 1741,6857.6 1741,6849.6 1741,6841 1741,6832.3"];
		ffn_up_10	[color=lightblue,
			height=0.94444,
			label="Layer10_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1373,6585.8",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_10 -> ffn_up_10	[pos="e,1453.8,6620 1541.1,6655.8 1513.8,6644.6 1487.1,6633.6 1463.1,6623.8"];
		ffn_gate_10	[color=lightblue,
			height=0.94444,
			label="Layer10_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1741,6585.8",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_10 -> ffn_gate_10	[pos="e,1741,6620.1 1741,6655.7 1741,6647 1741,6638.3 1741,6630.3"];
		ffn_residual_10	[color=lightyellow,
			height=2.3056,
			label="Layer10_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1700,6196.7",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_10 -> ffn_residual_10	[pos="e,1895.2,6279.7 1897.8,6655.6 1908.5,6644.9 1917.8,6633 1925,6619.8 1989.8,6501.2 1995,6431.3 1925,6315.7 1918.7,6305.2 1911.1,6295.6 \
1902.6,6286.8"];
		ffn_down_10	[color=lightblue,
			height=0.94444,
			label="Layer10_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1660,6481.8",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_10 -> ffn_down_10	[pos="e,1566.8,6516 1466.2,6551.7 1495.3,6541.4 1527.5,6529.9 1557.2,6519.4"];
		ffn_gate_10 -> ffn_down_10	[pos="e,1686.4,6516.1 1714.7,6551.7 1707.7,6542.9 1700,6533.3 1692.7,6524.1"];
		ffn_allreduce_10	[color=red,
			height=1.3356,
			label="Layer10_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1660,6363.7",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_10 -> ffn_allreduce_10	[pos="e,1660,6411.9 1660,6447.6 1660,6439.6 1660,6430.8 1660,6422.1"];
		ffn_allreduce_10 -> ffn_residual_10	[pos="e,1680.1,6279.9 1671.5,6315.4 1673.4,6307.3 1675.5,6298.7 1677.7,6289.9"];
		mha_q_11	[color=lightblue,
			height=0.94444,
			label="Layer11_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1037,6043.7",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_10 -> mha_q_11	[pos="e,1182.1,6077.7 1339.9,6113.6 1287.9,6101.8 1236.9,6090.2 1191.9,6080"];
		mha_k_11	[color=lightblue,
			height=0.94444,
			label="Layer11_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1464,6043.7",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_10 -> mha_k_11	[pos="e,1515.7,6077.8 1571.6,6113.5 1555.1,6103 1538.8,6092.5 1524.2,6083.2"];
		mha_v_11	[color=lightblue,
			height=0.94444,
			label="Layer11_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1891,6043.7",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_10 -> mha_v_11	[pos="e,1848.9,6077.9 1803.9,6113.5 1816.7,6103.4 1829.3,6093.4 1840.8,6084.4"];
		attn_residual_11	[color=lightyellow,
			height=2.3056,
			label="Layer11_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1741,5550.5",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_10 -> attn_residual_11	[pos="e,1964,5633.5 1952.3,6146.7 2021.2,6128.1 2083,6104.8 2105,6077.7 2143.4,6030.2 2115,6001.7 2115,5940.7 2115,5940.7 2115,5940.7 \
2115,5834.7 2115,5747.2 2049.2,5683.3 1972.8,5638.6"];
		attn_score_11	[color=lightblue,
			height=0.94444,
			label="Layer11_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
8-15 (TP)",
			pos="1464,5939.7",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_11 -> attn_score_11	[pos="e,1325.5,5973.7 1175.3,6009.6 1220.2,5998.9 1270,5987 1315.6,5976.1"];
		mha_k_11 -> attn_score_11	[pos="e,1464,5974 1464,6009.5 1464,6001.4 1464,5992.6 1464,5984.1"];
		attn_out_11	[color=lightblue,
			height=0.94444,
			label="Layer11_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1741,5835.7",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_11 -> attn_out_11	[pos="e,1651,5869.8 1553.9,5905.5 1581.9,5895.3 1612.9,5883.8 1641.5,5873.3"];
		attn_allreduce_11	[color=red,
			height=1.3356,
			label="Layer11_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1741,5717.6",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_11 -> attn_allreduce_11	[pos="e,1741,5765.7 1741,5801.4 1741,5793.4 1741,5784.6 1741,5775.9"];
		attn_allreduce_11 -> attn_residual_11	[pos="e,1741,5633.7 1741,5669.3 1741,5661.3 1741,5652.7 1741,5644"];
		ffn_up_11	[color=lightblue,
			height=0.94444,
			label="Layer11_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1373,5397.5",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_11 -> ffn_up_11	[pos="e,1453.8,5431.6 1541.1,5467.5 1513.8,5456.3 1487.1,5445.3 1463.1,5435.5"];
		ffn_gate_11	[color=lightblue,
			height=0.94444,
			label="Layer11_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1741,5397.5",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_11 -> ffn_gate_11	[pos="e,1741,5431.8 1741,5467.4 1741,5458.6 1741,5450 1741,5441.9"];
		ffn_residual_11	[color=lightyellow,
			height=2.3056,
			label="Layer11_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1700,5008.3",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_11 -> ffn_residual_11	[pos="e,1895.2,5091.3 1897.8,5467.3 1908.5,5456.6 1917.8,5444.7 1925,5431.5 1989.8,5312.9 1995,5243 1925,5127.3 1918.7,5116.9 1911.1,5107.3 \
1902.6,5098.5"];
		ffn_down_11	[color=lightblue,
			height=0.94444,
			label="Layer11_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1660,5293.5",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_11 -> ffn_down_11	[pos="e,1566.8,5327.6 1466.2,5363.4 1495.3,5353 1527.5,5341.6 1557.2,5331"];
		ffn_gate_11 -> ffn_down_11	[pos="e,1686.4,5327.8 1714.7,5363.4 1707.7,5354.6 1700,5344.9 1692.7,5335.7"];
		ffn_allreduce_11	[color=red,
			height=1.3356,
			label="Layer11_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1660,5175.4",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_11 -> ffn_allreduce_11	[pos="e,1660,5223.5 1660,5259.2 1660,5251.2 1660,5242.5 1660,5233.7"];
		ffn_allreduce_11 -> ffn_residual_11	[pos="e,1680.1,5091.6 1671.5,5127.1 1673.4,5119 1675.5,5110.4 1677.7,5101.5"];
		mha_q_12	[color=lightblue,
			height=0.94444,
			label="Layer12_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1037,4855.3",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_11 -> mha_q_12	[pos="e,1182.1,4889.4 1339.9,4925.3 1287.9,4913.5 1236.9,4901.9 1191.9,4891.6"];
		mha_k_12	[color=lightblue,
			height=0.94444,
			label="Layer12_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1464,4855.3",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_11 -> mha_k_12	[pos="e,1515.7,4889.4 1571.6,4925.2 1555.1,4914.6 1538.8,4904.2 1524.2,4894.8"];
		mha_v_12	[color=lightblue,
			height=0.94444,
			label="Layer12_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1891,4855.3",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_11 -> mha_v_12	[pos="e,1848.9,4889.6 1803.9,4925.2 1816.7,4915.1 1829.3,4905.1 1840.8,4896"];
		attn_residual_12	[color=lightyellow,
			height=2.3056,
			label="Layer12_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1741,4362.2",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_11 -> attn_residual_12	[pos="e,1964,4445.2 1952.3,4958.3 2021.2,4939.8 2083,4916.5 2105,4889.3 2143.4,4841.9 2115,4813.4 2115,4752.3 2115,4752.3 2115,4752.3 \
2115,4646.3 2115,4558.9 2049.2,4495 1972.8,4450.3"];
		attn_score_12	[color=lightblue,
			height=0.94444,
			label="Layer12_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
8-15 (TP)",
			pos="1464,4751.3",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_12 -> attn_score_12	[pos="e,1325.5,4785.4 1175.3,4821.3 1220.2,4810.6 1270,4798.7 1315.6,4787.8"];
		mha_k_12 -> attn_score_12	[pos="e,1464,4785.6 1464,4821.2 1464,4813.1 1464,4804.3 1464,4795.8"];
		attn_out_12	[color=lightblue,
			height=0.94444,
			label="Layer12_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1741,4647.3",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_12 -> attn_out_12	[pos="e,1651,4681.5 1553.9,4717.2 1581.9,4706.9 1612.9,4695.5 1641.5,4685"];
		attn_allreduce_12	[color=red,
			height=1.3356,
			label="Layer12_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1741,4529.2",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_12 -> attn_allreduce_12	[pos="e,1741,4577.4 1741,4613.1 1741,4605.1 1741,4596.3 1741,4587.6"];
		attn_allreduce_12 -> attn_residual_12	[pos="e,1741,4445.4 1741,4480.9 1741,4472.9 1741,4464.4 1741,4455.7"];
		ffn_up_12	[color=lightblue,
			height=0.94444,
			label="Layer12_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1373,4209.2",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_12 -> ffn_up_12	[pos="e,1453.8,4243.3 1541.1,4279.1 1513.8,4268 1487.1,4257 1463.1,4247.2"];
		ffn_gate_12	[color=lightblue,
			height=0.94444,
			label="Layer12_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1741,4209.2",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_12 -> ffn_gate_12	[pos="e,1741,4243.4 1741,4279 1741,4270.3 1741,4261.7 1741,4253.6"];
		ffn_residual_12	[color=lightyellow,
			height=2.3056,
			label="Layer12_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1700,3820",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_12 -> ffn_residual_12	[pos="e,1895.2,3903 1897.8,4278.9 1908.5,4268.2 1917.8,4256.3 1925,4243.2 1989.8,4124.5 1995,4054.7 1925,3939 1918.7,3928.5 1911.1,3918.9 \
1902.6,3910.2"];
		ffn_down_12	[color=lightblue,
			height=0.94444,
			label="Layer12_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1660,4105.2",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_12 -> ffn_down_12	[pos="e,1566.8,4139.3 1466.2,4175 1495.3,4164.7 1527.5,4153.2 1557.2,4142.7"];
		ffn_gate_12 -> ffn_down_12	[pos="e,1686.4,4139.5 1714.7,4175 1707.7,4166.2 1700,4156.6 1692.7,4147.4"];
		ffn_allreduce_12	[color=red,
			height=1.3356,
			label="Layer12_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1660,3987.1",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_12 -> ffn_allreduce_12	[pos="e,1660,4035.2 1660,4070.9 1660,4062.9 1660,4054.1 1660,4045.4"];
		ffn_allreduce_12 -> ffn_residual_12	[pos="e,1680.1,3903.2 1671.5,3938.8 1673.4,3930.7 1675.5,3922 1677.7,3913.2"];
		mha_q_13	[color=lightblue,
			height=0.94444,
			label="Layer13_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1037,3667",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_12 -> mha_q_13	[pos="e,1182.1,3701.1 1339.9,3737 1287.9,3725.1 1236.9,3713.5 1191.9,3703.3"];
		mha_k_13	[color=lightblue,
			height=0.94444,
			label="Layer13_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1464,3667",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_12 -> mha_k_13	[pos="e,1515.7,3701.1 1571.6,3736.9 1555.1,3726.3 1538.8,3715.9 1524.2,3706.5"];
		mha_v_13	[color=lightblue,
			height=0.94444,
			label="Layer13_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1891,3667",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_12 -> mha_v_13	[pos="e,1848.9,3701.3 1803.9,3736.9 1816.7,3726.7 1829.3,3716.8 1840.8,3707.7"];
		attn_residual_13	[color=lightyellow,
			height=2.3056,
			label="Layer13_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1741,3173.8",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_12 -> attn_residual_13	[pos="e,1964,3256.8 1952.3,3770 2021.2,3751.5 2083,3728.2 2105,3701 2143.4,3653.5 2115,3625.1 2115,3564 2115,3564 2115,3564 2115,3458 \
2115,3370.6 2049.2,3306.7 1972.8,3261.9"];
		attn_score_13	[color=lightblue,
			height=0.94444,
			label="Layer13_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
8-15 (TP)",
			pos="1464,3563",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_13 -> attn_score_13	[pos="e,1325.5,3597.1 1175.3,3633 1220.2,3622.2 1270,3610.3 1315.6,3599.5"];
		mha_k_13 -> attn_score_13	[pos="e,1464,3597.3 1464,3632.9 1464,3624.8 1464,3616 1464,3607.5"];
		attn_out_13	[color=lightblue,
			height=0.94444,
			label="Layer13_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1741,3459",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_13 -> attn_out_13	[pos="e,1651,3493.1 1553.9,3528.9 1581.9,3518.6 1612.9,3507.2 1641.5,3496.7"];
		attn_allreduce_13	[color=red,
			height=1.3356,
			label="Layer13_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1741,3340.9",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_13 -> attn_allreduce_13	[pos="e,1741,3389 1741,3424.7 1741,3416.7 1741,3408 1741,3399.2"];
		attn_allreduce_13 -> attn_residual_13	[pos="e,1741,3257.1 1741,3292.6 1741,3284.6 1741,3276 1741,3267.3"];
		ffn_up_13	[color=lightblue,
			height=0.94444,
			label="Layer13_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1373,3020.8",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_13 -> ffn_up_13	[pos="e,1453.8,3055 1541.1,3090.8 1513.8,3079.6 1487.1,3068.6 1463.1,3058.8"];
		ffn_gate_13	[color=lightblue,
			height=0.94444,
			label="Layer13_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1741,3020.8",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_13 -> ffn_gate_13	[pos="e,1741,3055.1 1741,3090.7 1741,3082 1741,3073.3 1741,3065.3"];
		ffn_residual_13	[color=lightyellow,
			height=2.3056,
			label="Layer13_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1700,2631.7",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_13 -> ffn_residual_13	[pos="e,1895.2,2714.7 1897.8,3090.6 1908.5,3079.9 1917.8,3068 1925,3054.8 1989.8,2936.2 1995,2866.3 1925,2750.7 1918.7,2740.2 1911.1,2730.6 \
1902.6,2721.8"];
		ffn_down_13	[color=lightblue,
			height=0.94444,
			label="Layer13_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1660,2916.8",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_13 -> ffn_down_13	[pos="e,1566.8,2951 1466.2,2986.7 1495.3,2976.4 1527.5,2964.9 1557.2,2954.4"];
		ffn_gate_13 -> ffn_down_13	[pos="e,1686.4,2951.1 1714.7,2986.7 1707.7,2977.9 1700,2968.3 1692.7,2959.1"];
		ffn_allreduce_13	[color=red,
			height=1.3356,
			label="Layer13_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1660,2798.7",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_13 -> ffn_allreduce_13	[pos="e,1660,2846.9 1660,2882.6 1660,2874.6 1660,2865.8 1660,2857.1"];
		ffn_allreduce_13 -> ffn_residual_13	[pos="e,1680.1,2714.9 1671.5,2750.4 1673.4,2742.3 1675.5,2733.7 1677.7,2724.9"];
		mha_q_14	[color=lightblue,
			height=0.94444,
			label="Layer14_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1037,2478.7",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_13 -> mha_q_14	[pos="e,1182.1,2512.7 1339.9,2548.6 1287.9,2536.8 1236.9,2525.2 1191.9,2515"];
		mha_k_14	[color=lightblue,
			height=0.94444,
			label="Layer14_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1464,2478.7",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_13 -> mha_k_14	[pos="e,1515.7,2512.8 1571.6,2548.5 1555.1,2538 1538.8,2527.5 1524.2,2518.2"];
		mha_v_14	[color=lightblue,
			height=0.94444,
			label="Layer14_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1891,2478.7",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_13 -> mha_v_14	[pos="e,1848.9,2512.9 1803.9,2548.5 1816.7,2538.4 1829.3,2528.4 1840.8,2519.4"];
		attn_residual_14	[color=lightyellow,
			height=2.3056,
			label="Layer14_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1741,1985.5",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_13 -> attn_residual_14	[pos="e,1964,2068.5 1952.3,2581.7 2021.2,2563.1 2083,2539.8 2105,2512.7 2143.4,2465.2 2115,2436.7 2115,2375.7 2115,2375.7 2115,2375.7 \
2115,2269.7 2115,2182.2 2049.2,2118.3 1972.8,2073.6"];
		attn_score_14	[color=lightblue,
			height=0.94444,
			label="Layer14_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
8-15 (TP)",
			pos="1464,2374.7",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_14 -> attn_score_14	[pos="e,1325.5,2408.7 1175.3,2444.6 1220.2,2433.9 1270,2422 1315.6,2411.1"];
		mha_k_14 -> attn_score_14	[pos="e,1464,2409 1464,2444.5 1464,2436.4 1464,2427.6 1464,2419.1"];
		attn_out_14	[color=lightblue,
			height=0.94444,
			label="Layer14_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1741,2270.7",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_14 -> attn_out_14	[pos="e,1651,2304.8 1553.9,2340.5 1581.9,2330.3 1612.9,2318.8 1641.5,2308.3"];
		attn_allreduce_14	[color=red,
			height=1.3356,
			label="Layer14_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1741,2152.6",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_14 -> attn_allreduce_14	[pos="e,1741,2200.7 1741,2236.4 1741,2228.4 1741,2219.6 1741,2210.9"];
		attn_allreduce_14 -> attn_residual_14	[pos="e,1741,2068.7 1741,2104.3 1741,2096.3 1741,2087.7 1741,2079"];
		ffn_up_14	[color=lightblue,
			height=0.94444,
			label="Layer14_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1373,1832.5",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_14 -> ffn_up_14	[pos="e,1453.8,1866.6 1541.1,1902.5 1513.8,1891.3 1487.1,1880.3 1463.1,1870.5"];
		ffn_gate_14	[color=lightblue,
			height=0.94444,
			label="Layer14_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1741,1832.5",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_14 -> ffn_gate_14	[pos="e,1741,1866.8 1741,1902.4 1741,1893.6 1741,1885 1741,1876.9"];
		ffn_residual_14	[color=lightyellow,
			height=2.3056,
			label="Layer14_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1700,1443.3",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_14 -> ffn_residual_14	[pos="e,1895.2,1526.3 1897.8,1902.3 1908.5,1891.6 1917.8,1879.7 1925,1866.5 1989.8,1747.9 1995,1678 1925,1562.3 1918.7,1551.9 1911.1,1542.3 \
1902.6,1533.5"];
		ffn_down_14	[color=lightblue,
			height=0.94444,
			label="Layer14_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1660,1728.5",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_14 -> ffn_down_14	[pos="e,1566.8,1762.6 1466.2,1798.4 1495.3,1788 1527.5,1776.6 1557.2,1766"];
		ffn_gate_14 -> ffn_down_14	[pos="e,1686.4,1762.8 1714.7,1798.4 1707.7,1789.6 1700,1779.9 1692.7,1770.7"];
		ffn_allreduce_14	[color=red,
			height=1.3356,
			label="Layer14_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1660,1610.4",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_14 -> ffn_allreduce_14	[pos="e,1660,1658.5 1660,1694.2 1660,1686.2 1660,1677.5 1660,1668.7"];
		ffn_allreduce_14 -> ffn_residual_14	[pos="e,1680.1,1526.6 1671.5,1562.1 1673.4,1554 1675.5,1545.4 1677.7,1536.5"];
		mha_q_15	[color=lightblue,
			height=0.94444,
			label="Layer15_MHA_Q
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1037,1290.3",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_14 -> mha_q_15	[pos="e,1182.1,1324.4 1339.9,1360.3 1287.9,1348.5 1236.9,1336.9 1191.9,1326.6"];
		mha_k_15	[color=lightblue,
			height=0.94444,
			label="Layer15_MHA_K
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1464,1290.3",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_14 -> mha_k_15	[pos="e,1515.7,1324.4 1571.6,1360.2 1555.1,1349.6 1538.8,1339.2 1524.2,1329.8"];
		mha_v_15	[color=lightblue,
			height=0.94444,
			label="Layer15_MHA_V
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, heads=32, d_k=128]
Device: 8-15 (TP)",
			pos="1891,1290.3",
			shape=rectangle,
			style=filled,
			width=5.6806];
		ffn_residual_14 -> mha_v_15	[pos="e,1848.9,1324.6 1803.9,1360.2 1816.7,1350.1 1829.3,1340.1 1840.8,1331"];
		attn_residual_15	[color=lightyellow,
			height=2.3056,
			label="Layer15_Attn_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1741,797.17",
			shape=parallelogram,
			style=filled,
			width=10.425];
		ffn_residual_14 -> attn_residual_15	[pos="e,1964,880.18 1952.3,1393.3 2021.2,1374.8 2083,1351.5 2105,1324.3 2143.4,1276.9 2115,1248.4 2115,1187.3 2115,1187.3 2115,1187.3 \
2115,1081.3 2115,993.9 2049.2,929.99 1972.8,885.26"];
		attn_score_15	[color=lightblue,
			height=0.94444,
			label="Layer15_Attn_Score
Input: [batch=128, seq=10000, heads=32, seq=10000]
Output: [batch=128, seq=10000, heads=32, seq=10000]
Device: \
8-15 (TP)",
			pos="1464,1186.3",
			shape=rectangle,
			style=filled,
			width=5.9444];
		mha_q_15 -> attn_score_15	[pos="e,1325.5,1220.4 1175.3,1256.3 1220.2,1245.6 1270,1233.7 1315.6,1222.8"];
		mha_k_15 -> attn_score_15	[pos="e,1464,1220.6 1464,1256.2 1464,1248.1 1464,1239.3 1464,1230.8"];
		attn_out_15	[color=lightblue,
			height=0.94444,
			label="Layer15_Attn_Out
Input: [batch=128, seq=10000, heads=32, d_k=128]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1741,1082.3",
			shape=rectangle,
			style=filled,
			width=5.5139];
		attn_score_15 -> attn_out_15	[pos="e,1651,1116.5 1553.9,1152.2 1581.9,1141.9 1612.9,1130.5 1641.5,1120"];
		attn_allreduce_15	[color=red,
			height=1.3356,
			label="Layer15_Attn_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1741,964.25",
			shape=ellipse,
			style=filled,
			width=7.1104];
		attn_out_15 -> attn_allreduce_15	[pos="e,1741,1012.4 1741,1048.1 1741,1040.1 1741,1031.3 1741,1022.6"];
		attn_allreduce_15 -> attn_residual_15	[pos="e,1741,880.4 1741,915.94 1741,907.93 1741,899.37 1741,890.67"];
		ffn_up_15	[color=lightblue,
			height=0.94444,
			label="Layer15_FFN_Up
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1373,644.17",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_15 -> ffn_up_15	[pos="e,1453.8,678.31 1541.1,714.15 1513.8,702.96 1487.1,691.97 1463.1,682.15"];
		ffn_gate_15	[color=lightblue,
			height=0.94444,
			label="Layer15_FFN_Gate
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, ffn=16384]
Device: 8-15 (TP)",
			pos="1741,644.17",
			shape=rectangle,
			style=filled,
			width=4.8611];
		attn_residual_15 -> ffn_gate_15	[pos="e,1741,678.43 1741,714.04 1741,705.28 1741,696.65 1741,688.61"];
		ffn_residual_15	[color=lightyellow,
			height=2.3056,
			label="Layer15_FFN_Residual
Input1: [batch=128, seq=10000, hidden=4096]
Input2: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, \
seq=10000, hidden=4096]
Device: 8-15",
			pos="1700,255",
			shape=parallelogram,
			style=filled,
			width=10.425];
		attn_residual_15 -> ffn_residual_15	[pos="e,1895.2,338 1897.8,713.95 1908.5,703.22 1917.8,691.32 1925,678.17 1989.8,559.55 1995,489.68 1925,374 1918.7,363.53 1911.1,353.95 \
1902.6,345.18"];
		ffn_down_15	[color=lightblue,
			height=0.94444,
			label="Layer15_FFN_Down
Input: [batch=128, seq=10000, ffn=16384]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (TP)",
			pos="1660,540.17",
			shape=rectangle,
			style=filled,
			width=5.0278];
		ffn_up_15 -> ffn_down_15	[pos="e,1566.8,574.29 1466.2,610.05 1495.3,599.71 1527.5,588.25 1557.2,577.69"];
		ffn_gate_15 -> ffn_down_15	[pos="e,1686.4,574.47 1714.7,610.05 1707.7,601.23 1700,591.6 1692.7,582.4"];
		ffn_allreduce_15	[color=red,
			height=1.3356,
			label="Layer15_FFN_AllReduce
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, hidden=4096]
Device: 8-15 (All-Reduce)",
			pos="1660,422.08",
			shape=ellipse,
			style=filled,
			width=7.1104];
		ffn_down_15 -> ffn_allreduce_15	[pos="e,1660,470.19 1660,505.9 1660,497.91 1660,489.14 1660,480.4"];
		ffn_allreduce_15 -> ffn_residual_15	[pos="e,1680.1,338.23 1671.5,373.78 1673.4,365.68 1675.5,357.02 1677.7,348.22"];
	}
	pipeline_comm_0	[color=lightgreen,
		height=1.041,
		label="Pipeline Communication
Transfer: [batch=128, seq=10000, hidden=4096]
Devices: 0-7  8-15",
		pos="1806,9719.1",
		shape=ellipse,
		style=filled,
		width=7.3068];
	ffn_residual_7 -> pipeline_comm_0	[pos="e,1806,9756.6 1806,9792.4 1806,9783.6 1806,9774.9 1806,9766.7"];
	output	[color=lightyellow,
		height=1.8889,
		label="Output Projection
Input: [batch=128, seq=10000, hidden=4096]
Output: [batch=128, seq=10000, vocab_size]
Device: 8-15 (TP)",
		pos="1700,68",
		shape=parallelogram,
		style=filled,
		width=10.08];
	ffn_residual_15 -> output	[pos="e,1700,136.28 1700,171.78 1700,163.34 1700,154.79 1700,146.4"];
	pipeline_comm_0 -> mha_q_8	[pos="e,1152.1,9642.7 1554.5,9708 1464.5,9701.6 1362.7,9691 1271,9673.7 1234.9,9666.8 1196.1,9656.3 1161.7,9645.7"];
	pipeline_comm_0 -> mha_k_8	[pos="e,1878.8,9642.9 1841.3,9681.7 1851,9671.6 1861.7,9660.6 1871.7,9650.3"];
	pipeline_comm_0 -> mha_v_8	[pos="e,1582.1,9642.7 1705.9,9684.4 1669.7,9672.2 1628.7,9658.4 1591.9,9646"];
	pipeline_comm_0 -> attn_residual_8	[pos="e,2104.3,9198.6 2026.4,9698.7 2068.8,9692.3 2105.9,9684.1 2125,9673.7 2209.1,9627.8 2260,9601.5 2260,9505.7 2260,9505.7 2260,9505.7 \
2260,9399.7 2260,9307.6 2196.2,9245.2 2113.4,9203.1"];
	attn_v_0	[height=0.5,
		pos="2290,19125",
		width=1.3902];
	attn_v_0 -> attn_out_0	[pos="e,2046,19044 2268.3,19109 2258.3,19103 2246,19096 2234,19091 2177.9,19071 2114.8,19056 2056.1,19046"];
	attn_v_1	[height=0.5,
		pos="2290,17937",
		width=1.3902];
	attn_v_1 -> attn_out_1	[pos="e,2046,17856 2268.3,17921 2258.3,17914 2246,17907 2234,17903 2177.9,17882 2114.8,17868 2056.1,17857"];
	attn_v_2	[height=0.5,
		pos="2290,16749",
		width=1.3902];
	attn_v_2 -> attn_out_2	[pos="e,2046,16667 2268.3,16732 2258.3,16726 2246,16719 2234,16715 2177.9,16694 2114.8,16679 2056.1,16669"];
	attn_v_3	[height=0.5,
		pos="2290,15560",
		width=1.3902];
	attn_v_3 -> attn_out_3	[pos="e,2046,15479 2268.3,15544 2258.3,15538 2246,15531 2234,15526 2177.9,15506 2114.8,15491 2056.1,15481"];
	attn_v_4	[height=0.5,
		pos="2290,14372",
		width=1.3902];
	attn_v_4 -> attn_out_4	[pos="e,2046,14291 2268.3,14356 2258.3,14349 2246,14342 2234,14338 2177.9,14317 2114.8,14303 2056.1,14292"];
	attn_v_5	[height=0.5,
		pos="2290,13184",
		width=1.3902];
	attn_v_5 -> attn_out_5	[pos="e,2046,13102 2268.3,13167 2258.3,13161 2246,13154 2234,13150 2177.9,13129 2114.8,13114 2056.1,13104"];
	attn_v_6	[height=0.5,
		pos="2290,11995",
		width=1.3902];
	attn_v_6 -> attn_out_6	[pos="e,2046,11914 2268.3,11979 2258.3,11973 2246,11966 2234,11961 2177.9,11941 2114.8,11926 2056.1,11916"];
	attn_v_7	[height=0.5,
		pos="2290,10807",
		width=1.3902];
	attn_v_7 -> attn_out_7	[pos="e,2046,10726 2268.3,10791 2258.3,10784 2246,10777 2234,10773 2177.9,10752 2114.8,10738 2056.1,10727"];
	attn_v_8	[height=0.5,
		pos="2182,9504.7",
		width=1.3902];
	attn_v_8 -> attn_out_8	[pos="e,2013.1,9434.7 2160.5,9488.3 2151,9482.1 2139.3,9475.2 2128,9470.7 2094.9,9457.3 2058.7,9446.2 2023.1,9437.2"];
	attn_v_9	[height=0.5,
		pos="2184,8316.3",
		width=1.3902];
	attn_v_9 -> attn_out_9	[pos="e,1940,8234.9 2162.3,8300.1 2152.3,8293.7 2140,8286.8 2128,8282.3 2071.9,8261.6 2008.8,8246.9 1950.1,8236.7"];
	attn_v_10	[height=0.5,
		pos="2190,7128",
		width=1.5526];
	attn_v_10 -> attn_out_10	[pos="e,1939.9,7048.2 2165.5,7111.7 2154.4,7105.3 2140.9,7098.5 2128,7094 2071.6,7074.5 2008.6,7060.3 1950,7050"];
	attn_v_11	[height=0.5,
		pos="2190,5939.7",
		width=1.5526];
	attn_v_11 -> attn_out_11	[pos="e,1939.9,5859.9 2165.5,5923.3 2154.4,5917 2140.9,5910.1 2128,5905.7 2071.6,5886.2 2008.6,5871.9 1950,5861.7"];
	attn_v_12	[height=0.5,
		pos="2190,4751.3",
		width=1.5526];
	attn_v_12 -> attn_out_12	[pos="e,1939.9,4671.6 2165.5,4735 2154.4,4728.7 2140.9,4721.8 2128,4717.3 2071.6,4697.8 2008.6,4683.6 1950,4673.3"];
	attn_v_13	[height=0.5,
		pos="2190,3563",
		width=1.5526];
	attn_v_13 -> attn_out_13	[pos="e,1939.9,3483.2 2165.5,3546.7 2154.4,3540.3 2140.9,3533.5 2128,3529 2071.6,3509.5 2008.6,3495.3 1950,3485"];
	attn_v_14	[height=0.5,
		pos="2190,2374.7",
		width=1.5526];
	attn_v_14 -> attn_out_14	[pos="e,1939.9,2294.9 2165.5,2358.3 2154.4,2352 2140.9,2345.1 2128,2340.7 2071.6,2321.2 2008.6,2306.9 1950,2296.7"];
	attn_v_15	[height=0.5,
		pos="2190,1186.3",
		width=1.5526];
	attn_v_15 -> attn_out_15	[pos="e,1939.9,1106.6 2165.5,1170 2154.4,1163.7 2140.9,1156.8 2128,1152.3 2071.6,1132.8 2008.6,1118.6 1950,1108.3"];
}
