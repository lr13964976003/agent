{
  "deployment_configurations": {
    "baseline_tensor_pipeline_parallelism": {
      "name": "Baseline TP=8, PP=2",
      "total_gpus": 16,
      "parallel_strategy": {
        "tensor_parallelism": 8,
        "pipeline_parallelism": 2,
        "data_parallelism": 1
      },
      "model_configuration": {
        "total_layers": 4,
        "model_size": "30B",
        "precision": "BF16",
        "batch_size": 128,
        "sequence_length": 10000,
        "num_heads": 32,
        "head_dimension": 128,
        "mlp_hidden_size": 16384
      },
      "gpu_mappings": [
        {
          "gpu_id": 0,
          "tensor_parallel_rank": 0,
          "pipeline_parallel_rank": 0,
          "layers_assigned": [0],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 1,
          "tensor_parallel_rank": 1,
          "pipeline_parallel_rank": 0,
          "layers_assigned": [0],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 2,
          "tensor_parallel_rank": 2,
          "pipeline_parallel_rank": 0,
          "layers_assigned": [0],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 3,
          "tensor_parallel_rank": 3,
          "pipeline_parallel_rank": 0,
          "layers_assigned": [0],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 4,
          "tensor_parallel_rank": 4,
          "pipeline_parallel_rank": 0,
          "layers_assigned": [0],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 5,
          "tensor_parallel_rank": 5,
          "pipeline_parallel_rank": 0,
          "layers_assigned": [0],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 6,
          "tensor_parallel_rank": 6,
          "pipeline_parallel_rank": 0,
          "layers_assigned": [0],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 7,
          "tensor_parallel_rank": 7,
          "pipeline_parallel_rank": 0,
          "layers_assigned": [0],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 8,
          "tensor_parallel_rank": 0,
          "pipeline_parallel_rank": 1,
          "layers_assigned": [1],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 9,
          "tensor_parallel_rank": 1,
          "pipeline_parallel_rank": 1,
          "layers_assigned": [1],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 10,
          "tensor_parallel_rank": 2,
          "pipeline_parallel_rank": 1,
          "layers_assigned": [1],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 11,
          "tensor_parallel_rank": 3,
          "pipeline_parallel_rank": 1,
          "layers_assigned": [1],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 12,
          "tensor_parallel_rank": 4,
          "pipeline_parallel_rank": 1,
          "layers_assigned": [1],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 13,
          "tensor_parallel_rank": 5,
          "pipeline_parallel_rank": 1,
          "layers_assigned": [1],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 14,
          "tensor_parallel_rank": 6,
          "pipeline_parallel_rank": 1,
          "layers_assigned": [1],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        },
        {
          "gpu_id": 15,
          "tensor_parallel_rank": 7,
          "pipeline_parallel_rank": 1,
          "layers_assigned": [1],
          "memory_allocation": {
            "weights": "1.875GB",
            "activations": "estimated_8GB",
            "buffers": "estimated_2GB",
            "total": "estimated_11.875GB"
          }
        }
      ],
      "communication_pattern": {
        "intra_stage": "tensor_parallel_communication",
        "inter_stage": "pipeline_parallel_communication"
      },
      "performance": {
        "tps": 12800,
        "tpot_ms": 0.078
      }
    },
    "proposed_layer_wise": {
      "name": "Proposed Layer-wise Partitioning",
      "total_gpus": 16,
      "parallel_strategy": {
        "type": "layer_wise_partitioning",
        "cache_constraint": "SRAM_L2_cache_fit",
        "partitioning_algorithm": "greedy_with_dp_optimization"
      },
      "model_configuration": {
        "total_layers": 4,
        "model_size": "30B",
        "precision": "BF16",
        "batch_size": 128,
        "sequence_length": 10000,
        "num_heads": 32,
        "head_dimension": 128,
        "mlp_hidden_size": 16384
      },
      "memory_estimation": {
        "per_layer_breakdown": {
          "formula": "size(l_j) = weight_size(l_j) + activation_size(l_j) + buffer_size(l_j)",
          "weight_size": "7.5GB_per_layer",
          "activation_size": "estimated_8GB_per_layer",
          "buffer_size": "estimated_2GB_per_layer",
          "total_per_layer": "estimated_17.5GB"
        }
      },
      "partitioning_results": {
        "partitioning_strategy": "greedy_layer_aggregation",
        "cache_capacity_constraint": "C",
        "partitions": [
          {
            "partition_id": 0,
            "layers": [0],
            "estimated_size": "17.5GB",
            "cache_fit": true
          },
          {
            "partition_id": 1,
            "layers": [1],
            "estimated_size": "17.5GB",
            "cache_fit": true
          },
          {
            "partition_id": 2,
            "layers": [2],
            "estimated_size": "17.5GB",
            "cache_fit": true
          },
          {
            "partition_id": 3,
            "layers": [3],
            "estimated_size": "17.5GB",
            "cache_fit": true
          }
        ]
      },
      "gpu_mappings": [
        {
          "gpu_id": 0,
          "partition_id": 0,
          "layers_assigned": [0],
          "memory_allocation": {
            "weights": "7.5GB",
            "activations": "8GB",
            "buffers": "2GB",
            "total": "17.5GB",
            "cache_location": "SRAM_L2"
          },
          "execution_order": 0
        },
        {
          "gpu_id": 1,
          "partition_id": 1,
          "layers_assigned": [1],
          "memory_allocation": {
            "weights": "7.5GB",
            "activations": "8GB",
            "buffers": "2GB",
            "total": "17.5GB",
            "cache_location": "SRAM_L2"
          },
          "execution_order": 1
        },
        {
          "gpu_id": 2,
          "partition_id": 2,
          "layers_assigned": [2],
          "memory_allocation": {
            "weights": "7.5GB",
            "activations": "8GB",
            "buffers": "2GB",
            "total": "17.5GB",
            "cache_location": "SRAM_L2"
          },
          "execution_order": 2
        },
        {
          "gpu_id": 3,
          "partition_id": 3,
          "layers_assigned": [3],
          "memory_allocation": {
            "weights": "7.5GB",
            "activations": "8GB",
            "buffers": "2GB",
            "total": "17.5GB",
            "cache_location": "SRAM_L2"
          },
          "execution_order": 3
        }
      ],
      "remaining_gpus": {
        "gpu_ids": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
        "status": "available_for_scaling_or_replication"
      },
      "communication_pattern": {
        "inter_partition": "point_to_point_transfer",
        "data_transfer": "intermediate_outputs_only",
        "optimization": "minimized_inter_card_communication"
      },
      "performance": {
        "tps": 15360,
        "tpot_ms": 0.065,
        "improvement": {
          "tps_increase": "20%",
          "tpot_reduction": "17%"
        }
      },
      "advantages": [
        "Reduced memory access latency",
        "Improved throughput",
        "Scalability across multiple devices",
        "Optimized on-chip memory utilization"
      ]
    }
  },
  "deployment_notes": {
    "critical_requirements": [
      "Each partition must fit entirely in target device SRAM/L2 cache",
      "Layer execution order must be preserved contiguously",
      "Memory footprint estimation must include weights, activations, and buffers",
      "Inter-card communication minimized to intermediate transfers only"
    ],
    "edge_case_handling": [
      "Single layer exceeding cache: apply intra-layer partitioning or compression",
      "Variable layer sizes: adjust partitioning heuristics",
      "Batch size tuning: reduce activation footprint if needed"
    ],
    "implementation_parameters": {
      "cache_capacity_C": "device_specific_SRAM_L2_size",
      "partitioning_algorithm": "greedy_or_dynamic_programming",
      "memory_estimation": "static_pre_deployment_or_dynamic_profiled"
    }
  }
}