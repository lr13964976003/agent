digraph baseline_tp8_pp2 {
	graph [bb="0,0,1347.5,19959",
		nodesep=0.5,
		rankdir=TB,
		ranksep=1.0,
		splines=ortho
	];
	node [fillcolor=lightblue,
		label="\N",
		shape=ellipse,
		style=filled
	];
	subgraph cluster_stage0 {
		graph [bb="8,10008,976,19794",
			color=red,
			label="Stage 0 (Layers 0-7)\nGPUs 0-7 (TP=8)",
			lheight=0.42,
			lp="492,19775",
			lwidth=2.01,
			style="rounded,dashed"
		];
		subgraph cluster_layer0 {
			graph [bb="252,18432,732,19597",
				label="Layer 0\nGPUs 0-7",
				lheight=0.42,
				lp="492,19578",
				lwidth=0.93,
				style=dotted
			];
			lay0_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 0-7",
				pos="492,19517",
				shape=rectangle,
				width=3.0139];
			lay0_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,19376",
				shape=rectangle,
				width=3.0972];
			lay0_qkv_proj -> lay0_attn	[pos="e,492,19411 492,19483 492,19483 492,19421 492,19421"];
			lay0_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,19235",
				shape=rectangle,
				width=3.75];
			lay0_attn -> lay0_attn_out	[pos="e,492,19270 492,19342 492,19342 492,19280 492,19280"];
			lay0_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,19080",
				width=6.4425];
			lay0_attn_out -> lay0_res1	[pos="e,492,19129 492,19201 492,19201 492,19139 492,19139"];
			lay0_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="368,18925",
				shape=rectangle,
				width=2.9444];
			lay0_res1 -> lay0_mlp_gate	[pos="e,368,18959 368,19039 368,19039 368,18969 368,18969"];
			lay0_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="616,18925",
				shape=rectangle,
				width=2.9444];
			lay0_res1 -> lay0_mlp_up	[pos="e,616,18959 616,19039 616,19039 616,18969 616,18969"];
			lay0_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="492,18784",
				shape=rectangle,
				width=2.9444];
			lay0_mlp_gate -> lay0_mlp_act	[pos="e,430,18818 430,18891 430,18891 430,18828 430,18828"];
			lay0_mlp_up -> lay0_mlp_act	[pos="e,554,18818 554,18891 554,18891 554,18828 554,18828"];
			lay0_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,18643",
				shape=rectangle,
				width=2.8194];
			lay0_mlp_act -> lay0_mlp_down	[pos="e,492,18677 492,18750 492,18750 492,18687 492,18687"];
			lay0_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,18488",
				width=6.4425];
			lay0_mlp_down -> lay0_res2	[pos="e,492,18536 492,18609 492,18609 492,18546 492,18546"];
		}
		subgraph cluster_layer1 {
			graph [bb="252,17230,732,18395",
				label="Layer 1\nGPUs 0-7",
				lheight=0.42,
				lp="492,18376",
				lwidth=0.93,
				style=dotted
			];
			lay1_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 0-7",
				pos="492,18315",
				shape=rectangle,
				width=3.0139];
			lay1_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,18174",
				shape=rectangle,
				width=3.0972];
			lay1_qkv_proj -> lay1_attn	[pos="e,492,18208 492,18281 492,18281 492,18218 492,18218"];
			lay1_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,18033",
				shape=rectangle,
				width=3.75];
			lay1_attn -> lay1_attn_out	[pos="e,492,18067 492,18140 492,18140 492,18077 492,18077"];
			lay1_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,17878",
				width=6.4425];
			lay1_attn_out -> lay1_res1	[pos="e,492,17926 492,17999 492,17999 492,17936 492,17936"];
			lay1_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="368,17723",
				shape=rectangle,
				width=2.9444];
			lay1_res1 -> lay1_mlp_gate	[pos="e,368,17757 368,17837 368,17837 368,17767 368,17767"];
			lay1_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="616,17723",
				shape=rectangle,
				width=2.9444];
			lay1_res1 -> lay1_mlp_up	[pos="e,616,17757 616,17837 616,17837 616,17767 616,17767"];
			lay1_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="492,17582",
				shape=rectangle,
				width=2.9444];
			lay1_mlp_gate -> lay1_mlp_act	[pos="e,430,17616 430,17689 430,17689 430,17626 430,17626"];
			lay1_mlp_up -> lay1_mlp_act	[pos="e,554,17616 554,17689 554,17689 554,17626 554,17626"];
			lay1_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,17441",
				shape=rectangle,
				width=2.8194];
			lay1_mlp_act -> lay1_mlp_down	[pos="e,492,17475 492,17548 492,17548 492,17485 492,17485"];
			lay1_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,17286",
				width=6.4425];
			lay1_mlp_down -> lay1_res2	[pos="e,492,17334 492,17407 492,17407 492,17344 492,17344"];
		}
		subgraph cluster_layer2 {
			graph [bb="252,16027,732,17193",
				label="Layer 2\nGPUs 0-7",
				lheight=0.42,
				lp="492,17174",
				lwidth=0.93,
				style=dotted
			];
			lay2_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 0-7",
				pos="492,17113",
				shape=rectangle,
				width=3.0139];
			lay2_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,16972",
				shape=rectangle,
				width=3.0972];
			lay2_qkv_proj -> lay2_attn	[pos="e,492,17006 492,17079 492,17079 492,17016 492,17016"];
			lay2_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,16831",
				shape=rectangle,
				width=3.75];
			lay2_attn -> lay2_attn_out	[pos="e,492,16865 492,16938 492,16938 492,16875 492,16875"];
			lay2_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,16676",
				width=6.4425];
			lay2_attn_out -> lay2_res1	[pos="e,492,16724 492,16797 492,16797 492,16734 492,16734"];
			lay2_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="368,16520",
				shape=rectangle,
				width=2.9444];
			lay2_res1 -> lay2_mlp_gate	[pos="e,368,16555 368,16635 368,16635 368,16565 368,16565"];
			lay2_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="616,16520",
				shape=rectangle,
				width=2.9444];
			lay2_res1 -> lay2_mlp_up	[pos="e,616,16555 616,16635 616,16635 616,16565 616,16565"];
			lay2_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="492,16379",
				shape=rectangle,
				width=2.9444];
			lay2_mlp_gate -> lay2_mlp_act	[pos="e,430,16414 430,16486 430,16486 430,16424 430,16424"];
			lay2_mlp_up -> lay2_mlp_act	[pos="e,554,16414 554,16486 554,16486 554,16424 554,16424"];
			lay2_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,16238",
				shape=rectangle,
				width=2.8194];
			lay2_mlp_act -> lay2_mlp_down	[pos="e,492,16273 492,16345 492,16345 492,16283 492,16283"];
			lay2_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,16083",
				width=6.4425];
			lay2_mlp_down -> lay2_res2	[pos="e,492,16132 492,16204 492,16204 492,16142 492,16142"];
		}
		subgraph cluster_layer3 {
			graph [bb="252,14825,732,15990",
				label="Layer 3\nGPUs 0-7",
				lheight=0.42,
				lp="492,15971",
				lwidth=0.93,
				style=dotted
			];
			lay3_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 0-7",
				pos="492,15910",
				shape=rectangle,
				width=3.0139];
			lay3_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,15769",
				shape=rectangle,
				width=3.0972];
			lay3_qkv_proj -> lay3_attn	[pos="e,492,15804 492,15876 492,15876 492,15814 492,15814"];
			lay3_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,15628",
				shape=rectangle,
				width=3.75];
			lay3_attn -> lay3_attn_out	[pos="e,492,15663 492,15735 492,15735 492,15673 492,15673"];
			lay3_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,15473",
				width=6.4425];
			lay3_attn_out -> lay3_res1	[pos="e,492,15522 492,15594 492,15594 492,15532 492,15532"];
			lay3_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="368,15318",
				shape=rectangle,
				width=2.9444];
			lay3_res1 -> lay3_mlp_gate	[pos="e,368,15352 368,15433 368,15433 368,15362 368,15362"];
			lay3_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="616,15318",
				shape=rectangle,
				width=2.9444];
			lay3_res1 -> lay3_mlp_up	[pos="e,616,15352 616,15433 616,15433 616,15362 616,15362"];
			lay3_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="492,15177",
				shape=rectangle,
				width=2.9444];
			lay3_mlp_gate -> lay3_mlp_act	[pos="e,430,15211 430,15284 430,15284 430,15221 430,15221"];
			lay3_mlp_up -> lay3_mlp_act	[pos="e,554,15211 554,15284 554,15284 554,15221 554,15221"];
			lay3_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,15036",
				shape=rectangle,
				width=2.8194];
			lay3_mlp_act -> lay3_mlp_down	[pos="e,492,15070 492,15143 492,15143 492,15080 492,15080"];
			lay3_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,14881",
				width=6.4425];
			lay3_mlp_down -> lay3_res2	[pos="e,492,14929 492,15002 492,15002 492,14939 492,14939"];
		}
		subgraph cluster_layer4 {
			graph [bb="252,13623,732,14788",
				label="Layer 4\nGPUs 0-7",
				lheight=0.42,
				lp="492,14769",
				lwidth=0.93,
				style=dotted
			];
			lay4_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 0-7",
				pos="492,14708",
				shape=rectangle,
				width=3.0139];
			lay4_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,14567",
				shape=rectangle,
				width=3.0972];
			lay4_qkv_proj -> lay4_attn	[pos="e,492,14601 492,14674 492,14674 492,14611 492,14611"];
			lay4_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,14426",
				shape=rectangle,
				width=3.75];
			lay4_attn -> lay4_attn_out	[pos="e,492,14460 492,14533 492,14533 492,14470 492,14470"];
			lay4_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,14271",
				width=6.4425];
			lay4_attn_out -> lay4_res1	[pos="e,492,14319 492,14392 492,14392 492,14329 492,14329"];
			lay4_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="368,14116",
				shape=rectangle,
				width=2.9444];
			lay4_res1 -> lay4_mlp_gate	[pos="e,368,14150 368,14230 368,14230 368,14160 368,14160"];
			lay4_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="616,14116",
				shape=rectangle,
				width=2.9444];
			lay4_res1 -> lay4_mlp_up	[pos="e,616,14150 616,14230 616,14230 616,14160 616,14160"];
			lay4_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="492,13975",
				shape=rectangle,
				width=2.9444];
			lay4_mlp_gate -> lay4_mlp_act	[pos="e,430,14009 430,14082 430,14082 430,14019 430,14019"];
			lay4_mlp_up -> lay4_mlp_act	[pos="e,554,14009 554,14082 554,14082 554,14019 554,14019"];
			lay4_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,13834",
				shape=rectangle,
				width=2.8194];
			lay4_mlp_act -> lay4_mlp_down	[pos="e,492,13868 492,13941 492,13941 492,13878 492,13878"];
			lay4_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,13679",
				width=6.4425];
			lay4_mlp_down -> lay4_res2	[pos="e,492,13727 492,13800 492,13800 492,13737 492,13737"];
		}
		subgraph cluster_layer5 {
			graph [bb="252,12420,732,13586",
				label="Layer 5\nGPUs 0-7",
				lheight=0.42,
				lp="492,13567",
				lwidth=0.93,
				style=dotted
			];
			lay5_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 0-7",
				pos="492,13506",
				shape=rectangle,
				width=3.0139];
			lay5_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,13365",
				shape=rectangle,
				width=3.0972];
			lay5_qkv_proj -> lay5_attn	[pos="e,492,13399 492,13472 492,13472 492,13409 492,13409"];
			lay5_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,13224",
				shape=rectangle,
				width=3.75];
			lay5_attn -> lay5_attn_out	[pos="e,492,13258 492,13331 492,13331 492,13268 492,13268"];
			lay5_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,13069",
				width=6.4425];
			lay5_attn_out -> lay5_res1	[pos="e,492,13117 492,13190 492,13190 492,13127 492,13127"];
			lay5_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="368,12913",
				shape=rectangle,
				width=2.9444];
			lay5_res1 -> lay5_mlp_gate	[pos="e,368,12948 368,13028 368,13028 368,12958 368,12958"];
			lay5_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="616,12913",
				shape=rectangle,
				width=2.9444];
			lay5_res1 -> lay5_mlp_up	[pos="e,616,12948 616,13028 616,13028 616,12958 616,12958"];
			lay5_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="492,12772",
				shape=rectangle,
				width=2.9444];
			lay5_mlp_gate -> lay5_mlp_act	[pos="e,430,12807 430,12879 430,12879 430,12817 430,12817"];
			lay5_mlp_up -> lay5_mlp_act	[pos="e,554,12807 554,12879 554,12879 554,12817 554,12817"];
			lay5_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,12631",
				shape=rectangle,
				width=2.8194];
			lay5_mlp_act -> lay5_mlp_down	[pos="e,492,12666 492,12738 492,12738 492,12676 492,12676"];
			lay5_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,12476",
				width=6.4425];
			lay5_mlp_down -> lay5_res2	[pos="e,492,12525 492,12597 492,12597 492,12535 492,12535"];
		}
		subgraph cluster_layer6 {
			graph [bb="252,11218,732,12383",
				label="Layer 6\nGPUs 0-7",
				lheight=0.42,
				lp="492,12364",
				lwidth=0.93,
				style=dotted
			];
			lay6_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 0-7",
				pos="492,12303",
				shape=rectangle,
				width=3.0139];
			lay6_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,12162",
				shape=rectangle,
				width=3.0972];
			lay6_qkv_proj -> lay6_attn	[pos="e,492,12197 492,12269 492,12269 492,12207 492,12207"];
			lay6_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,12021",
				shape=rectangle,
				width=3.75];
			lay6_attn -> lay6_attn_out	[pos="e,492,12056 492,12128 492,12128 492,12066 492,12066"];
			lay6_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,11866",
				width=6.4425];
			lay6_attn_out -> lay6_res1	[pos="e,492,11915 492,11987 492,11987 492,11925 492,11925"];
			lay6_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="368,11711",
				shape=rectangle,
				width=2.9444];
			lay6_res1 -> lay6_mlp_gate	[pos="e,368,11745 368,11826 368,11826 368,11755 368,11755"];
			lay6_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="616,11711",
				shape=rectangle,
				width=2.9444];
			lay6_res1 -> lay6_mlp_up	[pos="e,616,11745 616,11826 616,11826 616,11755 616,11755"];
			lay6_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="492,11570",
				shape=rectangle,
				width=2.9444];
			lay6_mlp_gate -> lay6_mlp_act	[pos="e,430,11604 430,11677 430,11677 430,11614 430,11614"];
			lay6_mlp_up -> lay6_mlp_act	[pos="e,554,11604 554,11677 554,11677 554,11614 554,11614"];
			lay6_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,11429",
				shape=rectangle,
				width=2.8194];
			lay6_mlp_act -> lay6_mlp_down	[pos="e,492,11463 492,11536 492,11536 492,11473 492,11473"];
			lay6_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,11274",
				width=6.4425];
			lay6_mlp_down -> lay6_res2	[pos="e,492,11322 492,11395 492,11395 492,11332 492,11332"];
		}
		subgraph cluster_layer7 {
			graph [bb="252,10016,732,11181",
				label="Layer 7\nGPUs 0-7",
				lheight=0.42,
				lp="492,11162",
				lwidth=0.93,
				style=dotted
			];
			lay7_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 0-7",
				pos="492,11101",
				shape=rectangle,
				width=3.0139];
			lay7_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,10960",
				shape=rectangle,
				width=3.0972];
			lay7_qkv_proj -> lay7_attn	[pos="e,492,10994 492,11067 492,11067 492,11004 492,11004"];
			lay7_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,10819",
				shape=rectangle,
				width=3.75];
			lay7_attn -> lay7_attn_out	[pos="e,492,10853 492,10926 492,10926 492,10863 492,10863"];
			lay7_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,10664",
				width=6.4425];
			lay7_attn_out -> lay7_res1	[pos="e,492,10712 492,10785 492,10785 492,10722 492,10722"];
			lay7_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="368,10509",
				shape=rectangle,
				width=2.9444];
			lay7_res1 -> lay7_mlp_gate	[pos="e,368,10543 368,10623 368,10623 368,10553 368,10553"];
			lay7_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="616,10509",
				shape=rectangle,
				width=2.9444];
			lay7_res1 -> lay7_mlp_up	[pos="e,616,10543 616,10623 616,10623 616,10553 616,10553"];
			lay7_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 0-7",
				pos="492,10368",
				shape=rectangle,
				width=2.9444];
			lay7_mlp_gate -> lay7_mlp_act	[pos="e,430,10402 430,10475 430,10475 430,10412 430,10412"];
			lay7_mlp_up -> lay7_mlp_act	[pos="e,554,10402 554,10475 554,10475 554,10412 554,10412"];
			lay7_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,10227",
				shape=rectangle,
				width=2.8194];
			lay7_mlp_act -> lay7_mlp_down	[pos="e,492,10261 492,10334 492,10334 492,10271 492,10271"];
			lay7_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 0-7",
				pos="492,10072",
				width=6.4425];
			lay7_mlp_down -> lay7_res2	[pos="e,492,10120 492,10193 492,10193 492,10130 492,10130"];
		}
		stage0_input	[fillcolor=lightyellow,
			height=1.4722,
			label="Input to Stage 0\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=\
4096]",
			pos="492,19695",
			shape=parallelogram,
			width=13.219];
		stage0_input -> lay0_qkv_proj	[pos="e,492,19551 492,19642 492,19642 492,19561 492,19561"];
		lay0_res2 -> lay1_qkv_proj	[pos="e,492,18349 492,18440 492,18440 492,18359 492,18359"];
		lay1_res2 -> lay2_qkv_proj	[pos="e,492,17147 492,17238 492,17238 492,17157 492,17157"];
		lay2_res2 -> lay3_qkv_proj	[pos="e,492,15945 492,16035 492,16035 492,15955 492,15955"];
		lay3_res2 -> lay4_qkv_proj	[pos="e,492,14742 492,14833 492,14833 492,14752 492,14752"];
		lay4_res2 -> lay5_qkv_proj	[pos="e,492,13540 492,13631 492,13631 492,13550 492,13550"];
		lay5_res2 -> lay6_qkv_proj	[pos="e,492,12338 492,12428 492,12428 492,12348 492,12348"];
		lay6_res2 -> lay7_qkv_proj	[pos="e,492,11135 492,11226 492,11226 492,11145 492,11145"];
	}
	subgraph cluster_stage1 {
		graph [bb="8,163,976,9949.7",
			color=blue,
			label="Stage 1 (Layers 8-15)\nGPUs 8-15 (TP=8)",
			lheight=0.42,
			lp="492,9930.7",
			lwidth=2.14,
			style="rounded,dashed"
		];
		subgraph cluster_layer8 {
			graph [bb="252,8587.3,732,9752.7",
				label="Layer 8\nGPUs 8-15",
				lheight=0.42,
				lp="492,9733.7",
				lwidth=1.06,
				style=dotted
			];
			lay8_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 8-15",
				pos="492,9672.7",
				shape=rectangle,
				width=3.0139];
			lay8_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,9531.7",
				shape=rectangle,
				width=3.0972];
			lay8_qkv_proj -> lay8_attn	[pos="e,492,9566 492,9638.5 492,9638.5 492,9576 492,9576"];
			lay8_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,9390.7",
				shape=rectangle,
				width=3.75];
			lay8_attn -> lay8_attn_out	[pos="e,492,9425 492,9497.5 492,9497.5 492,9435 492,9435"];
			lay8_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,9235.6",
				width=6.4425];
			lay8_attn_out -> lay8_res1	[pos="e,492,9284 492,9356.6 492,9356.6 492,9294 492,9294"];
			lay8_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="368,9080.5",
				shape=rectangle,
				width=2.9444];
			lay8_res1 -> lay8_mlp_gate	[pos="e,368,9114.7 368,9194.8 368,9194.8 368,9124.7 368,9124.7"];
			lay8_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="616,9080.5",
				shape=rectangle,
				width=2.9444];
			lay8_res1 -> lay8_mlp_up	[pos="e,616,9114.7 616,9194.8 616,9194.8 616,9124.7 616,9124.7"];
			lay8_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="492,8939.5",
				shape=rectangle,
				width=2.9444];
			lay8_mlp_gate -> lay8_mlp_act	[pos="e,430,8973.8 430,9046.4 430,9046.4 430,8983.8 430,8983.8"];
			lay8_mlp_up -> lay8_mlp_act	[pos="e,554,8973.8 554,9046.4 554,9046.4 554,8983.8 554,8983.8"];
			lay8_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,8798.5",
				shape=rectangle,
				width=2.8194];
			lay8_mlp_act -> lay8_mlp_down	[pos="e,492,8832.8 492,8905.4 492,8905.4 492,8842.8 492,8842.8"];
			lay8_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,8643.4",
				width=6.4425];
			lay8_mlp_down -> lay8_res2	[pos="e,492,8691.8 492,8764.5 492,8764.5 492,8701.8 492,8701.8"];
		}
		subgraph cluster_layer9 {
			graph [bb="252,7385,732,8550.3",
				label="Layer 9\nGPUs 8-15",
				lheight=0.42,
				lp="492,8531.3",
				lwidth=1.06,
				style=dotted
			];
			lay9_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 8-15",
				pos="492,8470.3",
				shape=rectangle,
				width=3.0139];
			lay9_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,8329.3",
				shape=rectangle,
				width=3.0972];
			lay9_qkv_proj -> lay9_attn	[pos="e,492,8363.6 492,8436.2 492,8436.2 492,8373.6 492,8373.6"];
			lay9_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,8188.3",
				shape=rectangle,
				width=3.75];
			lay9_attn -> lay9_attn_out	[pos="e,492,8222.6 492,8295.2 492,8295.2 492,8232.6 492,8232.6"];
			lay9_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,8033.2",
				width=6.4425];
			lay9_attn_out -> lay9_res1	[pos="e,492,8081.6 492,8154.3 492,8154.3 492,8091.6 492,8091.6"];
			lay9_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="368,7878.2",
				shape=rectangle,
				width=2.9444];
			lay9_res1 -> lay9_mlp_gate	[pos="e,368,7912.3 368,7992.5 368,7992.5 368,7922.3 368,7922.3"];
			lay9_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="616,7878.2",
				shape=rectangle,
				width=2.9444];
			lay9_res1 -> lay9_mlp_up	[pos="e,616,7912.3 616,7992.5 616,7992.5 616,7922.3 616,7922.3"];
			lay9_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="492,7737.2",
				shape=rectangle,
				width=2.9444];
			lay9_mlp_gate -> lay9_mlp_act	[pos="e,430,7771.5 430,7844 430,7844 430,7781.5 430,7781.5"];
			lay9_mlp_up -> lay9_mlp_act	[pos="e,554,7771.5 554,7844 554,7844 554,7781.5 554,7781.5"];
			lay9_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,7596.2",
				shape=rectangle,
				width=2.8194];
			lay9_mlp_act -> lay9_mlp_down	[pos="e,492,7630.5 492,7703 492,7703 492,7640.5 492,7640.5"];
			lay9_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,7441.1",
				width=6.4425];
			lay9_mlp_down -> lay9_res2	[pos="e,492,7489.5 492,7562.1 492,7562.1 492,7499.5 492,7499.5"];
		}
		subgraph cluster_layer10 {
			graph [bb="252,6182.7,732,7348",
				label="Layer 10\nGPUs 8-15",
				lheight=0.42,
				lp="492,7329",
				lwidth=1.06,
				style=dotted
			];
			lay10_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 8-15",
				pos="492,7268",
				shape=rectangle,
				width=3.0139];
			lay10_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,7127",
				shape=rectangle,
				width=3.0972];
			lay10_qkv_proj -> lay10_attn	[pos="e,492,7161.3 492,7233.9 492,7233.9 492,7171.3 492,7171.3"];
			lay10_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,6986",
				shape=rectangle,
				width=3.75];
			lay10_attn -> lay10_attn_out	[pos="e,492,7020.3 492,7092.9 492,7092.9 492,7030.3 492,7030.3"];
			lay10_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,6830.9",
				width=6.4425];
			lay10_attn_out -> lay10_res1	[pos="e,492,6879.3 492,6952 492,6952 492,6889.3 492,6889.3"];
			lay10_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="368,6675.8",
				shape=rectangle,
				width=2.9444];
			lay10_res1 -> lay10_mlp_gate	[pos="e,368,6710 368,6790.2 368,6790.2 368,6720 368,6720"];
			lay10_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="616,6675.8",
				shape=rectangle,
				width=2.9444];
			lay10_res1 -> lay10_mlp_up	[pos="e,616,6710 616,6790.2 616,6790.2 616,6720 616,6720"];
			lay10_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="492,6534.8",
				shape=rectangle,
				width=2.9444];
			lay10_mlp_gate -> lay10_mlp_act	[pos="e,430,6569.1 430,6641.7 430,6641.7 430,6579.1 430,6579.1"];
			lay10_mlp_up -> lay10_mlp_act	[pos="e,554,6569.1 554,6641.7 554,6641.7 554,6579.1 554,6579.1"];
			lay10_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,6393.8",
				shape=rectangle,
				width=2.8194];
			lay10_mlp_act -> lay10_mlp_down	[pos="e,492,6428.1 492,6500.7 492,6500.7 492,6438.1 492,6438.1"];
			lay10_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,6238.7",
				width=6.4425];
			lay10_mlp_down -> lay10_res2	[pos="e,492,6287.1 492,6359.8 492,6359.8 492,6297.1 492,6297.1"];
		}
		subgraph cluster_layer11 {
			graph [bb="252,4980.3,732,6145.7",
				label="Layer 11\nGPUs 8-15",
				lheight=0.42,
				lp="492,6126.7",
				lwidth=1.06,
				style=dotted
			];
			lay11_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 8-15",
				pos="492,6065.7",
				shape=rectangle,
				width=3.0139];
			lay11_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,5924.7",
				shape=rectangle,
				width=3.0972];
			lay11_qkv_proj -> lay11_attn	[pos="e,492,5959 492,6031.5 492,6031.5 492,5969 492,5969"];
			lay11_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,5783.7",
				shape=rectangle,
				width=3.75];
			lay11_attn -> lay11_attn_out	[pos="e,492,5818 492,5890.5 492,5890.5 492,5828 492,5828"];
			lay11_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,5628.6",
				width=6.4425];
			lay11_attn_out -> lay11_res1	[pos="e,492,5677 492,5749.6 492,5749.6 492,5687 492,5687"];
			lay11_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="368,5473.5",
				shape=rectangle,
				width=2.9444];
			lay11_res1 -> lay11_mlp_gate	[pos="e,368,5507.7 368,5587.8 368,5587.8 368,5517.7 368,5517.7"];
			lay11_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="616,5473.5",
				shape=rectangle,
				width=2.9444];
			lay11_res1 -> lay11_mlp_up	[pos="e,616,5507.7 616,5587.8 616,5587.8 616,5517.7 616,5517.7"];
			lay11_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="492,5332.5",
				shape=rectangle,
				width=2.9444];
			lay11_mlp_gate -> lay11_mlp_act	[pos="e,430,5366.8 430,5439.4 430,5439.4 430,5376.8 430,5376.8"];
			lay11_mlp_up -> lay11_mlp_act	[pos="e,554,5366.8 554,5439.4 554,5439.4 554,5376.8 554,5376.8"];
			lay11_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,5191.5",
				shape=rectangle,
				width=2.8194];
			lay11_mlp_act -> lay11_mlp_down	[pos="e,492,5225.8 492,5298.4 492,5298.4 492,5235.8 492,5235.8"];
			lay11_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,5036.4",
				width=6.4425];
			lay11_mlp_down -> lay11_res2	[pos="e,492,5084.8 492,5157.5 492,5157.5 492,5094.8 492,5094.8"];
		}
		subgraph cluster_layer12 {
			graph [bb="252,3778,732,4943.3",
				label="Layer 12\nGPUs 8-15",
				lheight=0.42,
				lp="492,4924.3",
				lwidth=1.06,
				style=dotted
			];
			lay12_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 8-15",
				pos="492,4863.3",
				shape=rectangle,
				width=3.0139];
			lay12_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,4722.3",
				shape=rectangle,
				width=3.0972];
			lay12_qkv_proj -> lay12_attn	[pos="e,492,4756.6 492,4829.2 492,4829.2 492,4766.6 492,4766.6"];
			lay12_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,4581.3",
				shape=rectangle,
				width=3.75];
			lay12_attn -> lay12_attn_out	[pos="e,492,4615.6 492,4688.2 492,4688.2 492,4625.6 492,4625.6"];
			lay12_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,4426.2",
				width=6.4425];
			lay12_attn_out -> lay12_res1	[pos="e,492,4474.6 492,4547.3 492,4547.3 492,4484.6 492,4484.6"];
			lay12_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="368,4271.2",
				shape=rectangle,
				width=2.9444];
			lay12_res1 -> lay12_mlp_gate	[pos="e,368,4305.3 368,4385.5 368,4385.5 368,4315.3 368,4315.3"];
			lay12_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="616,4271.2",
				shape=rectangle,
				width=2.9444];
			lay12_res1 -> lay12_mlp_up	[pos="e,616,4305.3 616,4385.5 616,4385.5 616,4315.3 616,4315.3"];
			lay12_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="492,4130.2",
				shape=rectangle,
				width=2.9444];
			lay12_mlp_gate -> lay12_mlp_act	[pos="e,430,4164.5 430,4237 430,4237 430,4174.5 430,4174.5"];
			lay12_mlp_up -> lay12_mlp_act	[pos="e,554,4164.5 554,4237 554,4237 554,4174.5 554,4174.5"];
			lay12_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,3989.2",
				shape=rectangle,
				width=2.8194];
			lay12_mlp_act -> lay12_mlp_down	[pos="e,492,4023.5 492,4096 492,4096 492,4033.5 492,4033.5"];
			lay12_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,3834.1",
				width=6.4425];
			lay12_mlp_down -> lay12_res2	[pos="e,492,3882.5 492,3955.1 492,3955.1 492,3892.5 492,3892.5"];
		}
		subgraph cluster_layer13 {
			graph [bb="252,2575.7,732,3741",
				label="Layer 13\nGPUs 8-15",
				lheight=0.42,
				lp="492,3722",
				lwidth=1.06,
				style=dotted
			];
			lay13_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 8-15",
				pos="492,3661",
				shape=rectangle,
				width=3.0139];
			lay13_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,3520",
				shape=rectangle,
				width=3.0972];
			lay13_qkv_proj -> lay13_attn	[pos="e,492,3554.3 492,3626.9 492,3626.9 492,3564.3 492,3564.3"];
			lay13_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,3379",
				shape=rectangle,
				width=3.75];
			lay13_attn -> lay13_attn_out	[pos="e,492,3413.3 492,3485.9 492,3485.9 492,3423.3 492,3423.3"];
			lay13_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,3223.9",
				width=6.4425];
			lay13_attn_out -> lay13_res1	[pos="e,492,3272.3 492,3345 492,3345 492,3282.3 492,3282.3"];
			lay13_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="368,3068.8",
				shape=rectangle,
				width=2.9444];
			lay13_res1 -> lay13_mlp_gate	[pos="e,368,3103 368,3183.2 368,3183.2 368,3113 368,3113"];
			lay13_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="616,3068.8",
				shape=rectangle,
				width=2.9444];
			lay13_res1 -> lay13_mlp_up	[pos="e,616,3103 616,3183.2 616,3183.2 616,3113 616,3113"];
			lay13_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="492,2927.8",
				shape=rectangle,
				width=2.9444];
			lay13_mlp_gate -> lay13_mlp_act	[pos="e,430,2962.1 430,3034.7 430,3034.7 430,2972.1 430,2972.1"];
			lay13_mlp_up -> lay13_mlp_act	[pos="e,554,2962.1 554,3034.7 554,3034.7 554,2972.1 554,2972.1"];
			lay13_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,2786.8",
				shape=rectangle,
				width=2.8194];
			lay13_mlp_act -> lay13_mlp_down	[pos="e,492,2821.1 492,2893.7 492,2893.7 492,2831.1 492,2831.1"];
			lay13_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,2631.7",
				width=6.4425];
			lay13_mlp_down -> lay13_res2	[pos="e,492,2680.1 492,2752.8 492,2752.8 492,2690.1 492,2690.1"];
		}
		subgraph cluster_layer14 {
			graph [bb="252,1373.3,732,2538.7",
				label="Layer 14\nGPUs 8-15",
				lheight=0.42,
				lp="492,2519.7",
				lwidth=1.06,
				style=dotted
			];
			lay14_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 8-15",
				pos="492,2458.7",
				shape=rectangle,
				width=3.0139];
			lay14_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,2317.7",
				shape=rectangle,
				width=3.0972];
			lay14_qkv_proj -> lay14_attn	[pos="e,492,2352 492,2424.5 492,2424.5 492,2362 492,2362"];
			lay14_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,2176.7",
				shape=rectangle,
				width=3.75];
			lay14_attn -> lay14_attn_out	[pos="e,492,2211 492,2283.5 492,2283.5 492,2221 492,2221"];
			lay14_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,2021.6",
				width=6.4425];
			lay14_attn_out -> lay14_res1	[pos="e,492,2070 492,2142.6 492,2142.6 492,2080 492,2080"];
			lay14_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="368,1866.5",
				shape=rectangle,
				width=2.9444];
			lay14_res1 -> lay14_mlp_gate	[pos="e,368,1900.7 368,1980.8 368,1980.8 368,1910.7 368,1910.7"];
			lay14_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="616,1866.5",
				shape=rectangle,
				width=2.9444];
			lay14_res1 -> lay14_mlp_up	[pos="e,616,1900.7 616,1980.8 616,1980.8 616,1910.7 616,1910.7"];
			lay14_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="492,1725.5",
				shape=rectangle,
				width=2.9444];
			lay14_mlp_gate -> lay14_mlp_act	[pos="e,430,1759.8 430,1832.4 430,1832.4 430,1769.8 430,1769.8"];
			lay14_mlp_up -> lay14_mlp_act	[pos="e,554,1759.8 554,1832.4 554,1832.4 554,1769.8 554,1769.8"];
			lay14_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,1584.5",
				shape=rectangle,
				width=2.8194];
			lay14_mlp_act -> lay14_mlp_down	[pos="e,492,1618.8 492,1691.4 492,1691.4 492,1628.8 492,1628.8"];
			lay14_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,1429.4",
				width=6.4425];
			lay14_mlp_down -> lay14_res2	[pos="e,492,1477.8 492,1550.5 492,1550.5 492,1487.8 492,1487.8"];
		}
		subgraph cluster_layer15 {
			graph [bb="252,171,732,1336.3",
				label="Layer 15\nGPUs 8-15",
				lheight=0.42,
				lp="492,1317.3",
				lwidth=1.06,
				style=dotted
			];
			lay15_qkv_proj	[fillcolor=lightcoral,
				height=0.94444,
				label="QKV Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,32,128]\nGPUs 8-15",
				pos="492,1256.3",
				shape=rectangle,
				width=3.0139];
			lay15_attn	[fillcolor=lightpink,
				height=0.94444,
				label="Multi-Head Attention (TP=8)\nInput: [128,10000,32,128]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,1115.3",
				shape=rectangle,
				width=3.0972];
			lay15_qkv_proj -> lay15_attn	[pos="e,492,1149.6 492,1222.2 492,1222.2 492,1159.6 492,1159.6"];
			lay15_attn_out	[fillcolor=lightcoral,
				height=0.94444,
				label="Attention Output Projection (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,974.33",
				shape=rectangle,
				width=3.75];
			lay15_attn -> lay15_attn_out	[pos="e,492,1008.6 492,1081.2 492,1081.2 492,1018.6 492,1018.6"];
			lay15_res1	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 1\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,819.25",
				width=6.4425];
			lay15_attn_out -> lay15_res1	[pos="e,492,867.62 492,940.3 492,940.3 492,877.62 492,877.62"];
			lay15_mlp_gate	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Gate (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="368,664.17",
				shape=rectangle,
				width=2.9444];
			lay15_res1 -> lay15_mlp_gate	[pos="e,368,698.35 368,778.5 368,778.5 368,708.35 368,708.35"];
			lay15_mlp_up	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Up (TP=8)\nInput: [128,10000,4096]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="616,664.17",
				shape=rectangle,
				width=2.9444];
			lay15_res1 -> lay15_mlp_up	[pos="e,616,698.35 616,778.5 616,778.5 616,708.35 616,708.35"];
			lay15_mlp_act	[height=0.94444,
				label="GELU Activation\nInput: [128,10000,16384]\nOutput: [128,10000,16384]\nGPUs 8-15",
				pos="492,523.17",
				shape=rectangle,
				width=2.9444];
			lay15_mlp_gate -> lay15_mlp_act	[pos="e,430,557.47 430,630.03 430,630.03 430,567.47 430,567.47"];
			lay15_mlp_up -> lay15_mlp_act	[pos="e,554,557.47 554,630.03 554,630.03 554,567.47 554,567.47"];
			lay15_mlp_down	[fillcolor=lightseagreen,
				height=0.94444,
				label="MLP Down (TP=8)\nInput: [128,10000,16384]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,382.17",
				shape=rectangle,
				width=2.8194];
			lay15_mlp_act -> lay15_mlp_down	[pos="e,492,416.47 492,489.03 492,489.03 492,426.47 492,426.47"];
			lay15_res2	[fillcolor=lightgray,
				height=1.3356,
				label="Residual Add 2\nInput: [128,10000,4096], [128,10000,4096]\nOutput: [128,10000,4096]\nGPUs 8-15",
				pos="492,227.08",
				width=6.4425];
			lay15_mlp_down -> lay15_res2	[pos="e,492,275.45 492,348.13 492,348.13 492,285.45 492,285.45"];
		}
		stage1_input	[fillcolor=lightyellow,
			height=1.4722,
			label="Input to Stage 1\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=\
4096]",
			pos="492,9850.7",
			shape=parallelogram,
			width=13.219];
		stage1_input -> lay8_qkv_proj	[pos="e,492,9706.7 492,9797.3 492,9797.3 492,9716.7 492,9716.7"];
		lay8_res2 -> lay9_qkv_proj	[pos="e,492,8504.6 492,8595.2 492,8595.2 492,8514.6 492,8514.6"];
		lay9_res2 -> lay10_qkv_proj	[pos="e,492,7302.2 492,7392.9 492,7392.9 492,7312.2 492,7312.2"];
		lay10_res2 -> lay11_qkv_proj	[pos="e,492,6099.9 492,6190.5 492,6190.5 492,6109.9 492,6109.9"];
		lay11_res2 -> lay12_qkv_proj	[pos="e,492,4897.6 492,4988.2 492,4988.2 492,4907.6 492,4907.6"];
		lay12_res2 -> lay13_qkv_proj	[pos="e,492,3695.2 492,3785.9 492,3785.9 492,3705.2 492,3705.2"];
		lay13_res2 -> lay14_qkv_proj	[pos="e,492,2492.9 492,2583.5 492,2583.5 492,2502.9 492,2502.9"];
		lay14_res2 -> lay15_qkv_proj	[pos="e,492,1290.6 492,1381.2 492,1381.2 492,1300.6 492,1300.6"];
	}
	input	[fillcolor=lightgreen,
		height=1.4722,
		label="Input\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]",
		pos="492,19906",
		shape=parallelogram,
		width=13.219];
	input -> stage0_input	[label="Full Model Input",
		lp="552,19810",
		pos="e,492,19749 492,19853 492,19853 492,19759 492,19759"];
	tp_allreduce	[fillcolor=yellow,
		height=1.041,
		label="Tensor Parallel All-Reduce\nAcross GPUs 0-7 or 8-15\nSize: 4096 elements per token",
		pos="1182,19906",
		style=dashed,
		width=4.5962];
	lay0_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1021.6,19897 627.02,19246 783.93,19246 1021.6,19246 1021.6,19246 1021.6,19246 1021.6,19887 1021.6,19887",
		style=dashed];
	lay0_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1031.8,19890 593.53,18654 749.9,18654 1031.8,18654 1031.8,18654 1031.8,18654 1031.8,19880 1031.8,19880",
		style=dashed];
	lay1_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1042,19886 627.25,18044 790.1,18044 1042,18044 1042,18044 1042,18044 1042,19876 1042,19876",
		style=dashed];
	lay1_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1052.2,19883 593.77,17452 755.27,17452 1052.2,17452 1052.2,17452 1052.2,17452 1052.2,19873 1052.2,19873",
		style=dashed];
	lay2_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1062.4,19880 627.17,16841 795.83,16841 1062.4,16841 1062.4,16841 1062.4,16841 1062.4,19870 1062.4,19870",
		style=dashed];
	lay2_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1072.5,19878 593.79,16249 760.28,16249 1072.5,16249 1072.5,16249 1072.5,16249 1072.5,19868 1072.5,19868",
		style=dashed];
	lay3_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1082.7,19876 627.15,15639 801.53,15639 1082.7,15639 1082.7,15639 1082.7,15639 1082.7,19866 1082.7,19866",
		style=dashed];
	lay3_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1092.9,19875 593.6,15047 764.94,15047 1092.9,15047 1092.9,15047 1092.9,15047 1092.9,19865 1092.9,19865",
		style=dashed];
	lay4_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1103.1,19873 627.24,14437 807.24,14437 1103.1,14437 1103.1,14437 1103.1,14437 1103.1,19863 1103.1,19863",
		style=dashed];
	lay4_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1113.3,19872 593.55,13845 769.7,13845 1113.3,13845 1113.3,13845 1113.3,13845 1113.3,19862 1113.3,19862",
		style=dashed];
	lay5_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1123.5,19871 627.07,13234 812.57,13234 1123.5,13234 1123.5,13234 1123.5,13234 1123.5,19861 1123.5,19861",
		style=dashed];
	lay5_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1133.6,19870 593.67,12642 774.59,12642 1133.6,12642 1133.6,12642 1133.6,12642 1133.6,19860 1133.6,19860",
		style=dashed];
	lay6_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1143.8,19869 627.06,12032 817.98,12032 1143.8,12032 1143.8,12032 1143.8,12032 1143.8,19859 1143.8,19859",
		style=dashed];
	lay6_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1154,19869 593.62,11440 779.2,11440 1154,11440 1154,11440 1154,11440 1154,19859 1154,19859",
		style=dashed];
	lay7_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1164.2,19869 627.22,10830 823.51,10830 1164.2,10830 1164.2,10830 1164.2,10830 1164.2,19859 1164.2,19859",
		style=dashed];
	lay7_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1174.4,19869 593.79,10238 784.01,10238 1174.4,10238 1174.4,10238 1174.4,10238 1174.4,19859 1174.4,19859",
		style=dashed];
	lay7_res2 -> stage1_input	[label="Pipeline Transfer\nSize: 5.24GB",
		lp="554,9972.7",
		pos="e,492,9903.8 492,10024 492,10024 492,9913.8 492,9913.8"];
	lay8_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1184.5,19869 627.17,9401 828.72,9401 1184.5,9401 1184.5,9401 1184.5,9401 1184.5,19859 1184.5,19859",
		style=dashed];
	lay8_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1194.7,19869 593.81,8809 788.56,8809 1194.7,8809 1194.7,8809 1194.7,8809 1194.7,19859 1194.7,19859",
		style=dashed];
	lay9_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1204.9,19869 627.35,8199 834.11,8199 1204.9,8199 1204.9,8199 1204.9,8199 1204.9,19859 1204.9,19859",
		style=dashed];
	lay9_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1215.1,19869 593.69,7607 792.85,7607 1215.1,7607 1215.1,7607 1215.1,7607 1215.1,19859 1215.1,19859",
		style=dashed];
	lay10_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1225.3,19870 627.35,6997 839.22,6997 1225.3,6997 1225.3,6997 1225.3,6997 1225.3,19860 1225.3,19860",
		style=dashed];
	lay10_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1235.5,19871 593.82,6405 797.43,6405 1235.5,6405 1235.5,6405 1235.5,6405 1235.5,19861 1235.5,19861",
		style=dashed];
	lay11_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1245.6,19872 627.17,5794 844.05,5794 1245.6,5794 1245.6,5794 1245.6,5794 1245.6,19862 1245.6,19862",
		style=dashed];
	lay11_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1255.8,19872 593.82,5202 801.78,5202 1255.8,5202 1255.8,5202 1255.8,5202 1255.8,19862 1255.8,19862",
		style=dashed];
	lay12_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1266,19874 627.26,4592 849.13,4592 1266,4592 1266,4592 1266,4592 1266,19864 1266,19864",
		style=dashed];
	lay12_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1276.2,19876 593.71,4000 805.9,4000 1276.2,4000 1276.2,4000 1276.2,4000 1276.2,19866 1276.2,19866",
		style=dashed];
	lay13_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1286.4,19877 627.2,3390 853.96,3390 1286.4,3390 1286.4,3390 1286.4,3390 1286.4,19867 1286.4,19867",
		style=dashed];
	lay13_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1296.6,19879 593.9,2798 810.39,2798 1296.6,2798 1296.6,2798 1296.6,2798 1296.6,19869 1296.6,19869",
		style=dashed];
	lay14_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1306.7,19882 627.45,2187 859.12,2187 1306.7,2187 1306.7,2187 1306.7,2187 1306.7,19872 1306.7,19872",
		style=dashed];
	lay14_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1316.9,19885 593.56,1595 814.06,1595 1316.9,1595 1316.9,1595 1316.9,1595 1316.9,19875 1316.9,19875",
		style=dashed];
	lay15_attn_out -> tp_allreduce	[constraint=false,
		pos="e,1327.1,19888 627.1,985 863.45,985 1327.1,985 1327.1,985 1327.1,985 1327.1,19878 1327.1,19878",
		style=dashed];
	lay15_mlp_down -> tp_allreduce	[constraint=false,
		pos="e,1337.3,19893 593.54,393 818.13,393 1337.3,393 1337.3,393 1337.3,393 1337.3,19883 1337.3,19883",
		style=dashed];
	output	[fillcolor=lightgreen,
		height=1.4722,
		label="Output\nInput: [batch_size=128, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_size=4096]",
		pos="492,53",
		shape=parallelogram,
		width=13.219];
	lay15_res2 -> output	[pos="e,492,106.12 492,178.59 492,178.59 492,116.12 492,116.12"];
	tp_allreduce -> lay0_attn_out	[constraint=false,
		pos="e,627.28,19223 1026.7,19893 1026.7,19799 1026.7,19223 1026.7,19223 1026.7,19223 637.28,19223 637.28,19223",
		style=dashed];
	tp_allreduce -> lay0_mlp_down	[constraint=false,
		pos="e,593.53,18631 1036.9,19888 1036.9,19733 1036.9,18631 1036.9,18631 1036.9,18631 603.53,18631 603.53,18631",
		style=dashed];
	tp_allreduce -> lay1_attn_out	[constraint=false,
		pos="e,627.08,18021 1047.1,19885 1047.1,19675 1047.1,18021 1047.1,18021 1047.1,18021 637.08,18021 637.08,18021",
		style=dashed];
	tp_allreduce -> lay1_mlp_down	[constraint=false,
		pos="e,593.71,17429 1057.3,19881 1057.3,19622 1057.3,17429 1057.3,17429 1057.3,17429 603.71,17429 603.71,17429",
		style=dashed];
	tp_allreduce -> lay2_attn_out	[constraint=false,
		pos="e,627.28,16819 1067.4,19879 1067.4,19575 1067.4,16819 1067.4,16819 1067.4,16819 637.28,16819 637.28,16819",
		style=dashed];
	tp_allreduce -> lay2_mlp_down	[constraint=false,
		pos="e,593.67,16227 1077.6,19877 1077.6,19530 1077.6,16227 1077.6,16227 1077.6,16227 603.67,16227 603.67,16227",
		style=dashed];
	tp_allreduce -> lay3_attn_out	[constraint=false,
		pos="e,627.2,15616 1087.8,19875 1087.8,19490 1087.8,15616 1087.8,15616 1087.8,15616 637.2,15616 637.2,15616",
		style=dashed];
	tp_allreduce -> lay3_mlp_down	[constraint=false,
		pos="e,593.77,15024 1098,19874 1098,19450 1098,15024 1098,15024 1098,15024 603.77,15024 603.77,15024",
		style=dashed];
	tp_allreduce -> lay4_attn_out	[constraint=false,
		pos="e,627.22,14414 1108.2,19873 1108.2,19414 1108.2,14414 1108.2,14414 1108.2,14414 637.22,14414 637.22,14414",
		style=dashed];
	tp_allreduce -> lay4_mlp_down	[constraint=false,
		pos="e,593.68,13822 1118.4,19871 1118.4,19378 1118.4,13822 1118.4,13822 1118.4,13822 603.68,13822 603.68,13822",
		style=dashed];
	tp_allreduce -> lay5_attn_out	[constraint=false,
		pos="e,627.38,13212 1128.5,19871 1128.5,19347 1128.5,13212 1128.5,13212 1128.5,13212 637.38,13212 637.38,13212",
		style=dashed];
	tp_allreduce -> lay5_mlp_down	[constraint=false,
		pos="e,593.76,12620 1138.7,19870 1138.7,19317 1138.7,12620 1138.7,12620 1138.7,12620 603.76,12620 603.76,12620",
		style=dashed];
	tp_allreduce -> lay6_attn_out	[constraint=false,
		pos="e,627.32,12009 1148.9,19869 1148.9,19287 1148.9,12009 1148.9,12009 1148.9,12009 637.32,12009 637.32,12009",
		style=dashed];
	tp_allreduce -> lay6_mlp_down	[constraint=false,
		pos="e,593.68,11417 1159.1,19869 1159.1,19260 1159.1,11417 1159.1,11417 1159.1,11417 603.68,11417 603.68,11417",
		style=dashed];
	tp_allreduce -> lay7_attn_out	[constraint=false,
		pos="e,627.02,10807 1169.3,19869 1169.3,19235 1169.3,10807 1169.3,10807 1169.3,10807 637.02,10807 637.02,10807",
		style=dashed];
	tp_allreduce -> lay7_mlp_down	[constraint=false,
		pos="e,593.81,10215 1179.5,19869 1179.5,19214 1179.5,10215 1179.5,10215 1179.5,10215 603.81,10215 603.81,10215",
		style=dashed];
	tp_allreduce -> lay8_attn_out	[constraint=false,
		pos="e,627.34,9379 1189.6,19869 1189.6,19184 1189.6,9379 1189.6,9379 1189.6,9379 637.34,9379 637.34,9379",
		style=dashed];
	tp_allreduce -> lay8_mlp_down	[constraint=false,
		pos="e,593.79,8787 1199.8,19869 1199.8,19167 1199.8,8787 1199.8,8787 1199.8,8787 603.79,8787 603.79,8787",
		style=dashed];
	tp_allreduce -> lay9_attn_out	[constraint=false,
		pos="e,627.05,8176 1210,19869 1210,19149 1210,8176 1210,8176 1210,8176 637.05,8176 637.05,8176",
		style=dashed];
	tp_allreduce -> lay9_mlp_down	[constraint=false,
		pos="e,593.63,7584 1220.2,19870 1220.2,19134 1220.2,7584 1220.2,7584 1220.2,7584 603.63,7584 603.63,7584",
		style=dashed];
	tp_allreduce -> lay10_attn_out	[constraint=false,
		pos="e,627.43,6974 1230.4,19870 1230.4,19123 1230.4,6974 1230.4,6974 1230.4,6974 637.43,6974 637.43,6974",
		style=dashed];
	tp_allreduce -> lay10_mlp_down	[constraint=false,
		pos="e,593.73,6382 1240.5,19871 1240.5,19113 1240.5,6382 1240.5,6382 1240.5,6382 603.73,6382 603.73,6382",
		style=dashed];
	tp_allreduce -> lay11_attn_out	[constraint=false,
		pos="e,627.21,5772 1250.7,19872 1250.7,19107 1250.7,5772 1250.7,5772 1250.7,5772 637.21,5772 637.21,5772",
		style=dashed];
	tp_allreduce -> lay11_mlp_down	[constraint=false,
		pos="e,593.71,5180 1260.9,19873 1260.9,19105 1260.9,5180 1260.9,5180 1260.9,5180 603.71,5180 603.71,5180",
		style=dashed];
	tp_allreduce -> lay12_attn_out	[constraint=false,
		pos="e,627.26,4569 1271.1,19875 1271.1,19105 1271.1,4569 1271.1,4569 1271.1,4569 637.26,4569 637.26,4569",
		style=dashed];
	tp_allreduce -> lay12_mlp_down	[constraint=false,
		pos="e,593.57,3977 1281.3,19876 1281.3,19111 1281.3,3977 1281.3,3977 1281.3,3977 603.57,3977 603.57,3977",
		style=dashed];
	tp_allreduce -> lay13_attn_out	[constraint=false,
		pos="e,627.17,3367 1291.5,19878 1291.5,19120 1291.5,3367 1291.5,3367 1291.5,3367 637.17,3367 637.17,3367",
		style=dashed];
	tp_allreduce -> lay13_mlp_down	[constraint=false,
		pos="e,593.72,2775 1301.6,19880 1301.6,19137 1301.6,2775 1301.6,2775 1301.6,2775 603.72,2775 603.72,2775",
		style=dashed];
	tp_allreduce -> lay14_attn_out	[constraint=false,
		pos="e,627.38,2165 1311.8,19883 1311.8,19165 1311.8,2165 1311.8,2165 1311.8,2165 637.38,2165 637.38,2165",
		style=dashed];
	tp_allreduce -> lay14_mlp_down	[constraint=false,
		pos="e,593.78,1573 1322,19886 1322,19206 1322,1573 1322,1573 1322,1573 603.78,1573 603.78,1573",
		style=dashed];
	tp_allreduce -> lay15_attn_out	[constraint=false,
		pos="e,627.46,962 1332.2,19890 1332.2,19273 1332.2,962 1332.2,962 1332.2,962 637.46,962 637.46,962",
		style=dashed];
	tp_allreduce -> lay15_mlp_down	[constraint=false,
		pos="e,593.73,370 1342.4,19897 1342.4,19413 1342.4,370 1342.4,370 1342.4,370 603.73,370 603.73,370",
		style=dashed];
}
