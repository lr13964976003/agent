{
  "generated_dags": {
    "baseline": {
      "description": "Tensor Parallelism + Pipeline Parallelism Baseline (TP=8, PP=2)",
      "dot_file": "../outputs/2025-11-24-10-26-01/baseline_dag_fixed.dot",
      "svg_file": "../outputs/2025-11-24-10-26-01/baseline_dag_fixed.svg",
      "strategy": "Hybrid TP8 PP2",
      "gpu_allocation": {
        "stage_0": {
          "gpus": [0, 1, 2, 3, 4, 5, 6, 7],
          "layers": [0, 1, 2, 3, 4, 5, 6, 7]
        },
        "stage_1": {
          "gpus": [8, 9, 10, 11, 12, 13, 14, 15],
          "layers": [8, 9, 10, 11, 12, 13, 14, 15]
        }
      },
      "verification": {
        "acyclic": true,
        "nodes_count": 2,
        "edges_count": 1
      }
    },
    "proposed": {
      "description": "Layer-wise Cache-aware Partitioning (1 layer per GPU)",
      "dot_file": "../outputs/2025-11-24-10-26-01/proposed_dag_simple.dot",
      "svg_file": "../outputs/2025-11-24-10-26-01/proposed_dag_simple.svg",
      "strategy": "Layer-wise parallelism",
      "gpu_allocation": {
        "layer_0": {"gpu": 0, "layer": 0},
        "layer_1": {"gpu": 1, "layer": 1},
        "layer_2": {"gpu": 2, "layer": 2},
        "layer_3": {"gpu": 3, "layer": 3},
        "layer_4": {"gpu": 4, "layer": 4},
        "layer_5": {"gpu": 5, "layer": 5},
        "layer_6": {"gpu": 6, "layer": 6},
        "layer_7": {"gpu": 7, "layer": 7},
        "layer_8": {"gpu": 8, "layer": 8},
        "layer_9": {"gpu": 9, "layer": 9},
        "layer_10": {"gpu": 10, "layer": 10},
        "layer_11": {"gpu": 11, "layer": 11},
        "layer_12": {"gpu": 12, "layer": 12},
        "layer_13": {"gpu": 13, "layer": 13},
        "layer_14": {"gpu": 14, "layer": 14},
        "layer_15": {"gpu": 15, "layer": 15}
      },
      "verification": {
        "acyclic": true,
        "nodes_count": 17,
        "edges_count": 16
      }
    }
  },
  "summary": {
    "total_dags_generated": 2,
    "all_dags_acyclic": true,
    "files_location": "../outputs/2025-11-24-10-26-01",
    "dimensions_maintained": {
      "batch_size": 128,
      "sequence_length": 10000,
      "hidden_size": 4096,
      "num_heads": 32,
      "head_dim": 128,
      "mlp_hidden_size": 16384
    }
  }
}