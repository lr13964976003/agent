// Layer-wise Deployment Strategy DAG
digraph {
	dpi=300 rankdir=TB size="30,40"
	node [fontname=Arial fontsize=10]
	edge [fontname=Arial fontsize=9]
	node [fillcolor=lightblue shape=rectangle style=filled]
	node [fillcolor=lightgreen shape=ellipse style=filled]
	node [fillcolor=lightyellow shape=parallelogram style=filled]
	subgraph cluster_input {
		label="Input Batch" style=dashed
		input [label="Input\nGPU: Host\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightcyan shape=ellipse]
	}
	subgraph cluster_gpu0 {
		fillcolor=lightgray label="GPU 0: Layers 0-1" style="rounded,filled"
		gpu0_layernorm0 [label="LayerNorm\nGPU: 0\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu0_mha_qkv [label="MHA Q/K/V Projection\nGPU: 0\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]" fillcolor=lightblue shape=rectangle]
		gpu0_mha_attention [label="MHA Attention\nGPU: 0\nInput: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]" fillcolor=lightblue shape=rectangle]
		gpu0_mha_out [label="MHA Output Projection\nGPU: 0\nInput: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu0_residual0 [label="Residual Add\nGPU: 0\nInput1: [batch_size=128, seq_len=10000, hidden_size=16384]\nInput2: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightyellow shape=parallelogram]
		gpu0_layernorm1 [label="LayerNorm\nGPU: 0\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu0_ffn_gate [label="FFN Gate\nGPU: 0\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu0_ffn_experts [label="FFN Experts\nGPU: 0\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu0_ffn_out [label="FFN Output Projection\nGPU: 0\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu0_residual1 [label="Residual Add\nGPU: 0\nInput1: [batch_size=128, seq_len=10000, hidden_size=16384]\nInput2: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightyellow shape=parallelogram]
	}
	comm_gpu0_gpu1 [label="Inter-GPU Transfer\nGPU: 0 â†’ 1\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightgreen shape=ellipse]
	subgraph cluster_gpu4 {
		fillcolor=lightgray label="GPU 4: Layers 8-9 (Representative Middle)" style="rounded,filled"
		gpu4_layernorm8 [label="LayerNorm\nGPU: 4\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu4_mha_qkv [label="MHA Q/K/V Projection\nGPU: 4\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]" fillcolor=lightblue shape=rectangle]
		gpu4_mha_attention [label="MHA Attention\nGPU: 4\nInput: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]" fillcolor=lightblue shape=rectangle]
		gpu4_mha_out [label="MHA Output Projection\nGPU: 4\nInput: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu4_residual8 [label="Residual Add\nGPU: 4\nInput1: [batch_size=128, seq_len=10000, hidden_size=16384]\nInput2: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightyellow shape=parallelogram]
		gpu4_layernorm9 [label="LayerNorm\nGPU: 4\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu4_ffn_gate [label="FFN Gate (Expert Selection)\nGPU: 4\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]" fillcolor=lightblue shape=rectangle style=dashed]
		gpu4_split_experts [label="Split for Experts\nGPU: 4\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=8192]" fillcolor=lightyellow shape=parallelogram]
		gpu4_expert1 [label="Expert 1\nGPU: 4\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=8192]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=8192]" fillcolor=lightblue shape=rectangle]
		gpu4_expert2 [label="Expert 2\nGPU: 4\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=8192]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=8192]" fillcolor=lightblue shape=rectangle]
		gpu4_aggregate_experts [label="Aggregate Experts\nGPU: 4\nInput1: [batch_size=128, seq_len=10000, mlp_hidden_size=8192]\nInput2: [batch_size=128, seq_len=10000, mlp_hidden_size=8192]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]" fillcolor=lightyellow shape=parallelogram]
		gpu4_ffn_out [label="FFN Output Projection\nGPU: 4\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu4_residual9 [label="Residual Add\nGPU: 4\nInput1: [batch_size=128, seq_len=10000, hidden_size=16384]\nInput2: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightyellow shape=parallelogram]
	}
	comm_gpu4_gpu5 [label="Inter-GPU Transfer\nGPU: 4 â†’ 5\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightgreen shape=ellipse]
	subgraph cluster_gpu7 {
		fillcolor=lightgray label="GPU 7: Layers 14-15 (Final)" style="rounded,filled"
		gpu7_layernorm14 [label="LayerNorm\nGPU: 7\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu7_mha_qkv [label="MHA Q/K/V Projection\nGPU: 7\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]" fillcolor=lightblue shape=rectangle]
		gpu7_mha_attention [label="MHA Attention\nGPU: 7\nInput: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]" fillcolor=lightblue shape=rectangle]
		gpu7_mha_out [label="MHA Output Projection\nGPU: 7\nInput: [batch_size=128, seq_len=10000, num_heads=32, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu7_residual14 [label="Residual Add\nGPU: 7\nInput1: [batch_size=128, seq_len=10000, hidden_size=16384]\nInput2: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightyellow shape=parallelogram]
		gpu7_layernorm15 [label="LayerNorm\nGPU: 7\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu7_ffn_gate [label="FFN Gate\nGPU: 7\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu7_ffn_experts [label="FFN Experts\nGPU: 7\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu7_ffn_out [label="FFN Output Projection\nGPU: 7\nInput: [batch_size=128, seq_len=10000, mlp_hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightblue shape=rectangle]
		gpu7_residual15 [label="Residual Add\nGPU: 7\nInput1: [batch_size=128, seq_len=10000, hidden_size=16384]\nInput2: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightyellow shape=parallelogram]
	}
	output [label="Output\nGPU: Host\nInput: [batch_size=128, seq_len=10000, hidden_size=16384]\nOutput: [batch_size=128, seq_len=10000, hidden_size=16384]" fillcolor=lightcyan shape=ellipse]
	input -> gpu0_layernorm0
	gpu0_layernorm0 -> gpu0_mha_qkv
	gpu0_mha_qkv -> gpu0_mha_attention
	gpu0_mha_attention -> gpu0_mha_out
	gpu0_mha_out -> gpu0_residual0
	input -> gpu0_residual0
	gpu0_residual0 -> gpu0_layernorm1
	gpu0_layernorm1 -> gpu0_ffn_gate
	gpu0_ffn_gate -> gpu0_ffn_experts
	gpu0_ffn_experts -> gpu0_ffn_out
	gpu0_ffn_out -> gpu0_residual1
	gpu0_residual0 -> gpu0_residual1
	gpu0_residual1 -> comm_gpu0_gpu1
	comm_gpu0_gpu1 -> gpu4_layernorm8 [label="... through GPUs 1-3 ..." style=dashed]
	gpu4_layernorm8 -> gpu4_mha_qkv
	gpu4_mha_qkv -> gpu4_mha_attention
	gpu4_mha_attention -> gpu4_mha_out
	gpu4_mha_out -> gpu4_residual8
	gpu4_layernorm8 -> gpu4_residual8
	gpu4_residual8 -> gpu4_layernorm9
	gpu4_layernorm9 -> gpu4_ffn_gate
	gpu4_ffn_gate -> gpu4_split_experts [style=dashed]
	gpu4_split_experts -> gpu4_expert1
	gpu4_split_experts -> gpu4_expert2
	gpu4_expert1 -> gpu4_aggregate_experts
	gpu4_expert2 -> gpu4_aggregate_experts
	gpu4_aggregate_experts -> gpu4_ffn_out
	gpu4_ffn_out -> gpu4_residual9
	gpu4_residual8 -> gpu4_residual9
	gpu4_residual9 -> comm_gpu4_gpu5
	comm_gpu4_gpu5 -> gpu7_layernorm14 [label="... through GPUs 5-6 ..." style=dashed]
	gpu7_layernorm14 -> gpu7_mha_qkv
	gpu7_mha_qkv -> gpu7_mha_attention
	gpu7_mha_attention -> gpu7_mha_out
	gpu7_mha_out -> gpu7_residual14
	gpu7_layernorm14 -> gpu7_residual14
	gpu7_residual14 -> gpu7_layernorm15
	gpu7_layernorm15 -> gpu7_ffn_gate
	gpu7_ffn_gate -> gpu7_ffn_experts
	gpu7_ffn_experts -> gpu7_ffn_out
	gpu7_ffn_out -> gpu7_residual15
	gpu7_residual14 -> gpu7_residual15
	gpu7_residual15 -> output
}
