{
  "generated_files": {
    "complete_dag_dot": "../outputs/2025-11-28-17-19-06/complete_layer_wise_deployment.dag",
    "complete_dag_svg": "../outputs/2025-11-28-17-19-06/complete_layer_wise_deployment.svg"
  },
  "dag_specifications": {
    "total_gpus_shown": 8,
    "layers_per_gpu": 2,
    "total_layers_covered": 16,
    "gpu_mapping": {
      "GPU_0": "Layers 0-1",
      "GPU_1": "Layers 2-3", 
      "GPU_2": "Layers 4-5",
      "GPU_3": "Layers 6-7",
      "GPU_4": "Layers 8-9 (Representative Middle)",
      "GPU_5": "Layers 10-11",
      "GPU_6": "Layers 12-13", 
      "GPU_7": "Layers 14-15 (Final)"
    },
    "node_types": {
      "computation": "Rectangle (lightblue)",
      "communication": "Ellipse (lightgreen)", 
      "routing_aggregation": "Parallelogram (lightyellow)"
    },
    "features_included": {
      "input_output_dimensions": "All nodes include [batch_size=128, seq_len=10000, ...] format",
      "gpu_id_specification": "Each node specifies GPU: X explicitly",
      "multi_head_attention": "Complete MHA with Q/K/V projection, attention, output projection",
      "ffn_with_experts": "FFN gate, expert splitting, multiple experts, aggregation",
      "expert_selection_dashed": "Gate to expert splitting shown with dashed line",
      "residual_connections": "All residual adds with multiple inputs shown",
      "inter_gpu_communication": "Complete communication chain between all GPUs",
      "tensor_split_aggregation": "Expert split and aggregation operations shown",
      "no_cycles": "Verified: DAG has no cycles",
      "complete_connectivity": "All nodes properly connected in pipeline"
    },
    "requirements_met": {
      "single_consolidated_dag": true,
      "three_representative_layers": "GPU 0 (start), GPU 4 (middle), GPU 7 (end) highlighted",
      "gpu_boundaries": "Each cluster represents one GPU with 2 layers",
      "operator_level_detail": "All operations broken down to individual operators",
      "shape_conventions": "Rectangles=compute, Ellipses=comm, Parallelograms=routing",
      "input_output_dimensions": "Every node includes tensor shapes",
      "gpu_numbers_explicit": "No abbreviations like 'ALL' or 'Shared'",
      "multi_operator_nodes_split": "Each operator is separate node",
      "mha_plus_ffn_per_layer": "Each layer has complete MHA and FFN structure",
      "residual_multiple_inputs": "All residual adds show both inputs",
      "tensor_split_aggregation_shown": "Expert operations properly represented",
      "no_cycles": "Verified acyclic",
      "proper_connectivity": "Input→GPU0→GPU1→...→GPU7→Output complete chain"
    }
  },
  "deployment_strategy_followed": {
    "layer_wise_partitioning": "2 layers per GPU across 8 GPUs",
    "cache_optimization": "Weights cached in L2 (50MB constraint)",
    "minimal_communication": "Only layer outputs transferred between GPUs",
    "sequential_execution": "Layers 0-15 executed in order across GPUs 0-7",
    "activation_streaming": "Large activations (83.8GB) streamed through memory hierarchy"
  },
  "performance_targets": {
    "tps_improvement": "20% (12800 → 15360)",
    "tpot_reduction": "17% (0.078 → 0.065 ms)",
    "memory_optimization": "Weight caching in L2 cache reduces off-chip access"
  }
}