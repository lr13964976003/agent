{
  "generated_dags": {
    "baseline": {
      "name": "baseline_moe_deployment",
      "description": "Traditional MoE deployment with Tensor Parallelism (TP=8) and Pipeline Parallelism (PP=2)",
      "parallel_strategy": {
        "tensor_parallelism": 8,
        "pipeline_parallelism": 2,
        "expert_parallelism": 2,
        "experts_per_gpu": 2
      },
      "total_gpus": 16,
      "device_mapping": {
        "stage_0": {
          "gpus": [0,1,2,3,4,5,6,7],
          "layers": "0-7",
          "expert_allocation": "2 experts per GPU"
        },
        "stage_1": {
          "gpus": [8,9,10,11,12,13,14,15],
          "layers": "8-15",
          "expert_allocation": "2 experts per GPU"
        }
      },
      "files": {
        "dot_path": "../outputs/2025-11-21-12-08-57/baseline_moe_deployment.dot",
        "svg_path": "../outputs/2025-11-21-12-08-57/baseline_moe_deployment.svg"
      }
    },
    "proposed": {
      "name": "proposed_moe_deployment",
      "description": "Large-scale cross-node Expert Parallelism (EP=16) with one expert per GPU",
      "parallel_strategy": {
        "expert_parallelism": 16,
        "experts_per_gpu": 1,
        "tensor_parallelism": 1,
        "pipeline_parallelism": 1
      },
      "total_gpus": 16,
      "device_mapping": {
        "expert_distribution": "1 expert per GPU across 16 GPUs",
        "gpu_allocation": {
          "gpu_0": "expert_0",
          "gpu_1": "expert_1",
          "gpu_2": "expert_2",
          "gpu_3": "expert_3",
          "gpu_4": "expert_4",
          "gpu_5": "expert_5",
          "gpu_6": "expert_6",
          "gpu_7": "expert_7",
          "gpu_8": "expert_8",
          "gpu_9": "expert_9",
          "gpu_10": "expert_10",
          "gpu_11": "expert_11",
          "gpu_12": "expert_12",
          "gpu_13": "expert_13",
          "gpu_14": "expert_14",
          "gpu_15": "expert_15"
        }
      },
      "files": {
        "dot_path": "../outputs/2025-11-21-12-08-57/proposed_moe_deployment.dot",
        "svg_path": "../outputs/2025-11-21-12-08-57/proposed_moe_deployment.svg"
      }
    }
  },
  "verification": {
    "baseline_dag": {
      "cycles_detected": false,
      "total_nodes": 720,
      "total_edges": 720,
      "input_output_connected": true
    },
    "proposed_dag": {
      "cycles_detected": false,
      "total_nodes": 337,
      "total_edges": 337,
      "input_output_connected": true
    }
  },
  "engineering_analysis": {
    "baseline": {
      "tensor_dimension_tracking": "Complete - all tensor shapes specified",
      "gpu_assignment": "Explicit GPU IDs for each node",
      "communication_paths": "Cross-stage communication via NVLink/InfiniBand",
      "load_balancing": "Static allocation - 2 experts per GPU"
    },
    "proposed": {
      "tensor_dimension_tracking": "Complete - includes dynamic batch routing",
      "gpu_assignment": "One-to-one expert-to-GPU mapping",
      "communication_paths": "Cross-node expert routing via dashed edges",
      "load_balancing": "Dynamic routing via gating mechanism"
    }
  },
  "all_files": [
    "../outputs/2025-11-21-12-08-57/baseline_moe_deployment.dot",
    "../outputs/2025-11-21-12-08-57/baseline_moe_deployment.svg",
    "../outputs/2025-11-21-12-08-57/proposed_moe_deployment.dot",
    "../outputs/2025-11-21-12-08-57/proposed_moe_deployment.svg"
  ]
}