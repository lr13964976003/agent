// Baseline MoE Deployment: TP8_PP2
digraph baseline_moe_deployment {
	rankdir=TB size="20,30"
	node [shape=rectangle]
	input [label="Input\nInput: [batch_size=128, seq_len=10000, hidden=4096]" shape=ellipse]
	layer0_mha_qkv_0 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer0_mha_qkv_1 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer0_mha_qkv_2 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer0_mha_qkv_3 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer0_mha_qkv_4 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer0_mha_qkv_5 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer0_mha_qkv_6 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer0_mha_qkv_7 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer0_mha_attn_7 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer0_mha_out_0 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer0_mha_out_1 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer0_mha_out_2 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer0_mha_out_3 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer0_mha_out_4 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer0_mha_out_5 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer0_mha_out_6 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer0_mha_out_7 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer0_res1_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer0_ln1_7 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer0_ffn_gate_0 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer0_ffn_gate_1 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer0_ffn_gate_2 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer0_ffn_gate_3 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer0_ffn_gate_4 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer0_ffn_gate_5 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer0_ffn_gate_6 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer0_ffn_gate_7 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer0_ffn_up_0 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer0_ffn_up_1 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer0_ffn_up_2 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer0_ffn_up_3 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer0_ffn_up_4 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer0_ffn_up_5 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer0_ffn_up_6 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer0_ffn_up_7 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer0_ffn_act_7 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer0_ffn_down_0 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer0_ffn_down_1 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer0_ffn_down_2 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer0_ffn_down_3 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer0_ffn_down_4 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer0_ffn_down_5 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer0_ffn_down_6 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer0_ffn_down_7 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer0_res2_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	input -> layer0_mha_qkv_0
	layer0_mha_qkv_0 -> layer0_mha_attn_0
	layer0_mha_attn_0 -> layer0_mha_out_0
	layer0_mha_out_0 -> layer0_res1_0
	layer0_res1_0 -> layer0_ln1_0
	layer0_ln1_0 -> layer0_ffn_gate_0
	layer0_ln1_0 -> layer0_ffn_up_0
	layer0_ffn_gate_0 -> layer0_ffn_act_0
	layer0_ffn_up_0 -> layer0_ffn_act_0
	layer0_ffn_act_0 -> layer0_ffn_down_0
	layer0_ffn_down_0 -> layer0_res2_0
	input -> layer0_mha_qkv_1
	layer0_mha_qkv_1 -> layer0_mha_attn_1
	layer0_mha_attn_1 -> layer0_mha_out_1
	layer0_mha_out_1 -> layer0_res1_1
	layer0_res1_1 -> layer0_ln1_1
	layer0_ln1_1 -> layer0_ffn_gate_1
	layer0_ln1_1 -> layer0_ffn_up_1
	layer0_ffn_gate_1 -> layer0_ffn_act_1
	layer0_ffn_up_1 -> layer0_ffn_act_1
	layer0_ffn_act_1 -> layer0_ffn_down_1
	layer0_ffn_down_1 -> layer0_res2_1
	input -> layer0_mha_qkv_2
	layer0_mha_qkv_2 -> layer0_mha_attn_2
	layer0_mha_attn_2 -> layer0_mha_out_2
	layer0_mha_out_2 -> layer0_res1_2
	layer0_res1_2 -> layer0_ln1_2
	layer0_ln1_2 -> layer0_ffn_gate_2
	layer0_ln1_2 -> layer0_ffn_up_2
	layer0_ffn_gate_2 -> layer0_ffn_act_2
	layer0_ffn_up_2 -> layer0_ffn_act_2
	layer0_ffn_act_2 -> layer0_ffn_down_2
	layer0_ffn_down_2 -> layer0_res2_2
	input -> layer0_mha_qkv_3
	layer0_mha_qkv_3 -> layer0_mha_attn_3
	layer0_mha_attn_3 -> layer0_mha_out_3
	layer0_mha_out_3 -> layer0_res1_3
	layer0_res1_3 -> layer0_ln1_3
	layer0_ln1_3 -> layer0_ffn_gate_3
	layer0_ln1_3 -> layer0_ffn_up_3
	layer0_ffn_gate_3 -> layer0_ffn_act_3
	layer0_ffn_up_3 -> layer0_ffn_act_3
	layer0_ffn_act_3 -> layer0_ffn_down_3
	layer0_ffn_down_3 -> layer0_res2_3
	input -> layer0_mha_qkv_4
	layer0_mha_qkv_4 -> layer0_mha_attn_4
	layer0_mha_attn_4 -> layer0_mha_out_4
	layer0_mha_out_4 -> layer0_res1_4
	layer0_res1_4 -> layer0_ln1_4
	layer0_ln1_4 -> layer0_ffn_gate_4
	layer0_ln1_4 -> layer0_ffn_up_4
	layer0_ffn_gate_4 -> layer0_ffn_act_4
	layer0_ffn_up_4 -> layer0_ffn_act_4
	layer0_ffn_act_4 -> layer0_ffn_down_4
	layer0_ffn_down_4 -> layer0_res2_4
	input -> layer0_mha_qkv_5
	layer0_mha_qkv_5 -> layer0_mha_attn_5
	layer0_mha_attn_5 -> layer0_mha_out_5
	layer0_mha_out_5 -> layer0_res1_5
	layer0_res1_5 -> layer0_ln1_5
	layer0_ln1_5 -> layer0_ffn_gate_5
	layer0_ln1_5 -> layer0_ffn_up_5
	layer0_ffn_gate_5 -> layer0_ffn_act_5
	layer0_ffn_up_5 -> layer0_ffn_act_5
	layer0_ffn_act_5 -> layer0_ffn_down_5
	layer0_ffn_down_5 -> layer0_res2_5
	input -> layer0_mha_qkv_6
	layer0_mha_qkv_6 -> layer0_mha_attn_6
	layer0_mha_attn_6 -> layer0_mha_out_6
	layer0_mha_out_6 -> layer0_res1_6
	layer0_res1_6 -> layer0_ln1_6
	layer0_ln1_6 -> layer0_ffn_gate_6
	layer0_ln1_6 -> layer0_ffn_up_6
	layer0_ffn_gate_6 -> layer0_ffn_act_6
	layer0_ffn_up_6 -> layer0_ffn_act_6
	layer0_ffn_act_6 -> layer0_ffn_down_6
	layer0_ffn_down_6 -> layer0_res2_6
	input -> layer0_mha_qkv_7
	layer0_mha_qkv_7 -> layer0_mha_attn_7
	layer0_mha_attn_7 -> layer0_mha_out_7
	layer0_mha_out_7 -> layer0_res1_7
	layer0_res1_7 -> layer0_ln1_7
	layer0_ln1_7 -> layer0_ffn_gate_7
	layer0_ln1_7 -> layer0_ffn_up_7
	layer0_ffn_gate_7 -> layer0_ffn_act_7
	layer0_ffn_up_7 -> layer0_ffn_act_7
	layer0_ffn_act_7 -> layer0_ffn_down_7
	layer0_ffn_down_7 -> layer0_res2_7
	layer1_mha_qkv_0 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer1_mha_qkv_1 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer1_mha_qkv_2 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer1_mha_qkv_3 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer1_mha_qkv_4 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer1_mha_qkv_5 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer1_mha_qkv_6 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer1_mha_qkv_7 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer1_mha_attn_7 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer1_mha_out_0 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer1_mha_out_1 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer1_mha_out_2 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer1_mha_out_3 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer1_mha_out_4 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer1_mha_out_5 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer1_mha_out_6 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer1_mha_out_7 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer1_res1_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer1_ln1_7 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer1_ffn_gate_0 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer1_ffn_gate_1 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer1_ffn_gate_2 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer1_ffn_gate_3 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer1_ffn_gate_4 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer1_ffn_gate_5 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer1_ffn_gate_6 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer1_ffn_gate_7 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer1_ffn_up_0 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer1_ffn_up_1 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer1_ffn_up_2 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer1_ffn_up_3 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer1_ffn_up_4 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer1_ffn_up_5 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer1_ffn_up_6 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer1_ffn_up_7 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer1_ffn_act_7 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer1_ffn_down_0 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer1_ffn_down_1 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer1_ffn_down_2 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer1_ffn_down_3 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer1_ffn_down_4 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer1_ffn_down_5 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer1_ffn_down_6 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer1_ffn_down_7 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer1_res2_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer0_res2_0 -> layer1_mha_qkv_0
	layer1_mha_qkv_0 -> layer1_mha_attn_0
	layer1_mha_attn_0 -> layer1_mha_out_0
	layer1_mha_out_0 -> layer1_res1_0
	layer1_res1_0 -> layer1_ln1_0
	layer1_ln1_0 -> layer1_ffn_gate_0
	layer1_ln1_0 -> layer1_ffn_up_0
	layer1_ffn_gate_0 -> layer1_ffn_act_0
	layer1_ffn_up_0 -> layer1_ffn_act_0
	layer1_ffn_act_0 -> layer1_ffn_down_0
	layer1_ffn_down_0 -> layer1_res2_0
	layer0_res2_1 -> layer1_mha_qkv_1
	layer1_mha_qkv_1 -> layer1_mha_attn_1
	layer1_mha_attn_1 -> layer1_mha_out_1
	layer1_mha_out_1 -> layer1_res1_1
	layer1_res1_1 -> layer1_ln1_1
	layer1_ln1_1 -> layer1_ffn_gate_1
	layer1_ln1_1 -> layer1_ffn_up_1
	layer1_ffn_gate_1 -> layer1_ffn_act_1
	layer1_ffn_up_1 -> layer1_ffn_act_1
	layer1_ffn_act_1 -> layer1_ffn_down_1
	layer1_ffn_down_1 -> layer1_res2_1
	layer0_res2_2 -> layer1_mha_qkv_2
	layer1_mha_qkv_2 -> layer1_mha_attn_2
	layer1_mha_attn_2 -> layer1_mha_out_2
	layer1_mha_out_2 -> layer1_res1_2
	layer1_res1_2 -> layer1_ln1_2
	layer1_ln1_2 -> layer1_ffn_gate_2
	layer1_ln1_2 -> layer1_ffn_up_2
	layer1_ffn_gate_2 -> layer1_ffn_act_2
	layer1_ffn_up_2 -> layer1_ffn_act_2
	layer1_ffn_act_2 -> layer1_ffn_down_2
	layer1_ffn_down_2 -> layer1_res2_2
	layer0_res2_3 -> layer1_mha_qkv_3
	layer1_mha_qkv_3 -> layer1_mha_attn_3
	layer1_mha_attn_3 -> layer1_mha_out_3
	layer1_mha_out_3 -> layer1_res1_3
	layer1_res1_3 -> layer1_ln1_3
	layer1_ln1_3 -> layer1_ffn_gate_3
	layer1_ln1_3 -> layer1_ffn_up_3
	layer1_ffn_gate_3 -> layer1_ffn_act_3
	layer1_ffn_up_3 -> layer1_ffn_act_3
	layer1_ffn_act_3 -> layer1_ffn_down_3
	layer1_ffn_down_3 -> layer1_res2_3
	layer0_res2_4 -> layer1_mha_qkv_4
	layer1_mha_qkv_4 -> layer1_mha_attn_4
	layer1_mha_attn_4 -> layer1_mha_out_4
	layer1_mha_out_4 -> layer1_res1_4
	layer1_res1_4 -> layer1_ln1_4
	layer1_ln1_4 -> layer1_ffn_gate_4
	layer1_ln1_4 -> layer1_ffn_up_4
	layer1_ffn_gate_4 -> layer1_ffn_act_4
	layer1_ffn_up_4 -> layer1_ffn_act_4
	layer1_ffn_act_4 -> layer1_ffn_down_4
	layer1_ffn_down_4 -> layer1_res2_4
	layer0_res2_5 -> layer1_mha_qkv_5
	layer1_mha_qkv_5 -> layer1_mha_attn_5
	layer1_mha_attn_5 -> layer1_mha_out_5
	layer1_mha_out_5 -> layer1_res1_5
	layer1_res1_5 -> layer1_ln1_5
	layer1_ln1_5 -> layer1_ffn_gate_5
	layer1_ln1_5 -> layer1_ffn_up_5
	layer1_ffn_gate_5 -> layer1_ffn_act_5
	layer1_ffn_up_5 -> layer1_ffn_act_5
	layer1_ffn_act_5 -> layer1_ffn_down_5
	layer1_ffn_down_5 -> layer1_res2_5
	layer0_res2_6 -> layer1_mha_qkv_6
	layer1_mha_qkv_6 -> layer1_mha_attn_6
	layer1_mha_attn_6 -> layer1_mha_out_6
	layer1_mha_out_6 -> layer1_res1_6
	layer1_res1_6 -> layer1_ln1_6
	layer1_ln1_6 -> layer1_ffn_gate_6
	layer1_ln1_6 -> layer1_ffn_up_6
	layer1_ffn_gate_6 -> layer1_ffn_act_6
	layer1_ffn_up_6 -> layer1_ffn_act_6
	layer1_ffn_act_6 -> layer1_ffn_down_6
	layer1_ffn_down_6 -> layer1_res2_6
	layer0_res2_7 -> layer1_mha_qkv_7
	layer1_mha_qkv_7 -> layer1_mha_attn_7
	layer1_mha_attn_7 -> layer1_mha_out_7
	layer1_mha_out_7 -> layer1_res1_7
	layer1_res1_7 -> layer1_ln1_7
	layer1_ln1_7 -> layer1_ffn_gate_7
	layer1_ln1_7 -> layer1_ffn_up_7
	layer1_ffn_gate_7 -> layer1_ffn_act_7
	layer1_ffn_up_7 -> layer1_ffn_act_7
	layer1_ffn_act_7 -> layer1_ffn_down_7
	layer1_ffn_down_7 -> layer1_res2_7
	layer2_mha_qkv_0 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer2_mha_qkv_1 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer2_mha_qkv_2 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer2_mha_qkv_3 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer2_mha_qkv_4 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer2_mha_qkv_5 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer2_mha_qkv_6 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer2_mha_qkv_7 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer2_mha_attn_7 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer2_mha_out_0 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer2_mha_out_1 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer2_mha_out_2 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer2_mha_out_3 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer2_mha_out_4 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer2_mha_out_5 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer2_mha_out_6 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer2_mha_out_7 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer2_res1_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer2_ln1_7 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer2_ffn_gate_0 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer2_ffn_gate_1 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer2_ffn_gate_2 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer2_ffn_gate_3 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer2_ffn_gate_4 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer2_ffn_gate_5 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer2_ffn_gate_6 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer2_ffn_gate_7 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer2_ffn_up_0 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer2_ffn_up_1 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer2_ffn_up_2 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer2_ffn_up_3 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer2_ffn_up_4 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer2_ffn_up_5 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer2_ffn_up_6 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer2_ffn_up_7 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer2_ffn_act_7 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer2_ffn_down_0 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer2_ffn_down_1 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer2_ffn_down_2 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer2_ffn_down_3 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer2_ffn_down_4 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer2_ffn_down_5 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer2_ffn_down_6 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer2_ffn_down_7 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer2_res2_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer1_res2_0 -> layer2_mha_qkv_0
	layer2_mha_qkv_0 -> layer2_mha_attn_0
	layer2_mha_attn_0 -> layer2_mha_out_0
	layer2_mha_out_0 -> layer2_res1_0
	layer2_res1_0 -> layer2_ln1_0
	layer2_ln1_0 -> layer2_ffn_gate_0
	layer2_ln1_0 -> layer2_ffn_up_0
	layer2_ffn_gate_0 -> layer2_ffn_act_0
	layer2_ffn_up_0 -> layer2_ffn_act_0
	layer2_ffn_act_0 -> layer2_ffn_down_0
	layer2_ffn_down_0 -> layer2_res2_0
	layer1_res2_1 -> layer2_mha_qkv_1
	layer2_mha_qkv_1 -> layer2_mha_attn_1
	layer2_mha_attn_1 -> layer2_mha_out_1
	layer2_mha_out_1 -> layer2_res1_1
	layer2_res1_1 -> layer2_ln1_1
	layer2_ln1_1 -> layer2_ffn_gate_1
	layer2_ln1_1 -> layer2_ffn_up_1
	layer2_ffn_gate_1 -> layer2_ffn_act_1
	layer2_ffn_up_1 -> layer2_ffn_act_1
	layer2_ffn_act_1 -> layer2_ffn_down_1
	layer2_ffn_down_1 -> layer2_res2_1
	layer1_res2_2 -> layer2_mha_qkv_2
	layer2_mha_qkv_2 -> layer2_mha_attn_2
	layer2_mha_attn_2 -> layer2_mha_out_2
	layer2_mha_out_2 -> layer2_res1_2
	layer2_res1_2 -> layer2_ln1_2
	layer2_ln1_2 -> layer2_ffn_gate_2
	layer2_ln1_2 -> layer2_ffn_up_2
	layer2_ffn_gate_2 -> layer2_ffn_act_2
	layer2_ffn_up_2 -> layer2_ffn_act_2
	layer2_ffn_act_2 -> layer2_ffn_down_2
	layer2_ffn_down_2 -> layer2_res2_2
	layer1_res2_3 -> layer2_mha_qkv_3
	layer2_mha_qkv_3 -> layer2_mha_attn_3
	layer2_mha_attn_3 -> layer2_mha_out_3
	layer2_mha_out_3 -> layer2_res1_3
	layer2_res1_3 -> layer2_ln1_3
	layer2_ln1_3 -> layer2_ffn_gate_3
	layer2_ln1_3 -> layer2_ffn_up_3
	layer2_ffn_gate_3 -> layer2_ffn_act_3
	layer2_ffn_up_3 -> layer2_ffn_act_3
	layer2_ffn_act_3 -> layer2_ffn_down_3
	layer2_ffn_down_3 -> layer2_res2_3
	layer1_res2_4 -> layer2_mha_qkv_4
	layer2_mha_qkv_4 -> layer2_mha_attn_4
	layer2_mha_attn_4 -> layer2_mha_out_4
	layer2_mha_out_4 -> layer2_res1_4
	layer2_res1_4 -> layer2_ln1_4
	layer2_ln1_4 -> layer2_ffn_gate_4
	layer2_ln1_4 -> layer2_ffn_up_4
	layer2_ffn_gate_4 -> layer2_ffn_act_4
	layer2_ffn_up_4 -> layer2_ffn_act_4
	layer2_ffn_act_4 -> layer2_ffn_down_4
	layer2_ffn_down_4 -> layer2_res2_4
	layer1_res2_5 -> layer2_mha_qkv_5
	layer2_mha_qkv_5 -> layer2_mha_attn_5
	layer2_mha_attn_5 -> layer2_mha_out_5
	layer2_mha_out_5 -> layer2_res1_5
	layer2_res1_5 -> layer2_ln1_5
	layer2_ln1_5 -> layer2_ffn_gate_5
	layer2_ln1_5 -> layer2_ffn_up_5
	layer2_ffn_gate_5 -> layer2_ffn_act_5
	layer2_ffn_up_5 -> layer2_ffn_act_5
	layer2_ffn_act_5 -> layer2_ffn_down_5
	layer2_ffn_down_5 -> layer2_res2_5
	layer1_res2_6 -> layer2_mha_qkv_6
	layer2_mha_qkv_6 -> layer2_mha_attn_6
	layer2_mha_attn_6 -> layer2_mha_out_6
	layer2_mha_out_6 -> layer2_res1_6
	layer2_res1_6 -> layer2_ln1_6
	layer2_ln1_6 -> layer2_ffn_gate_6
	layer2_ln1_6 -> layer2_ffn_up_6
	layer2_ffn_gate_6 -> layer2_ffn_act_6
	layer2_ffn_up_6 -> layer2_ffn_act_6
	layer2_ffn_act_6 -> layer2_ffn_down_6
	layer2_ffn_down_6 -> layer2_res2_6
	layer1_res2_7 -> layer2_mha_qkv_7
	layer2_mha_qkv_7 -> layer2_mha_attn_7
	layer2_mha_attn_7 -> layer2_mha_out_7
	layer2_mha_out_7 -> layer2_res1_7
	layer2_res1_7 -> layer2_ln1_7
	layer2_ln1_7 -> layer2_ffn_gate_7
	layer2_ln1_7 -> layer2_ffn_up_7
	layer2_ffn_gate_7 -> layer2_ffn_act_7
	layer2_ffn_up_7 -> layer2_ffn_act_7
	layer2_ffn_act_7 -> layer2_ffn_down_7
	layer2_ffn_down_7 -> layer2_res2_7
	layer3_mha_qkv_0 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer3_mha_qkv_1 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer3_mha_qkv_2 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer3_mha_qkv_3 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer3_mha_qkv_4 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer3_mha_qkv_5 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer3_mha_qkv_6 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer3_mha_qkv_7 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer3_mha_attn_7 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer3_mha_out_0 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer3_mha_out_1 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer3_mha_out_2 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer3_mha_out_3 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer3_mha_out_4 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer3_mha_out_5 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer3_mha_out_6 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer3_mha_out_7 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer3_res1_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer3_ln1_7 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer3_ffn_gate_0 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer3_ffn_gate_1 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer3_ffn_gate_2 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer3_ffn_gate_3 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer3_ffn_gate_4 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer3_ffn_gate_5 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer3_ffn_gate_6 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer3_ffn_gate_7 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer3_ffn_up_0 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer3_ffn_up_1 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer3_ffn_up_2 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer3_ffn_up_3 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer3_ffn_up_4 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer3_ffn_up_5 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer3_ffn_up_6 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer3_ffn_up_7 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer3_ffn_act_7 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer3_ffn_down_0 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer3_ffn_down_1 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer3_ffn_down_2 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer3_ffn_down_3 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer3_ffn_down_4 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer3_ffn_down_5 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer3_ffn_down_6 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer3_ffn_down_7 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer3_res2_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer2_res2_0 -> layer3_mha_qkv_0
	layer3_mha_qkv_0 -> layer3_mha_attn_0
	layer3_mha_attn_0 -> layer3_mha_out_0
	layer3_mha_out_0 -> layer3_res1_0
	layer3_res1_0 -> layer3_ln1_0
	layer3_ln1_0 -> layer3_ffn_gate_0
	layer3_ln1_0 -> layer3_ffn_up_0
	layer3_ffn_gate_0 -> layer3_ffn_act_0
	layer3_ffn_up_0 -> layer3_ffn_act_0
	layer3_ffn_act_0 -> layer3_ffn_down_0
	layer3_ffn_down_0 -> layer3_res2_0
	layer2_res2_1 -> layer3_mha_qkv_1
	layer3_mha_qkv_1 -> layer3_mha_attn_1
	layer3_mha_attn_1 -> layer3_mha_out_1
	layer3_mha_out_1 -> layer3_res1_1
	layer3_res1_1 -> layer3_ln1_1
	layer3_ln1_1 -> layer3_ffn_gate_1
	layer3_ln1_1 -> layer3_ffn_up_1
	layer3_ffn_gate_1 -> layer3_ffn_act_1
	layer3_ffn_up_1 -> layer3_ffn_act_1
	layer3_ffn_act_1 -> layer3_ffn_down_1
	layer3_ffn_down_1 -> layer3_res2_1
	layer2_res2_2 -> layer3_mha_qkv_2
	layer3_mha_qkv_2 -> layer3_mha_attn_2
	layer3_mha_attn_2 -> layer3_mha_out_2
	layer3_mha_out_2 -> layer3_res1_2
	layer3_res1_2 -> layer3_ln1_2
	layer3_ln1_2 -> layer3_ffn_gate_2
	layer3_ln1_2 -> layer3_ffn_up_2
	layer3_ffn_gate_2 -> layer3_ffn_act_2
	layer3_ffn_up_2 -> layer3_ffn_act_2
	layer3_ffn_act_2 -> layer3_ffn_down_2
	layer3_ffn_down_2 -> layer3_res2_2
	layer2_res2_3 -> layer3_mha_qkv_3
	layer3_mha_qkv_3 -> layer3_mha_attn_3
	layer3_mha_attn_3 -> layer3_mha_out_3
	layer3_mha_out_3 -> layer3_res1_3
	layer3_res1_3 -> layer3_ln1_3
	layer3_ln1_3 -> layer3_ffn_gate_3
	layer3_ln1_3 -> layer3_ffn_up_3
	layer3_ffn_gate_3 -> layer3_ffn_act_3
	layer3_ffn_up_3 -> layer3_ffn_act_3
	layer3_ffn_act_3 -> layer3_ffn_down_3
	layer3_ffn_down_3 -> layer3_res2_3
	layer2_res2_4 -> layer3_mha_qkv_4
	layer3_mha_qkv_4 -> layer3_mha_attn_4
	layer3_mha_attn_4 -> layer3_mha_out_4
	layer3_mha_out_4 -> layer3_res1_4
	layer3_res1_4 -> layer3_ln1_4
	layer3_ln1_4 -> layer3_ffn_gate_4
	layer3_ln1_4 -> layer3_ffn_up_4
	layer3_ffn_gate_4 -> layer3_ffn_act_4
	layer3_ffn_up_4 -> layer3_ffn_act_4
	layer3_ffn_act_4 -> layer3_ffn_down_4
	layer3_ffn_down_4 -> layer3_res2_4
	layer2_res2_5 -> layer3_mha_qkv_5
	layer3_mha_qkv_5 -> layer3_mha_attn_5
	layer3_mha_attn_5 -> layer3_mha_out_5
	layer3_mha_out_5 -> layer3_res1_5
	layer3_res1_5 -> layer3_ln1_5
	layer3_ln1_5 -> layer3_ffn_gate_5
	layer3_ln1_5 -> layer3_ffn_up_5
	layer3_ffn_gate_5 -> layer3_ffn_act_5
	layer3_ffn_up_5 -> layer3_ffn_act_5
	layer3_ffn_act_5 -> layer3_ffn_down_5
	layer3_ffn_down_5 -> layer3_res2_5
	layer2_res2_6 -> layer3_mha_qkv_6
	layer3_mha_qkv_6 -> layer3_mha_attn_6
	layer3_mha_attn_6 -> layer3_mha_out_6
	layer3_mha_out_6 -> layer3_res1_6
	layer3_res1_6 -> layer3_ln1_6
	layer3_ln1_6 -> layer3_ffn_gate_6
	layer3_ln1_6 -> layer3_ffn_up_6
	layer3_ffn_gate_6 -> layer3_ffn_act_6
	layer3_ffn_up_6 -> layer3_ffn_act_6
	layer3_ffn_act_6 -> layer3_ffn_down_6
	layer3_ffn_down_6 -> layer3_res2_6
	layer2_res2_7 -> layer3_mha_qkv_7
	layer3_mha_qkv_7 -> layer3_mha_attn_7
	layer3_mha_attn_7 -> layer3_mha_out_7
	layer3_mha_out_7 -> layer3_res1_7
	layer3_res1_7 -> layer3_ln1_7
	layer3_ln1_7 -> layer3_ffn_gate_7
	layer3_ln1_7 -> layer3_ffn_up_7
	layer3_ffn_gate_7 -> layer3_ffn_act_7
	layer3_ffn_up_7 -> layer3_ffn_act_7
	layer3_ffn_act_7 -> layer3_ffn_down_7
	layer3_ffn_down_7 -> layer3_res2_7
	layer4_mha_qkv_0 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer4_mha_qkv_1 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer4_mha_qkv_2 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer4_mha_qkv_3 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer4_mha_qkv_4 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer4_mha_qkv_5 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer4_mha_qkv_6 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer4_mha_qkv_7 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer4_mha_attn_7 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer4_mha_out_0 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer4_mha_out_1 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer4_mha_out_2 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer4_mha_out_3 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer4_mha_out_4 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer4_mha_out_5 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer4_mha_out_6 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer4_mha_out_7 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer4_res1_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer4_ln1_7 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer4_ffn_gate_0 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer4_ffn_gate_1 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer4_ffn_gate_2 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer4_ffn_gate_3 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer4_ffn_gate_4 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer4_ffn_gate_5 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer4_ffn_gate_6 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer4_ffn_gate_7 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer4_ffn_up_0 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer4_ffn_up_1 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer4_ffn_up_2 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer4_ffn_up_3 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer4_ffn_up_4 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer4_ffn_up_5 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer4_ffn_up_6 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer4_ffn_up_7 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer4_ffn_act_7 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer4_ffn_down_0 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer4_ffn_down_1 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer4_ffn_down_2 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer4_ffn_down_3 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer4_ffn_down_4 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer4_ffn_down_5 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer4_ffn_down_6 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer4_ffn_down_7 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer4_res2_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer3_res2_0 -> layer4_mha_qkv_0
	layer4_mha_qkv_0 -> layer4_mha_attn_0
	layer4_mha_attn_0 -> layer4_mha_out_0
	layer4_mha_out_0 -> layer4_res1_0
	layer4_res1_0 -> layer4_ln1_0
	layer4_ln1_0 -> layer4_ffn_gate_0
	layer4_ln1_0 -> layer4_ffn_up_0
	layer4_ffn_gate_0 -> layer4_ffn_act_0
	layer4_ffn_up_0 -> layer4_ffn_act_0
	layer4_ffn_act_0 -> layer4_ffn_down_0
	layer4_ffn_down_0 -> layer4_res2_0
	layer3_res2_1 -> layer4_mha_qkv_1
	layer4_mha_qkv_1 -> layer4_mha_attn_1
	layer4_mha_attn_1 -> layer4_mha_out_1
	layer4_mha_out_1 -> layer4_res1_1
	layer4_res1_1 -> layer4_ln1_1
	layer4_ln1_1 -> layer4_ffn_gate_1
	layer4_ln1_1 -> layer4_ffn_up_1
	layer4_ffn_gate_1 -> layer4_ffn_act_1
	layer4_ffn_up_1 -> layer4_ffn_act_1
	layer4_ffn_act_1 -> layer4_ffn_down_1
	layer4_ffn_down_1 -> layer4_res2_1
	layer3_res2_2 -> layer4_mha_qkv_2
	layer4_mha_qkv_2 -> layer4_mha_attn_2
	layer4_mha_attn_2 -> layer4_mha_out_2
	layer4_mha_out_2 -> layer4_res1_2
	layer4_res1_2 -> layer4_ln1_2
	layer4_ln1_2 -> layer4_ffn_gate_2
	layer4_ln1_2 -> layer4_ffn_up_2
	layer4_ffn_gate_2 -> layer4_ffn_act_2
	layer4_ffn_up_2 -> layer4_ffn_act_2
	layer4_ffn_act_2 -> layer4_ffn_down_2
	layer4_ffn_down_2 -> layer4_res2_2
	layer3_res2_3 -> layer4_mha_qkv_3
	layer4_mha_qkv_3 -> layer4_mha_attn_3
	layer4_mha_attn_3 -> layer4_mha_out_3
	layer4_mha_out_3 -> layer4_res1_3
	layer4_res1_3 -> layer4_ln1_3
	layer4_ln1_3 -> layer4_ffn_gate_3
	layer4_ln1_3 -> layer4_ffn_up_3
	layer4_ffn_gate_3 -> layer4_ffn_act_3
	layer4_ffn_up_3 -> layer4_ffn_act_3
	layer4_ffn_act_3 -> layer4_ffn_down_3
	layer4_ffn_down_3 -> layer4_res2_3
	layer3_res2_4 -> layer4_mha_qkv_4
	layer4_mha_qkv_4 -> layer4_mha_attn_4
	layer4_mha_attn_4 -> layer4_mha_out_4
	layer4_mha_out_4 -> layer4_res1_4
	layer4_res1_4 -> layer4_ln1_4
	layer4_ln1_4 -> layer4_ffn_gate_4
	layer4_ln1_4 -> layer4_ffn_up_4
	layer4_ffn_gate_4 -> layer4_ffn_act_4
	layer4_ffn_up_4 -> layer4_ffn_act_4
	layer4_ffn_act_4 -> layer4_ffn_down_4
	layer4_ffn_down_4 -> layer4_res2_4
	layer3_res2_5 -> layer4_mha_qkv_5
	layer4_mha_qkv_5 -> layer4_mha_attn_5
	layer4_mha_attn_5 -> layer4_mha_out_5
	layer4_mha_out_5 -> layer4_res1_5
	layer4_res1_5 -> layer4_ln1_5
	layer4_ln1_5 -> layer4_ffn_gate_5
	layer4_ln1_5 -> layer4_ffn_up_5
	layer4_ffn_gate_5 -> layer4_ffn_act_5
	layer4_ffn_up_5 -> layer4_ffn_act_5
	layer4_ffn_act_5 -> layer4_ffn_down_5
	layer4_ffn_down_5 -> layer4_res2_5
	layer3_res2_6 -> layer4_mha_qkv_6
	layer4_mha_qkv_6 -> layer4_mha_attn_6
	layer4_mha_attn_6 -> layer4_mha_out_6
	layer4_mha_out_6 -> layer4_res1_6
	layer4_res1_6 -> layer4_ln1_6
	layer4_ln1_6 -> layer4_ffn_gate_6
	layer4_ln1_6 -> layer4_ffn_up_6
	layer4_ffn_gate_6 -> layer4_ffn_act_6
	layer4_ffn_up_6 -> layer4_ffn_act_6
	layer4_ffn_act_6 -> layer4_ffn_down_6
	layer4_ffn_down_6 -> layer4_res2_6
	layer3_res2_7 -> layer4_mha_qkv_7
	layer4_mha_qkv_7 -> layer4_mha_attn_7
	layer4_mha_attn_7 -> layer4_mha_out_7
	layer4_mha_out_7 -> layer4_res1_7
	layer4_res1_7 -> layer4_ln1_7
	layer4_ln1_7 -> layer4_ffn_gate_7
	layer4_ln1_7 -> layer4_ffn_up_7
	layer4_ffn_gate_7 -> layer4_ffn_act_7
	layer4_ffn_up_7 -> layer4_ffn_act_7
	layer4_ffn_act_7 -> layer4_ffn_down_7
	layer4_ffn_down_7 -> layer4_res2_7
	layer5_mha_qkv_0 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer5_mha_qkv_1 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer5_mha_qkv_2 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer5_mha_qkv_3 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer5_mha_qkv_4 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer5_mha_qkv_5 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer5_mha_qkv_6 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer5_mha_qkv_7 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer5_mha_attn_7 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer5_mha_out_0 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer5_mha_out_1 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer5_mha_out_2 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer5_mha_out_3 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer5_mha_out_4 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer5_mha_out_5 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer5_mha_out_6 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer5_mha_out_7 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer5_res1_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer5_ln1_7 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer5_ffn_gate_0 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer5_ffn_gate_1 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer5_ffn_gate_2 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer5_ffn_gate_3 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer5_ffn_gate_4 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer5_ffn_gate_5 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer5_ffn_gate_6 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer5_ffn_gate_7 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer5_ffn_up_0 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer5_ffn_up_1 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer5_ffn_up_2 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer5_ffn_up_3 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer5_ffn_up_4 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer5_ffn_up_5 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer5_ffn_up_6 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer5_ffn_up_7 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer5_ffn_act_7 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer5_ffn_down_0 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer5_ffn_down_1 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer5_ffn_down_2 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer5_ffn_down_3 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer5_ffn_down_4 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer5_ffn_down_5 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer5_ffn_down_6 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer5_ffn_down_7 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer5_res2_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer4_res2_0 -> layer5_mha_qkv_0
	layer5_mha_qkv_0 -> layer5_mha_attn_0
	layer5_mha_attn_0 -> layer5_mha_out_0
	layer5_mha_out_0 -> layer5_res1_0
	layer5_res1_0 -> layer5_ln1_0
	layer5_ln1_0 -> layer5_ffn_gate_0
	layer5_ln1_0 -> layer5_ffn_up_0
	layer5_ffn_gate_0 -> layer5_ffn_act_0
	layer5_ffn_up_0 -> layer5_ffn_act_0
	layer5_ffn_act_0 -> layer5_ffn_down_0
	layer5_ffn_down_0 -> layer5_res2_0
	layer4_res2_1 -> layer5_mha_qkv_1
	layer5_mha_qkv_1 -> layer5_mha_attn_1
	layer5_mha_attn_1 -> layer5_mha_out_1
	layer5_mha_out_1 -> layer5_res1_1
	layer5_res1_1 -> layer5_ln1_1
	layer5_ln1_1 -> layer5_ffn_gate_1
	layer5_ln1_1 -> layer5_ffn_up_1
	layer5_ffn_gate_1 -> layer5_ffn_act_1
	layer5_ffn_up_1 -> layer5_ffn_act_1
	layer5_ffn_act_1 -> layer5_ffn_down_1
	layer5_ffn_down_1 -> layer5_res2_1
	layer4_res2_2 -> layer5_mha_qkv_2
	layer5_mha_qkv_2 -> layer5_mha_attn_2
	layer5_mha_attn_2 -> layer5_mha_out_2
	layer5_mha_out_2 -> layer5_res1_2
	layer5_res1_2 -> layer5_ln1_2
	layer5_ln1_2 -> layer5_ffn_gate_2
	layer5_ln1_2 -> layer5_ffn_up_2
	layer5_ffn_gate_2 -> layer5_ffn_act_2
	layer5_ffn_up_2 -> layer5_ffn_act_2
	layer5_ffn_act_2 -> layer5_ffn_down_2
	layer5_ffn_down_2 -> layer5_res2_2
	layer4_res2_3 -> layer5_mha_qkv_3
	layer5_mha_qkv_3 -> layer5_mha_attn_3
	layer5_mha_attn_3 -> layer5_mha_out_3
	layer5_mha_out_3 -> layer5_res1_3
	layer5_res1_3 -> layer5_ln1_3
	layer5_ln1_3 -> layer5_ffn_gate_3
	layer5_ln1_3 -> layer5_ffn_up_3
	layer5_ffn_gate_3 -> layer5_ffn_act_3
	layer5_ffn_up_3 -> layer5_ffn_act_3
	layer5_ffn_act_3 -> layer5_ffn_down_3
	layer5_ffn_down_3 -> layer5_res2_3
	layer4_res2_4 -> layer5_mha_qkv_4
	layer5_mha_qkv_4 -> layer5_mha_attn_4
	layer5_mha_attn_4 -> layer5_mha_out_4
	layer5_mha_out_4 -> layer5_res1_4
	layer5_res1_4 -> layer5_ln1_4
	layer5_ln1_4 -> layer5_ffn_gate_4
	layer5_ln1_4 -> layer5_ffn_up_4
	layer5_ffn_gate_4 -> layer5_ffn_act_4
	layer5_ffn_up_4 -> layer5_ffn_act_4
	layer5_ffn_act_4 -> layer5_ffn_down_4
	layer5_ffn_down_4 -> layer5_res2_4
	layer4_res2_5 -> layer5_mha_qkv_5
	layer5_mha_qkv_5 -> layer5_mha_attn_5
	layer5_mha_attn_5 -> layer5_mha_out_5
	layer5_mha_out_5 -> layer5_res1_5
	layer5_res1_5 -> layer5_ln1_5
	layer5_ln1_5 -> layer5_ffn_gate_5
	layer5_ln1_5 -> layer5_ffn_up_5
	layer5_ffn_gate_5 -> layer5_ffn_act_5
	layer5_ffn_up_5 -> layer5_ffn_act_5
	layer5_ffn_act_5 -> layer5_ffn_down_5
	layer5_ffn_down_5 -> layer5_res2_5
	layer4_res2_6 -> layer5_mha_qkv_6
	layer5_mha_qkv_6 -> layer5_mha_attn_6
	layer5_mha_attn_6 -> layer5_mha_out_6
	layer5_mha_out_6 -> layer5_res1_6
	layer5_res1_6 -> layer5_ln1_6
	layer5_ln1_6 -> layer5_ffn_gate_6
	layer5_ln1_6 -> layer5_ffn_up_6
	layer5_ffn_gate_6 -> layer5_ffn_act_6
	layer5_ffn_up_6 -> layer5_ffn_act_6
	layer5_ffn_act_6 -> layer5_ffn_down_6
	layer5_ffn_down_6 -> layer5_res2_6
	layer4_res2_7 -> layer5_mha_qkv_7
	layer5_mha_qkv_7 -> layer5_mha_attn_7
	layer5_mha_attn_7 -> layer5_mha_out_7
	layer5_mha_out_7 -> layer5_res1_7
	layer5_res1_7 -> layer5_ln1_7
	layer5_ln1_7 -> layer5_ffn_gate_7
	layer5_ln1_7 -> layer5_ffn_up_7
	layer5_ffn_gate_7 -> layer5_ffn_act_7
	layer5_ffn_up_7 -> layer5_ffn_act_7
	layer5_ffn_act_7 -> layer5_ffn_down_7
	layer5_ffn_down_7 -> layer5_res2_7
	layer6_mha_qkv_0 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer6_mha_qkv_1 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer6_mha_qkv_2 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer6_mha_qkv_3 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer6_mha_qkv_4 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer6_mha_qkv_5 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer6_mha_qkv_6 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer6_mha_qkv_7 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer6_mha_attn_7 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer6_mha_out_0 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer6_mha_out_1 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer6_mha_out_2 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer6_mha_out_3 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer6_mha_out_4 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer6_mha_out_5 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer6_mha_out_6 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer6_mha_out_7 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer6_res1_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer6_ln1_7 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer6_ffn_gate_0 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer6_ffn_gate_1 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer6_ffn_gate_2 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer6_ffn_gate_3 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer6_ffn_gate_4 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer6_ffn_gate_5 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer6_ffn_gate_6 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer6_ffn_gate_7 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer6_ffn_up_0 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer6_ffn_up_1 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer6_ffn_up_2 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer6_ffn_up_3 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer6_ffn_up_4 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer6_ffn_up_5 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer6_ffn_up_6 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer6_ffn_up_7 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer6_ffn_act_7 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer6_ffn_down_0 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer6_ffn_down_1 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer6_ffn_down_2 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer6_ffn_down_3 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer6_ffn_down_4 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer6_ffn_down_5 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer6_ffn_down_6 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer6_ffn_down_7 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer6_res2_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer5_res2_0 -> layer6_mha_qkv_0
	layer6_mha_qkv_0 -> layer6_mha_attn_0
	layer6_mha_attn_0 -> layer6_mha_out_0
	layer6_mha_out_0 -> layer6_res1_0
	layer6_res1_0 -> layer6_ln1_0
	layer6_ln1_0 -> layer6_ffn_gate_0
	layer6_ln1_0 -> layer6_ffn_up_0
	layer6_ffn_gate_0 -> layer6_ffn_act_0
	layer6_ffn_up_0 -> layer6_ffn_act_0
	layer6_ffn_act_0 -> layer6_ffn_down_0
	layer6_ffn_down_0 -> layer6_res2_0
	layer5_res2_1 -> layer6_mha_qkv_1
	layer6_mha_qkv_1 -> layer6_mha_attn_1
	layer6_mha_attn_1 -> layer6_mha_out_1
	layer6_mha_out_1 -> layer6_res1_1
	layer6_res1_1 -> layer6_ln1_1
	layer6_ln1_1 -> layer6_ffn_gate_1
	layer6_ln1_1 -> layer6_ffn_up_1
	layer6_ffn_gate_1 -> layer6_ffn_act_1
	layer6_ffn_up_1 -> layer6_ffn_act_1
	layer6_ffn_act_1 -> layer6_ffn_down_1
	layer6_ffn_down_1 -> layer6_res2_1
	layer5_res2_2 -> layer6_mha_qkv_2
	layer6_mha_qkv_2 -> layer6_mha_attn_2
	layer6_mha_attn_2 -> layer6_mha_out_2
	layer6_mha_out_2 -> layer6_res1_2
	layer6_res1_2 -> layer6_ln1_2
	layer6_ln1_2 -> layer6_ffn_gate_2
	layer6_ln1_2 -> layer6_ffn_up_2
	layer6_ffn_gate_2 -> layer6_ffn_act_2
	layer6_ffn_up_2 -> layer6_ffn_act_2
	layer6_ffn_act_2 -> layer6_ffn_down_2
	layer6_ffn_down_2 -> layer6_res2_2
	layer5_res2_3 -> layer6_mha_qkv_3
	layer6_mha_qkv_3 -> layer6_mha_attn_3
	layer6_mha_attn_3 -> layer6_mha_out_3
	layer6_mha_out_3 -> layer6_res1_3
	layer6_res1_3 -> layer6_ln1_3
	layer6_ln1_3 -> layer6_ffn_gate_3
	layer6_ln1_3 -> layer6_ffn_up_3
	layer6_ffn_gate_3 -> layer6_ffn_act_3
	layer6_ffn_up_3 -> layer6_ffn_act_3
	layer6_ffn_act_3 -> layer6_ffn_down_3
	layer6_ffn_down_3 -> layer6_res2_3
	layer5_res2_4 -> layer6_mha_qkv_4
	layer6_mha_qkv_4 -> layer6_mha_attn_4
	layer6_mha_attn_4 -> layer6_mha_out_4
	layer6_mha_out_4 -> layer6_res1_4
	layer6_res1_4 -> layer6_ln1_4
	layer6_ln1_4 -> layer6_ffn_gate_4
	layer6_ln1_4 -> layer6_ffn_up_4
	layer6_ffn_gate_4 -> layer6_ffn_act_4
	layer6_ffn_up_4 -> layer6_ffn_act_4
	layer6_ffn_act_4 -> layer6_ffn_down_4
	layer6_ffn_down_4 -> layer6_res2_4
	layer5_res2_5 -> layer6_mha_qkv_5
	layer6_mha_qkv_5 -> layer6_mha_attn_5
	layer6_mha_attn_5 -> layer6_mha_out_5
	layer6_mha_out_5 -> layer6_res1_5
	layer6_res1_5 -> layer6_ln1_5
	layer6_ln1_5 -> layer6_ffn_gate_5
	layer6_ln1_5 -> layer6_ffn_up_5
	layer6_ffn_gate_5 -> layer6_ffn_act_5
	layer6_ffn_up_5 -> layer6_ffn_act_5
	layer6_ffn_act_5 -> layer6_ffn_down_5
	layer6_ffn_down_5 -> layer6_res2_5
	layer5_res2_6 -> layer6_mha_qkv_6
	layer6_mha_qkv_6 -> layer6_mha_attn_6
	layer6_mha_attn_6 -> layer6_mha_out_6
	layer6_mha_out_6 -> layer6_res1_6
	layer6_res1_6 -> layer6_ln1_6
	layer6_ln1_6 -> layer6_ffn_gate_6
	layer6_ln1_6 -> layer6_ffn_up_6
	layer6_ffn_gate_6 -> layer6_ffn_act_6
	layer6_ffn_up_6 -> layer6_ffn_act_6
	layer6_ffn_act_6 -> layer6_ffn_down_6
	layer6_ffn_down_6 -> layer6_res2_6
	layer5_res2_7 -> layer6_mha_qkv_7
	layer6_mha_qkv_7 -> layer6_mha_attn_7
	layer6_mha_attn_7 -> layer6_mha_out_7
	layer6_mha_out_7 -> layer6_res1_7
	layer6_res1_7 -> layer6_ln1_7
	layer6_ln1_7 -> layer6_ffn_gate_7
	layer6_ln1_7 -> layer6_ffn_up_7
	layer6_ffn_gate_7 -> layer6_ffn_act_7
	layer6_ffn_up_7 -> layer6_ffn_act_7
	layer6_ffn_act_7 -> layer6_ffn_down_7
	layer6_ffn_down_7 -> layer6_res2_7
	layer7_mha_qkv_0 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer7_mha_qkv_1 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer7_mha_qkv_2 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer7_mha_qkv_3 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer7_mha_qkv_4 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer7_mha_qkv_5 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer7_mha_qkv_6 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer7_mha_qkv_7 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer7_mha_attn_7 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer7_mha_out_0 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer7_mha_out_1 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer7_mha_out_2 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer7_mha_out_3 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer7_mha_out_4 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer7_mha_out_5 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer7_mha_out_6 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer7_mha_out_7 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer7_res1_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer7_ln1_7 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer7_ffn_gate_0 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer7_ffn_gate_1 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer7_ffn_gate_2 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer7_ffn_gate_3 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer7_ffn_gate_4 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer7_ffn_gate_5 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer7_ffn_gate_6 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer7_ffn_gate_7 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer7_ffn_up_0 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer7_ffn_up_1 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer7_ffn_up_2 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer7_ffn_up_3 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer7_ffn_up_4 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer7_ffn_up_5 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer7_ffn_up_6 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer7_ffn_up_7 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer7_ffn_act_7 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer7_ffn_down_0 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 0" fillcolor=lightblue style=filled]
	layer7_ffn_down_1 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 1" fillcolor=lightblue style=filled]
	layer7_ffn_down_2 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 2" fillcolor=lightblue style=filled]
	layer7_ffn_down_3 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 3" fillcolor=lightblue style=filled]
	layer7_ffn_down_4 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 4" fillcolor=lightblue style=filled]
	layer7_ffn_down_5 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 5" fillcolor=lightblue style=filled]
	layer7_ffn_down_6 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 6" fillcolor=lightblue style=filled]
	layer7_ffn_down_7 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 7" fillcolor=lightblue style=filled]
	layer7_res2_7 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer6_res2_0 -> layer7_mha_qkv_0
	layer7_mha_qkv_0 -> layer7_mha_attn_0
	layer7_mha_attn_0 -> layer7_mha_out_0
	layer7_mha_out_0 -> layer7_res1_0
	layer7_res1_0 -> layer7_ln1_0
	layer7_ln1_0 -> layer7_ffn_gate_0
	layer7_ln1_0 -> layer7_ffn_up_0
	layer7_ffn_gate_0 -> layer7_ffn_act_0
	layer7_ffn_up_0 -> layer7_ffn_act_0
	layer7_ffn_act_0 -> layer7_ffn_down_0
	layer7_ffn_down_0 -> layer7_res2_0
	layer6_res2_1 -> layer7_mha_qkv_1
	layer7_mha_qkv_1 -> layer7_mha_attn_1
	layer7_mha_attn_1 -> layer7_mha_out_1
	layer7_mha_out_1 -> layer7_res1_1
	layer7_res1_1 -> layer7_ln1_1
	layer7_ln1_1 -> layer7_ffn_gate_1
	layer7_ln1_1 -> layer7_ffn_up_1
	layer7_ffn_gate_1 -> layer7_ffn_act_1
	layer7_ffn_up_1 -> layer7_ffn_act_1
	layer7_ffn_act_1 -> layer7_ffn_down_1
	layer7_ffn_down_1 -> layer7_res2_1
	layer6_res2_2 -> layer7_mha_qkv_2
	layer7_mha_qkv_2 -> layer7_mha_attn_2
	layer7_mha_attn_2 -> layer7_mha_out_2
	layer7_mha_out_2 -> layer7_res1_2
	layer7_res1_2 -> layer7_ln1_2
	layer7_ln1_2 -> layer7_ffn_gate_2
	layer7_ln1_2 -> layer7_ffn_up_2
	layer7_ffn_gate_2 -> layer7_ffn_act_2
	layer7_ffn_up_2 -> layer7_ffn_act_2
	layer7_ffn_act_2 -> layer7_ffn_down_2
	layer7_ffn_down_2 -> layer7_res2_2
	layer6_res2_3 -> layer7_mha_qkv_3
	layer7_mha_qkv_3 -> layer7_mha_attn_3
	layer7_mha_attn_3 -> layer7_mha_out_3
	layer7_mha_out_3 -> layer7_res1_3
	layer7_res1_3 -> layer7_ln1_3
	layer7_ln1_3 -> layer7_ffn_gate_3
	layer7_ln1_3 -> layer7_ffn_up_3
	layer7_ffn_gate_3 -> layer7_ffn_act_3
	layer7_ffn_up_3 -> layer7_ffn_act_3
	layer7_ffn_act_3 -> layer7_ffn_down_3
	layer7_ffn_down_3 -> layer7_res2_3
	layer6_res2_4 -> layer7_mha_qkv_4
	layer7_mha_qkv_4 -> layer7_mha_attn_4
	layer7_mha_attn_4 -> layer7_mha_out_4
	layer7_mha_out_4 -> layer7_res1_4
	layer7_res1_4 -> layer7_ln1_4
	layer7_ln1_4 -> layer7_ffn_gate_4
	layer7_ln1_4 -> layer7_ffn_up_4
	layer7_ffn_gate_4 -> layer7_ffn_act_4
	layer7_ffn_up_4 -> layer7_ffn_act_4
	layer7_ffn_act_4 -> layer7_ffn_down_4
	layer7_ffn_down_4 -> layer7_res2_4
	layer6_res2_5 -> layer7_mha_qkv_5
	layer7_mha_qkv_5 -> layer7_mha_attn_5
	layer7_mha_attn_5 -> layer7_mha_out_5
	layer7_mha_out_5 -> layer7_res1_5
	layer7_res1_5 -> layer7_ln1_5
	layer7_ln1_5 -> layer7_ffn_gate_5
	layer7_ln1_5 -> layer7_ffn_up_5
	layer7_ffn_gate_5 -> layer7_ffn_act_5
	layer7_ffn_up_5 -> layer7_ffn_act_5
	layer7_ffn_act_5 -> layer7_ffn_down_5
	layer7_ffn_down_5 -> layer7_res2_5
	layer6_res2_6 -> layer7_mha_qkv_6
	layer7_mha_qkv_6 -> layer7_mha_attn_6
	layer7_mha_attn_6 -> layer7_mha_out_6
	layer7_mha_out_6 -> layer7_res1_6
	layer7_res1_6 -> layer7_ln1_6
	layer7_ln1_6 -> layer7_ffn_gate_6
	layer7_ln1_6 -> layer7_ffn_up_6
	layer7_ffn_gate_6 -> layer7_ffn_act_6
	layer7_ffn_up_6 -> layer7_ffn_act_6
	layer7_ffn_act_6 -> layer7_ffn_down_6
	layer7_ffn_down_6 -> layer7_res2_6
	layer6_res2_7 -> layer7_mha_qkv_7
	layer7_mha_qkv_7 -> layer7_mha_attn_7
	layer7_mha_attn_7 -> layer7_mha_out_7
	layer7_mha_out_7 -> layer7_res1_7
	layer7_res1_7 -> layer7_ln1_7
	layer7_ln1_7 -> layer7_ffn_gate_7
	layer7_ln1_7 -> layer7_ffn_up_7
	layer7_ffn_gate_7 -> layer7_ffn_act_7
	layer7_ffn_up_7 -> layer7_ffn_act_7
	layer7_ffn_act_7 -> layer7_ffn_down_7
	layer7_ffn_down_7 -> layer7_res2_7
	stage0_to_stage1 [label="Cross-Stage Transfer\nAll-to-All\n128100004096 BF16\nGPUs 0-7  8-15" shape=parallelogram]
	layer8_mha_qkv_8 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer8_mha_qkv_9 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer8_mha_qkv_10 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer8_mha_qkv_11 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer8_mha_qkv_12 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer8_mha_qkv_13 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer8_mha_qkv_14 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer8_mha_qkv_15 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer8_mha_attn_15 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer8_mha_out_8 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer8_mha_out_9 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer8_mha_out_10 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer8_mha_out_11 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer8_mha_out_12 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer8_mha_out_13 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer8_mha_out_14 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer8_mha_out_15 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer8_res1_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer8_ln1_15 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer8_ffn_gate_8 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer8_ffn_gate_9 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer8_ffn_gate_10 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer8_ffn_gate_11 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer8_ffn_gate_12 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer8_ffn_gate_13 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer8_ffn_gate_14 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer8_ffn_gate_15 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer8_ffn_up_8 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer8_ffn_up_9 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer8_ffn_up_10 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer8_ffn_up_11 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer8_ffn_up_12 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer8_ffn_up_13 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer8_ffn_up_14 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer8_ffn_up_15 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer8_ffn_act_15 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer8_ffn_down_8 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer8_ffn_down_9 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer8_ffn_down_10 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer8_ffn_down_11 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer8_ffn_down_12 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer8_ffn_down_13 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer8_ffn_down_14 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer8_ffn_down_15 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer8_res2_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer7_res2_8 -> layer8_mha_qkv_8
	layer8_mha_qkv_8 -> layer8_mha_attn_8
	layer8_mha_attn_8 -> layer8_mha_out_8
	layer8_mha_out_8 -> layer8_res1_8
	layer8_res1_8 -> layer8_ln1_8
	layer8_ln1_8 -> layer8_ffn_gate_8
	layer8_ln1_8 -> layer8_ffn_up_8
	layer8_ffn_gate_8 -> layer8_ffn_act_8
	layer8_ffn_up_8 -> layer8_ffn_act_8
	layer8_ffn_act_8 -> layer8_ffn_down_8
	layer8_ffn_down_8 -> layer8_res2_8
	layer7_res2_9 -> layer8_mha_qkv_9
	layer8_mha_qkv_9 -> layer8_mha_attn_9
	layer8_mha_attn_9 -> layer8_mha_out_9
	layer8_mha_out_9 -> layer8_res1_9
	layer8_res1_9 -> layer8_ln1_9
	layer8_ln1_9 -> layer8_ffn_gate_9
	layer8_ln1_9 -> layer8_ffn_up_9
	layer8_ffn_gate_9 -> layer8_ffn_act_9
	layer8_ffn_up_9 -> layer8_ffn_act_9
	layer8_ffn_act_9 -> layer8_ffn_down_9
	layer8_ffn_down_9 -> layer8_res2_9
	layer7_res2_10 -> layer8_mha_qkv_10
	layer8_mha_qkv_10 -> layer8_mha_attn_10
	layer8_mha_attn_10 -> layer8_mha_out_10
	layer8_mha_out_10 -> layer8_res1_10
	layer8_res1_10 -> layer8_ln1_10
	layer8_ln1_10 -> layer8_ffn_gate_10
	layer8_ln1_10 -> layer8_ffn_up_10
	layer8_ffn_gate_10 -> layer8_ffn_act_10
	layer8_ffn_up_10 -> layer8_ffn_act_10
	layer8_ffn_act_10 -> layer8_ffn_down_10
	layer8_ffn_down_10 -> layer8_res2_10
	layer7_res2_11 -> layer8_mha_qkv_11
	layer8_mha_qkv_11 -> layer8_mha_attn_11
	layer8_mha_attn_11 -> layer8_mha_out_11
	layer8_mha_out_11 -> layer8_res1_11
	layer8_res1_11 -> layer8_ln1_11
	layer8_ln1_11 -> layer8_ffn_gate_11
	layer8_ln1_11 -> layer8_ffn_up_11
	layer8_ffn_gate_11 -> layer8_ffn_act_11
	layer8_ffn_up_11 -> layer8_ffn_act_11
	layer8_ffn_act_11 -> layer8_ffn_down_11
	layer8_ffn_down_11 -> layer8_res2_11
	layer7_res2_12 -> layer8_mha_qkv_12
	layer8_mha_qkv_12 -> layer8_mha_attn_12
	layer8_mha_attn_12 -> layer8_mha_out_12
	layer8_mha_out_12 -> layer8_res1_12
	layer8_res1_12 -> layer8_ln1_12
	layer8_ln1_12 -> layer8_ffn_gate_12
	layer8_ln1_12 -> layer8_ffn_up_12
	layer8_ffn_gate_12 -> layer8_ffn_act_12
	layer8_ffn_up_12 -> layer8_ffn_act_12
	layer8_ffn_act_12 -> layer8_ffn_down_12
	layer8_ffn_down_12 -> layer8_res2_12
	layer7_res2_13 -> layer8_mha_qkv_13
	layer8_mha_qkv_13 -> layer8_mha_attn_13
	layer8_mha_attn_13 -> layer8_mha_out_13
	layer8_mha_out_13 -> layer8_res1_13
	layer8_res1_13 -> layer8_ln1_13
	layer8_ln1_13 -> layer8_ffn_gate_13
	layer8_ln1_13 -> layer8_ffn_up_13
	layer8_ffn_gate_13 -> layer8_ffn_act_13
	layer8_ffn_up_13 -> layer8_ffn_act_13
	layer8_ffn_act_13 -> layer8_ffn_down_13
	layer8_ffn_down_13 -> layer8_res2_13
	layer7_res2_14 -> layer8_mha_qkv_14
	layer8_mha_qkv_14 -> layer8_mha_attn_14
	layer8_mha_attn_14 -> layer8_mha_out_14
	layer8_mha_out_14 -> layer8_res1_14
	layer8_res1_14 -> layer8_ln1_14
	layer8_ln1_14 -> layer8_ffn_gate_14
	layer8_ln1_14 -> layer8_ffn_up_14
	layer8_ffn_gate_14 -> layer8_ffn_act_14
	layer8_ffn_up_14 -> layer8_ffn_act_14
	layer8_ffn_act_14 -> layer8_ffn_down_14
	layer8_ffn_down_14 -> layer8_res2_14
	layer7_res2_15 -> layer8_mha_qkv_15
	layer8_mha_qkv_15 -> layer8_mha_attn_15
	layer8_mha_attn_15 -> layer8_mha_out_15
	layer8_mha_out_15 -> layer8_res1_15
	layer8_res1_15 -> layer8_ln1_15
	layer8_ln1_15 -> layer8_ffn_gate_15
	layer8_ln1_15 -> layer8_ffn_up_15
	layer8_ffn_gate_15 -> layer8_ffn_act_15
	layer8_ffn_up_15 -> layer8_ffn_act_15
	layer8_ffn_act_15 -> layer8_ffn_down_15
	layer8_ffn_down_15 -> layer8_res2_15
	layer9_mha_qkv_8 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer9_mha_qkv_9 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer9_mha_qkv_10 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer9_mha_qkv_11 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer9_mha_qkv_12 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer9_mha_qkv_13 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer9_mha_qkv_14 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer9_mha_qkv_15 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer9_mha_attn_15 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer9_mha_out_8 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer9_mha_out_9 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer9_mha_out_10 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer9_mha_out_11 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer9_mha_out_12 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer9_mha_out_13 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer9_mha_out_14 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer9_mha_out_15 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer9_res1_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer9_ln1_15 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer9_ffn_gate_8 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer9_ffn_gate_9 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer9_ffn_gate_10 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer9_ffn_gate_11 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer9_ffn_gate_12 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer9_ffn_gate_13 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer9_ffn_gate_14 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer9_ffn_gate_15 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer9_ffn_up_8 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer9_ffn_up_9 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer9_ffn_up_10 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer9_ffn_up_11 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer9_ffn_up_12 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer9_ffn_up_13 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer9_ffn_up_14 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer9_ffn_up_15 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer9_ffn_act_15 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer9_ffn_down_8 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer9_ffn_down_9 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer9_ffn_down_10 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer9_ffn_down_11 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer9_ffn_down_12 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer9_ffn_down_13 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer9_ffn_down_14 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer9_ffn_down_15 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer9_res2_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer8_res2_8 -> layer9_mha_qkv_8
	layer9_mha_qkv_8 -> layer9_mha_attn_8
	layer9_mha_attn_8 -> layer9_mha_out_8
	layer9_mha_out_8 -> layer9_res1_8
	layer9_res1_8 -> layer9_ln1_8
	layer9_ln1_8 -> layer9_ffn_gate_8
	layer9_ln1_8 -> layer9_ffn_up_8
	layer9_ffn_gate_8 -> layer9_ffn_act_8
	layer9_ffn_up_8 -> layer9_ffn_act_8
	layer9_ffn_act_8 -> layer9_ffn_down_8
	layer9_ffn_down_8 -> layer9_res2_8
	layer8_res2_9 -> layer9_mha_qkv_9
	layer9_mha_qkv_9 -> layer9_mha_attn_9
	layer9_mha_attn_9 -> layer9_mha_out_9
	layer9_mha_out_9 -> layer9_res1_9
	layer9_res1_9 -> layer9_ln1_9
	layer9_ln1_9 -> layer9_ffn_gate_9
	layer9_ln1_9 -> layer9_ffn_up_9
	layer9_ffn_gate_9 -> layer9_ffn_act_9
	layer9_ffn_up_9 -> layer9_ffn_act_9
	layer9_ffn_act_9 -> layer9_ffn_down_9
	layer9_ffn_down_9 -> layer9_res2_9
	layer8_res2_10 -> layer9_mha_qkv_10
	layer9_mha_qkv_10 -> layer9_mha_attn_10
	layer9_mha_attn_10 -> layer9_mha_out_10
	layer9_mha_out_10 -> layer9_res1_10
	layer9_res1_10 -> layer9_ln1_10
	layer9_ln1_10 -> layer9_ffn_gate_10
	layer9_ln1_10 -> layer9_ffn_up_10
	layer9_ffn_gate_10 -> layer9_ffn_act_10
	layer9_ffn_up_10 -> layer9_ffn_act_10
	layer9_ffn_act_10 -> layer9_ffn_down_10
	layer9_ffn_down_10 -> layer9_res2_10
	layer8_res2_11 -> layer9_mha_qkv_11
	layer9_mha_qkv_11 -> layer9_mha_attn_11
	layer9_mha_attn_11 -> layer9_mha_out_11
	layer9_mha_out_11 -> layer9_res1_11
	layer9_res1_11 -> layer9_ln1_11
	layer9_ln1_11 -> layer9_ffn_gate_11
	layer9_ln1_11 -> layer9_ffn_up_11
	layer9_ffn_gate_11 -> layer9_ffn_act_11
	layer9_ffn_up_11 -> layer9_ffn_act_11
	layer9_ffn_act_11 -> layer9_ffn_down_11
	layer9_ffn_down_11 -> layer9_res2_11
	layer8_res2_12 -> layer9_mha_qkv_12
	layer9_mha_qkv_12 -> layer9_mha_attn_12
	layer9_mha_attn_12 -> layer9_mha_out_12
	layer9_mha_out_12 -> layer9_res1_12
	layer9_res1_12 -> layer9_ln1_12
	layer9_ln1_12 -> layer9_ffn_gate_12
	layer9_ln1_12 -> layer9_ffn_up_12
	layer9_ffn_gate_12 -> layer9_ffn_act_12
	layer9_ffn_up_12 -> layer9_ffn_act_12
	layer9_ffn_act_12 -> layer9_ffn_down_12
	layer9_ffn_down_12 -> layer9_res2_12
	layer8_res2_13 -> layer9_mha_qkv_13
	layer9_mha_qkv_13 -> layer9_mha_attn_13
	layer9_mha_attn_13 -> layer9_mha_out_13
	layer9_mha_out_13 -> layer9_res1_13
	layer9_res1_13 -> layer9_ln1_13
	layer9_ln1_13 -> layer9_ffn_gate_13
	layer9_ln1_13 -> layer9_ffn_up_13
	layer9_ffn_gate_13 -> layer9_ffn_act_13
	layer9_ffn_up_13 -> layer9_ffn_act_13
	layer9_ffn_act_13 -> layer9_ffn_down_13
	layer9_ffn_down_13 -> layer9_res2_13
	layer8_res2_14 -> layer9_mha_qkv_14
	layer9_mha_qkv_14 -> layer9_mha_attn_14
	layer9_mha_attn_14 -> layer9_mha_out_14
	layer9_mha_out_14 -> layer9_res1_14
	layer9_res1_14 -> layer9_ln1_14
	layer9_ln1_14 -> layer9_ffn_gate_14
	layer9_ln1_14 -> layer9_ffn_up_14
	layer9_ffn_gate_14 -> layer9_ffn_act_14
	layer9_ffn_up_14 -> layer9_ffn_act_14
	layer9_ffn_act_14 -> layer9_ffn_down_14
	layer9_ffn_down_14 -> layer9_res2_14
	layer8_res2_15 -> layer9_mha_qkv_15
	layer9_mha_qkv_15 -> layer9_mha_attn_15
	layer9_mha_attn_15 -> layer9_mha_out_15
	layer9_mha_out_15 -> layer9_res1_15
	layer9_res1_15 -> layer9_ln1_15
	layer9_ln1_15 -> layer9_ffn_gate_15
	layer9_ln1_15 -> layer9_ffn_up_15
	layer9_ffn_gate_15 -> layer9_ffn_act_15
	layer9_ffn_up_15 -> layer9_ffn_act_15
	layer9_ffn_act_15 -> layer9_ffn_down_15
	layer9_ffn_down_15 -> layer9_res2_15
	layer10_mha_qkv_8 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer10_mha_qkv_9 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer10_mha_qkv_10 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer10_mha_qkv_11 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer10_mha_qkv_12 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer10_mha_qkv_13 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer10_mha_qkv_14 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer10_mha_qkv_15 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer10_mha_attn_15 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer10_mha_out_8 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer10_mha_out_9 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer10_mha_out_10 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer10_mha_out_11 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer10_mha_out_12 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer10_mha_out_13 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer10_mha_out_14 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer10_mha_out_15 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer10_res1_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer10_ln1_15 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer10_ffn_gate_8 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer10_ffn_gate_9 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer10_ffn_gate_10 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer10_ffn_gate_11 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer10_ffn_gate_12 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer10_ffn_gate_13 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer10_ffn_gate_14 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer10_ffn_gate_15 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer10_ffn_up_8 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer10_ffn_up_9 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer10_ffn_up_10 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer10_ffn_up_11 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer10_ffn_up_12 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer10_ffn_up_13 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer10_ffn_up_14 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer10_ffn_up_15 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer10_ffn_act_15 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer10_ffn_down_8 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer10_ffn_down_9 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer10_ffn_down_10 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer10_ffn_down_11 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer10_ffn_down_12 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer10_ffn_down_13 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer10_ffn_down_14 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer10_ffn_down_15 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer10_res2_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer9_res2_8 -> layer10_mha_qkv_8
	layer10_mha_qkv_8 -> layer10_mha_attn_8
	layer10_mha_attn_8 -> layer10_mha_out_8
	layer10_mha_out_8 -> layer10_res1_8
	layer10_res1_8 -> layer10_ln1_8
	layer10_ln1_8 -> layer10_ffn_gate_8
	layer10_ln1_8 -> layer10_ffn_up_8
	layer10_ffn_gate_8 -> layer10_ffn_act_8
	layer10_ffn_up_8 -> layer10_ffn_act_8
	layer10_ffn_act_8 -> layer10_ffn_down_8
	layer10_ffn_down_8 -> layer10_res2_8
	layer9_res2_9 -> layer10_mha_qkv_9
	layer10_mha_qkv_9 -> layer10_mha_attn_9
	layer10_mha_attn_9 -> layer10_mha_out_9
	layer10_mha_out_9 -> layer10_res1_9
	layer10_res1_9 -> layer10_ln1_9
	layer10_ln1_9 -> layer10_ffn_gate_9
	layer10_ln1_9 -> layer10_ffn_up_9
	layer10_ffn_gate_9 -> layer10_ffn_act_9
	layer10_ffn_up_9 -> layer10_ffn_act_9
	layer10_ffn_act_9 -> layer10_ffn_down_9
	layer10_ffn_down_9 -> layer10_res2_9
	layer9_res2_10 -> layer10_mha_qkv_10
	layer10_mha_qkv_10 -> layer10_mha_attn_10
	layer10_mha_attn_10 -> layer10_mha_out_10
	layer10_mha_out_10 -> layer10_res1_10
	layer10_res1_10 -> layer10_ln1_10
	layer10_ln1_10 -> layer10_ffn_gate_10
	layer10_ln1_10 -> layer10_ffn_up_10
	layer10_ffn_gate_10 -> layer10_ffn_act_10
	layer10_ffn_up_10 -> layer10_ffn_act_10
	layer10_ffn_act_10 -> layer10_ffn_down_10
	layer10_ffn_down_10 -> layer10_res2_10
	layer9_res2_11 -> layer10_mha_qkv_11
	layer10_mha_qkv_11 -> layer10_mha_attn_11
	layer10_mha_attn_11 -> layer10_mha_out_11
	layer10_mha_out_11 -> layer10_res1_11
	layer10_res1_11 -> layer10_ln1_11
	layer10_ln1_11 -> layer10_ffn_gate_11
	layer10_ln1_11 -> layer10_ffn_up_11
	layer10_ffn_gate_11 -> layer10_ffn_act_11
	layer10_ffn_up_11 -> layer10_ffn_act_11
	layer10_ffn_act_11 -> layer10_ffn_down_11
	layer10_ffn_down_11 -> layer10_res2_11
	layer9_res2_12 -> layer10_mha_qkv_12
	layer10_mha_qkv_12 -> layer10_mha_attn_12
	layer10_mha_attn_12 -> layer10_mha_out_12
	layer10_mha_out_12 -> layer10_res1_12
	layer10_res1_12 -> layer10_ln1_12
	layer10_ln1_12 -> layer10_ffn_gate_12
	layer10_ln1_12 -> layer10_ffn_up_12
	layer10_ffn_gate_12 -> layer10_ffn_act_12
	layer10_ffn_up_12 -> layer10_ffn_act_12
	layer10_ffn_act_12 -> layer10_ffn_down_12
	layer10_ffn_down_12 -> layer10_res2_12
	layer9_res2_13 -> layer10_mha_qkv_13
	layer10_mha_qkv_13 -> layer10_mha_attn_13
	layer10_mha_attn_13 -> layer10_mha_out_13
	layer10_mha_out_13 -> layer10_res1_13
	layer10_res1_13 -> layer10_ln1_13
	layer10_ln1_13 -> layer10_ffn_gate_13
	layer10_ln1_13 -> layer10_ffn_up_13
	layer10_ffn_gate_13 -> layer10_ffn_act_13
	layer10_ffn_up_13 -> layer10_ffn_act_13
	layer10_ffn_act_13 -> layer10_ffn_down_13
	layer10_ffn_down_13 -> layer10_res2_13
	layer9_res2_14 -> layer10_mha_qkv_14
	layer10_mha_qkv_14 -> layer10_mha_attn_14
	layer10_mha_attn_14 -> layer10_mha_out_14
	layer10_mha_out_14 -> layer10_res1_14
	layer10_res1_14 -> layer10_ln1_14
	layer10_ln1_14 -> layer10_ffn_gate_14
	layer10_ln1_14 -> layer10_ffn_up_14
	layer10_ffn_gate_14 -> layer10_ffn_act_14
	layer10_ffn_up_14 -> layer10_ffn_act_14
	layer10_ffn_act_14 -> layer10_ffn_down_14
	layer10_ffn_down_14 -> layer10_res2_14
	layer9_res2_15 -> layer10_mha_qkv_15
	layer10_mha_qkv_15 -> layer10_mha_attn_15
	layer10_mha_attn_15 -> layer10_mha_out_15
	layer10_mha_out_15 -> layer10_res1_15
	layer10_res1_15 -> layer10_ln1_15
	layer10_ln1_15 -> layer10_ffn_gate_15
	layer10_ln1_15 -> layer10_ffn_up_15
	layer10_ffn_gate_15 -> layer10_ffn_act_15
	layer10_ffn_up_15 -> layer10_ffn_act_15
	layer10_ffn_act_15 -> layer10_ffn_down_15
	layer10_ffn_down_15 -> layer10_res2_15
	layer11_mha_qkv_8 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer11_mha_qkv_9 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer11_mha_qkv_10 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer11_mha_qkv_11 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer11_mha_qkv_12 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer11_mha_qkv_13 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer11_mha_qkv_14 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer11_mha_qkv_15 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer11_mha_attn_15 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer11_mha_out_8 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer11_mha_out_9 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer11_mha_out_10 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer11_mha_out_11 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer11_mha_out_12 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer11_mha_out_13 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer11_mha_out_14 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer11_mha_out_15 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer11_res1_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer11_ln1_15 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer11_ffn_gate_8 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer11_ffn_gate_9 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer11_ffn_gate_10 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer11_ffn_gate_11 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer11_ffn_gate_12 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer11_ffn_gate_13 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer11_ffn_gate_14 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer11_ffn_gate_15 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer11_ffn_up_8 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer11_ffn_up_9 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer11_ffn_up_10 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer11_ffn_up_11 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer11_ffn_up_12 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer11_ffn_up_13 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer11_ffn_up_14 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer11_ffn_up_15 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer11_ffn_act_15 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer11_ffn_down_8 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer11_ffn_down_9 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer11_ffn_down_10 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer11_ffn_down_11 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer11_ffn_down_12 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer11_ffn_down_13 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer11_ffn_down_14 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer11_ffn_down_15 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer11_res2_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer10_res2_8 -> layer11_mha_qkv_8
	layer11_mha_qkv_8 -> layer11_mha_attn_8
	layer11_mha_attn_8 -> layer11_mha_out_8
	layer11_mha_out_8 -> layer11_res1_8
	layer11_res1_8 -> layer11_ln1_8
	layer11_ln1_8 -> layer11_ffn_gate_8
	layer11_ln1_8 -> layer11_ffn_up_8
	layer11_ffn_gate_8 -> layer11_ffn_act_8
	layer11_ffn_up_8 -> layer11_ffn_act_8
	layer11_ffn_act_8 -> layer11_ffn_down_8
	layer11_ffn_down_8 -> layer11_res2_8
	layer10_res2_9 -> layer11_mha_qkv_9
	layer11_mha_qkv_9 -> layer11_mha_attn_9
	layer11_mha_attn_9 -> layer11_mha_out_9
	layer11_mha_out_9 -> layer11_res1_9
	layer11_res1_9 -> layer11_ln1_9
	layer11_ln1_9 -> layer11_ffn_gate_9
	layer11_ln1_9 -> layer11_ffn_up_9
	layer11_ffn_gate_9 -> layer11_ffn_act_9
	layer11_ffn_up_9 -> layer11_ffn_act_9
	layer11_ffn_act_9 -> layer11_ffn_down_9
	layer11_ffn_down_9 -> layer11_res2_9
	layer10_res2_10 -> layer11_mha_qkv_10
	layer11_mha_qkv_10 -> layer11_mha_attn_10
	layer11_mha_attn_10 -> layer11_mha_out_10
	layer11_mha_out_10 -> layer11_res1_10
	layer11_res1_10 -> layer11_ln1_10
	layer11_ln1_10 -> layer11_ffn_gate_10
	layer11_ln1_10 -> layer11_ffn_up_10
	layer11_ffn_gate_10 -> layer11_ffn_act_10
	layer11_ffn_up_10 -> layer11_ffn_act_10
	layer11_ffn_act_10 -> layer11_ffn_down_10
	layer11_ffn_down_10 -> layer11_res2_10
	layer10_res2_11 -> layer11_mha_qkv_11
	layer11_mha_qkv_11 -> layer11_mha_attn_11
	layer11_mha_attn_11 -> layer11_mha_out_11
	layer11_mha_out_11 -> layer11_res1_11
	layer11_res1_11 -> layer11_ln1_11
	layer11_ln1_11 -> layer11_ffn_gate_11
	layer11_ln1_11 -> layer11_ffn_up_11
	layer11_ffn_gate_11 -> layer11_ffn_act_11
	layer11_ffn_up_11 -> layer11_ffn_act_11
	layer11_ffn_act_11 -> layer11_ffn_down_11
	layer11_ffn_down_11 -> layer11_res2_11
	layer10_res2_12 -> layer11_mha_qkv_12
	layer11_mha_qkv_12 -> layer11_mha_attn_12
	layer11_mha_attn_12 -> layer11_mha_out_12
	layer11_mha_out_12 -> layer11_res1_12
	layer11_res1_12 -> layer11_ln1_12
	layer11_ln1_12 -> layer11_ffn_gate_12
	layer11_ln1_12 -> layer11_ffn_up_12
	layer11_ffn_gate_12 -> layer11_ffn_act_12
	layer11_ffn_up_12 -> layer11_ffn_act_12
	layer11_ffn_act_12 -> layer11_ffn_down_12
	layer11_ffn_down_12 -> layer11_res2_12
	layer10_res2_13 -> layer11_mha_qkv_13
	layer11_mha_qkv_13 -> layer11_mha_attn_13
	layer11_mha_attn_13 -> layer11_mha_out_13
	layer11_mha_out_13 -> layer11_res1_13
	layer11_res1_13 -> layer11_ln1_13
	layer11_ln1_13 -> layer11_ffn_gate_13
	layer11_ln1_13 -> layer11_ffn_up_13
	layer11_ffn_gate_13 -> layer11_ffn_act_13
	layer11_ffn_up_13 -> layer11_ffn_act_13
	layer11_ffn_act_13 -> layer11_ffn_down_13
	layer11_ffn_down_13 -> layer11_res2_13
	layer10_res2_14 -> layer11_mha_qkv_14
	layer11_mha_qkv_14 -> layer11_mha_attn_14
	layer11_mha_attn_14 -> layer11_mha_out_14
	layer11_mha_out_14 -> layer11_res1_14
	layer11_res1_14 -> layer11_ln1_14
	layer11_ln1_14 -> layer11_ffn_gate_14
	layer11_ln1_14 -> layer11_ffn_up_14
	layer11_ffn_gate_14 -> layer11_ffn_act_14
	layer11_ffn_up_14 -> layer11_ffn_act_14
	layer11_ffn_act_14 -> layer11_ffn_down_14
	layer11_ffn_down_14 -> layer11_res2_14
	layer10_res2_15 -> layer11_mha_qkv_15
	layer11_mha_qkv_15 -> layer11_mha_attn_15
	layer11_mha_attn_15 -> layer11_mha_out_15
	layer11_mha_out_15 -> layer11_res1_15
	layer11_res1_15 -> layer11_ln1_15
	layer11_ln1_15 -> layer11_ffn_gate_15
	layer11_ln1_15 -> layer11_ffn_up_15
	layer11_ffn_gate_15 -> layer11_ffn_act_15
	layer11_ffn_up_15 -> layer11_ffn_act_15
	layer11_ffn_act_15 -> layer11_ffn_down_15
	layer11_ffn_down_15 -> layer11_res2_15
	layer12_mha_qkv_8 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer12_mha_qkv_9 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer12_mha_qkv_10 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer12_mha_qkv_11 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer12_mha_qkv_12 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer12_mha_qkv_13 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer12_mha_qkv_14 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer12_mha_qkv_15 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer12_mha_attn_15 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer12_mha_out_8 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer12_mha_out_9 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer12_mha_out_10 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer12_mha_out_11 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer12_mha_out_12 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer12_mha_out_13 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer12_mha_out_14 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer12_mha_out_15 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer12_res1_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer12_ln1_15 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer12_ffn_gate_8 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer12_ffn_gate_9 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer12_ffn_gate_10 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer12_ffn_gate_11 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer12_ffn_gate_12 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer12_ffn_gate_13 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer12_ffn_gate_14 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer12_ffn_gate_15 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer12_ffn_up_8 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer12_ffn_up_9 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer12_ffn_up_10 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer12_ffn_up_11 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer12_ffn_up_12 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer12_ffn_up_13 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer12_ffn_up_14 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer12_ffn_up_15 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer12_ffn_act_15 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer12_ffn_down_8 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer12_ffn_down_9 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer12_ffn_down_10 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer12_ffn_down_11 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer12_ffn_down_12 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer12_ffn_down_13 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer12_ffn_down_14 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer12_ffn_down_15 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer12_res2_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer11_res2_8 -> layer12_mha_qkv_8
	layer12_mha_qkv_8 -> layer12_mha_attn_8
	layer12_mha_attn_8 -> layer12_mha_out_8
	layer12_mha_out_8 -> layer12_res1_8
	layer12_res1_8 -> layer12_ln1_8
	layer12_ln1_8 -> layer12_ffn_gate_8
	layer12_ln1_8 -> layer12_ffn_up_8
	layer12_ffn_gate_8 -> layer12_ffn_act_8
	layer12_ffn_up_8 -> layer12_ffn_act_8
	layer12_ffn_act_8 -> layer12_ffn_down_8
	layer12_ffn_down_8 -> layer12_res2_8
	layer11_res2_9 -> layer12_mha_qkv_9
	layer12_mha_qkv_9 -> layer12_mha_attn_9
	layer12_mha_attn_9 -> layer12_mha_out_9
	layer12_mha_out_9 -> layer12_res1_9
	layer12_res1_9 -> layer12_ln1_9
	layer12_ln1_9 -> layer12_ffn_gate_9
	layer12_ln1_9 -> layer12_ffn_up_9
	layer12_ffn_gate_9 -> layer12_ffn_act_9
	layer12_ffn_up_9 -> layer12_ffn_act_9
	layer12_ffn_act_9 -> layer12_ffn_down_9
	layer12_ffn_down_9 -> layer12_res2_9
	layer11_res2_10 -> layer12_mha_qkv_10
	layer12_mha_qkv_10 -> layer12_mha_attn_10
	layer12_mha_attn_10 -> layer12_mha_out_10
	layer12_mha_out_10 -> layer12_res1_10
	layer12_res1_10 -> layer12_ln1_10
	layer12_ln1_10 -> layer12_ffn_gate_10
	layer12_ln1_10 -> layer12_ffn_up_10
	layer12_ffn_gate_10 -> layer12_ffn_act_10
	layer12_ffn_up_10 -> layer12_ffn_act_10
	layer12_ffn_act_10 -> layer12_ffn_down_10
	layer12_ffn_down_10 -> layer12_res2_10
	layer11_res2_11 -> layer12_mha_qkv_11
	layer12_mha_qkv_11 -> layer12_mha_attn_11
	layer12_mha_attn_11 -> layer12_mha_out_11
	layer12_mha_out_11 -> layer12_res1_11
	layer12_res1_11 -> layer12_ln1_11
	layer12_ln1_11 -> layer12_ffn_gate_11
	layer12_ln1_11 -> layer12_ffn_up_11
	layer12_ffn_gate_11 -> layer12_ffn_act_11
	layer12_ffn_up_11 -> layer12_ffn_act_11
	layer12_ffn_act_11 -> layer12_ffn_down_11
	layer12_ffn_down_11 -> layer12_res2_11
	layer11_res2_12 -> layer12_mha_qkv_12
	layer12_mha_qkv_12 -> layer12_mha_attn_12
	layer12_mha_attn_12 -> layer12_mha_out_12
	layer12_mha_out_12 -> layer12_res1_12
	layer12_res1_12 -> layer12_ln1_12
	layer12_ln1_12 -> layer12_ffn_gate_12
	layer12_ln1_12 -> layer12_ffn_up_12
	layer12_ffn_gate_12 -> layer12_ffn_act_12
	layer12_ffn_up_12 -> layer12_ffn_act_12
	layer12_ffn_act_12 -> layer12_ffn_down_12
	layer12_ffn_down_12 -> layer12_res2_12
	layer11_res2_13 -> layer12_mha_qkv_13
	layer12_mha_qkv_13 -> layer12_mha_attn_13
	layer12_mha_attn_13 -> layer12_mha_out_13
	layer12_mha_out_13 -> layer12_res1_13
	layer12_res1_13 -> layer12_ln1_13
	layer12_ln1_13 -> layer12_ffn_gate_13
	layer12_ln1_13 -> layer12_ffn_up_13
	layer12_ffn_gate_13 -> layer12_ffn_act_13
	layer12_ffn_up_13 -> layer12_ffn_act_13
	layer12_ffn_act_13 -> layer12_ffn_down_13
	layer12_ffn_down_13 -> layer12_res2_13
	layer11_res2_14 -> layer12_mha_qkv_14
	layer12_mha_qkv_14 -> layer12_mha_attn_14
	layer12_mha_attn_14 -> layer12_mha_out_14
	layer12_mha_out_14 -> layer12_res1_14
	layer12_res1_14 -> layer12_ln1_14
	layer12_ln1_14 -> layer12_ffn_gate_14
	layer12_ln1_14 -> layer12_ffn_up_14
	layer12_ffn_gate_14 -> layer12_ffn_act_14
	layer12_ffn_up_14 -> layer12_ffn_act_14
	layer12_ffn_act_14 -> layer12_ffn_down_14
	layer12_ffn_down_14 -> layer12_res2_14
	layer11_res2_15 -> layer12_mha_qkv_15
	layer12_mha_qkv_15 -> layer12_mha_attn_15
	layer12_mha_attn_15 -> layer12_mha_out_15
	layer12_mha_out_15 -> layer12_res1_15
	layer12_res1_15 -> layer12_ln1_15
	layer12_ln1_15 -> layer12_ffn_gate_15
	layer12_ln1_15 -> layer12_ffn_up_15
	layer12_ffn_gate_15 -> layer12_ffn_act_15
	layer12_ffn_up_15 -> layer12_ffn_act_15
	layer12_ffn_act_15 -> layer12_ffn_down_15
	layer12_ffn_down_15 -> layer12_res2_15
	layer13_mha_qkv_8 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer13_mha_qkv_9 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer13_mha_qkv_10 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer13_mha_qkv_11 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer13_mha_qkv_12 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer13_mha_qkv_13 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer13_mha_qkv_14 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer13_mha_qkv_15 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer13_mha_attn_15 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer13_mha_out_8 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer13_mha_out_9 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer13_mha_out_10 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer13_mha_out_11 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer13_mha_out_12 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer13_mha_out_13 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer13_mha_out_14 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer13_mha_out_15 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer13_res1_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer13_ln1_15 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer13_ffn_gate_8 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer13_ffn_gate_9 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer13_ffn_gate_10 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer13_ffn_gate_11 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer13_ffn_gate_12 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer13_ffn_gate_13 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer13_ffn_gate_14 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer13_ffn_gate_15 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer13_ffn_up_8 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer13_ffn_up_9 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer13_ffn_up_10 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer13_ffn_up_11 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer13_ffn_up_12 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer13_ffn_up_13 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer13_ffn_up_14 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer13_ffn_up_15 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer13_ffn_act_15 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer13_ffn_down_8 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer13_ffn_down_9 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer13_ffn_down_10 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer13_ffn_down_11 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer13_ffn_down_12 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer13_ffn_down_13 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer13_ffn_down_14 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer13_ffn_down_15 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer13_res2_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer12_res2_8 -> layer13_mha_qkv_8
	layer13_mha_qkv_8 -> layer13_mha_attn_8
	layer13_mha_attn_8 -> layer13_mha_out_8
	layer13_mha_out_8 -> layer13_res1_8
	layer13_res1_8 -> layer13_ln1_8
	layer13_ln1_8 -> layer13_ffn_gate_8
	layer13_ln1_8 -> layer13_ffn_up_8
	layer13_ffn_gate_8 -> layer13_ffn_act_8
	layer13_ffn_up_8 -> layer13_ffn_act_8
	layer13_ffn_act_8 -> layer13_ffn_down_8
	layer13_ffn_down_8 -> layer13_res2_8
	layer12_res2_9 -> layer13_mha_qkv_9
	layer13_mha_qkv_9 -> layer13_mha_attn_9
	layer13_mha_attn_9 -> layer13_mha_out_9
	layer13_mha_out_9 -> layer13_res1_9
	layer13_res1_9 -> layer13_ln1_9
	layer13_ln1_9 -> layer13_ffn_gate_9
	layer13_ln1_9 -> layer13_ffn_up_9
	layer13_ffn_gate_9 -> layer13_ffn_act_9
	layer13_ffn_up_9 -> layer13_ffn_act_9
	layer13_ffn_act_9 -> layer13_ffn_down_9
	layer13_ffn_down_9 -> layer13_res2_9
	layer12_res2_10 -> layer13_mha_qkv_10
	layer13_mha_qkv_10 -> layer13_mha_attn_10
	layer13_mha_attn_10 -> layer13_mha_out_10
	layer13_mha_out_10 -> layer13_res1_10
	layer13_res1_10 -> layer13_ln1_10
	layer13_ln1_10 -> layer13_ffn_gate_10
	layer13_ln1_10 -> layer13_ffn_up_10
	layer13_ffn_gate_10 -> layer13_ffn_act_10
	layer13_ffn_up_10 -> layer13_ffn_act_10
	layer13_ffn_act_10 -> layer13_ffn_down_10
	layer13_ffn_down_10 -> layer13_res2_10
	layer12_res2_11 -> layer13_mha_qkv_11
	layer13_mha_qkv_11 -> layer13_mha_attn_11
	layer13_mha_attn_11 -> layer13_mha_out_11
	layer13_mha_out_11 -> layer13_res1_11
	layer13_res1_11 -> layer13_ln1_11
	layer13_ln1_11 -> layer13_ffn_gate_11
	layer13_ln1_11 -> layer13_ffn_up_11
	layer13_ffn_gate_11 -> layer13_ffn_act_11
	layer13_ffn_up_11 -> layer13_ffn_act_11
	layer13_ffn_act_11 -> layer13_ffn_down_11
	layer13_ffn_down_11 -> layer13_res2_11
	layer12_res2_12 -> layer13_mha_qkv_12
	layer13_mha_qkv_12 -> layer13_mha_attn_12
	layer13_mha_attn_12 -> layer13_mha_out_12
	layer13_mha_out_12 -> layer13_res1_12
	layer13_res1_12 -> layer13_ln1_12
	layer13_ln1_12 -> layer13_ffn_gate_12
	layer13_ln1_12 -> layer13_ffn_up_12
	layer13_ffn_gate_12 -> layer13_ffn_act_12
	layer13_ffn_up_12 -> layer13_ffn_act_12
	layer13_ffn_act_12 -> layer13_ffn_down_12
	layer13_ffn_down_12 -> layer13_res2_12
	layer12_res2_13 -> layer13_mha_qkv_13
	layer13_mha_qkv_13 -> layer13_mha_attn_13
	layer13_mha_attn_13 -> layer13_mha_out_13
	layer13_mha_out_13 -> layer13_res1_13
	layer13_res1_13 -> layer13_ln1_13
	layer13_ln1_13 -> layer13_ffn_gate_13
	layer13_ln1_13 -> layer13_ffn_up_13
	layer13_ffn_gate_13 -> layer13_ffn_act_13
	layer13_ffn_up_13 -> layer13_ffn_act_13
	layer13_ffn_act_13 -> layer13_ffn_down_13
	layer13_ffn_down_13 -> layer13_res2_13
	layer12_res2_14 -> layer13_mha_qkv_14
	layer13_mha_qkv_14 -> layer13_mha_attn_14
	layer13_mha_attn_14 -> layer13_mha_out_14
	layer13_mha_out_14 -> layer13_res1_14
	layer13_res1_14 -> layer13_ln1_14
	layer13_ln1_14 -> layer13_ffn_gate_14
	layer13_ln1_14 -> layer13_ffn_up_14
	layer13_ffn_gate_14 -> layer13_ffn_act_14
	layer13_ffn_up_14 -> layer13_ffn_act_14
	layer13_ffn_act_14 -> layer13_ffn_down_14
	layer13_ffn_down_14 -> layer13_res2_14
	layer12_res2_15 -> layer13_mha_qkv_15
	layer13_mha_qkv_15 -> layer13_mha_attn_15
	layer13_mha_attn_15 -> layer13_mha_out_15
	layer13_mha_out_15 -> layer13_res1_15
	layer13_res1_15 -> layer13_ln1_15
	layer13_ln1_15 -> layer13_ffn_gate_15
	layer13_ln1_15 -> layer13_ffn_up_15
	layer13_ffn_gate_15 -> layer13_ffn_act_15
	layer13_ffn_up_15 -> layer13_ffn_act_15
	layer13_ffn_act_15 -> layer13_ffn_down_15
	layer13_ffn_down_15 -> layer13_res2_15
	layer14_mha_qkv_8 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer14_mha_qkv_9 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer14_mha_qkv_10 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer14_mha_qkv_11 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer14_mha_qkv_12 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer14_mha_qkv_13 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer14_mha_qkv_14 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer14_mha_qkv_15 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer14_mha_attn_15 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer14_mha_out_8 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer14_mha_out_9 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer14_mha_out_10 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer14_mha_out_11 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer14_mha_out_12 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer14_mha_out_13 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer14_mha_out_14 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer14_mha_out_15 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer14_res1_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer14_ln1_15 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer14_ffn_gate_8 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer14_ffn_gate_9 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer14_ffn_gate_10 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer14_ffn_gate_11 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer14_ffn_gate_12 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer14_ffn_gate_13 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer14_ffn_gate_14 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer14_ffn_gate_15 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer14_ffn_up_8 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer14_ffn_up_9 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer14_ffn_up_10 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer14_ffn_up_11 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer14_ffn_up_12 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer14_ffn_up_13 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer14_ffn_up_14 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer14_ffn_up_15 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer14_ffn_act_15 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer14_ffn_down_8 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer14_ffn_down_9 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer14_ffn_down_10 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer14_ffn_down_11 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer14_ffn_down_12 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer14_ffn_down_13 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer14_ffn_down_14 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer14_ffn_down_15 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer14_res2_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer13_res2_8 -> layer14_mha_qkv_8
	layer14_mha_qkv_8 -> layer14_mha_attn_8
	layer14_mha_attn_8 -> layer14_mha_out_8
	layer14_mha_out_8 -> layer14_res1_8
	layer14_res1_8 -> layer14_ln1_8
	layer14_ln1_8 -> layer14_ffn_gate_8
	layer14_ln1_8 -> layer14_ffn_up_8
	layer14_ffn_gate_8 -> layer14_ffn_act_8
	layer14_ffn_up_8 -> layer14_ffn_act_8
	layer14_ffn_act_8 -> layer14_ffn_down_8
	layer14_ffn_down_8 -> layer14_res2_8
	layer13_res2_9 -> layer14_mha_qkv_9
	layer14_mha_qkv_9 -> layer14_mha_attn_9
	layer14_mha_attn_9 -> layer14_mha_out_9
	layer14_mha_out_9 -> layer14_res1_9
	layer14_res1_9 -> layer14_ln1_9
	layer14_ln1_9 -> layer14_ffn_gate_9
	layer14_ln1_9 -> layer14_ffn_up_9
	layer14_ffn_gate_9 -> layer14_ffn_act_9
	layer14_ffn_up_9 -> layer14_ffn_act_9
	layer14_ffn_act_9 -> layer14_ffn_down_9
	layer14_ffn_down_9 -> layer14_res2_9
	layer13_res2_10 -> layer14_mha_qkv_10
	layer14_mha_qkv_10 -> layer14_mha_attn_10
	layer14_mha_attn_10 -> layer14_mha_out_10
	layer14_mha_out_10 -> layer14_res1_10
	layer14_res1_10 -> layer14_ln1_10
	layer14_ln1_10 -> layer14_ffn_gate_10
	layer14_ln1_10 -> layer14_ffn_up_10
	layer14_ffn_gate_10 -> layer14_ffn_act_10
	layer14_ffn_up_10 -> layer14_ffn_act_10
	layer14_ffn_act_10 -> layer14_ffn_down_10
	layer14_ffn_down_10 -> layer14_res2_10
	layer13_res2_11 -> layer14_mha_qkv_11
	layer14_mha_qkv_11 -> layer14_mha_attn_11
	layer14_mha_attn_11 -> layer14_mha_out_11
	layer14_mha_out_11 -> layer14_res1_11
	layer14_res1_11 -> layer14_ln1_11
	layer14_ln1_11 -> layer14_ffn_gate_11
	layer14_ln1_11 -> layer14_ffn_up_11
	layer14_ffn_gate_11 -> layer14_ffn_act_11
	layer14_ffn_up_11 -> layer14_ffn_act_11
	layer14_ffn_act_11 -> layer14_ffn_down_11
	layer14_ffn_down_11 -> layer14_res2_11
	layer13_res2_12 -> layer14_mha_qkv_12
	layer14_mha_qkv_12 -> layer14_mha_attn_12
	layer14_mha_attn_12 -> layer14_mha_out_12
	layer14_mha_out_12 -> layer14_res1_12
	layer14_res1_12 -> layer14_ln1_12
	layer14_ln1_12 -> layer14_ffn_gate_12
	layer14_ln1_12 -> layer14_ffn_up_12
	layer14_ffn_gate_12 -> layer14_ffn_act_12
	layer14_ffn_up_12 -> layer14_ffn_act_12
	layer14_ffn_act_12 -> layer14_ffn_down_12
	layer14_ffn_down_12 -> layer14_res2_12
	layer13_res2_13 -> layer14_mha_qkv_13
	layer14_mha_qkv_13 -> layer14_mha_attn_13
	layer14_mha_attn_13 -> layer14_mha_out_13
	layer14_mha_out_13 -> layer14_res1_13
	layer14_res1_13 -> layer14_ln1_13
	layer14_ln1_13 -> layer14_ffn_gate_13
	layer14_ln1_13 -> layer14_ffn_up_13
	layer14_ffn_gate_13 -> layer14_ffn_act_13
	layer14_ffn_up_13 -> layer14_ffn_act_13
	layer14_ffn_act_13 -> layer14_ffn_down_13
	layer14_ffn_down_13 -> layer14_res2_13
	layer13_res2_14 -> layer14_mha_qkv_14
	layer14_mha_qkv_14 -> layer14_mha_attn_14
	layer14_mha_attn_14 -> layer14_mha_out_14
	layer14_mha_out_14 -> layer14_res1_14
	layer14_res1_14 -> layer14_ln1_14
	layer14_ln1_14 -> layer14_ffn_gate_14
	layer14_ln1_14 -> layer14_ffn_up_14
	layer14_ffn_gate_14 -> layer14_ffn_act_14
	layer14_ffn_up_14 -> layer14_ffn_act_14
	layer14_ffn_act_14 -> layer14_ffn_down_14
	layer14_ffn_down_14 -> layer14_res2_14
	layer13_res2_15 -> layer14_mha_qkv_15
	layer14_mha_qkv_15 -> layer14_mha_attn_15
	layer14_mha_attn_15 -> layer14_mha_out_15
	layer14_mha_out_15 -> layer14_res1_15
	layer14_res1_15 -> layer14_ln1_15
	layer14_ln1_15 -> layer14_ffn_gate_15
	layer14_ln1_15 -> layer14_ffn_up_15
	layer14_ffn_gate_15 -> layer14_ffn_act_15
	layer14_ffn_up_15 -> layer14_ffn_act_15
	layer14_ffn_act_15 -> layer14_ffn_down_15
	layer14_ffn_down_15 -> layer14_res2_15
	layer15_mha_qkv_8 [label="MHA QKV Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer15_mha_qkv_9 [label="MHA QKV Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer15_mha_qkv_10 [label="MHA QKV Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer15_mha_qkv_11 [label="MHA QKV Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer15_mha_qkv_12 [label="MHA QKV Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer15_mha_qkv_13 [label="MHA QKV Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer15_mha_qkv_14 [label="MHA QKV Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer15_mha_qkv_15 [label="MHA QKV Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128, width=1/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer15_mha_attn_15 [label="MHA Attention\nAll heads combined\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer15_mha_out_8 [label="MHA Output Proj\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer15_mha_out_9 [label="MHA Output Proj\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer15_mha_out_10 [label="MHA Output Proj\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer15_mha_out_11 [label="MHA Output Proj\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer15_mha_out_12 [label="MHA Output Proj\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer15_mha_out_13 [label="MHA Output Proj\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer15_mha_out_14 [label="MHA Output Proj\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer15_mha_out_15 [label="MHA Output Proj\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer15_res1_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer15_ln1_15 [label="Layer Norm 1\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" fillcolor=yellow style=filled]
	layer15_ffn_gate_8 [label="FFN Gate\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer15_ffn_gate_9 [label="FFN Gate\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer15_ffn_gate_10 [label="FFN Gate\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer15_ffn_gate_11 [label="FFN Gate\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer15_ffn_gate_12 [label="FFN Gate\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer15_ffn_gate_13 [label="FFN Gate\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer15_ffn_gate_14 [label="FFN Gate\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer15_ffn_gate_15 [label="FFN Gate\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer15_ffn_up_8 [label="FFN Up\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer15_ffn_up_9 [label="FFN Up\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer15_ffn_up_10 [label="FFN Up\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer15_ffn_up_11 [label="FFN Up\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer15_ffn_up_12 [label="FFN Up\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer15_ffn_up_13 [label="FFN Up\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer15_ffn_up_14 [label="FFN Up\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer15_ffn_up_15 [label="FFN Up\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer15_ffn_act_15 [label="FFN Activation\nSiLU(gate) * up\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nAll GPUs" fillcolor=lightgreen shape=parallelogram style=filled]
	layer15_ffn_down_8 [label="FFN Down\nTP Shard 0/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 8" fillcolor=lightblue style=filled]
	layer15_ffn_down_9 [label="FFN Down\nTP Shard 1/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 9" fillcolor=lightblue style=filled]
	layer15_ffn_down_10 [label="FFN Down\nTP Shard 2/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 10" fillcolor=lightblue style=filled]
	layer15_ffn_down_11 [label="FFN Down\nTP Shard 3/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 11" fillcolor=lightblue style=filled]
	layer15_ffn_down_12 [label="FFN Down\nTP Shard 4/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 12" fillcolor=lightblue style=filled]
	layer15_ffn_down_13 [label="FFN Down\nTP Shard 5/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 13" fillcolor=lightblue style=filled]
	layer15_ffn_down_14 [label="FFN Down\nTP Shard 6/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 14" fillcolor=lightblue style=filled]
	layer15_ffn_down_15 [label="FFN Down\nTP Shard 7/8\nInput: [batch_size=128, seq_len=10000, ffn_hidden=16384]\nOutput: [batch_size=128, seq_len=10000, hidden=4096/8]\nGPU 15" fillcolor=lightblue style=filled]
	layer15_res2_15 [label="Residual Add\nInput1: [batch_size=128, seq_len=10000, hidden=4096]\nInput2: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]\nAll GPUs" shape=parallelogram]
	layer14_res2_8 -> layer15_mha_qkv_8
	layer15_mha_qkv_8 -> layer15_mha_attn_8
	layer15_mha_attn_8 -> layer15_mha_out_8
	layer15_mha_out_8 -> layer15_res1_8
	layer15_res1_8 -> layer15_ln1_8
	layer15_ln1_8 -> layer15_ffn_gate_8
	layer15_ln1_8 -> layer15_ffn_up_8
	layer15_ffn_gate_8 -> layer15_ffn_act_8
	layer15_ffn_up_8 -> layer15_ffn_act_8
	layer15_ffn_act_8 -> layer15_ffn_down_8
	layer15_ffn_down_8 -> layer15_res2_8
	layer14_res2_9 -> layer15_mha_qkv_9
	layer15_mha_qkv_9 -> layer15_mha_attn_9
	layer15_mha_attn_9 -> layer15_mha_out_9
	layer15_mha_out_9 -> layer15_res1_9
	layer15_res1_9 -> layer15_ln1_9
	layer15_ln1_9 -> layer15_ffn_gate_9
	layer15_ln1_9 -> layer15_ffn_up_9
	layer15_ffn_gate_9 -> layer15_ffn_act_9
	layer15_ffn_up_9 -> layer15_ffn_act_9
	layer15_ffn_act_9 -> layer15_ffn_down_9
	layer15_ffn_down_9 -> layer15_res2_9
	layer14_res2_10 -> layer15_mha_qkv_10
	layer15_mha_qkv_10 -> layer15_mha_attn_10
	layer15_mha_attn_10 -> layer15_mha_out_10
	layer15_mha_out_10 -> layer15_res1_10
	layer15_res1_10 -> layer15_ln1_10
	layer15_ln1_10 -> layer15_ffn_gate_10
	layer15_ln1_10 -> layer15_ffn_up_10
	layer15_ffn_gate_10 -> layer15_ffn_act_10
	layer15_ffn_up_10 -> layer15_ffn_act_10
	layer15_ffn_act_10 -> layer15_ffn_down_10
	layer15_ffn_down_10 -> layer15_res2_10
	layer14_res2_11 -> layer15_mha_qkv_11
	layer15_mha_qkv_11 -> layer15_mha_attn_11
	layer15_mha_attn_11 -> layer15_mha_out_11
	layer15_mha_out_11 -> layer15_res1_11
	layer15_res1_11 -> layer15_ln1_11
	layer15_ln1_11 -> layer15_ffn_gate_11
	layer15_ln1_11 -> layer15_ffn_up_11
	layer15_ffn_gate_11 -> layer15_ffn_act_11
	layer15_ffn_up_11 -> layer15_ffn_act_11
	layer15_ffn_act_11 -> layer15_ffn_down_11
	layer15_ffn_down_11 -> layer15_res2_11
	layer14_res2_12 -> layer15_mha_qkv_12
	layer15_mha_qkv_12 -> layer15_mha_attn_12
	layer15_mha_attn_12 -> layer15_mha_out_12
	layer15_mha_out_12 -> layer15_res1_12
	layer15_res1_12 -> layer15_ln1_12
	layer15_ln1_12 -> layer15_ffn_gate_12
	layer15_ln1_12 -> layer15_ffn_up_12
	layer15_ffn_gate_12 -> layer15_ffn_act_12
	layer15_ffn_up_12 -> layer15_ffn_act_12
	layer15_ffn_act_12 -> layer15_ffn_down_12
	layer15_ffn_down_12 -> layer15_res2_12
	layer14_res2_13 -> layer15_mha_qkv_13
	layer15_mha_qkv_13 -> layer15_mha_attn_13
	layer15_mha_attn_13 -> layer15_mha_out_13
	layer15_mha_out_13 -> layer15_res1_13
	layer15_res1_13 -> layer15_ln1_13
	layer15_ln1_13 -> layer15_ffn_gate_13
	layer15_ln1_13 -> layer15_ffn_up_13
	layer15_ffn_gate_13 -> layer15_ffn_act_13
	layer15_ffn_up_13 -> layer15_ffn_act_13
	layer15_ffn_act_13 -> layer15_ffn_down_13
	layer15_ffn_down_13 -> layer15_res2_13
	layer14_res2_14 -> layer15_mha_qkv_14
	layer15_mha_qkv_14 -> layer15_mha_attn_14
	layer15_mha_attn_14 -> layer15_mha_out_14
	layer15_mha_out_14 -> layer15_res1_14
	layer15_res1_14 -> layer15_ln1_14
	layer15_ln1_14 -> layer15_ffn_gate_14
	layer15_ln1_14 -> layer15_ffn_up_14
	layer15_ffn_gate_14 -> layer15_ffn_act_14
	layer15_ffn_up_14 -> layer15_ffn_act_14
	layer15_ffn_act_14 -> layer15_ffn_down_14
	layer15_ffn_down_14 -> layer15_res2_14
	layer14_res2_15 -> layer15_mha_qkv_15
	layer15_mha_qkv_15 -> layer15_mha_attn_15
	layer15_mha_attn_15 -> layer15_mha_out_15
	layer15_mha_out_15 -> layer15_res1_15
	layer15_res1_15 -> layer15_ln1_15
	layer15_ln1_15 -> layer15_ffn_gate_15
	layer15_ln1_15 -> layer15_ffn_up_15
	layer15_ffn_gate_15 -> layer15_ffn_act_15
	layer15_ffn_up_15 -> layer15_ffn_act_15
	layer15_ffn_act_15 -> layer15_ffn_down_15
	layer15_ffn_down_15 -> layer15_res2_15
	output [label="Output\nOutput: [batch_size=128, seq_len=10000, hidden=4096]" shape=ellipse]
	input -> layer0_mha_qkv_0
	layer7_ffn2_7 -> stage0_to_stage1
	stage0_to_stage1 -> layer8_mha_qkv_8
	layer15_ffn2_15 -> output
}
