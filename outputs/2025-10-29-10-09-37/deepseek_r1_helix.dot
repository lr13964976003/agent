digraph DeepSeek_R1_Helix_Parallelism {
	graph [bb="0,0,7118.9,2511.1",
		rankdir=TB,
		size="30,20"
	];
	node [fillcolor=lightblue,
		label="\N",
		shape=rectangle,
		style=filled
	];
	input	[fillcolor=lightgreen,
		height=1.3356,
		label="Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: CPU",
		pos="2997,2463",
		shape=ellipse,
		width=8.5246];
	split_all_gpus	[fillcolor=yellow,
		height=1.8889,
		label="Split Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="2997,2310.9",
		shape=parallelogram,
		width=12.499];
	input -> split_all_gpus	[pos="e,2997,2379.1 2997,2414.5 2997,2406.4 2997,2397.9 2997,2389.2"];
	qkv_proj_gpu_0	[fillcolor=lightcoral,
		height=0.94444,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: \
gpu_0",
		pos="1001,2172.9",
		width=6.6806];
	split_all_gpus -> qkv_proj_gpu_0	[pos="e,1241.8,2206.1 2618.1,2295.6 2266.3,2280.3 1728,2252.3 1252,2207.1"];
	qkv_proj_gpu_1	[fillcolor=lightcoral,
		height=0.94444,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: \
gpu_1",
		pos="1500,2172.9",
		width=6.6806];
	split_all_gpus -> qkv_proj_gpu_1	[pos="e,1740.8,2206 2600.3,2282.2 2357.6,2264 2040.8,2238.1 1751.1,2207.1"];
	qkv_proj_gpu_2	[fillcolor=lightcoral,
		height=0.94444,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: \
gpu_2",
		pos="1999,2172.9",
		width=6.6806];
	split_all_gpus -> qkv_proj_gpu_2	[pos="e,2239.6,2206.7 2556.4,2249.9 2451.3,2235.6 2342.8,2220.8 2249.7,2208.1"];
	qkv_proj_gpu_3	[fillcolor=lightcoral,
		height=0.94444,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: \
gpu_3",
		pos="2498,2172.9",
		width=6.6806];
	split_all_gpus -> qkv_proj_gpu_3	[pos="e,2619.2,2207 2751.2,2242.9 2709,2231.4 2666.8,2219.9 2629.2,2209.7"];
	qkv_proj_gpu_4	[fillcolor=lightcoral,
		height=0.94444,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: \
gpu_4",
		pos="2997,2172.9",
		width=6.6806];
	split_all_gpus -> qkv_proj_gpu_4	[pos="e,2997,2206.9 2997,2242.7 2997,2234 2997,2225.2 2997,2217"];
	qkv_proj_gpu_5	[fillcolor=lightcoral,
		height=0.94444,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: \
gpu_5",
		pos="3496,2172.9",
		width=6.6806];
	split_all_gpus -> qkv_proj_gpu_5	[pos="e,3374.8,2207 3242.8,2242.9 3285,2231.4 3327.2,2219.9 3364.8,2209.7"];
	qkv_proj_gpu_6	[fillcolor=lightcoral,
		height=0.94444,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: \
gpu_6",
		pos="3995,2172.9",
		width=6.6806];
	split_all_gpus -> qkv_proj_gpu_6	[pos="e,3754.3,2206.7 3298.1,2268.9 3440.6,2249.5 3608.8,2226.6 3744.2,2208.1"];
	qkv_proj_gpu_7	[fillcolor=lightcoral,
		height=0.94444,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: \
gpu_7",
		pos="4494,2172.9",
		width=6.6806];
	split_all_gpus -> qkv_proj_gpu_7	[pos="e,4253.2,2205.9 3323,2287.4 3572.1,2269.1 3924.2,2241 4242.8,2207"];
	residual_attn	[fillcolor=lightgreen,
		height=1.6303,
		label="Residual Add\nAttention\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="3978,1268.2",
		shape=ellipse,
		width=8.5246];
	split_all_gpus -> residual_attn	[pos="e,4176.6,1313 3341.2,2300.8 3835.6,2286.1 4693.5,2253.9 4744,2206.9 4791.4,2162.8 4763,2127.2 4763,2062.4 4763,2062.4 4763,2062.4 \
4763,1605.4 4763,1479.8 4419.1,1373.2 4186.5,1315.5"];
	kv_cache_gpu_0	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: \
gpu_0\nTokens: 0-124999",
		pos="1014,2061.4",
		width=6.3056];
	qkv_proj_gpu_0 -> kv_cache_gpu_0	[pos="e,1009.2,2103.2 1004.9,2138.8 1005.9,2130.7 1006.9,2121.9 1008,2113.3"];
	kv_cache_gpu_1	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: \
gpu_1\nTokens: 125000-249999",
		pos="1508,2061.4",
		width=6.3056];
	qkv_proj_gpu_1 -> kv_cache_gpu_1	[pos="e,1505,2103.2 1502.4,2138.8 1503,2130.7 1503.7,2121.9 1504.3,2113.3"];
	kv_cache_gpu_2	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: \
gpu_2\nTokens: 250000-374999",
		pos="2007,2061.4",
		width=6.3056];
	qkv_proj_gpu_2 -> kv_cache_gpu_2	[pos="e,2004,2103.2 2001.4,2138.8 2002,2130.7 2002.7,2121.9 2003.3,2113.3"];
	kv_cache_gpu_3	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: \
gpu_3\nTokens: 375000-499999",
		pos="2499,2061.4",
		width=6.3056];
	qkv_proj_gpu_3 -> kv_cache_gpu_3	[pos="e,2498.6,2103.2 2498.3,2138.8 2498.4,2130.7 2498.5,2121.9 2498.5,2113.3"];
	kv_cache_gpu_4	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: \
gpu_4\nTokens: 500000-624999",
		pos="2996,2061.4",
		width=6.3056];
	qkv_proj_gpu_4 -> kv_cache_gpu_4	[pos="e,2996.4,2103.2 2996.7,2138.8 2996.6,2130.7 2996.5,2121.9 2996.5,2113.3"];
	kv_cache_gpu_5	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: \
gpu_5\nTokens: 625000-749999",
		pos="3493,2061.4",
		width=6.3056];
	qkv_proj_gpu_5 -> kv_cache_gpu_5	[pos="e,3494.1,2103.2 3495.1,2138.8 3494.9,2130.7 3494.6,2121.9 3494.4,2113.3"];
	kv_cache_gpu_6	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: \
gpu_6\nTokens: 750000-874999",
		pos="3991,2061.4",
		width=6.3056];
	qkv_proj_gpu_6 -> kv_cache_gpu_6	[pos="e,3992.5,2103.2 3993.8,2138.8 3993.5,2130.7 3993.2,2121.9 3992.9,2113.3"];
	kv_cache_gpu_7	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: \
gpu_7\nTokens: 875000-999999",
		pos="4487,2061.4",
		width=6.3056];
	qkv_proj_gpu_7 -> kv_cache_gpu_7	[pos="e,4489.6,2103.2 4491.9,2138.8 4491.4,2130.7 4490.8,2121.9 4490.2,2113.3"];
	flash_attn_gpu_0	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=\
128]\nDevice: gpu_0",
		pos="1018,1949.9",
		width=6.5556];
	kv_cache_gpu_0 -> flash_attn_gpu_0	[pos="e,1016.8,1984.2 1015.5,2019.7 1015.8,2011.4 1016.1,2002.8 1016.4,1994.5"];
	flash_attn_gpu_1	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=\
128]\nDevice: gpu_1",
		pos="1517,1949.9",
		width=6.5556];
	kv_cache_gpu_1 -> flash_attn_gpu_1	[pos="e,1514.3,1984.2 1511.4,2019.7 1512,2011.4 1512.8,2002.6 1513.4,1994.2"];
	flash_attn_gpu_2	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=\
128]\nDevice: gpu_2",
		pos="2011,1949.9",
		width=6.5556];
	kv_cache_gpu_2 -> flash_attn_gpu_2	[pos="e,2009.8,1984.2 2008.5,2019.7 2008.8,2011.4 2009.1,2002.8 2009.4,1994.5"];
	flash_attn_gpu_3	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=\
128]\nDevice: gpu_3",
		pos="2503,1949.9",
		width=6.5556];
	kv_cache_gpu_3 -> flash_attn_gpu_3	[pos="e,2501.8,1984.2 2500.5,2019.7 2500.8,2011.4 2501.1,2002.8 2501.4,1994.5"];
	flash_attn_gpu_4	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=\
128]\nDevice: gpu_4",
		pos="2994,1949.9",
		width=6.5556];
	kv_cache_gpu_4 -> flash_attn_gpu_4	[pos="e,2994.6,1984.2 2995.3,2019.7 2995.1,2011.4 2994.9,2002.8 2994.8,1994.5"];
	flash_attn_gpu_5	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=\
128]\nDevice: gpu_5",
		pos="3488,1949.9",
		width=6.5556];
	kv_cache_gpu_5 -> flash_attn_gpu_5	[pos="e,3489.5,1984.2 3491.1,2019.7 3490.8,2011.4 3490.4,2002.8 3490,1994.5"];
	flash_attn_gpu_6	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=\
128]\nDevice: gpu_6",
		pos="3986,1949.9",
		width=6.5556];
	kv_cache_gpu_6 -> flash_attn_gpu_6	[pos="e,3987.5,1984.2 3989.1,2019.7 3988.8,2011.4 3988.4,2002.8 3988,1994.5"];
	flash_attn_gpu_7	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=\
128]\nDevice: gpu_7",
		pos="4483,1949.9",
		width=6.5556];
	kv_cache_gpu_7 -> flash_attn_gpu_7	[pos="e,4484.2,1984.2 4485.5,2019.7 4485.2,2011.4 4484.9,2002.8 4484.6,1994.5"];
	all2all	[fillcolor=yellow,
		height=2.7222,
		label="All-to-All\nExchange Query Heads\nInput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, \
heads=128, d_k=128]\nVolume: 1×128×128×0.5 bytes\nDevice: all GPUs",
		pos="2872,1781.9",
		shape=parallelogram,
		width=13.852];
	flash_attn_gpu_0 -> all2all	[pos="e,2503,1808.7 1254.2,1917.9 1260.2,1917.2 1266.1,1916.5 1272,1915.9 1689,1871.5 2168.4,1833.4 2492.9,1809.5"];
	flash_attn_gpu_1 -> all2all	[pos="e,2519.1,1824.3 1753.2,1917.5 1757.5,1917 1761.8,1916.5 1766,1915.9 2013.8,1884.6 2293,1851 2508.9,1825.5"];
	flash_attn_gpu_2 -> all2all	[pos="e,2542.3,1846.5 2182.4,1915.9 2282.5,1896.6 2412.3,1871.6 2532.2,1848.4"];
	flash_attn_gpu_3 -> all2all	[pos="e,2656.1,1880 2576.8,1915.7 2597.8,1906.3 2621.8,1895.5 2646.8,1884.2"];
	flash_attn_gpu_4 -> all2all	[pos="e,2943.4,1880 2969.6,1915.7 2963.5,1907.5 2956.7,1898.1 2949.5,1888.3"];
	flash_attn_gpu_5 -> all2all	[pos="e,3232.1,1880 3365.2,1915.8 3328.4,1905.9 3286,1894.5 3241.9,1882.6"];
	flash_attn_gpu_6 -> all2all	[pos="e,3343.2,1853.1 3764.2,1915.9 3646.3,1898.3 3496.5,1876 3353.5,1854.7"];
	flash_attn_gpu_7 -> all2all	[pos="e,3310.8,1822.2 4246.9,1917.8 4241.6,1917.1 4236.3,1916.5 4231,1915.9 3927.8,1881.2 3586,1847.8 3320.9,1823.1"];
	attn_out_gpu_0	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_0\nRows: 0-2047",
		pos="1421,1606.4",
		width=5.9028];
	all2all -> attn_out_gpu_0	[pos="e,1633.6,1646.7 2432.7,1741.3 2200.6,1718.4 1910.9,1686.8 1643.8,1648.2"];
	attn_out_gpu_1	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_1\nRows: 2048-4095",
		pos="1864,1606.4",
		width=5.9028];
	all2all -> attn_out_gpu_1	[pos="e,2076.6,1646.4 2390.3,1700.6 2292.6,1684 2190.3,1666.4 2086.6,1648.2"];
	attn_out_gpu_2	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_2\nRows: 4096-6143",
		pos="2307,1606.4",
		width=5.9028];
	all2all -> attn_out_gpu_2	[pos="e,2439,1648 2556.1,1683.9 2518.6,1672.4 2481.9,1661.1 2448.8,1651"];
	attn_out_gpu_3	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_3\nRows: 6144-8191",
		pos="2750,1606.4",
		width=5.9028];
	all2all -> attn_out_gpu_3	[pos="e,2778.6,1648.1 2803.7,1683.8 2797.1,1674.4 2790.6,1665.1 2784.5,1656.5"];
	attn_out_gpu_4	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_4\nRows: 8192-10239",
		pos="3193,1606.4",
		width=5.9028];
	all2all -> attn_out_gpu_4	[pos="e,3117.8,1648.1 3051.7,1683.8 3071.5,1673.1 3090.9,1662.6 3108.6,1653"];
	attn_out_gpu_5	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_5\nRows: 10240-12287",
		pos="3636,1606.4",
		width=5.9028];
	all2all -> attn_out_gpu_5	[pos="e,3457.4,1648 3192.1,1708.2 3279.1,1688.5 3370.5,1667.7 3447.6,1650.2"];
	attn_out_gpu_6	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_6\nRows: 12288-14335",
		pos="4079,1606.4",
		width=5.9028];
	all2all -> attn_out_gpu_6	[pos="e,3866.4,1646.4 3222.7,1737.6 3408.2,1713.6 3639.9,1682.2 3856.4,1648"];
	attn_out_gpu_7	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_7\nRows: 14336-16383",
		pos="4522,1606.4",
		width=5.9028];
	all2all -> attn_out_gpu_7	[pos="e,4309.4,1646.6 3241.4,1755.5 3527.7,1733.6 3934.3,1697.8 4299.4,1648"];
	attn_all_reduce	[fillcolor=yellow,
		height=2.3056,
		label="All-Reduce\nAttention Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=\
16384]\nDevice: all GPUs",
		pos="3193,1445.9",
		shape=parallelogram,
		width=12.499];
	attn_out_gpu_0 -> attn_all_reduce	[pos="e,2853.5,1462.4 1633.5,1566 1636.4,1565.7 1639.2,1565.3 1642,1564.9 2052.8,1511.8 2528.2,1480 2843.3,1463"];
	attn_out_gpu_1 -> attn_all_reduce	[pos="e,2867.4,1475.1 2076.6,1566.2 2079.4,1565.7 2082.2,1565.3 2085,1564.9 2343.3,1527.6 2637.1,1496.8 2857.3,1476.1"];
	attn_out_gpu_2 -> attn_all_reduce	[pos="e,2894.2,1499.6 2519.6,1566.4 2522.4,1565.9 2525.2,1565.4 2528,1564.9 2644.2,1544 2771.9,1521.2 2884.1,1501.4"];
	attn_out_gpu_3 -> attn_all_reduce	[pos="e,2963.6,1529 2863.2,1564.9 2891.2,1554.9 2922.3,1543.8 2953.8,1532.5"];
	attn_out_gpu_4 -> attn_all_reduce	[pos="e,3193,1529.2 3193,1564.6 3193,1556.7 3193,1548.1 3193,1539.2"];
	attn_out_gpu_5 -> attn_all_reduce	[pos="e,3422.4,1529 3522.8,1564.9 3494.8,1554.9 3463.7,1543.8 3432.2,1532.5"];
	attn_out_gpu_6 -> attn_all_reduce	[pos="e,3640,1526.1 3866.3,1566.6 3863.2,1566 3860.1,1565.5 3857,1564.9 3790.1,1552.9 3719.5,1540.2 3650.2,1527.9"];
	attn_out_gpu_7 -> attn_all_reduce	[pos="e,3591.3,1482.2 4309.4,1566.3 4306.2,1565.8 4303.1,1565.4 4300,1564.9 4069.6,1531.7 3810.9,1503.6 3601.5,1483.2"];
	attn_all_reduce -> residual_attn	[pos="e,3781.1,1313.3 3478.9,1380.9 3575.6,1359.3 3681.7,1335.5 3771.1,1315.5"];
	rmsnorm1	[fillcolor=lightgray,
		height=0.94444,
		label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all \
GPUs",
		pos="3978,1139.5",
		width=6.0278];
	residual_attn -> rmsnorm1	[pos="e,3978,1173.7 3978,1209.5 3978,1200.8 3978,1192.1 3978,1183.8"];
	expert_gate_gpu_0	[fillcolor=orange,
		height=2.3056,
		label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
gpu_0\nExperts: 224-255",
		pos="434,986.55",
		shape=diamond,
		style=dashed,
		width=12.056];
	rmsnorm1 -> expert_gate_gpu_0	[pos="e,633.42,1031.5 3760.8,1138.3 3152,1136.7 1437.2,1127.2 877,1069.5 800.21,1061.6 716.79,1047.6 643.26,1033.4"];
	expert_gate_gpu_1	[fillcolor=orange,
		height=2.3056,
		label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
gpu_1\nExperts: 224-255",
		pos="1320,986.55",
		shape=diamond,
		style=dashed,
		width=12.056];
	rmsnorm1 -> expert_gate_gpu_1	[pos="e,1520.3,1031.3 3760.9,1136 3281.6,1129.6 2145.4,1110.7 1763,1069.5 1686.5,1061.3 1603.4,1047.3 1530.1,1033.2"];
	expert_gate_gpu_2	[fillcolor=orange,
		height=2.3056,
		label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
gpu_2\nExperts: 224-255",
		pos="2206,986.55",
		shape=diamond,
		style=dashed,
		width=12.056];
	rmsnorm1 -> expert_gate_gpu_2	[pos="e,2408.6,1030.8 3760.9,1136 3497.1,1130.9 3039.1,1115.6 2649,1069.5 2573.5,1060.6 2491.5,1046.7 2418.8,1032.8"];
	expert_gate_gpu_3	[fillcolor=orange,
		height=2.3056,
		label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
gpu_3\nExperts: 224-255",
		pos="3092,986.55",
		shape=diamond,
		style=dashed,
		width=12.056];
	rmsnorm1 -> expert_gate_gpu_3	[pos="e,3306.7,1028.6 3760.9,1106 3689,1094.9 3608.5,1082.1 3535,1069.5 3464,1057.4 3386.7,1043.4 3316.9,1030.5"];
	expert_gate_gpu_4	[fillcolor=orange,
		height=2.3056,
		label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
gpu_4\nExperts: 224-255",
		pos="3978,986.55",
		shape=diamond,
		style=dashed,
		width=12.056];
	rmsnorm1 -> expert_gate_gpu_4	[pos="e,3978,1069.8 3978,1105.4 3978,1097.7 3978,1089 3978,1080"];
	expert_gate_gpu_5	[fillcolor=orange,
		height=2.3056,
		label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
gpu_5\nExperts: 224-255",
		pos="4864,986.55",
		shape=diamond,
		style=dashed,
		width=12.056];
	rmsnorm1 -> expert_gate_gpu_5	[pos="e,4649.3,1028.6 4195.1,1106 4267,1094.9 4347.5,1082.1 4421,1069.5 4492,1057.4 4569.3,1043.4 4639.1,1030.5"];
	expert_gate_gpu_6	[fillcolor=orange,
		height=2.3056,
		label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
gpu_6\nExperts: 224-255",
		pos="5750,986.55",
		shape=diamond,
		style=dashed,
		width=12.056];
	rmsnorm1 -> expert_gate_gpu_6	[pos="e,5547.4,1030.8 4195.1,1136 4458.9,1130.9 4916.9,1115.6 5307,1069.5 5382.5,1060.6 5464.5,1046.7 5537.2,1032.8"];
	expert_gate_gpu_7	[fillcolor=orange,
		height=2.3056,
		label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
gpu_7\nExperts: 224-255",
		pos="6636,986.55",
		shape=diamond,
		style=dashed,
		width=12.056];
	rmsnorm1 -> expert_gate_gpu_7	[pos="e,6435.7,1031.3 4195.1,1136 4674.4,1129.6 5810.6,1110.7 6193,1069.5 6269.5,1061.3 6352.6,1047.3 6425.9,1033.2"];
	residual_ffn	[fillcolor=lightgreen,
		height=1.6303,
		label="Residual Add\nFFN\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="5538,294.86",
		shape=ellipse,
		width=8.5246];
	rmsnorm1 -> residual_ffn	[pos="e,5844.3,298.96 4195.3,1137.6 4891.7,1134 7026.6,1119 7079,1069.5 7155.2,997.65 7098,939.33 7098,834.55 7098,834.55 7098,834.55 \
7098,624.55 7098,372.8 6310.3,312.86 5854.4,299.26"];
	expert_fc1_gpu_0	[height=0.94444,
		label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
0",
		pos="651,833.55",
		width=5.8611];
	expert_gate_gpu_0 -> expert_fc1_gpu_0	[pos="e,603.5,867.6 526.66,921.07 549.94,904.87 574.23,887.97 595.09,873.45"];
	expert_fc1_gpu_1	[height=0.94444,
		label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
1",
		pos="1438,833.55",
		width=5.8611];
	expert_gate_gpu_1 -> expert_fc1_gpu_1	[pos="e,1412,867.79 1375.9,914.03 1386.2,900.9 1396.5,887.6 1405.8,875.75"];
	expert_fc1_gpu_2	[height=0.94444,
		label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
2",
		pos="2314,833.55",
		width=5.8611];
	expert_gate_gpu_2 -> expert_fc1_gpu_2	[pos="e,2290.3,867.73 2257.8,913.14 2266.9,900.35 2276.2,887.44 2284.4,875.89"];
	expert_fc1_gpu_3	[height=0.94444,
		label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
3",
		pos="3146,833.55",
		width=5.8611];
	expert_gate_gpu_3 -> expert_fc1_gpu_3	[pos="e,3134.1,867.74 3119.5,908.72 3123.3,897.9 3127.2,887.12 3130.7,877.26"];
	expert_fc1_gpu_4	[height=0.94444,
		label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
4",
		pos="3978,833.55",
		width=5.8611];
	expert_gate_gpu_4 -> expert_fc1_gpu_4	[pos="e,3978,867.81 3978,903.42 3978,894.66 3978,886.03 3978,877.99"];
	expert_fc1_gpu_5	[height=0.94444,
		label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
5",
		pos="4647,833.55",
		width=5.8611];
	expert_gate_gpu_5 -> expert_fc1_gpu_5	[pos="e,4694.5,867.6 4771.3,921.07 4748.1,904.87 4723.8,887.97 4702.9,873.45"];
	expert_fc1_gpu_6	[height=0.94444,
		label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
6",
		pos="5425,833.55",
		width=5.8611];
	expert_gate_gpu_6 -> expert_fc1_gpu_6	[pos="e,5496.1,867.56 5624.8,927.38 5584.8,908.77 5541.5,888.68 5505.5,871.96"];
	expert_fc1_gpu_7	[height=0.94444,
		label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
7",
		pos="6257,833.55",
		width=5.8611];
	expert_gate_gpu_7 -> expert_fc1_gpu_7	[pos="e,6339.9,867.57 6496.8,930.07 6447.6,910.47 6393.5,888.94 6349.2,871.28"];
	expert_act_gpu_0	[height=0.94444,
		label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
0",
		pos="1160,729.55",
		width=5.4861];
	expert_fc1_gpu_0 -> expert_act_gpu_0	[pos="e,994.92,763.63 815.91,799.5 870.01,788.66 930.3,776.58 985.11,765.59"];
	expert_act_gpu_1	[height=0.94444,
		label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
1",
		pos="1674,729.55",
		width=5.4861];
	expert_fc1_gpu_1 -> expert_act_gpu_1	[pos="e,1597.4,763.67 1514.6,799.43 1538,789.31 1563.9,778.12 1587.9,767.75"];
	expert_act_gpu_2	[height=0.94444,
		label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
2",
		pos="2531,729.55",
		width=5.4861];
	expert_fc1_gpu_2 -> expert_act_gpu_2	[pos="e,2460.5,763.67 2384.5,799.43 2405.8,789.4 2429.4,778.32 2451.3,768.03"];
	expert_act_gpu_3	[height=0.94444,
		label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
3",
		pos="3254,729.55",
		width=5.4861];
	expert_fc1_gpu_3 -> expert_act_gpu_3	[pos="e,3218.7,763.85 3181.1,799.43 3190.7,790.34 3201.2,780.39 3211.2,770.95"];
	expert_act_gpu_4	[height=0.94444,
		label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
4",
		pos="3978,729.55",
		width=5.4861];
	expert_fc1_gpu_4 -> expert_act_gpu_4	[pos="e,3978,763.85 3978,799.43 3978,791.32 3978,782.53 3978,774.02"];
	expert_act_gpu_5	[height=0.94444,
		label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
5",
		pos="4539,729.55",
		width=5.4861];
	expert_fc1_gpu_5 -> expert_act_gpu_5	[pos="e,4574.3,763.85 4611.9,799.43 4602.3,790.34 4591.8,780.39 4581.8,770.95"];
	expert_act_gpu_6	[height=0.94444,
		label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
6",
		pos="5208,729.55",
		width=5.4861];
	expert_fc1_gpu_6 -> expert_act_gpu_6	[pos="e,5278.5,763.67 5354.5,799.43 5333.2,789.4 5309.6,778.32 5287.7,768.03"];
	expert_act_gpu_7	[height=0.94444,
		label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_\
7",
		pos="5959,729.55",
		width=5.4861];
	expert_fc1_gpu_7 -> expert_act_gpu_7	[pos="e,6055.5,763.58 6160.2,799.43 6129.8,789.02 6096.1,777.47 6065.1,766.85"];
	expert_fc2_gpu_0	[height=0.94444,
		label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_\
0",
		pos="1268,625.55",
		width=6.0278];
	expert_act_gpu_0 -> expert_fc2_gpu_0	[pos="e,1232.7,659.85 1195.1,695.43 1204.7,686.34 1215.2,676.39 1225.2,666.95"];
	expert_fc2_gpu_1	[height=0.94444,
		label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_\
1",
		pos="1774,625.55",
		width=6.0278];
	expert_act_gpu_1 -> expert_fc2_gpu_1	[pos="e,1741.4,659.85 1706.5,695.43 1715.3,686.43 1724.9,676.59 1734.1,667.22"];
	expert_fc2_gpu_2	[height=0.94444,
		label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_\
2",
		pos="2748,625.55",
		width=6.0278];
	expert_act_gpu_2 -> expert_fc2_gpu_2	[pos="e,2677.5,659.67 2601.5,695.43 2622.8,685.4 2646.4,674.32 2668.3,664.03"];
	expert_fc2_gpu_3	[height=0.94444,
		label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_\
3",
		pos="3471,625.55",
		width=6.0278];
	expert_act_gpu_3 -> expert_fc2_gpu_3	[pos="e,3400.5,659.67 3324.5,695.43 3345.8,685.4 3369.4,674.32 3391.3,664.03"];
	expert_fc2_gpu_4	[height=0.94444,
		label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_\
4",
		pos="3978,625.55",
		width=6.0278];
	expert_act_gpu_4 -> expert_fc2_gpu_4	[pos="e,3978,659.85 3978,695.43 3978,687.32 3978,678.53 3978,670.02"];
	expert_fc2_gpu_5	[height=0.94444,
		label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_\
5",
		pos="4485,625.55",
		width=6.0278];
	expert_act_gpu_5 -> expert_fc2_gpu_5	[pos="e,4502.6,659.85 4521.5,695.43 4517,686.97 4512.1,677.75 4507.4,668.9"];
	expert_fc2_gpu_6	[height=0.94444,
		label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_\
6",
		pos="5073,625.55",
		width=6.0278];
	expert_act_gpu_6 -> expert_fc2_gpu_6	[pos="e,5117.1,659.85 5164.2,695.43 5151.8,686.08 5138.2,675.81 5125.4,666.11"];
	expert_fc2_gpu_7	[height=0.94444,
		label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_\
7",
		pos="5742,625.55",
		width=6.0278];
	expert_act_gpu_7 -> expert_fc2_gpu_7	[pos="e,5812.5,659.67 5888.5,695.43 5867.2,685.4 5843.6,674.32 5821.7,664.03"];
	ffn_all_reduce	[fillcolor=yellow,
		height=2.3056,
		label="All-Reduce\nFFN Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="3978,472.55",
		shape=parallelogram,
		width=12.499];
	expert_fc2_gpu_0 -> ffn_all_reduce	[pos="e,3631.4,482.88 1485.3,597.45 1506.5,595.26 1527.6,593.24 1548,591.55 2283.6,530.41 3147.2,497.76 3621.4,483.19"];
	expert_fc2_gpu_1 -> ffn_all_reduce	[pos="e,3646.1,496.28 1991.1,609.67 2375.2,583.36 3177.1,528.42 3636,496.98"];
	expert_fc2_gpu_2 -> ffn_all_reduce	[pos="e,3663.8,512.11 2965.1,597.9 3155.9,574.47 3436.4,540.03 3653.9,513.34"];
	expert_fc2_gpu_3 -> ffn_all_reduce	[pos="e,3709.6,553.48 3581.9,591.5 3617.3,580.97 3658.1,568.81 3699.9,556.38"];
	expert_fc2_gpu_4 -> ffn_all_reduce	[pos="e,3978,555.78 3978,591.41 3978,583.65 3978,575.01 3978,566.03"];
	expert_fc2_gpu_5 -> ffn_all_reduce	[pos="e,4253.4,555.55 4374.1,591.5 4340.7,581.58 4302.6,570.21 4263.3,558.53"];
	expert_fc2_gpu_6 -> ffn_all_reduce	[pos="e,4401.9,532 4855.7,594.58 4728.8,577.09 4564.3,554.39 4411.9,533.39"];
	expert_fc2_gpu_7 -> ffn_all_reduce	[pos="e,4374.9,507.52 5524.8,605.96 5241.5,581.7 4740.1,538.78 4385.1,508.39"];
	ffn_all_reduce -> residual_ffn	[pos="e,5275.9,325.38 4295.6,435.78 4580.9,403.65 4995.5,356.96 5265.9,326.5"];
	rmsnorm2	[fillcolor=lightgray,
		height=0.94444,
		label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all \
GPUs",
		pos="5538,166.17",
		width=6.0278];
	residual_ffn -> rmsnorm2	[pos="e,5538,200.28 5538,236.07 5538,227.45 5538,218.68 5538,210.42"];
	output	[fillcolor=lightgreen,
		height=1.3356,
		label="Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, vocab_size=32000]\nDevice: \
CPU",
		pos="5538,48.083",
		shape=ellipse,
		width=9.0549];
	rmsnorm2 -> output	[pos="e,5538,96.193 5538,131.9 5538,123.91 5538,115.14 5538,106.4"];
}
