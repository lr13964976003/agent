digraph Llama_405B_Helix_Parallelism {
	graph [bb="0,0,9527.5,2586.1",
		rankdir=TB,
		size="40,30"
	];
	node [fillcolor=lightblue,
		label="\N",
		shape=rectangle,
		style=filled
	];
	input	[fillcolor=lightgreen,
		height=1.3356,
		label="Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: CPU",
		pos="4156,2538",
		shape=ellipse,
		width=8.5246];
	split_all_gpus	[fillcolor=yellow,
		height=1.8889,
		label="Split Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="4156,2385.9",
		shape=parallelogram,
		width=12.499];
	input -> split_all_gpus	[pos="e,4156,2454.1 4156,2489.5 4156,2481.4 4156,2472.9 4156,2464.2"];
	qkv_proj_gpu_0	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_0\nQueries: 0-31, KV: 0-1",
		pos="236,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_0	[pos="e,472.26,2281.1 3791.3,2381 2997.8,2371.5 1134.2,2343.2 482.24,2282"];
	qkv_proj_gpu_1	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_1\nQueries: 0-31, KV: 0-1",
		pos="726,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_1	[pos="e,962.26,2281 3788.7,2379.1 3075.7,2366.9 1526.3,2335.1 972.35,2282"];
	qkv_proj_gpu_2	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_2\nQueries: 0-31, KV: 0-1",
		pos="1216,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_2	[pos="e,1452.3,2281 3797.5,2385.5 3282.3,2383.1 2302.8,2366.6 1462.5,2282"];
	qkv_proj_gpu_3	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_3\nQueries: 0-31, KV: 0-1",
		pos="1706,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_3	[pos="e,1942.3,2281 3791.2,2381 3355.3,2373.2 2604.3,2350.4 1952.3,2282"];
	qkv_proj_gpu_4	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_4\nQueries: 32-63, KV: 2-3",
		pos="2196,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_4	[pos="e,2432.3,2280.9 3781.2,2373.6 3435.6,2360.6 2908.3,2334 2442.2,2282"];
	qkv_proj_gpu_5	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_5\nQueries: 32-63, KV: 2-3",
		pos="2686,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_5	[pos="e,2922.3,2280.8 3762.8,2359.9 3524.9,2342.8 3215.5,2317 2932.3,2282.1"];
	qkv_proj_gpu_6	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_6\nQueries: 32-63, KV: 2-3",
		pos="3176,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_6	[pos="e,3412.3,2280.6 3715.6,2325 3621.6,2311.7 3522.8,2297.4 3422.4,2282.1"];
	qkv_proj_gpu_7	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_7\nQueries: 32-63, KV: 2-3",
		pos="3666,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_7	[pos="e,3804.5,2282 3927.1,2317.9 3888.9,2306.7 3850,2295.3 3814.5,2284.9"];
	qkv_proj_gpu_8	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_8\nQueries: 64-95, KV: 4-5",
		pos="4156,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_8	[pos="e,4156,2281.9 4156,2317.8 4156,2309.2 4156,2300.5 4156,2292.2"];
	qkv_proj_gpu_9	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_9\nQueries: 64-95, KV: 4-5",
		pos="4646,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_9	[pos="e,4507.5,2282 4384.9,2317.9 4423.1,2306.7 4462,2295.3 4497.5,2284.9"];
	qkv_proj_gpu_10	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_10\nQueries: 64-95, KV: 4-5",
		pos="5136,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_10	[pos="e,4899.7,2280.6 4458,2344.3 4587.9,2326.4 4741.7,2304.6 4889.5,2282.1"];
	qkv_proj_gpu_11	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_11\nQueries: 64-95, KV: 4-5",
		pos="5626,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_11	[pos="e,5389.7,2280.8 4484.9,2364.4 4729.4,2347.5 5071,2320.2 5379.7,2282.1"];
	qkv_proj_gpu_12	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_12\nQueries: 96-127, KV: 6-7",
		pos="6116,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_12	[pos="e,5879.7,2280.9 4499,2374.8 4843.4,2362.2 5389.1,2335.6 5869.5,2282.1"];
	qkv_proj_gpu_13	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_13\nQueries: 96-127, KV: 6-7",
		pos="6606,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_13	[pos="e,6369.7,2281 4507.8,2381.2 4941.6,2373.7 5701.2,2351 6359.6,2282"];
	qkv_proj_gpu_14	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_14\nQueries: 96-127, KV: 6-7",
		pos="7096,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_14	[pos="e,6859.7,2281 4513.4,2385.5 5028.3,2383.2 6008.6,2366.6 6849.5,2282"];
	qkv_proj_gpu_15	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: \
gpu_15\nQueries: 96-127, KV: 6-7",
		pos="7586,2240.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_15	[pos="e,7349.7,2281 4505.2,2379.4 5209.7,2367.4 6780.9,2335.5 7339.6,2282"];
	residual_attn	[fillcolor=lightgreen,
		height=1.6303,
		label="Residual Add\nAttention\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="6179,1096.2",
		shape=ellipse,
		width=8.5246];
	split_all_gpus -> residual_attn	[pos="e,6466.1,1117.1 4507.4,2381.1 5414.8,2370.4 7770.2,2337.5 7831,2281.9 7883.7,2233.8 7850,2193.8 7850,2122.4 7850,2122.4 7850,2122.4 \
7850,1433.4 7850,1413.5 7846.8,1405 7831,1392.9 7623.3,1233.5 6893,1152.2 6476.2,1117.9"];
	kv_cache_gpu_0	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_0\nSequence: 0-249999",
		pos="245,2121.4",
		width=6.1806];
	qkv_proj_gpu_0 -> kv_cache_gpu_0	[pos="e,241.88,2162.9 239.12,2198.8 239.76,2190.5 240.45,2181.6 241.11,2172.9"];
	kv_cache_gpu_1	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_1\nSequence: 0-249999",
		pos="740,2121.4",
		width=6.1806];
	qkv_proj_gpu_1 -> kv_cache_gpu_1	[pos="e,735.15,2162.9 730.86,2198.8 731.86,2190.5 732.92,2181.6 733.95,2172.9"];
	kv_cache_gpu_2	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_2\nSequence: 0-249999",
		pos="1224,2121.4",
		width=6.1806];
	qkv_proj_gpu_2 -> kv_cache_gpu_2	[pos="e,1221.2,2162.9 1218.8,2198.8 1219.3,2190.5 1220,2181.6 1220.5,2172.9"];
	kv_cache_gpu_3	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_3\nSequence: 0-249999",
		pos="1713,2121.4",
		width=6.1806];
	qkv_proj_gpu_3 -> kv_cache_gpu_3	[pos="e,1710.6,2162.9 1708.4,2198.8 1708.9,2190.5 1709.5,2181.6 1710,2172.9"];
	kv_cache_gpu_4	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_4\nSequence: 250000-499999",
		pos="2214,2121.4",
		width=6.1806];
	qkv_proj_gpu_4 -> kv_cache_gpu_4	[pos="e,2207.8,2162.9 2202.2,2198.8 2203.5,2190.5 2204.9,2181.6 2206.2,2172.9"];
	kv_cache_gpu_5	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_5\nSequence: 250000-499999",
		pos="2702,2121.4",
		width=6.1806];
	qkv_proj_gpu_5 -> kv_cache_gpu_5	[pos="e,2696.5,2162.9 2691.6,2198.8 2692.7,2190.5 2693.9,2181.6 2695.1,2172.9"];
	kv_cache_gpu_6	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_6\nSequence: 250000-499999",
		pos="3178,2121.4",
		width=6.1806];
	qkv_proj_gpu_6 -> kv_cache_gpu_6	[pos="e,3177.3,2162.9 3176.7,2198.8 3176.8,2190.5 3177,2181.6 3177.1,2172.9"];
	kv_cache_gpu_7	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_7\nSequence: 250000-499999",
		pos="3662,2121.4",
		width=6.1806];
	qkv_proj_gpu_7 -> kv_cache_gpu_7	[pos="e,3663.4,2162.9 3664.6,2198.8 3664.3,2190.5 3664,2181.6 3663.7,2172.9"];
	kv_cache_gpu_8	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_8\nSequence: 500000-749999",
		pos="4158,2121.4",
		width=6.1806];
	qkv_proj_gpu_8 -> kv_cache_gpu_8	[pos="e,4157.3,2162.9 4156.7,2198.8 4156.8,2190.5 4157,2181.6 4157.1,2172.9"];
	kv_cache_gpu_9	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_9\nSequence: 500000-749999",
		pos="4642,2121.4",
		width=6.1806];
	qkv_proj_gpu_9 -> kv_cache_gpu_9	[pos="e,4643.4,2162.9 4644.6,2198.8 4644.3,2190.5 4644,2181.6 4643.7,2172.9"];
	kv_cache_gpu_10	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_10\nSequence: 500000-749999",
		pos="5131,2121.4",
		width=6.1806];
	qkv_proj_gpu_10 -> kv_cache_gpu_10	[pos="e,5132.7,2162.9 5134.3,2198.8 5133.9,2190.5 5133.5,2181.6 5133.2,2172.9"];
	kv_cache_gpu_11	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_11\nSequence: 500000-749999",
		pos="5610,2121.4",
		width=6.1806];
	qkv_proj_gpu_11 -> kv_cache_gpu_11	[pos="e,5615.5,2162.9 5620.4,2198.8 5619.3,2190.5 5618.1,2181.6 5616.9,2172.9"];
	kv_cache_gpu_12	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_12\nSequence: 750000-999999",
		pos="6099,2121.4",
		width=6.1806];
	qkv_proj_gpu_12 -> kv_cache_gpu_12	[pos="e,6104.9,2162.9 6110.1,2198.8 6108.9,2190.5 6107.6,2181.6 6106.3,2172.9"];
	kv_cache_gpu_13	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_13\nSequence: 750000-999999",
		pos="6589,2121.4",
		width=6.1806];
	qkv_proj_gpu_13 -> kv_cache_gpu_13	[pos="e,6594.9,2162.9 6600.1,2198.8 6598.9,2190.5 6597.6,2181.6 6596.3,2172.9"];
	kv_cache_gpu_14	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_14\nSequence: 750000-999999",
		pos="7083,2121.4",
		width=6.1806];
	qkv_proj_gpu_14 -> kv_cache_gpu_14	[pos="e,7087.5,2162.9 7091.5,2198.8 7090.6,2190.5 7089.6,2181.6 7088.6,2172.9"];
	kv_cache_gpu_15	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: \
gpu_15\nSequence: 750000-999999",
		pos="7566,2121.4",
		width=6.1806];
	qkv_proj_gpu_15 -> kv_cache_gpu_15	[pos="e,7572.9,2162.9 7579.1,2198.8 7577.6,2190.5 7576.1,2181.6 7574.6,2172.9"];
	flash_attn_gpu_0	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_0",
		pos="273,2009.9",
		width=6.3056];
	kv_cache_gpu_0 -> flash_attn_gpu_0	[pos="e,264.49,2044.2 255.43,2079.7 257.57,2071.3 259.84,2062.4 262,2053.9"];
	flash_attn_gpu_1	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_1",
		pos="758,2009.9",
		width=6.3056];
	kv_cache_gpu_1 -> flash_attn_gpu_1	[pos="e,752.53,2044.2 746.7,2079.7 748.07,2071.4 749.51,2062.6 750.88,2054.2"];
	flash_attn_gpu_2	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_2",
		pos="1238,2009.9",
		width=6.3056];
	kv_cache_gpu_2 -> flash_attn_gpu_2	[pos="e,1233.7,2044.2 1229.2,2079.7 1230.3,2071.4 1231.4,2062.6 1232.5,2054.2"];
	flash_attn_gpu_3	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_3",
		pos="1717,2009.9",
		width=6.3056];
	kv_cache_gpu_3 -> flash_attn_gpu_3	[pos="e,1715.8,2044.2 1714.5,2079.7 1714.8,2071.4 1715.1,2062.8 1715.4,2054.5"];
	flash_attn_gpu_4	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_4",
		pos="2224,2009.9",
		width=6.3056];
	kv_cache_gpu_4 -> flash_attn_gpu_4	[pos="e,2221,2044.2 2217.7,2079.7 2218.5,2071.4 2219.3,2062.6 2220,2054.2"];
	flash_attn_gpu_5	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_5",
		pos="2706,2009.9",
		width=6.3056];
	kv_cache_gpu_5 -> flash_attn_gpu_5	[pos="e,2704.8,2044.2 2703.5,2079.7 2703.8,2071.4 2704.1,2062.8 2704.4,2054.5"];
	flash_attn_gpu_6	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_6",
		pos="3180,2009.9",
		width=6.3056];
	kv_cache_gpu_6 -> flash_attn_gpu_6	[pos="e,3179.4,2044.2 3178.7,2079.7 3178.9,2071.4 3179.1,2062.8 3179.2,2054.5"];
	flash_attn_gpu_7	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_7",
		pos="3661,2009.9",
		width=6.3056];
	kv_cache_gpu_7 -> flash_attn_gpu_7	[pos="e,3661.3,2044.2 3661.6,2079.7 3661.6,2071.4 3661.5,2062.8 3661.4,2054.5"];
	flash_attn_gpu_8	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_8",
		pos="4167,2009.9",
		width=6.3056];
	kv_cache_gpu_8 -> flash_attn_gpu_8	[pos="e,4164.3,2044.2 4161.4,2079.7 4162,2071.4 4162.8,2062.6 4163.4,2054.2"];
	flash_attn_gpu_9	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_9",
		pos="4641,2009.9",
		width=6.3056];
	kv_cache_gpu_9 -> flash_attn_gpu_9	[pos="e,4641.3,2044.2 4641.6,2079.7 4641.6,2071.4 4641.5,2062.8 4641.4,2054.5"];
	flash_attn_gpu_10	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_10",
		pos="5120,2009.9",
		width=6.3056];
	kv_cache_gpu_10 -> flash_attn_gpu_10	[pos="e,5123.3,2044.2 5126.9,2079.7 5126.1,2071.4 5125.2,2062.6 5124.3,2054.2"];
	flash_attn_gpu_11	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_11",
		pos="5601,2009.9",
		width=6.3056];
	kv_cache_gpu_11 -> flash_attn_gpu_11	[pos="e,5603.7,2044.2 5606.6,2079.7 5606,2071.4 5605.2,2062.6 5604.6,2054.2"];
	flash_attn_gpu_12	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_12",
		pos="6086,2009.9",
		width=6.3056];
	kv_cache_gpu_12 -> flash_attn_gpu_12	[pos="e,6090,2044.2 6094.2,2079.7 6093.2,2071.4 6092.1,2062.6 6091.1,2054.2"];
	flash_attn_gpu_13	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_13",
		pos="6574,2009.9",
		width=6.3056];
	kv_cache_gpu_13 -> flash_attn_gpu_13	[pos="e,6578.6,2044.2 6583.4,2079.7 6582.3,2071.4 6581.1,2062.6 6579.9,2054.2"];
	flash_attn_gpu_14	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_14",
		pos="7074,2009.9",
		width=6.3056];
	kv_cache_gpu_14 -> flash_attn_gpu_14	[pos="e,7076.7,2044.2 7079.6,2079.7 7079,2071.4 7078.2,2062.6 7077.6,2054.2"];
	flash_attn_gpu_15	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: \
gpu_15",
		pos="7556,2009.9",
		width=6.3056];
	kv_cache_gpu_15 -> flash_attn_gpu_15	[pos="e,7559,2044.2 7562.3,2079.7 7561.5,2071.4 7560.7,2062.6 7560,2054.2"];
	all2all_group_0	[fillcolor=yellow,
		height=2.7222,
		label="All-to-All\nGroup 0\nInput: [batch_size=1, seq_len=250000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=250000, heads=128, \
d_k=128]\nVolume: 1×16384×0.5 bytes\nDevice: group_0",
		pos="1962,1841.9",
		shape=parallelogram,
		width=13.593];
	flash_attn_gpu_0 -> all2all_group_0	[pos="e,1603.6,1872.1 500.26,1978.4 507.59,1977.6 514.85,1976.7 522,1975.9 885.69,1935 1302.1,1897.6 1593.3,1872.9"];
	flash_attn_gpu_1 -> all2all_group_0	[pos="e,1621.7,1889.9 985.17,1977.6 1161.7,1953.3 1409.6,1919.1 1611.5,1891.3"];
	flash_attn_gpu_2 -> all2all_group_0	[pos="e,1647.3,1915.1 1382.3,1975.8 1455.8,1959 1548.2,1937.8 1637.6,1917.3"];
	flash_attn_gpu_3 -> all2all_group_0	[pos="e,1818.7,1940 1766,1975.7 1779.3,1966.7 1794.5,1956.4 1810.3,1945.7"];
	all2all_group_1	[fillcolor=yellow,
		height=2.7222,
		label="All-to-All\nGroup 1\nInput: [batch_size=1, seq_len=250000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=250000, heads=128, \
d_k=128]\nVolume: 1×16384×0.5 bytes\nDevice: group_1",
		pos="3411,1841.9",
		shape=parallelogram,
		width=13.593];
	flash_attn_gpu_4 -> all2all_group_1	[pos="e,3071.2,1890.4 2451.1,1977.2 2623.7,1953 2864.2,1919.4 3061.3,1891.8"];
	flash_attn_gpu_5 -> all2all_group_1	[pos="e,3098,1916.6 2846.6,1975.8 2916.2,1959.4 3003.3,1938.9 3088.2,1918.9"];
	flash_attn_gpu_6 -> all2all_group_1	[pos="e,3275.9,1940 3226.2,1975.7 3238.6,1966.8 3252.8,1956.6 3267.6,1946"];
	flash_attn_gpu_7 -> all2all_group_1	[pos="e,3557.3,1940 3611,1975.7 3597.4,1966.7 3582,1956.4 3565.8,1945.7"];
	all2all_group_2	[fillcolor=yellow,
		height=2.7222,
		label="All-to-All\nGroup 2\nInput: [batch_size=1, seq_len=250000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=250000, heads=128, \
d_k=128]\nVolume: 1×16384×0.5 bytes\nDevice: group_2",
		pos="4408,1841.9",
		shape=parallelogram,
		width=13.593];
	flash_attn_gpu_8 -> all2all_group_2	[pos="e,4267,1940 4215.2,1975.7 4228.2,1966.8 4242.9,1956.6 4258.4,1946"];
	flash_attn_gpu_9 -> all2all_group_2	[pos="e,4544.3,1940 4594.4,1975.7 4581.9,1966.8 4567.6,1956.6 4552.7,1946"];
	flash_attn_gpu_10 -> all2all_group_2	[pos="e,4824.2,1940 4978.1,1975.8 4935.2,1965.8 4885.7,1954.3 4834.2,1942.3"];
	flash_attn_gpu_11 -> all2all_group_2	[pos="e,4862.9,1906.2 5373.8,1977.3 5232.2,1957.6 5044.9,1931.5 4872.9,1907.6"];
	all2all_group_3	[fillcolor=yellow,
		height=2.7222,
		label="All-to-All\nGroup 3\nInput: [batch_size=1, seq_len=250000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=250000, heads=128, \
d_k=128]\nVolume: 1×16384×0.5 bytes\nDevice: group_3",
		pos="5624,1841.9",
		shape=parallelogram,
		width=13.593];
	flash_attn_gpu_12 -> all2all_group_3	[pos="e,5894.1,1940 5993.9,1975.8 5966.9,1966.1 5936,1955 5903.7,1943.4"];
	flash_attn_gpu_13 -> all2all_group_3	[pos="e,6099,1925.9 6384.9,1975.9 6304.7,1961.9 6207.2,1944.8 6109.1,1927.7"];
	flash_attn_gpu_14 -> all2all_group_3	[pos="e,6063.1,1890.8 6846.9,1980.3 6834.4,1978.8 6822,1977.4 6810,1975.9 6566.7,1947.1 6295.1,1916.5 6073.3,1892"];
	flash_attn_gpu_15 -> all2all_group_3	[pos="e,6041.6,1869.5 7328.9,1978 7322.5,1977.3 7316.2,1976.6 7310,1975.9 6882.6,1930.4 6392.8,1893.5 6051.7,1870.2"];
	global_all2all	[fillcolor=yellow,
		height=2.7222,
		label="Global All-to-All\nCross-Group Exchange\nInput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_\
len=1000000, heads=128, d_k=128]\nVolume: 1×16384×0.5 bytes\nDevice: all GPUs",
		pos="4121,1609.9",
		shape=parallelogram,
		width=13.852];
	all2all_group_0 -> global_all2all	[pos="e,3762.8,1647.5 2310.5,1802.1 2706.2,1759.3 3349,1690.8 3752.8,1648.5"];
	all2all_group_0 -> global_all2all	[pos="e,3764,1648.4 2311.5,1803.1 2707.7,1760.7 3350.7,1692.2 3753.9,1649.5"];
	all2all_group_0 -> global_all2all	[pos="e,3764.9,1649.4 2312.8,1804.1 2709.4,1762.1 3352,1693.6 3754.8,1650.5"];
	all2all_group_0 -> global_all2all	[pos="e,3766.2,1650.4 2313.6,1805.1 2710.7,1763.5 3353.6,1695 3756.1,1651.5"];
	all2all_group_1 -> global_all2all	[pos="e,3819.8,1702.1 3690.5,1743.9 3729.8,1731.1 3770.4,1717.9 3810.3,1705.1"];
	all2all_group_1 -> global_all2all	[pos="e,3823.2,1705.2 3701.1,1744.8 3738,1732.8 3776,1720.5 3813.2,1708.5"];
	all2all_group_1 -> global_all2all	[pos="e,3828,1708 3704.4,1748 3742,1735.9 3780.6,1723.4 3818.4,1711.1"];
	all2all_group_1 -> global_all2all	[pos="e,3841.4,1708 3707.6,1751.3 3748.7,1738.1 3790.9,1724.5 3831.8,1711.1"];
	all2all_group_2 -> global_all2all	[pos="e,4222.3,1708.2 4266.7,1743.7 4254.4,1734.1 4242.1,1724.2 4230.1,1714.5"];
	all2all_group_2 -> global_all2all	[pos="e,4235.6,1708.2 4280,1743.7 4267.9,1734.1 4255.6,1724.2 4243.5,1714.5"];
	all2all_group_2 -> global_all2all	[pos="e,4249,1708.2 4293.4,1743.7 4281.4,1734.1 4269.2,1724.2 4257,1714.5"];
	all2all_group_2 -> global_all2all	[pos="e,4262.4,1708.2 4306.7,1743.7 4294.9,1734.1 4282.7,1724.2 4270.4,1714.5"];
	all2all_group_3 -> global_all2all	[pos="e,4598.2,1686.9 5164.8,1773.5 4988.6,1746.8 4787,1716 4608.3,1688.4"];
	all2all_group_3 -> global_all2all	[pos="e,4595.5,1684.5 5162.7,1771.3 4986.1,1744.4 4784.3,1713.5 4605.6,1686.1"];
	all2all_group_3 -> global_all2all	[pos="e,4593.1,1682.2 5160.4,1769 4983.6,1741.9 4781.7,1711 4603.2,1683.8"];
	all2all_group_3 -> global_all2all	[pos="e,4591,1680 5157.9,1766.7 4981,1739.4 4779.3,1708.5 4601,1681.5"];
	attn_out_gpu_0	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_0\nRows: 0-1023",
		pos="964,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_0	[pos="e,1176.5,1474.9 3721.9,1608 3157.9,1602.8 2095.9,1578.8 1186.5,1476.1"];
	attn_out_gpu_1	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_1\nRows: 1024-2047",
		pos="1407,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_1	[pos="e,1619.5,1474.9 3717,1603.2 3225,1592.7 2369.2,1562.6 1629.6,1476.1"];
	attn_out_gpu_2	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_2\nRows: 2048-3071",
		pos="1850,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_2	[pos="e,2062.6,1474.7 3709.6,1596.2 3297.3,1580.2 2644.9,1546 2072.7,1476"];
	attn_out_gpu_3	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_3\nRows: 3072-4095",
		pos="2293,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_3	[pos="e,2505.6,1474.7 3698.4,1585.3 3376.5,1564.5 2922.5,1529.4 2515.8,1476"];
	attn_out_gpu_4	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_4\nRows: 4096-5119",
		pos="2736,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_4	[pos="e,2948.6,1474.5 3677.6,1565.2 3463.3,1542.3 3202.2,1511.9 2958.7,1476"];
	attn_out_gpu_5	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_5\nRows: 5120-6143",
		pos="3179,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_5	[pos="e,3391.8,1474.6 3628.6,1518.2 3549.2,1503.6 3470.6,1489.1 3401.8,1476.5"];
	attn_out_gpu_6	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_6\nRows: 6144-7167",
		pos="3622,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_6	[pos="e,3738.6,1476 3842,1511.9 3809.3,1500.6 3777.3,1489.4 3748.3,1479.4"];
	attn_out_gpu_7	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_7\nRows: 7168-8191",
		pos="4065,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_7	[pos="e,4078.1,1476.1 4089.6,1511.8 4086.8,1502.9 4083.9,1494.1 4081.3,1485.9"];
	attn_out_gpu_8	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_8\nRows: 8192-9215",
		pos="4508,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_8	[pos="e,4417.6,1476 4337.4,1511.9 4362.1,1500.8 4386.2,1490 4408.3,1480.1"];
	attn_out_gpu_9	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_9\nRows: 9216-10239",
		pos="4951,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_9	[pos="e,4757.1,1476 4446.1,1541 4547.6,1519.8 4656.6,1497 4747.3,1478"];
	attn_out_gpu_10	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_10\nRows: 10240-11263",
		pos="5394,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_10	[pos="e,5181.4,1474.6 4475.4,1569.1 4676.8,1545.2 4933.9,1512.7 5171.2,1476.2"];
	attn_out_gpu_11	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_11\nRows: 11264-12287",
		pos="5837,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_11	[pos="e,5624.5,1474.8 4492.6,1585.5 4793.2,1564.2 5227.2,1528.3 5614.5,1476.1"];
	attn_out_gpu_12	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_12\nRows: 12288-13311",
		pos="6280,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_12	[pos="e,6067.5,1474.8 4502.8,1595.4 4892.2,1578.7 5513.5,1543.9 6057.5,1476.1"];
	attn_out_gpu_13	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_13\nRows: 13312-14335",
		pos="6723,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_13	[pos="e,6510.5,1474.9 4509.7,1602.2 4979.9,1590.6 5795.2,1559.5 6500.4,1476.1"];
	attn_out_gpu_14	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_14\nRows: 14336-15359",
		pos="7166,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_14	[pos="e,6953.5,1474.9 4514.9,1607 5059.8,1600.6 6073.8,1575.1 6943.3,1476.1"];
	attn_out_gpu_15	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: \
gpu_15\nRows: 15360-16383",
		pos="7609,1434.4",
		width=5.9028];
	global_all2all -> attn_out_gpu_15	[pos="e,7396.5,1474.9 4507.5,1600 5243.3,1581.9 6821.5,1537.4 7386.2,1476.1"];
	attn_all_reduce	[fillcolor=yellow,
		height=2.3056,
		label="All-Reduce\nAttention Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=\
16384]\nDevice: all GPUs",
		pos="4508,1273.9",
		shape=parallelogram,
		width=12.499];
	attn_out_gpu_0 -> attn_all_reduce	[pos="e,4157.6,1280.9 1176.5,1393.9 1179.4,1393.6 1182.2,1393.2 1185,1392.9 1749.6,1331.5 3409.5,1294.8 4147.3,1281.1"];
	attn_out_gpu_1 -> attn_all_reduce	[pos="e,4160,1283.1 1619.5,1393.9 1622.4,1393.6 1625.2,1393.2 1628,1392.9 2106,1339.6 3490,1299.8 4149.9,1283.3"];
	attn_out_gpu_2 -> attn_all_reduce	[pos="e,4153.8,1277.5 2062.6,1394 2065.8,1393.7 2068.9,1393.3 2072,1392.9 2804,1308.8 3666.3,1284.5 4143.7,1277.6"];
	attn_out_gpu_3 -> attn_all_reduce	[pos="e,4159.9,1282.8 2505.6,1394.1 2508.8,1393.7 2511.9,1393.3 2515,1392.9 3084.1,1324.4 3749.7,1295.2 4149.6,1283.1"];
	attn_out_gpu_4 -> attn_all_reduce	[pos="e,4168.3,1290.6 2948.6,1394.2 2951.8,1393.7 2954.9,1393.3 2958,1392.9 3368.4,1340 3843.3,1308.2 4158.1,1291.1"];
	attn_out_gpu_5 -> attn_all_reduce	[pos="e,4182.4,1303.2 3391.6,1394.3 3394.8,1393.8 3397.9,1393.4 3401,1392.9 3659,1355.7 3952.4,1324.9 4172.3,1304.2"];
	attn_out_gpu_6 -> attn_all_reduce	[pos="e,4209.6,1327.6 3834.7,1394.6 3837.8,1394 3840.9,1393.5 3844,1392.9 3960,1372 4087.6,1349.3 4199.5,1329.4"];
	attn_out_gpu_7 -> attn_all_reduce	[pos="e,4278.6,1357 4178.2,1392.9 4206.2,1382.9 4237.3,1371.8 4268.8,1360.5"];
	attn_out_gpu_8 -> attn_all_reduce	[pos="e,4508,1357.2 4508,1392.6 4508,1384.7 4508,1376.1 4508,1367.2"];
	attn_out_gpu_9 -> attn_all_reduce	[pos="e,4737.4,1357 4837.8,1392.9 4809.8,1382.9 4778.7,1371.8 4747.2,1360.5"];
	attn_out_gpu_10 -> attn_all_reduce	[pos="e,4954.8,1353.8 5181.4,1394.4 5178.6,1393.9 5175.8,1393.4 5173,1392.9 5105.8,1380.8 5034.7,1368.1 4965,1355.7"];
	attn_out_gpu_11 -> attn_all_reduce	[pos="e,4906,1310 5624.4,1394.2 5621.6,1393.7 5618.8,1393.3 5616,1392.9 5385.1,1359.5 5125.9,1331.4 4916.3,1311"];
	attn_out_gpu_12 -> attn_all_reduce	[pos="e,4886.8,1292.6 6067.5,1394 6064.6,1393.7 6061.8,1393.3 6059,1392.9 5664.9,1342 5211.4,1310.6 4897,1293.1"];
	attn_out_gpu_13 -> attn_all_reduce	[pos="e,4876.3,1283.3 6510.5,1394 6507.6,1393.6 6504.8,1393.3 6502,1392.9 5941.8,1325.3 5288.3,1296 4886.3,1283.6"];
	attn_out_gpu_14 -> attn_all_reduce	[pos="e,4870,1277.5 6953.5,1393.9 6950.6,1393.6 6947.8,1393.2 6945,1392.9 6216.7,1308.9 5359.2,1284.6 4880.2,1277.6"];
	attn_out_gpu_15 -> attn_all_reduce	[pos="e,4876.6,1283.6 7396.5,1393.9 7393.6,1393.6 7390.8,1393.2 7388,1392.9 6914.9,1340.2 5554.6,1300.7 4886.8,1283.8"];
	attn_all_reduce -> residual_attn	[pos="e,5912,1125.3 4827.7,1239.3 5138.7,1206.6 5606.4,1157.4 5902,1126.4"];
	rmsnorm1	[fillcolor=lightgray,
		height=0.94444,
		label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all \
GPUs",
		pos="6179,967.55",
		width=6.0278];
	residual_attn -> rmsnorm1	[pos="e,6179,1001.7 6179,1037.5 6179,1028.8 6179,1020.1 6179,1011.8"];
	ffn_fc1_gpu_0	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_0\nColumns: \
0-4095",
		pos="2648,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_0	[pos="e,2859.3,895.96 5961.8,966.45 5326.1,965.46 3475.7,957.37 2874,897.55 2872.5,897.4 2871,897.24 2869.4,897.08"];
	ffn_fc1_gpu_1	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_1\nColumns: \
4096-8191",
		pos="3094,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_1	[pos="e,3305.2,896.26 5961.9,965.69 5386,962.7 3829.1,949.9 3317,897.55 3316.4,897.48 3315.8,897.42 3315.2,897.36"];
	ffn_fc1_gpu_2	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_2\nColumns: \
8192-12287",
		pos="3537,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_2	[pos="e,3748,896.44 5961.9,964.61 5450.6,959.36 4181.5,942.13 3758,897.55 3757.9,897.54 3757.8,897.52 3757.7,897.51"];
	ffn_fc1_gpu_3	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_3\nColumns: \
12288-16383",
		pos="3978,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_3	[pos="e,4189,896.41 5961.9,962.99 5521.1,955.12 4534.9,933.97 4199,897.55 4198.9,897.54 4198.8,897.52 4198.7,897.51"];
	ffn_fc1_gpu_4	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_4\nColumns: \
16384-20479",
		pos="4419,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_4	[pos="e,4630.3,896.51 5961.8,966.19 5665,963.58 5116.3,951.2 4640.4,897.66"];
	ffn_fc1_gpu_5	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_5\nColumns: \
20480-24575",
		pos="4859,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_5	[pos="e,5070.3,896.44 5961.7,960.57 5741.7,952.74 5391.7,935.43 5080.4,897.68"];
	ffn_fc1_gpu_6	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_6\nColumns: \
24576-28671",
		pos="5299,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_6	[pos="e,5510,896.23 5961.8,948.14 5835.6,936.39 5673.4,919.51 5520.1,897.67"];
	ffn_fc1_gpu_7	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_7\nColumns: \
28672-32767",
		pos="5739,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_7	[pos="e,5902.1,897.63 6046.4,933.54 6004.1,923.01 5956.6,911.2 5911.9,900.07"];
	ffn_fc1_gpu_8	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_8\nColumns: \
32768-36863",
		pos="6179,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_8	[pos="e,6179,897.82 6179,933.4 6179,925.34 6179,916.55 6179,907.88"];
	ffn_fc1_gpu_9	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_9\nColumns: \
36864-40959",
		pos="6620,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_9	[pos="e,6456.6,897.63 6311.9,933.54 6354.3,923.01 6401.9,911.2 6446.7,900.07"];
	ffn_fc1_gpu_10	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_10\nColumns: \
40960-45055",
		pos="7061,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_10	[pos="e,6849.9,896.09 6396.1,948.14 6525.3,936.13 6692.4,918.74 6840,897.55 6840.1,897.53 6840.2,897.52 6840.3,897.5"];
	ffn_fc1_gpu_11	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_11\nColumns: \
45056-49151",
		pos="7503,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_11	[pos="e,7291.9,896.17 6396.1,960.51 6618.9,952.54 6975.1,934.89 7281,897.55 7281.2,897.52 7281.4,897.49 7281.7,897.46"];
	ffn_fc1_gpu_12	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_12\nColumns: \
49152-53247",
		pos="7946,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_12	[pos="e,7734.8,896.15 6396.1,966.06 6695.6,963.29 7251.9,950.61 7723,897.55 7723.6,897.48 7724.2,897.41 7724.8,897.34"];
	ffn_fc1_gpu_13	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_13\nColumns: \
53248-57343",
		pos="8389,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_13	[pos="e,8177.8,896.2 6396.1,962.98 6837.9,955.08 7828.7,933.84 8166,897.55 8166.6,897.48 8167.2,897.41 8167.8,897.35"];
	ffn_fc1_gpu_14	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_14\nColumns: \
57344-61439",
		pos="8832,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_14	[pos="e,8620.8,896.23 6396.4,964.59 6909.3,959.29 8183.7,941.94 8609,897.55 8609.6,897.48 8610.2,897.42 8610.8,897.35"];
	ffn_fc1_gpu_15	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_15\nColumns: \
61440-65535",
		pos="9275,856.05",
		width=5.8611];
	rmsnorm1 -> ffn_fc1_gpu_15	[pos="e,9063.8,896.26 6396.2,965.71 6973.6,962.78 8537.7,950.1 9052,897.55 9052.6,897.48 9053.2,897.42 9053.8,897.36"];
	residual_ffn	[fillcolor=lightgreen,
		height=1.6303,
		label="Residual Add\nFFN\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="7850,294.86",
		shape=ellipse,
		width=8.5246];
	rmsnorm1 -> residual_ffn	[pos="e,8137.1,315.68 6396,965.72 7123.9,962.47 9436.2,948.19 9495,897.55 9546.9,852.81 9521,814.08 9521,745.55 9521,745.55 9521,745.55 \
9521,632.05 9521,612.16 9517.8,603.65 9502,591.55 9294.3,432.13 8564,350.81 8147.2,316.51"];
	ffn_act_gpu_0	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_0",
		pos="2636,744.55",
		width=5.4861];
	ffn_fc1_gpu_0 -> ffn_act_gpu_0	[pos="e,2639.6,778.85 2643.5,814.27 2642.6,805.97 2641.7,797.21 2640.7,788.83"];
	ffn_act_gpu_1	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_1",
		pos="3086,744.55",
		width=5.4861];
	ffn_fc1_gpu_1 -> ffn_act_gpu_1	[pos="e,3088.4,778.85 3091,814.27 3090.4,805.97 3089.8,797.21 3089.2,788.83"];
	ffn_act_gpu_2	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_2",
		pos="3529,744.55",
		width=5.4861];
	ffn_fc1_gpu_2 -> ffn_act_gpu_2	[pos="e,3531.4,778.85 3534,814.27 3533.4,805.97 3532.8,797.21 3532.2,788.83"];
	ffn_act_gpu_3	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_3",
		pos="3971,744.55",
		width=5.4861];
	ffn_fc1_gpu_3 -> ffn_act_gpu_3	[pos="e,3973.1,778.85 3975.4,814.27 3974.9,805.97 3974.3,797.21 3973.8,788.83"];
	ffn_act_gpu_4	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_4",
		pos="4413,744.55",
		width=5.4861];
	ffn_fc1_gpu_4 -> ffn_act_gpu_4	[pos="e,4414.8,778.85 4416.8,814.27 4416.3,806.06 4415.8,797.41 4415.4,789.11"];
	ffn_act_gpu_5	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_5",
		pos="4855,744.55",
		width=5.4861];
	ffn_fc1_gpu_5 -> ffn_act_gpu_5	[pos="e,4856.2,778.85 4857.5,814.27 4857.2,806.06 4856.9,797.41 4856.6,789.11"];
	ffn_act_gpu_6	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_6",
		pos="5296,744.55",
		width=5.4861];
	ffn_fc1_gpu_6 -> ffn_act_gpu_6	[pos="e,5296.9,778.85 5297.9,814.27 5297.7,806.06 5297.4,797.41 5297.2,789.11"];
	ffn_act_gpu_7	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_7",
		pos="5737,744.55",
		width=5.4861];
	ffn_fc1_gpu_7 -> ffn_act_gpu_7	[pos="e,5737.6,778.85 5738.3,814.27 5738.1,806.06 5737.9,797.41 5737.8,789.11"];
	ffn_act_gpu_8	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_8",
		pos="6179,744.55",
		width=5.4861];
	ffn_fc1_gpu_8 -> ffn_act_gpu_8	[pos="e,6179,778.85 6179,814.27 6179,806.06 6179,797.41 6179,789.11"];
	ffn_act_gpu_9	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_9",
		pos="6621,744.55",
		width=5.4861];
	ffn_fc1_gpu_9 -> ffn_act_gpu_9	[pos="e,6620.7,778.85 6620.4,814.27 6620.4,806.06 6620.5,797.41 6620.6,789.11"];
	ffn_act_gpu_10	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_10",
		pos="7062,744.55",
		width=5.4861];
	ffn_fc1_gpu_10 -> ffn_act_gpu_10	[pos="e,7061.7,778.85 7061.4,814.27 7061.4,806.06 7061.5,797.41 7061.6,789.11"];
	ffn_act_gpu_11	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_11",
		pos="7505,744.55",
		width=5.4861];
	ffn_fc1_gpu_11 -> ffn_act_gpu_11	[pos="e,7504.4,778.85 7503.7,814.27 7503.9,806.06 7504.1,797.41 7504.2,789.11"];
	ffn_act_gpu_12	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_12",
		pos="7949,744.55",
		width=5.4861];
	ffn_fc1_gpu_12 -> ffn_act_gpu_12	[pos="e,7948.1,778.85 7947.1,814.27 7947.3,806.06 7947.6,797.41 7947.8,789.11"];
	ffn_act_gpu_13	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_13",
		pos="8391,744.55",
		width=5.4861];
	ffn_fc1_gpu_13 -> ffn_act_gpu_13	[pos="e,8390.4,778.85 8389.7,814.27 8389.9,806.06 8390.1,797.41 8390.2,789.11"];
	ffn_act_gpu_14	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_14",
		pos="8835,744.55",
		width=5.4861];
	ffn_fc1_gpu_14 -> ffn_act_gpu_14	[pos="e,8834.1,778.85 8833.1,814.27 8833.3,806.06 8833.6,797.41 8833.8,789.11"];
	ffn_act_gpu_15	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: \
gpu_15",
		pos="9277,744.55",
		width=5.4861];
	ffn_fc1_gpu_15 -> ffn_act_gpu_15	[pos="e,9276.4,778.85 9275.7,814.27 9275.9,806.06 9276.1,797.41 9276.2,789.11"];
	ffn_fc2_gpu_0	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_0\nRows: \
0-1023",
		pos="2635,633.05",
		width=5.9028];
	ffn_act_gpu_0 -> ffn_fc2_gpu_0	[pos="e,2635.4,674.82 2635.7,710.4 2635.6,702.34 2635.5,693.55 2635.5,684.88"];
	ffn_fc2_gpu_1	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_1\nRows: \
1024-2047",
		pos="3078,633.05",
		width=5.9028];
	ffn_act_gpu_1 -> ffn_fc2_gpu_1	[pos="e,3081,674.82 3083.6,710.4 3083,702.34 3082.3,693.55 3081.7,684.88"];
	ffn_fc2_gpu_2	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_2\nRows: \
2048-3071",
		pos="3521,633.05",
		width=5.9028];
	ffn_act_gpu_2 -> ffn_fc2_gpu_2	[pos="e,3524,674.82 3526.6,710.4 3526,702.34 3525.3,693.55 3524.7,684.88"];
	ffn_fc2_gpu_3	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_3\nRows: \
3072-4095",
		pos="3964,633.05",
		width=5.9028];
	ffn_act_gpu_3 -> ffn_fc2_gpu_3	[pos="e,3966.6,674.82 3968.9,710.4 3968.4,702.34 3967.8,693.55 3967.2,684.88"];
	ffn_fc2_gpu_4	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_4\nRows: \
4096-5119",
		pos="4407,633.05",
		width=5.9028];
	ffn_act_gpu_4 -> ffn_fc2_gpu_4	[pos="e,4409.2,674.82 4411.2,710.4 4410.7,702.34 4410.3,693.55 4409.8,684.88"];
	ffn_fc2_gpu_5	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_5\nRows: \
5120-6143",
		pos="4850,633.05",
		width=5.9028];
	ffn_act_gpu_5 -> ffn_fc2_gpu_5	[pos="e,4851.9,674.82 4853.5,710.4 4853.1,702.34 4852.7,693.55 4852.3,684.88"];
	ffn_fc2_gpu_6	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_6\nRows: \
6144-7167",
		pos="5293,633.05",
		width=5.9028];
	ffn_act_gpu_6 -> ffn_fc2_gpu_6	[pos="e,5294.1,674.82 5295.1,710.4 5294.9,702.34 5294.6,693.55 5294.4,684.88"];
	ffn_fc2_gpu_7	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_7\nRows: \
7168-8191",
		pos="5736,633.05",
		width=5.9028];
	ffn_act_gpu_7 -> ffn_fc2_gpu_7	[pos="e,5736.4,674.82 5736.7,710.4 5736.6,702.34 5736.5,693.55 5736.5,684.88"];
	ffn_fc2_gpu_8	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_8\nRows: \
8192-9215",
		pos="6179,633.05",
		width=5.9028];
	ffn_act_gpu_8 -> ffn_fc2_gpu_8	[pos="e,6179,674.82 6179,710.4 6179,702.34 6179,693.55 6179,684.88"];
	ffn_fc2_gpu_9	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_9\nRows: \
9216-10239",
		pos="6622,633.05",
		width=5.9028];
	ffn_act_gpu_9 -> ffn_fc2_gpu_9	[pos="e,6621.6,674.82 6621.3,710.4 6621.4,702.34 6621.5,693.55 6621.5,684.88"];
	ffn_fc2_gpu_10	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_10\nRows: \
10240-11263",
		pos="7065,633.05",
		width=5.9028];
	ffn_act_gpu_10 -> ffn_fc2_gpu_10	[pos="e,7063.9,674.82 7062.9,710.4 7063.1,702.34 7063.4,693.55 7063.6,684.88"];
	ffn_fc2_gpu_11	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_11\nRows: \
11264-12287",
		pos="7508,633.05",
		width=5.9028];
	ffn_act_gpu_11 -> ffn_fc2_gpu_11	[pos="e,7506.9,674.82 7505.9,710.4 7506.1,702.34 7506.4,693.55 7506.6,684.88"];
	ffn_fc2_gpu_12	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_12\nRows: \
12288-13311",
		pos="7951,633.05",
		width=5.9028];
	ffn_act_gpu_12 -> ffn_fc2_gpu_12	[pos="e,7950.3,674.82 7949.6,710.4 7949.8,702.34 7949.9,693.55 7950.1,684.88"];
	ffn_fc2_gpu_13	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_13\nRows: \
13312-14335",
		pos="8394,633.05",
		width=5.9028];
	ffn_act_gpu_13 -> ffn_fc2_gpu_13	[pos="e,8392.9,674.82 8391.9,710.4 8392.1,702.34 8392.4,693.55 8392.6,684.88"];
	ffn_fc2_gpu_14	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_14\nRows: \
14336-15359",
		pos="8837,633.05",
		width=5.9028];
	ffn_act_gpu_14 -> ffn_fc2_gpu_14	[pos="e,8836.3,674.82 8835.6,710.4 8835.8,702.34 8835.9,693.55 8836.1,684.88"];
	ffn_fc2_gpu_15	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_15\nRows: \
15360-16383",
		pos="9280,633.05",
		width=5.9028];
	ffn_act_gpu_15 -> ffn_fc2_gpu_15	[pos="e,9278.9,674.82 9277.9,710.4 9278.1,702.34 9278.4,693.55 9278.6,684.88"];
	ffn_all_reduce	[fillcolor=yellow,
		height=2.3056,
		label="All-Reduce\nFFN Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="6179,472.55",
		shape=parallelogram,
		width=12.499];
	ffn_fc2_gpu_0 -> ffn_all_reduce	[pos="e,5828.7,479.56 2847.6,592.61 2850.7,592.24 2853.9,591.88 2857,591.55 3421.5,530.36 5080.8,493.52 5818.5,479.75"];
	ffn_fc2_gpu_1 -> ffn_all_reduce	[pos="e,5831.1,481.72 3290.6,592.64 3293.7,592.26 3296.9,591.89 3300,591.55 3777.9,538.43 5161.4,498.55 5821,481.98"];
	ffn_fc2_gpu_2 -> ffn_all_reduce	[pos="e,5824.8,476.07 3733.6,592.67 3736.8,592.28 3739.9,591.9 3743,591.55 4475,507.42 5337.3,483.14 5814.7,476.22"];
	ffn_fc2_gpu_3 -> ffn_all_reduce	[pos="e,5830.9,481.37 4176.6,592.72 4179.8,592.31 4182.9,591.92 4186,591.55 4755.1,523.04 5420.7,493.8 5820.6,481.68"];
	ffn_fc2_gpu_4 -> ffn_all_reduce	[pos="e,5839.3,489.18 4619.6,592.79 4622.8,592.36 4625.9,591.95 4629,591.55 5039.4,538.63 5514.3,506.77 5829.1,489.73"];
	ffn_fc2_gpu_5 -> ffn_all_reduce	[pos="e,5853.4,501.85 5062.6,592.92 5065.8,592.45 5068.9,591.99 5072,591.55 5330,554.32 5623.4,523.56 5843.3,502.8"];
	ffn_fc2_gpu_6 -> ffn_all_reduce	[pos="e,5880.6,526.27 5505.7,593.23 5508.8,592.66 5511.9,592.1 5515,591.55 5631,570.65 5758.6,547.93 5870.5,528.06"];
	ffn_fc2_gpu_7 -> ffn_all_reduce	[pos="e,5949.6,555.62 5849.2,591.54 5877.2,581.52 5908.3,570.41 5939.8,559.13"];
	ffn_fc2_gpu_8 -> ffn_all_reduce	[pos="e,6179,555.82 6179,591.23 6179,583.3 6179,574.69 6179,565.86"];
	ffn_fc2_gpu_9 -> ffn_all_reduce	[pos="e,6408.4,555.62 6508.8,591.54 6480.8,581.52 6449.7,570.41 6418.2,559.13"];
	ffn_fc2_gpu_10 -> ffn_all_reduce	[pos="e,6625.8,552.46 6852.4,593.06 6849.6,592.55 6846.8,592.05 6844,591.55 6776.8,579.42 6705.7,566.7 6636,554.29"];
	ffn_fc2_gpu_11 -> ffn_all_reduce	[pos="e,6577,508.63 7295.4,592.79 7292.6,592.37 7289.8,591.95 7287,591.55 7056.1,558.15 6796.9,530.03 6587.3,509.63"];
	ffn_fc2_gpu_12 -> ffn_all_reduce	[pos="e,6557.8,491.2 7738.5,592.67 7735.6,592.28 7732.8,591.91 7730,591.55 7335.9,540.59 6882.4,509.22 6568,491.77"];
	ffn_fc2_gpu_13 -> ffn_all_reduce	[pos="e,6547.3,481.88 8181.5,592.6 8178.6,592.24 8175.8,591.89 8173,591.55 7612.8,523.9 6959.3,494.58 6557.3,482.19"];
	ffn_fc2_gpu_14 -> ffn_all_reduce	[pos="e,6541,476.08 8624.5,592.56 8621.6,592.21 8618.8,591.87 8616,591.55 7887.7,507.56 7030.2,483.2 6551.2,476.23"];
	ffn_fc2_gpu_15 -> ffn_all_reduce	[pos="e,6547.6,482.2 9067.5,592.53 9064.6,592.19 9061.8,591.86 9059,591.55 8585.9,538.77 7225.6,499.27 6557.8,482.46"];
	ffn_all_reduce -> residual_ffn	[pos="e,7583,323.93 6498.7,437.93 6809.7,405.23 7277.4,356.06 7573,324.98"];
	rmsnorm2	[fillcolor=lightgray,
		height=0.94444,
		label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all \
GPUs",
		pos="7850,166.17",
		width=6.0278];
	residual_ffn -> rmsnorm2	[pos="e,7850,200.28 7850,236.07 7850,227.45 7850,218.68 7850,210.42"];
	output	[fillcolor=lightgreen,
		height=1.3356,
		label="Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, vocab_size=128000]\nDevice: \
CPU",
		pos="7850,48.083",
		shape=ellipse,
		width=9.2317];
	rmsnorm2 -> output	[pos="e,7850,96.193 7850,131.9 7850,123.91 7850,115.14 7850,106.4"];
}
