digraph Llama_405B_Baseline_TP {
	graph [bb="0,0,4717.6,2339.1",
		rankdir=TB,
		size="30,20"
	];
	node [fillcolor=lightblue,
		label="\N",
		shape=rectangle,
		style=filled
	];
	input	[fillcolor=lightgreen,
		height=1.3356,
		label="Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: CPU",
		pos="2196,2291",
		shape=ellipse,
		width=8.5246];
	split_all_gpus	[fillcolor=yellow,
		height=1.8889,
		label="Split Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="2196,2138.9",
		shape=parallelogram,
		width=12.499];
	input -> split_all_gpus	[pos="e,2196,2207.1 2196,2242.5 2196,2234.4 2196,2225.9 2196,2217.2"];
	qkv_proj_gpu_0	[fillcolor=lightcoral,
		height=1.3611,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: \
gpu_0\nColumns: 0-2047\nQueries: 0-15, KV: 0-0",
		pos="236,1985.9",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_0	[pos="e,472.24,2033.8 1825.8,2130 1480.1,2119.2 949.79,2094 482.24,2035"];
	qkv_proj_gpu_1	[fillcolor=lightcoral,
		height=1.3611,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: \
gpu_1\nColumns: 2048-4095\nQueries: 16-31, KV: 1-1",
		pos="726,1985.9",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_1	[pos="e,962.26,2033.7 1807.2,2116.2 1568.6,2100.1 1256.9,2074.4 972.37,2035.1"];
	qkv_proj_gpu_2	[fillcolor=lightcoral,
		height=1.3611,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: \
gpu_2\nColumns: 4096-6143\nQueries: 32-47, KV: 2-2",
		pos="1216,1985.9",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_2	[pos="e,1452.3,2033.4 1759.1,2080.7 1664.1,2067.2 1563.9,2052.1 1462.4,2035.1"];
	qkv_proj_gpu_3	[fillcolor=lightcoral,
		height=1.3611,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: \
gpu_3\nColumns: 6144-8191\nQueries: 48-63, KV: 3-3",
		pos="1706,1985.9",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_3	[pos="e,1862,2035 1978.6,2070.9 1942.7,2059.9 1906,2048.6 1871.6,2038"];
	qkv_proj_gpu_4	[fillcolor=lightcoral,
		height=1.3611,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: \
gpu_4\nColumns: 8192-10239\nQueries: 64-79, KV: 4-4",
		pos="2196,1985.9",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_4	[pos="e,2196,2035.3 2196,2070.8 2196,2062.3 2196,2053.6 2196,2045.3"];
	qkv_proj_gpu_5	[fillcolor=lightcoral,
		height=1.3611,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: \
gpu_5\nColumns: 10240-12287\nQueries: 80-95, KV: 5-5",
		pos="2686,1985.9",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_5	[pos="e,2530,2035 2413.4,2070.9 2449.3,2059.9 2486,2048.6 2520.4,2038"];
	qkv_proj_gpu_6	[fillcolor=lightcoral,
		height=1.3611,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: \
gpu_6\nColumns: 12288-14335\nQueries: 96-111, KV: 6-6",
		pos="3176,1985.9",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_6	[pos="e,2939.7,2033.4 2500.1,2099.1 2629.7,2081.6 2782.7,2059.6 2929.6,2035.1"];
	qkv_proj_gpu_7	[fillcolor=lightcoral,
		height=1.3611,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: \
gpu_7\nColumns: 14336-16383\nQueries: 112-127, KV: 7-7",
		pos="3666,1985.9",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_7	[pos="e,3429.7,2033.7 2528.2,2119.9 2772.6,2104.2 3112.8,2077.5 3419.8,2035.1"];
	residual_attn	[fillcolor=lightgreen,
		height=1.6303,
		label="Residual Add\nAttention\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="3145,1096.2",
		shape=ellipse,
		width=8.5246];
	split_all_gpus -> residual_attn	[pos="e,3343.6,1141 2540,2128.5 3026.5,2113.6 3862.2,2081.4 3911,2034.9 3967.5,1981.1 3930,1938.4 3930,1860.4 3930,1860.4 3930,1860.4 \
3930,1433.4 3930,1307.8 3586.1,1201.2 3353.5,1143.5"];
	kv_cache_gpu_0	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: \
gpu_0\nDUPLICATED ACROSS GPUS",
		pos="236,1859.4",
		width=6.4306];
	qkv_proj_gpu_0 -> kv_cache_gpu_0	[pos="e,236,1901.2 236,1936.8 236,1928.4 236,1919.7 236,1911.3"];
	kv_cache_gpu_1	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: \
gpu_1\nDUPLICATED ACROSS GPUS",
		pos="726,1859.4",
		width=6.4306];
	qkv_proj_gpu_1 -> kv_cache_gpu_1	[pos="e,726,1901.2 726,1936.8 726,1928.4 726,1919.7 726,1911.3"];
	kv_cache_gpu_2	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: \
gpu_2\nDUPLICATED ACROSS GPUS",
		pos="1216,1859.4",
		width=6.4306];
	qkv_proj_gpu_2 -> kv_cache_gpu_2	[pos="e,1216,1901.2 1216,1936.8 1216,1928.4 1216,1919.7 1216,1911.3"];
	kv_cache_gpu_3	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: \
gpu_3\nDUPLICATED ACROSS GPUS",
		pos="1706,1859.4",
		width=6.4306];
	qkv_proj_gpu_3 -> kv_cache_gpu_3	[pos="e,1706,1901.2 1706,1936.8 1706,1928.4 1706,1919.7 1706,1911.3"];
	kv_cache_gpu_4	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: \
gpu_4\nDUPLICATED ACROSS GPUS",
		pos="2196,1859.4",
		width=6.4306];
	qkv_proj_gpu_4 -> kv_cache_gpu_4	[pos="e,2196,1901.2 2196,1936.8 2196,1928.4 2196,1919.7 2196,1911.3"];
	kv_cache_gpu_5	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: \
gpu_5\nDUPLICATED ACROSS GPUS",
		pos="2686,1859.4",
		width=6.4306];
	qkv_proj_gpu_5 -> kv_cache_gpu_5	[pos="e,2686,1901.2 2686,1936.8 2686,1928.4 2686,1919.7 2686,1911.3"];
	kv_cache_gpu_6	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: \
gpu_6\nDUPLICATED ACROSS GPUS",
		pos="3176,1859.4",
		width=6.4306];
	qkv_proj_gpu_6 -> kv_cache_gpu_6	[pos="e,3176,1901.2 3176,1936.8 3176,1928.4 3176,1919.7 3176,1911.3"];
	kv_cache_gpu_7	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: \
gpu_7\nDUPLICATED ACROSS GPUS",
		pos="3666,1859.4",
		width=6.4306];
	qkv_proj_gpu_7 -> kv_cache_gpu_7	[pos="e,3666,1901.2 3666,1936.8 3666,1928.4 3666,1919.7 3666,1911.3"];
	flash_attn_gpu_0	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=\
128]\nDevice: gpu_0",
		pos="236,1747.9",
		width=6.5556];
	kv_cache_gpu_0 -> flash_attn_gpu_0	[pos="e,236,1782.2 236,1817.7 236,1809.4 236,1800.8 236,1792.5"];
	flash_attn_gpu_1	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=\
128]\nDevice: gpu_1",
		pos="726,1747.9",
		width=6.5556];
	kv_cache_gpu_1 -> flash_attn_gpu_1	[pos="e,726,1782.2 726,1817.7 726,1809.4 726,1800.8 726,1792.5"];
	flash_attn_gpu_2	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=\
128]\nDevice: gpu_2",
		pos="1216,1747.9",
		width=6.5556];
	kv_cache_gpu_2 -> flash_attn_gpu_2	[pos="e,1216,1782.2 1216,1817.7 1216,1809.4 1216,1800.8 1216,1792.5"];
	flash_attn_gpu_3	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=\
128]\nDevice: gpu_3",
		pos="1706,1747.9",
		width=6.5556];
	kv_cache_gpu_3 -> flash_attn_gpu_3	[pos="e,1706,1782.2 1706,1817.7 1706,1809.4 1706,1800.8 1706,1792.5"];
	flash_attn_gpu_4	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=\
128]\nDevice: gpu_4",
		pos="2196,1747.9",
		width=6.5556];
	kv_cache_gpu_4 -> flash_attn_gpu_4	[pos="e,2196,1782.2 2196,1817.7 2196,1809.4 2196,1800.8 2196,1792.5"];
	flash_attn_gpu_5	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=\
128]\nDevice: gpu_5",
		pos="2686,1747.9",
		width=6.5556];
	kv_cache_gpu_5 -> flash_attn_gpu_5	[pos="e,2686,1782.2 2686,1817.7 2686,1809.4 2686,1800.8 2686,1792.5"];
	flash_attn_gpu_6	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=\
128]\nDevice: gpu_6",
		pos="3176,1747.9",
		width=6.5556];
	kv_cache_gpu_6 -> flash_attn_gpu_6	[pos="e,3176,1782.2 3176,1817.7 3176,1809.4 3176,1800.8 3176,1792.5"];
	flash_attn_gpu_7	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=\
128]\nDevice: gpu_7",
		pos="3666,1747.9",
		width=6.5556];
	kv_cache_gpu_7 -> flash_attn_gpu_7	[pos="e,3666,1782.2 3666,1817.7 3666,1809.4 3666,1800.8 3666,1792.5"];
	attn_all_reduce	[fillcolor=yellow,
		height=2.3056,
		label="All-Reduce\nAttention Output\nInput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, \
heads=128, d_k=128]\nDevice: all GPUs",
		pos="2056,1594.9",
		shape=parallelogram,
		width=13.852];
	flash_attn_gpu_0 -> attn_all_reduce	[pos="e,1686.7,1617.4 472.3,1714.9 475.22,1714.5 478.12,1714.2 481,1713.9 888.54,1671.3 1357,1638.2 1676.3,1618.1"];
	flash_attn_gpu_1 -> attn_all_reduce	[pos="e,1704.5,1631.9 962.32,1715 965.23,1714.6 968.12,1714.3 971,1713.9 1211.9,1684.6 1483.4,1655.1 1694.5,1632.9"];
	flash_attn_gpu_2 -> attn_all_reduce	[pos="e,1731.6,1654.2 1399.8,1713.9 1494.1,1696.9 1612,1675.7 1721.7,1656"];
	flash_attn_gpu_3 -> attn_all_reduce	[pos="e,1865.8,1678 1782.8,1713.8 1805,1704.2 1830.4,1693.3 1856.5,1682"];
	flash_attn_gpu_4 -> attn_all_reduce	[pos="e,2132.2,1678.2 2165.3,1713.8 2157.3,1705.2 2148.3,1695.5 2139.1,1685.5"];
	flash_attn_gpu_5 -> attn_all_reduce	[pos="e,2398.2,1677.9 2548.1,1713.9 2506,1703.8 2457.7,1692.2 2408.1,1680.3"];
	flash_attn_gpu_6 -> attn_all_reduce	[pos="e,2533.2,1660.3 2939.8,1715.1 2824.4,1699.5 2681,1680.2 2543.5,1661.7"];
	flash_attn_gpu_7 -> attn_all_reduce	[pos="e,2494.1,1628.5 3429.7,1714.9 3426.8,1714.6 3423.9,1714.2 3421,1713.9 3115.7,1680.2 2771,1650.5 2504.4,1629.3"];
	attn_out_gpu_0	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_0\nRows: 0-2047",
		pos="588,1434.4",
		width=5.9028];
	attn_all_reduce -> attn_out_gpu_0	[pos="e,800.54,1474.7 1618.5,1561.9 1381.8,1542.2 1084.3,1513.8 810.68,1476.1"];
	attn_out_gpu_1	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_1\nRows: 2048-4095",
		pos="1031,1434.4",
		width=5.9028];
	attn_all_reduce -> attn_out_gpu_1	[pos="e,1243.6,1474.5 1574.4,1526.1 1471.2,1510.8 1362.7,1494.1 1253.5,1476.2"];
	attn_out_gpu_2	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_2\nRows: 4096-6143",
		pos="1474,1434.4",
		width=5.9028];
	attn_all_reduce -> attn_out_gpu_2	[pos="e,1623,1476 1754.8,1511.9 1712.8,1500.4 1671,1489.1 1633.1,1478.8"];
	attn_out_gpu_3	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_3\nRows: 6144-8191",
		pos="1917,1434.4",
		width=5.9028];
	attn_all_reduce -> attn_out_gpu_3	[pos="e,1952.8,1476.2 1984.1,1511.9 1975.7,1502.3 1967.3,1492.8 1959.5,1483.9"];
	attn_out_gpu_4	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_4\nRows: 8192-10239",
		pos="2360,1434.4",
		width=5.9028];
	attn_all_reduce -> attn_out_gpu_4	[pos="e,2282.2,1476 2213.3,1511.9 2233.9,1501.2 2254.3,1490.5 2273.1,1480.7"];
	attn_out_gpu_5	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_5\nRows: 10240-12287",
		pos="2803,1434.4",
		width=5.9028];
	attn_all_reduce -> attn_out_gpu_5	[pos="e,2611.9,1476 2369.8,1527.3 2448.3,1510.7 2530.5,1493.2 2601.9,1478.1"];
	attn_out_gpu_6	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_6\nRows: 12288-14335",
		pos="3246,1434.4",
		width=5.9028];
	attn_all_reduce -> attn_out_gpu_6	[pos="e,3033.4,1474.5 2405.6,1556.4 2587,1535.6 2812.4,1507.9 3023.3,1476"];
	attn_out_gpu_7	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: \
gpu_7\nRows: 14336-16383",
		pos="3689,1434.4",
		width=5.9028];
	attn_all_reduce -> attn_out_gpu_7	[pos="e,3476.4,1474.6 2425.8,1572.8 2708.4,1554.4 3107.5,1523.2 3466.3,1476"];
	attn_out_all_reduce	[fillcolor=yellow,
		height=2.3056,
		label="All-Reduce\nFinal Attention\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=\
16384]\nDevice: all GPUs",
		pos="2360,1273.9",
		shape=parallelogram,
		width=12.499];
	attn_out_gpu_0 -> attn_out_all_reduce	[pos="e,2020.5,1290.4 800.53,1394 803.37,1393.7 806.2,1393.3 809,1392.9 1219.8,1339.8 1695.2,1308 2010.3,1291"];
	attn_out_gpu_1 -> attn_out_all_reduce	[pos="e,2034.4,1303.1 1243.6,1394.2 1246.4,1393.7 1249.2,1393.3 1252,1392.9 1510.3,1355.6 1804.1,1324.8 2024.3,1304.1"];
	attn_out_gpu_2 -> attn_out_all_reduce	[pos="e,2061.2,1327.6 1686.6,1394.4 1689.4,1393.9 1692.2,1393.4 1695,1392.9 1811.2,1372 1938.9,1349.2 2051.1,1329.4"];
	attn_out_gpu_3 -> attn_out_all_reduce	[pos="e,2130.6,1357 2030.2,1392.9 2058.2,1382.9 2089.3,1371.8 2120.8,1360.5"];
	attn_out_gpu_4 -> attn_out_all_reduce	[pos="e,2360,1357.2 2360,1392.6 2360,1384.7 2360,1376.1 2360,1367.2"];
	attn_out_gpu_5 -> attn_out_all_reduce	[pos="e,2589.4,1357 2689.8,1392.9 2661.8,1382.9 2630.7,1371.8 2599.2,1360.5"];
	attn_out_gpu_6 -> attn_out_all_reduce	[pos="e,2807,1354.1 3033.3,1394.6 3030.2,1394 3027.1,1393.5 3024,1392.9 2957.1,1380.9 2886.5,1368.2 2817.2,1355.9"];
	attn_out_gpu_7 -> attn_out_all_reduce	[pos="e,2758.3,1310.2 3476.4,1394.3 3473.2,1393.8 3470.1,1393.4 3467,1392.9 3236.6,1359.7 2977.9,1331.6 2768.5,1311.2"];
	attn_out_all_reduce -> residual_attn	[pos="e,2948.1,1141.3 2645.9,1208.9 2742.6,1187.3 2848.7,1163.5 2938.1,1143.5"];
	rmsnorm1	[fillcolor=lightgray,
		height=0.94444,
		label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all \
GPUs",
		pos="3145,967.55",
		width=6.0278];
	residual_attn -> rmsnorm1	[pos="e,3145,1001.7 3145,1037.5 3145,1028.8 3145,1020.1 3145,1011.8"];
	ffn_fc1_gpu_0	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_0\nColumns: \
0-8191",
		pos="1399,856.05",
		width=5.7361];
	rmsnorm1 -> ffn_fc1_gpu_0	[pos="e,1605.8,895.1 2928,965.62 2632.6,962.32 2087.8,949 1626,897.55 1622.7,897.17 1619.3,896.78 1615.9,896.38"];
	ffn_fc1_gpu_1	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_1\nColumns: \
8192-16383",
		pos="1842,856.05",
		width=5.7361];
	rmsnorm1 -> ffn_fc1_gpu_1	[pos="e,2048.6,895.69 2928,960.21 2709,952.05 2361.6,934.29 2063,897.55 2061.7,897.38 2060.3,897.21 2059,897.05"];
	ffn_fc1_gpu_2	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_2\nColumns: \
16384-24575",
		pos="2279,856.05",
		width=5.7361];
	rmsnorm1 -> ffn_fc1_gpu_2	[pos="e,2485.8,895.87 2927.7,947.66 2801.7,935.64 2640,918.41 2497,897.55 2496.7,897.5 2496.3,897.44 2496,897.39"];
	ffn_fc1_gpu_3	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_3\nColumns: \
24576-32767",
		pos="2713,856.05",
		width=5.7361];
	rmsnorm1 -> ffn_fc1_gpu_3	[pos="e,2873.1,897.63 3014.8,933.54 2973.3,923.04 2926.8,911.25 2883,900.14"];
	ffn_fc1_gpu_4	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_4\nColumns: \
32768-40959",
		pos="3145,856.05",
		width=5.7361];
	rmsnorm1 -> ffn_fc1_gpu_4	[pos="e,3145,897.82 3145,933.4 3145,925.34 3145,916.55 3145,907.88"];
	ffn_fc1_gpu_5	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_5\nColumns: \
40960-49151",
		pos="3579,856.05",
		width=5.7361];
	rmsnorm1 -> ffn_fc1_gpu_5	[pos="e,3418.2,897.63 3275.8,933.54 3317.5,923.04 3364.2,911.25 3408.2,900.14"];
	ffn_fc1_gpu_6	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_6\nColumns: \
49152-57343",
		pos="4016,856.05",
		width=5.7361];
	rmsnorm1 -> ffn_fc1_gpu_6	[pos="e,3809.3,895.42 3362.2,947.63 3488.7,935.57 3651.2,918.32 3795,897.55 3796.3,897.35 3797.7,897.16 3799,896.96"];
	ffn_fc1_gpu_7	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_7\nColumns: \
57344-65535",
		pos="4461,856.05",
		width=5.7361];
	rmsnorm1 -> ffn_fc1_gpu_7	[pos="e,4254.3,894.69 3362.2,959.9 3582.1,951.49 3931.6,933.53 4232,897.55 4236,897.07 4240,896.56 4244.1,896.04"];
	residual_ffn	[fillcolor=lightgreen,
		height=1.6303,
		label="Residual Add\nFFN\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="3930,294.86",
		shape=ellipse,
		width=8.5246];
	rmsnorm1 -> residual_ffn	[pos="e,4128.6,339.63 3362.3,963.56 3773,956.97 4627,938.41 4677,897.55 4730.9,853.47 4715,815.18 4715,745.55 4715,745.55 4715,745.55 \
4715,632.05 4715,506.44 4371.1,399.83 4138.5,342.09"];
	ffn_act_gpu_0	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: \
gpu_0",
		pos="1397,744.55",
		width=5.4861];
	ffn_fc1_gpu_0 -> ffn_act_gpu_0	[pos="e,1397.6,778.85 1398.3,814.27 1398.1,806.06 1397.9,797.41 1397.8,789.11"];
	ffn_act_gpu_1	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: \
gpu_1",
		pos="1824,744.55",
		width=5.4861];
	ffn_fc1_gpu_1 -> ffn_act_gpu_1	[pos="e,1829.5,778.85 1835.3,814.27 1833.9,805.97 1832.5,797.21 1831.1,788.83"];
	ffn_act_gpu_2	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: \
gpu_2",
		pos="2267,744.55",
		width=5.4861];
	ffn_fc1_gpu_2 -> ffn_act_gpu_2	[pos="e,2270.6,778.85 2274.5,814.27 2273.6,805.97 2272.7,797.21 2271.7,788.83"];
	ffn_act_gpu_3	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: \
gpu_3",
		pos="2707,744.55",
		width=5.4861];
	ffn_fc1_gpu_3 -> ffn_act_gpu_3	[pos="e,2708.8,778.85 2710.8,814.27 2710.3,806.06 2709.8,797.41 2709.4,789.11"];
	ffn_act_gpu_4	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: \
gpu_4",
		pos="3145,744.55",
		width=5.4861];
	ffn_fc1_gpu_4 -> ffn_act_gpu_4	[pos="e,3145,778.85 3145,814.27 3145,806.06 3145,797.41 3145,789.11"];
	ffn_act_gpu_5	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: \
gpu_5",
		pos="3582,744.55",
		width=5.4861];
	ffn_fc1_gpu_5 -> ffn_act_gpu_5	[pos="e,3581.1,778.85 3580.1,814.27 3580.3,806.06 3580.6,797.41 3580.8,789.11"];
	ffn_act_gpu_6	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: \
gpu_6",
		pos="4022,744.55",
		width=5.4861];
	ffn_fc1_gpu_6 -> ffn_act_gpu_6	[pos="e,4020.2,778.85 4018.2,814.27 4018.7,806.06 4019.2,797.41 4019.6,789.11"];
	ffn_act_gpu_7	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: \
gpu_7",
		pos="4467,744.55",
		width=5.4861];
	ffn_fc1_gpu_7 -> ffn_act_gpu_7	[pos="e,4465.2,778.85 4463.2,814.27 4463.7,806.06 4464.2,797.41 4464.6,789.11"];
	ffn_fc2_gpu_0	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_0\nRows: \
0-2047",
		pos="1373,633.05",
		width=5.9028];
	ffn_act_gpu_0 -> ffn_fc2_gpu_0	[pos="e,1381.9,674.82 1389.7,710.4 1387.9,702.25 1386,693.36 1384.1,684.6"];
	ffn_fc2_gpu_1	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_1\nRows: \
2048-4095",
		pos="1816,633.05",
		width=5.9028];
	ffn_act_gpu_1 -> ffn_fc2_gpu_1	[pos="e,1819,674.82 1821.6,710.4 1821,702.34 1820.3,693.55 1819.7,684.88"];
	ffn_fc2_gpu_2	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_2\nRows: \
4096-6143",
		pos="2259,633.05",
		width=5.9028];
	ffn_act_gpu_2 -> ffn_fc2_gpu_2	[pos="e,2262,674.82 2264.6,710.4 2264,702.34 2263.3,693.55 2262.7,684.88"];
	ffn_fc2_gpu_3	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_3\nRows: \
6144-8191",
		pos="2702,633.05",
		width=5.9028];
	ffn_act_gpu_3 -> ffn_fc2_gpu_3	[pos="e,2703.9,674.82 2705.5,710.4 2705.1,702.34 2704.7,693.55 2704.3,684.88"];
	ffn_fc2_gpu_4	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_4\nRows: \
8192-10239",
		pos="3145,633.05",
		width=5.9028];
	ffn_act_gpu_4 -> ffn_fc2_gpu_4	[pos="e,3145,674.82 3145,710.4 3145,702.34 3145,693.55 3145,684.88"];
	ffn_fc2_gpu_5	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_5\nRows: \
10240-12287",
		pos="3588,633.05",
		width=5.9028];
	ffn_act_gpu_5 -> ffn_fc2_gpu_5	[pos="e,3585.8,674.82 3583.8,710.4 3584.3,702.34 3584.7,693.55 3585.2,684.88"];
	ffn_fc2_gpu_6	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_6\nRows: \
12288-14335",
		pos="4031,633.05",
		width=5.9028];
	ffn_act_gpu_6 -> ffn_fc2_gpu_6	[pos="e,4027.6,674.82 4024.7,710.4 4025.4,702.34 4026.1,693.55 4026.8,684.88"];
	ffn_fc2_gpu_7	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_7\nRows: \
14336-16383",
		pos="4474,633.05",
		width=5.9028];
	ffn_act_gpu_7 -> ffn_fc2_gpu_7	[pos="e,4471.4,674.82 4469.1,710.4 4469.6,702.34 4470.2,693.55 4470.8,684.88"];
	ffn_all_reduce	[fillcolor=yellow,
		height=2.3056,
		label="All-Reduce\nFFN Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="3145,472.55",
		shape=parallelogram,
		width=12.499];
	ffn_fc2_gpu_0 -> ffn_all_reduce	[pos="e,2805.5,489.07 1585.5,592.67 1588.4,592.28 1591.2,591.91 1594,591.55 2004.8,538.42 2480.2,506.6 2795.3,489.62"];
	ffn_fc2_gpu_1 -> ffn_all_reduce	[pos="e,2819.4,501.74 2028.6,592.79 2031.4,592.37 2034.2,591.95 2037,591.55 2295.3,554.17 2589.1,523.42 2809.3,502.68"];
	ffn_fc2_gpu_2 -> ffn_all_reduce	[pos="e,2846.2,526.2 2471.6,593.06 2474.4,592.55 2477.2,592.05 2480,591.55 2596.2,570.59 2723.9,547.86 2836.1,527.99"];
	ffn_fc2_gpu_3 -> ffn_all_reduce	[pos="e,2915.6,555.62 2815.2,591.54 2843.2,581.52 2874.3,570.41 2905.8,559.13"];
	ffn_fc2_gpu_4 -> ffn_all_reduce	[pos="e,3145,555.82 3145,591.23 3145,583.3 3145,574.69 3145,565.86"];
	ffn_fc2_gpu_5 -> ffn_all_reduce	[pos="e,3374.4,555.62 3474.8,591.54 3446.8,581.52 3415.7,570.41 3384.2,559.13"];
	ffn_fc2_gpu_6 -> ffn_all_reduce	[pos="e,3591.8,552.46 3818.4,593.06 3815.6,592.55 3812.8,592.05 3810,591.55 3742.8,579.42 3671.7,566.7 3602,554.29"];
	ffn_fc2_gpu_7 -> ffn_all_reduce	[pos="e,3543,508.63 4261.4,592.79 4258.6,592.37 4255.8,591.95 4253,591.55 4022.1,558.15 3762.9,530.03 3553.3,509.63"];
	ffn_all_reduce -> residual_ffn	[pos="e,3733.1,339.92 3430.9,407.56 3527.6,385.91 3633.7,362.16 3723.1,342.16"];
	rmsnorm2	[fillcolor=lightgray,
		height=0.94444,
		label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all \
GPUs",
		pos="3930,166.17",
		width=6.0278];
	residual_ffn -> rmsnorm2	[pos="e,3930,200.28 3930,236.07 3930,227.45 3930,218.68 3930,210.42"];
	output	[fillcolor=lightgreen,
		height=1.3356,
		label="Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, vocab_size=128000]\nDevice: \
CPU",
		pos="3930,48.083",
		shape=ellipse,
		width=9.2317];
	rmsnorm2 -> output	[pos="e,3930,96.193 3930,131.9 3930,123.91 3930,115.14 3930,106.4"];
}
