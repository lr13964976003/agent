digraph DeepSeek_R1_Baseline_TP {
	rankdir=TB size="30,20"
	node [fillcolor=lightblue shape=rectangle style=filled]
	input [label="Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: CPU" fillcolor=lightgreen shape=ellipse]
	split_all_gpus [label="Split Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	qkv_proj_gpu_0 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=32]\nDevice: gpu_0\nColumns: 0-4095" fillcolor=lightcoral]
	qkv_proj_gpu_1 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=32]\nDevice: gpu_1\nColumns: 4096-8191" fillcolor=lightcoral]
	qkv_proj_gpu_2 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=32]\nDevice: gpu_2\nColumns: 8192-12287" fillcolor=lightcoral]
	qkv_proj_gpu_3 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=32]\nDevice: gpu_3\nColumns: 12288-16383" fillcolor=lightcoral]
	kv_cache_gpu_0 [label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nDevice: gpu_0\nDUPLICATED - Limitation" fillcolor=lightpink]
	kv_cache_gpu_1 [label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nDevice: gpu_1\nDUPLICATED - Limitation" fillcolor=lightpink]
	kv_cache_gpu_2 [label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nDevice: gpu_2\nDUPLICATED - Limitation" fillcolor=lightpink]
	kv_cache_gpu_3 [label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nDevice: gpu_3\nDUPLICATED - Limitation" fillcolor=lightpink]
	flash_attn_gpu_0 [label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_0" fillcolor=lightblue]
	flash_attn_gpu_1 [label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_1" fillcolor=lightblue]
	flash_attn_gpu_2 [label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_2" fillcolor=lightblue]
	flash_attn_gpu_3 [label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_3" fillcolor=lightblue]
	attn_all_reduce [label="All-Reduce\nAttention Output\nInput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	attn_out_gpu_0 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: gpu_0\nRows: 0-4095" fillcolor=lightcoral]
	attn_out_gpu_1 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: gpu_1\nRows: 4096-8191" fillcolor=lightcoral]
	attn_out_gpu_2 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: gpu_2\nRows: 8192-12287" fillcolor=lightcoral]
	attn_out_gpu_3 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: gpu_3\nRows: 12288-16383" fillcolor=lightcoral]
	attn_out_all_reduce [label="All-Reduce\nFinal Attention\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	residual_attn [label="Residual Add\nAttention\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgreen shape=ellipse]
	rmsnorm1 [label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgray]
	ffn_fc1_gpu_0 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: gpu_0\nColumns: 0-16383" fillcolor=lightblue]
	ffn_fc1_gpu_1 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: gpu_1\nColumns: 16384-32767" fillcolor=lightblue]
	ffn_fc1_gpu_2 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: gpu_2\nColumns: 32768-49151" fillcolor=lightblue]
	ffn_fc1_gpu_3 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: gpu_3\nColumns: 49152-65535" fillcolor=lightblue]
	ffn_act_gpu_0 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: gpu_0" fillcolor=lightblue]
	ffn_act_gpu_1 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: gpu_1" fillcolor=lightblue]
	ffn_act_gpu_2 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: gpu_2" fillcolor=lightblue]
	ffn_act_gpu_3 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: gpu_3" fillcolor=lightblue]
	ffn_fc2_gpu_0 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: gpu_0\nRows: 0-4095" fillcolor=lightblue]
	ffn_fc2_gpu_1 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: gpu_1\nRows: 4096-8191" fillcolor=lightblue]
	ffn_fc2_gpu_2 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: gpu_2\nRows: 8192-12287" fillcolor=lightblue]
	ffn_fc2_gpu_3 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: gpu_3\nRows: 12288-16383" fillcolor=lightblue]
	ffn_all_reduce [label="All-Reduce\nFFN Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	residual_ffn [label="Residual Add\nFFN\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgreen shape=ellipse]
	rmsnorm2 [label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgray]
	output [label="Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, vocab_size=32000]\nDevice: CPU" fillcolor=lightgreen shape=ellipse]
	input -> split_all_gpus
	split_all_gpus -> qkv_proj_gpu_0
	qkv_proj_gpu_0 -> kv_cache_gpu_0
	kv_cache_gpu_0 -> flash_attn_gpu_0
	split_all_gpus -> qkv_proj_gpu_1
	qkv_proj_gpu_1 -> kv_cache_gpu_1
	kv_cache_gpu_1 -> flash_attn_gpu_1
	split_all_gpus -> qkv_proj_gpu_2
	qkv_proj_gpu_2 -> kv_cache_gpu_2
	kv_cache_gpu_2 -> flash_attn_gpu_2
	split_all_gpus -> qkv_proj_gpu_3
	qkv_proj_gpu_3 -> kv_cache_gpu_3
	kv_cache_gpu_3 -> flash_attn_gpu_3
	flash_attn_gpu_0 -> attn_all_reduce
	attn_all_reduce -> attn_out_gpu_0
	attn_out_gpu_0 -> attn_out_all_reduce
	flash_attn_gpu_1 -> attn_all_reduce
	attn_all_reduce -> attn_out_gpu_1
	attn_out_gpu_1 -> attn_out_all_reduce
	flash_attn_gpu_2 -> attn_all_reduce
	attn_all_reduce -> attn_out_gpu_2
	attn_out_gpu_2 -> attn_out_all_reduce
	flash_attn_gpu_3 -> attn_all_reduce
	attn_all_reduce -> attn_out_gpu_3
	attn_out_gpu_3 -> attn_out_all_reduce
	attn_out_all_reduce -> residual_attn
	split_all_gpus -> residual_attn
	residual_attn -> rmsnorm1
	rmsnorm1 -> ffn_fc1_gpu_0
	ffn_fc1_gpu_0 -> ffn_act_gpu_0
	ffn_act_gpu_0 -> ffn_fc2_gpu_0
	ffn_fc2_gpu_0 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_1
	ffn_fc1_gpu_1 -> ffn_act_gpu_1
	ffn_act_gpu_1 -> ffn_fc2_gpu_1
	ffn_fc2_gpu_1 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_2
	ffn_fc1_gpu_2 -> ffn_act_gpu_2
	ffn_act_gpu_2 -> ffn_fc2_gpu_2
	ffn_fc2_gpu_2 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_3
	ffn_fc1_gpu_3 -> ffn_act_gpu_3
	ffn_act_gpu_3 -> ffn_fc2_gpu_3
	ffn_fc2_gpu_3 -> ffn_all_reduce
	ffn_all_reduce -> residual_ffn
	rmsnorm1 -> residual_ffn
	residual_ffn -> rmsnorm2
	rmsnorm2 -> output
}
