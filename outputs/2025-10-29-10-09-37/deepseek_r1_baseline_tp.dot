digraph DeepSeek_R1_Baseline_TP {
	graph [bb="0,0,2315.4,2324.1",
		rankdir=TB,
		size="30,20"
	];
	node [fillcolor=lightblue,
		label="\N",
		shape=rectangle,
		style=filled
	];
	input	[fillcolor=lightgreen,
		height=1.3356,
		label="Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: CPU",
		pos="1216,2276",
		shape=ellipse,
		width=8.5246];
	split_all_gpus	[fillcolor=yellow,
		height=1.8889,
		label="Split Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="1216,2123.9",
		shape=parallelogram,
		width=12.499];
	input -> split_all_gpus	[pos="e,1216,2192.1 1216,2227.5 1216,2219.4 1216,2210.9 1216,2202.2"];
	qkv_proj_gpu_0	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=32]\nDevice: \
gpu_0\nColumns: 0-4095",
		pos="236,1978.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_0	[pos="e,472.32,2018.6 775.58,2063 681.62,2049.7 582.8,2035.4 482.36,2020.1"];
	qkv_proj_gpu_1	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=32]\nDevice: \
gpu_1\nColumns: 4096-8191",
		pos="726,1978.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_1	[pos="e,864.53,2020 987.14,2055.9 948.86,2044.7 910.05,2033.3 874.47,2022.9"];
	qkv_proj_gpu_2	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=32]\nDevice: \
gpu_2\nColumns: 8192-12287",
		pos="1216,1978.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_2	[pos="e,1216,2019.9 1216,2055.8 1216,2047.2 1216,2038.5 1216,2030.2"];
	qkv_proj_gpu_3	[fillcolor=lightcoral,
		height=1.1528,
		label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=32]\nDevice: \
gpu_3\nColumns: 12288-16383",
		pos="1706,1978.4",
		width=6.5556];
	split_all_gpus -> qkv_proj_gpu_3	[pos="e,1567.5,2020 1444.9,2055.9 1483.1,2044.7 1522,2033.3 1557.5,2022.9"];
	residual_attn	[fillcolor=lightgreen,
		height=1.6303,
		label="Residual Add\nAttention\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="1628,1096.2",
		shape=ellipse,
		width=8.5246];
	split_all_gpus -> residual_attn	[pos="e,1742.4,1150.8 1561.1,2114.2 1724.4,2102.4 1895.7,2076.4 1951,2019.9 2001,1968.9 1970,1931.8 1970,1860.4 1970,1860.4 1970,1860.4 \
1970,1433.4 1970,1306.7 1849.2,1211.9 1751.2,1155.8"];
	kv_cache_gpu_0	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nDevice: \
gpu_0\nDUPLICATED - Limitation",
		pos="236,1859.4",
		width=6.4306];
	qkv_proj_gpu_0 -> kv_cache_gpu_0	[pos="e,236,1900.9 236,1936.8 236,1928.6 236,1919.8 236,1911.2"];
	kv_cache_gpu_1	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nDevice: \
gpu_1\nDUPLICATED - Limitation",
		pos="726,1859.4",
		width=6.4306];
	qkv_proj_gpu_1 -> kv_cache_gpu_1	[pos="e,726,1900.9 726,1936.8 726,1928.6 726,1919.8 726,1911.2"];
	kv_cache_gpu_2	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nDevice: \
gpu_2\nDUPLICATED - Limitation",
		pos="1216,1859.4",
		width=6.4306];
	qkv_proj_gpu_2 -> kv_cache_gpu_2	[pos="e,1216,1900.9 1216,1936.8 1216,1928.6 1216,1919.8 1216,1911.2"];
	kv_cache_gpu_3	[fillcolor=lightpink,
		height=1.1528,
		label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=1, d_k=128]\nDevice: \
gpu_3\nDUPLICATED - Limitation",
		pos="1706,1859.4",
		width=6.4306];
	qkv_proj_gpu_3 -> kv_cache_gpu_3	[pos="e,1706,1900.9 1706,1936.8 1706,1928.6 1706,1919.8 1706,1911.2"];
	flash_attn_gpu_0	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=\
128]\nDevice: gpu_0",
		pos="236,1747.9",
		width=6.5556];
	kv_cache_gpu_0 -> flash_attn_gpu_0	[pos="e,236,1782.2 236,1817.7 236,1809.4 236,1800.8 236,1792.5"];
	flash_attn_gpu_1	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=\
128]\nDevice: gpu_1",
		pos="726,1747.9",
		width=6.5556];
	kv_cache_gpu_1 -> flash_attn_gpu_1	[pos="e,726,1782.2 726,1817.7 726,1809.4 726,1800.8 726,1792.5"];
	flash_attn_gpu_2	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=\
128]\nDevice: gpu_2",
		pos="1216,1747.9",
		width=6.5556];
	kv_cache_gpu_2 -> flash_attn_gpu_2	[pos="e,1216,1782.2 1216,1817.7 1216,1809.4 1216,1800.8 1216,1792.5"];
	flash_attn_gpu_3	[height=0.94444,
		label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=\
128]\nDevice: gpu_3",
		pos="1706,1747.9",
		width=6.5556];
	kv_cache_gpu_3 -> flash_attn_gpu_3	[pos="e,1706,1782.2 1706,1817.7 1706,1809.4 1706,1800.8 1706,1792.5"];
	attn_all_reduce	[fillcolor=yellow,
		height=2.3056,
		label="All-Reduce\nAttention Output\nInput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, \
heads=128, d_k=128]\nDevice: all GPUs",
		pos="1029,1594.9",
		shape=parallelogram,
		width=13.852];
	flash_attn_gpu_0 -> attn_all_reduce	[pos="e,708.1,1657 409.54,1713.9 494.24,1697.8 599.04,1677.8 698.21,1658.9"];
	flash_attn_gpu_1 -> attn_all_reduce	[pos="e,864.3,1678 792.49,1713.8 811.4,1704.4 832.92,1693.6 855.1,1682.6"];
	flash_attn_gpu_2 -> attn_all_reduce	[pos="e,1130.8,1678.2 1175,1713.8 1164,1704.9 1151.6,1695 1138.8,1684.6"];
	flash_attn_gpu_3 -> attn_all_reduce	[pos="e,1396.7,1677.9 1557.8,1713.9 1512.4,1703.7 1460.2,1692.1 1406.7,1680.2"];
	attn_out_gpu_0	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: \
gpu_0\nRows: 0-4095",
		pos="400,1434.4",
		width=5.9028];
	attn_all_reduce -> attn_out_gpu_0	[pos="e,561.06,1476 703.45,1511.9 657.65,1500.4 612.18,1488.9 570.9,1478.5"];
	attn_out_gpu_1	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: \
gpu_1\nRows: 4096-8191",
		pos="843,1434.4",
		width=5.9028];
	attn_all_reduce -> attn_out_gpu_1	[pos="e,890.86,1476.2 932.73,1511.9 921.03,1501.9 909.4,1492 898.59,1482.8"];
	attn_out_gpu_2	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: \
gpu_2\nRows: 8192-12287",
		pos="1286,1434.4",
		width=5.9028];
	attn_all_reduce -> attn_out_gpu_2	[pos="e,1220.2,1476 1162,1511.9 1179.1,1501.4 1196,1490.9 1211.6,1481.3"];
	attn_out_gpu_3	[fillcolor=lightcoral,
		height=1.1528,
		label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: \
gpu_3\nRows: 12288-16383",
		pos="1729,1434.4",
		width=5.9028];
	attn_all_reduce -> attn_out_gpu_3	[pos="e,1549.9,1476 1338.6,1523.8 1407.2,1508.3 1477.9,1492.3 1540,1478.2"];
	attn_out_all_reduce	[fillcolor=yellow,
		height=2.3056,
		label="All-Reduce\nFinal Attention\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=\
16384]\nDevice: all GPUs",
		pos="1286,1273.9",
		shape=parallelogram,
		width=12.499];
	attn_out_gpu_0 -> attn_out_all_reduce	[pos="e,987.18,1327.6 612.6,1394.4 615.41,1393.9 618.22,1393.4 621,1392.9 737.21,1372 864.91,1349.2 977.06,1329.4"];
	attn_out_gpu_1 -> attn_out_all_reduce	[pos="e,1056.6,1357 956.22,1392.9 984.22,1382.9 1015.3,1371.8 1046.8,1360.5"];
	attn_out_gpu_2 -> attn_out_all_reduce	[pos="e,1286,1357.2 1286,1392.6 1286,1384.7 1286,1376.1 1286,1367.2"];
	attn_out_gpu_3 -> attn_out_all_reduce	[pos="e,1515.4,1357 1615.8,1392.9 1587.8,1382.9 1556.7,1371.8 1525.2,1360.5"];
	attn_out_all_reduce -> residual_attn	[pos="e,1522.6,1151.4 1445.7,1190.9 1468.7,1179.1 1491.9,1167.2 1513.7,1156"];
	rmsnorm1	[fillcolor=lightgray,
		height=0.94444,
		label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all \
GPUs",
		pos="1628,967.55",
		width=6.0278];
	residual_attn -> rmsnorm1	[pos="e,1628,1001.7 1628,1037.5 1628,1028.8 1628,1020.1 1628,1011.8"];
	ffn_fc1_gpu_0	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: gpu_0\nColumns: \
0-16383",
		pos="756,856.05",
		width=5.7361];
	rmsnorm1 -> ffn_fc1_gpu_0	[pos="e,962.71,895.28 1410.8,947.58 1284.3,935.51 1121.8,918.26 978,897.55 976.27,897.3 974.53,897.04 972.78,896.79"];
	ffn_fc1_gpu_1	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: gpu_1\nColumns: \
16384-32767",
		pos="1194,856.05",
		width=5.7361];
	rmsnorm1 -> ffn_fc1_gpu_1	[pos="e,1354.8,897.63 1497.2,933.54 1455.5,923.04 1408.8,911.25 1364.8,900.14"];
	ffn_fc1_gpu_2	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: gpu_2\nColumns: \
32768-49151",
		pos="1628,856.05",
		width=5.7361];
	rmsnorm1 -> ffn_fc1_gpu_2	[pos="e,1628,897.82 1628,933.4 1628,925.34 1628,916.55 1628,907.88"];
	ffn_fc1_gpu_3	[height=1.1528,
		label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: gpu_3\nColumns: \
49152-65535",
		pos="2062,856.05",
		width=5.7361];
	rmsnorm1 -> ffn_fc1_gpu_3	[pos="e,1901.2,897.63 1758.8,933.54 1800.5,923.04 1847.2,911.25 1891.2,900.14"];
	residual_ffn	[fillcolor=lightgreen,
		height=1.6303,
		label="Residual Add\nFFN\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="1970,294.86",
		shape=ellipse,
		width=8.5246];
	rmsnorm1 -> residual_ffn	[pos="e,2084.4,349.42 1845.4,955.92 2021.6,945.28 2245.7,926.32 2278,897.55 2329.7,851.47 2312,814.77 2312,745.55 2312,745.55 2312,745.55 \
2312,632.05 2312,505.36 2191.2,410.5 2093.2,354.4"];
	ffn_act_gpu_0	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: \
gpu_0",
		pos="752,744.55",
		width=5.6111];
	ffn_fc1_gpu_0 -> ffn_act_gpu_0	[pos="e,753.22,778.85 754.51,814.27 754.21,806.06 753.89,797.41 753.59,789.11"];
	ffn_act_gpu_1	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: \
gpu_1",
		pos="1189,744.55",
		width=5.6111];
	ffn_fc1_gpu_1 -> ffn_act_gpu_1	[pos="e,1190.5,778.85 1192.1,814.27 1191.8,806.06 1191.4,797.41 1191,789.11"];
	ffn_act_gpu_2	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: \
gpu_2",
		pos="1628,744.55",
		width=5.6111];
	ffn_fc1_gpu_2 -> ffn_act_gpu_2	[pos="e,1628,778.85 1628,814.27 1628,806.06 1628,797.41 1628,789.11"];
	ffn_act_gpu_3	[height=0.94444,
		label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=16384]\nDevice: \
gpu_3",
		pos="2065,744.55",
		width=5.6111];
	ffn_fc1_gpu_3 -> ffn_act_gpu_3	[pos="e,2064.1,778.85 2063.1,814.27 2063.3,806.06 2063.6,797.41 2063.8,789.11"];
	ffn_fc2_gpu_0	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: gpu_0\nRows: \
0-4095",
		pos="742,633.05",
		width=5.9028];
	ffn_act_gpu_0 -> ffn_fc2_gpu_0	[pos="e,745.72,674.82 748.97,710.4 748.24,702.34 747.43,693.55 746.64,684.88"];
	ffn_fc2_gpu_1	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: gpu_1\nRows: \
4096-8191",
		pos="1185,633.05",
		width=5.9028];
	ffn_act_gpu_1 -> ffn_fc2_gpu_1	[pos="e,1186.5,674.82 1187.8,710.4 1187.5,702.34 1187.2,693.55 1186.9,684.88"];
	ffn_fc2_gpu_2	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: gpu_2\nRows: \
8192-12287",
		pos="1628,633.05",
		width=5.9028];
	ffn_act_gpu_2 -> ffn_fc2_gpu_2	[pos="e,1628,674.82 1628,710.4 1628,702.34 1628,693.55 1628,684.88"];
	ffn_fc2_gpu_3	[height=1.1528,
		label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=4096]\nDevice: gpu_3\nRows: \
12288-16383",
		pos="2071,633.05",
		width=5.9028];
	ffn_act_gpu_3 -> ffn_fc2_gpu_3	[pos="e,2068.8,674.82 2066.8,710.4 2067.3,702.34 2067.7,693.55 2068.2,684.88"];
	ffn_all_reduce	[fillcolor=yellow,
		height=2.3056,
		label="All-Reduce\nFFN Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: \
all GPUs",
		pos="1628,472.55",
		shape=parallelogram,
		width=12.499];
	ffn_fc2_gpu_0 -> ffn_all_reduce	[pos="e,1329.2,526.2 954.6,593.06 957.41,592.55 960.22,592.05 963,591.55 1079.2,570.59 1206.9,547.86 1319.1,527.99"];
	ffn_fc2_gpu_1 -> ffn_all_reduce	[pos="e,1398.6,555.62 1298.2,591.54 1326.2,581.52 1357.3,570.41 1388.8,559.13"];
	ffn_fc2_gpu_2 -> ffn_all_reduce	[pos="e,1628,555.82 1628,591.23 1628,583.3 1628,574.69 1628,565.86"];
	ffn_fc2_gpu_3 -> ffn_all_reduce	[pos="e,1857.4,555.62 1957.8,591.54 1929.8,581.52 1898.7,570.41 1867.2,559.13"];
	ffn_all_reduce -> residual_ffn	[pos="e,1864.6,349.98 1787.7,389.49 1810.7,377.7 1833.9,365.78 1855.7,354.57"];
	rmsnorm2	[fillcolor=lightgray,
		height=0.94444,
		label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all \
GPUs",
		pos="1970,166.17",
		width=6.0278];
	residual_ffn -> rmsnorm2	[pos="e,1970,200.28 1970,236.07 1970,227.45 1970,218.68 1970,210.42"];
	output	[fillcolor=lightgreen,
		height=1.3356,
		label="Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, vocab_size=32000]\nDevice: \
CPU",
		pos="1970,48.083",
		shape=ellipse,
		width=9.0549];
	rmsnorm2 -> output	[pos="e,1970,96.193 1970,131.9 1970,123.91 1970,115.14 1970,106.4"];
}
