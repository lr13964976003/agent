digraph Llama_405B_Helix_Parallelism {
	rankdir=TB size="40,30"
	node [fillcolor=lightblue shape=rectangle style=filled]
	input [label="Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: CPU" fillcolor=lightgreen shape=ellipse]
	split_all_gpus [label="Split Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	qkv_proj_gpu_0 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_0\nQueries: 0-31, KV: 0-1" fillcolor=lightcoral]
	qkv_proj_gpu_1 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_1\nQueries: 0-31, KV: 0-1" fillcolor=lightcoral]
	qkv_proj_gpu_2 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_2\nQueries: 0-31, KV: 0-1" fillcolor=lightcoral]
	qkv_proj_gpu_3 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_3\nQueries: 0-31, KV: 0-1" fillcolor=lightcoral]
	qkv_proj_gpu_4 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_4\nQueries: 32-63, KV: 2-3" fillcolor=lightcoral]
	qkv_proj_gpu_5 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_5\nQueries: 32-63, KV: 2-3" fillcolor=lightcoral]
	qkv_proj_gpu_6 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_6\nQueries: 32-63, KV: 2-3" fillcolor=lightcoral]
	qkv_proj_gpu_7 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_7\nQueries: 32-63, KV: 2-3" fillcolor=lightcoral]
	qkv_proj_gpu_8 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_8\nQueries: 64-95, KV: 4-5" fillcolor=lightcoral]
	qkv_proj_gpu_9 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_9\nQueries: 64-95, KV: 4-5" fillcolor=lightcoral]
	qkv_proj_gpu_10 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_10\nQueries: 64-95, KV: 4-5" fillcolor=lightcoral]
	qkv_proj_gpu_11 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_11\nQueries: 64-95, KV: 4-5" fillcolor=lightcoral]
	qkv_proj_gpu_12 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_12\nQueries: 96-127, KV: 6-7" fillcolor=lightcoral]
	qkv_proj_gpu_13 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_13\nQueries: 96-127, KV: 6-7" fillcolor=lightcoral]
	qkv_proj_gpu_14 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_14\nQueries: 96-127, KV: 6-7" fillcolor=lightcoral]
	qkv_proj_gpu_15 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=32, d_k=128]\nDevice: gpu_15\nQueries: 96-127, KV: 6-7" fillcolor=lightcoral]
	kv_cache_gpu_0 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_0\nSequence: 0-249999" fillcolor=lightpink]
	kv_cache_gpu_1 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_1\nSequence: 0-249999" fillcolor=lightpink]
	kv_cache_gpu_2 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_2\nSequence: 0-249999" fillcolor=lightpink]
	kv_cache_gpu_3 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_3\nSequence: 0-249999" fillcolor=lightpink]
	kv_cache_gpu_4 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_4\nSequence: 250000-499999" fillcolor=lightpink]
	kv_cache_gpu_5 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_5\nSequence: 250000-499999" fillcolor=lightpink]
	kv_cache_gpu_6 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_6\nSequence: 250000-499999" fillcolor=lightpink]
	kv_cache_gpu_7 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_7\nSequence: 250000-499999" fillcolor=lightpink]
	kv_cache_gpu_8 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_8\nSequence: 500000-749999" fillcolor=lightpink]
	kv_cache_gpu_9 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_9\nSequence: 500000-749999" fillcolor=lightpink]
	kv_cache_gpu_10 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_10\nSequence: 500000-749999" fillcolor=lightpink]
	kv_cache_gpu_11 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_11\nSequence: 500000-749999" fillcolor=lightpink]
	kv_cache_gpu_12 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_12\nSequence: 750000-999999" fillcolor=lightpink]
	kv_cache_gpu_13 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_13\nSequence: 750000-999999" fillcolor=lightpink]
	kv_cache_gpu_14 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_14\nSequence: 750000-999999" fillcolor=lightpink]
	kv_cache_gpu_15 [label="KV Cache\nInput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=2, d_k=128]\nDevice: gpu_15\nSequence: 750000-999999" fillcolor=lightpink]
	flash_attn_gpu_0 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_0" fillcolor=lightblue]
	flash_attn_gpu_1 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_1" fillcolor=lightblue]
	flash_attn_gpu_2 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_2" fillcolor=lightblue]
	flash_attn_gpu_3 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_3" fillcolor=lightblue]
	flash_attn_gpu_4 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_4" fillcolor=lightblue]
	flash_attn_gpu_5 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_5" fillcolor=lightblue]
	flash_attn_gpu_6 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_6" fillcolor=lightblue]
	flash_attn_gpu_7 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_7" fillcolor=lightblue]
	flash_attn_gpu_8 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_8" fillcolor=lightblue]
	flash_attn_gpu_9 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_9" fillcolor=lightblue]
	flash_attn_gpu_10 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_10" fillcolor=lightblue]
	flash_attn_gpu_11 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_11" fillcolor=lightblue]
	flash_attn_gpu_12 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_12" fillcolor=lightblue]
	flash_attn_gpu_13 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_13" fillcolor=lightblue]
	flash_attn_gpu_14 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_14" fillcolor=lightblue]
	flash_attn_gpu_15 [label="FlashAttention\nInput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nOutput: [batch_size=1, seq_len=62500, heads=32, d_k=128]\nDevice: gpu_15" fillcolor=lightblue]
	all2all_group_0 [label="All-to-All\nGroup 0\nInput: [batch_size=1, seq_len=250000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=250000, heads=128, d_k=128]\nVolume: 1×16384×0.5 bytes\nDevice: group_0" fillcolor=yellow shape=parallelogram]
	all2all_group_1 [label="All-to-All\nGroup 1\nInput: [batch_size=1, seq_len=250000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=250000, heads=128, d_k=128]\nVolume: 1×16384×0.5 bytes\nDevice: group_1" fillcolor=yellow shape=parallelogram]
	all2all_group_2 [label="All-to-All\nGroup 2\nInput: [batch_size=1, seq_len=250000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=250000, heads=128, d_k=128]\nVolume: 1×16384×0.5 bytes\nDevice: group_2" fillcolor=yellow shape=parallelogram]
	all2all_group_3 [label="All-to-All\nGroup 3\nInput: [batch_size=1, seq_len=250000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=250000, heads=128, d_k=128]\nVolume: 1×16384×0.5 bytes\nDevice: group_3" fillcolor=yellow shape=parallelogram]
	global_all2all [label="Global All-to-All\nCross-Group Exchange\nInput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nVolume: 1×16384×0.5 bytes\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	attn_out_gpu_0 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_0\nRows: 0-1023" fillcolor=lightcoral]
	attn_out_gpu_1 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_1\nRows: 1024-2047" fillcolor=lightcoral]
	attn_out_gpu_2 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_2\nRows: 2048-3071" fillcolor=lightcoral]
	attn_out_gpu_3 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_3\nRows: 3072-4095" fillcolor=lightcoral]
	attn_out_gpu_4 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_4\nRows: 4096-5119" fillcolor=lightcoral]
	attn_out_gpu_5 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_5\nRows: 5120-6143" fillcolor=lightcoral]
	attn_out_gpu_6 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_6\nRows: 6144-7167" fillcolor=lightcoral]
	attn_out_gpu_7 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_7\nRows: 7168-8191" fillcolor=lightcoral]
	attn_out_gpu_8 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_8\nRows: 8192-9215" fillcolor=lightcoral]
	attn_out_gpu_9 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_9\nRows: 9216-10239" fillcolor=lightcoral]
	attn_out_gpu_10 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_10\nRows: 10240-11263" fillcolor=lightcoral]
	attn_out_gpu_11 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_11\nRows: 11264-12287" fillcolor=lightcoral]
	attn_out_gpu_12 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_12\nRows: 12288-13311" fillcolor=lightcoral]
	attn_out_gpu_13 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_13\nRows: 13312-14335" fillcolor=lightcoral]
	attn_out_gpu_14 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_14\nRows: 14336-15359" fillcolor=lightcoral]
	attn_out_gpu_15 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=1024]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_15\nRows: 15360-16383" fillcolor=lightcoral]
	attn_all_reduce [label="All-Reduce\nAttention Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	residual_attn [label="Residual Add\nAttention\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgreen shape=ellipse]
	rmsnorm1 [label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgray]
	ffn_fc1_gpu_0 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_0\nColumns: 0-4095" fillcolor=lightblue]
	ffn_fc1_gpu_1 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_1\nColumns: 4096-8191" fillcolor=lightblue]
	ffn_fc1_gpu_2 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_2\nColumns: 8192-12287" fillcolor=lightblue]
	ffn_fc1_gpu_3 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_3\nColumns: 12288-16383" fillcolor=lightblue]
	ffn_fc1_gpu_4 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_4\nColumns: 16384-20479" fillcolor=lightblue]
	ffn_fc1_gpu_5 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_5\nColumns: 20480-24575" fillcolor=lightblue]
	ffn_fc1_gpu_6 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_6\nColumns: 24576-28671" fillcolor=lightblue]
	ffn_fc1_gpu_7 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_7\nColumns: 28672-32767" fillcolor=lightblue]
	ffn_fc1_gpu_8 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_8\nColumns: 32768-36863" fillcolor=lightblue]
	ffn_fc1_gpu_9 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_9\nColumns: 36864-40959" fillcolor=lightblue]
	ffn_fc1_gpu_10 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_10\nColumns: 40960-45055" fillcolor=lightblue]
	ffn_fc1_gpu_11 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_11\nColumns: 45056-49151" fillcolor=lightblue]
	ffn_fc1_gpu_12 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_12\nColumns: 49152-53247" fillcolor=lightblue]
	ffn_fc1_gpu_13 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_13\nColumns: 53248-57343" fillcolor=lightblue]
	ffn_fc1_gpu_14 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_14\nColumns: 57344-61439" fillcolor=lightblue]
	ffn_fc1_gpu_15 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_15\nColumns: 61440-65535" fillcolor=lightblue]
	ffn_act_gpu_0 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_0" fillcolor=lightblue]
	ffn_act_gpu_1 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_1" fillcolor=lightblue]
	ffn_act_gpu_2 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_2" fillcolor=lightblue]
	ffn_act_gpu_3 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_3" fillcolor=lightblue]
	ffn_act_gpu_4 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_4" fillcolor=lightblue]
	ffn_act_gpu_5 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_5" fillcolor=lightblue]
	ffn_act_gpu_6 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_6" fillcolor=lightblue]
	ffn_act_gpu_7 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_7" fillcolor=lightblue]
	ffn_act_gpu_8 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_8" fillcolor=lightblue]
	ffn_act_gpu_9 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_9" fillcolor=lightblue]
	ffn_act_gpu_10 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_10" fillcolor=lightblue]
	ffn_act_gpu_11 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_11" fillcolor=lightblue]
	ffn_act_gpu_12 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_12" fillcolor=lightblue]
	ffn_act_gpu_13 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_13" fillcolor=lightblue]
	ffn_act_gpu_14 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_14" fillcolor=lightblue]
	ffn_act_gpu_15 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, ffn=4096]\nDevice: gpu_15" fillcolor=lightblue]
	ffn_fc2_gpu_0 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_0\nRows: 0-1023" fillcolor=lightblue]
	ffn_fc2_gpu_1 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_1\nRows: 1024-2047" fillcolor=lightblue]
	ffn_fc2_gpu_2 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_2\nRows: 2048-3071" fillcolor=lightblue]
	ffn_fc2_gpu_3 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_3\nRows: 3072-4095" fillcolor=lightblue]
	ffn_fc2_gpu_4 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_4\nRows: 4096-5119" fillcolor=lightblue]
	ffn_fc2_gpu_5 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_5\nRows: 5120-6143" fillcolor=lightblue]
	ffn_fc2_gpu_6 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_6\nRows: 6144-7167" fillcolor=lightblue]
	ffn_fc2_gpu_7 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_7\nRows: 7168-8191" fillcolor=lightblue]
	ffn_fc2_gpu_8 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_8\nRows: 8192-9215" fillcolor=lightblue]
	ffn_fc2_gpu_9 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_9\nRows: 9216-10239" fillcolor=lightblue]
	ffn_fc2_gpu_10 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_10\nRows: 10240-11263" fillcolor=lightblue]
	ffn_fc2_gpu_11 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_11\nRows: 11264-12287" fillcolor=lightblue]
	ffn_fc2_gpu_12 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_12\nRows: 12288-13311" fillcolor=lightblue]
	ffn_fc2_gpu_13 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_13\nRows: 13312-14335" fillcolor=lightblue]
	ffn_fc2_gpu_14 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_14\nRows: 14336-15359" fillcolor=lightblue]
	ffn_fc2_gpu_15 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=4096]\nOutput: [batch_size=1, seq_len=1000000, hidden=1024]\nDevice: gpu_15\nRows: 15360-16383" fillcolor=lightblue]
	ffn_all_reduce [label="All-Reduce\nFFN Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	residual_ffn [label="Residual Add\nFFN\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgreen shape=ellipse]
	rmsnorm2 [label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgray]
	output [label="Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, vocab_size=128000]\nDevice: CPU" fillcolor=lightgreen shape=ellipse]
	input -> split_all_gpus
	split_all_gpus -> qkv_proj_gpu_0
	qkv_proj_gpu_0 -> kv_cache_gpu_0
	kv_cache_gpu_0 -> flash_attn_gpu_0
	split_all_gpus -> qkv_proj_gpu_1
	qkv_proj_gpu_1 -> kv_cache_gpu_1
	kv_cache_gpu_1 -> flash_attn_gpu_1
	split_all_gpus -> qkv_proj_gpu_2
	qkv_proj_gpu_2 -> kv_cache_gpu_2
	kv_cache_gpu_2 -> flash_attn_gpu_2
	split_all_gpus -> qkv_proj_gpu_3
	qkv_proj_gpu_3 -> kv_cache_gpu_3
	kv_cache_gpu_3 -> flash_attn_gpu_3
	split_all_gpus -> qkv_proj_gpu_4
	qkv_proj_gpu_4 -> kv_cache_gpu_4
	kv_cache_gpu_4 -> flash_attn_gpu_4
	split_all_gpus -> qkv_proj_gpu_5
	qkv_proj_gpu_5 -> kv_cache_gpu_5
	kv_cache_gpu_5 -> flash_attn_gpu_5
	split_all_gpus -> qkv_proj_gpu_6
	qkv_proj_gpu_6 -> kv_cache_gpu_6
	kv_cache_gpu_6 -> flash_attn_gpu_6
	split_all_gpus -> qkv_proj_gpu_7
	qkv_proj_gpu_7 -> kv_cache_gpu_7
	kv_cache_gpu_7 -> flash_attn_gpu_7
	split_all_gpus -> qkv_proj_gpu_8
	qkv_proj_gpu_8 -> kv_cache_gpu_8
	kv_cache_gpu_8 -> flash_attn_gpu_8
	split_all_gpus -> qkv_proj_gpu_9
	qkv_proj_gpu_9 -> kv_cache_gpu_9
	kv_cache_gpu_9 -> flash_attn_gpu_9
	split_all_gpus -> qkv_proj_gpu_10
	qkv_proj_gpu_10 -> kv_cache_gpu_10
	kv_cache_gpu_10 -> flash_attn_gpu_10
	split_all_gpus -> qkv_proj_gpu_11
	qkv_proj_gpu_11 -> kv_cache_gpu_11
	kv_cache_gpu_11 -> flash_attn_gpu_11
	split_all_gpus -> qkv_proj_gpu_12
	qkv_proj_gpu_12 -> kv_cache_gpu_12
	kv_cache_gpu_12 -> flash_attn_gpu_12
	split_all_gpus -> qkv_proj_gpu_13
	qkv_proj_gpu_13 -> kv_cache_gpu_13
	kv_cache_gpu_13 -> flash_attn_gpu_13
	split_all_gpus -> qkv_proj_gpu_14
	qkv_proj_gpu_14 -> kv_cache_gpu_14
	kv_cache_gpu_14 -> flash_attn_gpu_14
	split_all_gpus -> qkv_proj_gpu_15
	qkv_proj_gpu_15 -> kv_cache_gpu_15
	kv_cache_gpu_15 -> flash_attn_gpu_15
	flash_attn_gpu_0 -> all2all_group_0
	flash_attn_gpu_1 -> all2all_group_0
	flash_attn_gpu_2 -> all2all_group_0
	flash_attn_gpu_3 -> all2all_group_0
	all2all_group_0 -> global_all2all
	all2all_group_0 -> global_all2all
	all2all_group_0 -> global_all2all
	all2all_group_0 -> global_all2all
	flash_attn_gpu_4 -> all2all_group_1
	flash_attn_gpu_5 -> all2all_group_1
	flash_attn_gpu_6 -> all2all_group_1
	flash_attn_gpu_7 -> all2all_group_1
	all2all_group_1 -> global_all2all
	all2all_group_1 -> global_all2all
	all2all_group_1 -> global_all2all
	all2all_group_1 -> global_all2all
	flash_attn_gpu_8 -> all2all_group_2
	flash_attn_gpu_9 -> all2all_group_2
	flash_attn_gpu_10 -> all2all_group_2
	flash_attn_gpu_11 -> all2all_group_2
	all2all_group_2 -> global_all2all
	all2all_group_2 -> global_all2all
	all2all_group_2 -> global_all2all
	all2all_group_2 -> global_all2all
	flash_attn_gpu_12 -> all2all_group_3
	flash_attn_gpu_13 -> all2all_group_3
	flash_attn_gpu_14 -> all2all_group_3
	flash_attn_gpu_15 -> all2all_group_3
	all2all_group_3 -> global_all2all
	all2all_group_3 -> global_all2all
	all2all_group_3 -> global_all2all
	all2all_group_3 -> global_all2all
	global_all2all -> attn_out_gpu_0
	attn_out_gpu_0 -> attn_all_reduce
	global_all2all -> attn_out_gpu_1
	attn_out_gpu_1 -> attn_all_reduce
	global_all2all -> attn_out_gpu_2
	attn_out_gpu_2 -> attn_all_reduce
	global_all2all -> attn_out_gpu_3
	attn_out_gpu_3 -> attn_all_reduce
	global_all2all -> attn_out_gpu_4
	attn_out_gpu_4 -> attn_all_reduce
	global_all2all -> attn_out_gpu_5
	attn_out_gpu_5 -> attn_all_reduce
	global_all2all -> attn_out_gpu_6
	attn_out_gpu_6 -> attn_all_reduce
	global_all2all -> attn_out_gpu_7
	attn_out_gpu_7 -> attn_all_reduce
	global_all2all -> attn_out_gpu_8
	attn_out_gpu_8 -> attn_all_reduce
	global_all2all -> attn_out_gpu_9
	attn_out_gpu_9 -> attn_all_reduce
	global_all2all -> attn_out_gpu_10
	attn_out_gpu_10 -> attn_all_reduce
	global_all2all -> attn_out_gpu_11
	attn_out_gpu_11 -> attn_all_reduce
	global_all2all -> attn_out_gpu_12
	attn_out_gpu_12 -> attn_all_reduce
	global_all2all -> attn_out_gpu_13
	attn_out_gpu_13 -> attn_all_reduce
	global_all2all -> attn_out_gpu_14
	attn_out_gpu_14 -> attn_all_reduce
	global_all2all -> attn_out_gpu_15
	attn_out_gpu_15 -> attn_all_reduce
	attn_all_reduce -> residual_attn
	split_all_gpus -> residual_attn
	residual_attn -> rmsnorm1
	rmsnorm1 -> ffn_fc1_gpu_0
	ffn_fc1_gpu_0 -> ffn_act_gpu_0
	ffn_act_gpu_0 -> ffn_fc2_gpu_0
	ffn_fc2_gpu_0 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_1
	ffn_fc1_gpu_1 -> ffn_act_gpu_1
	ffn_act_gpu_1 -> ffn_fc2_gpu_1
	ffn_fc2_gpu_1 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_2
	ffn_fc1_gpu_2 -> ffn_act_gpu_2
	ffn_act_gpu_2 -> ffn_fc2_gpu_2
	ffn_fc2_gpu_2 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_3
	ffn_fc1_gpu_3 -> ffn_act_gpu_3
	ffn_act_gpu_3 -> ffn_fc2_gpu_3
	ffn_fc2_gpu_3 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_4
	ffn_fc1_gpu_4 -> ffn_act_gpu_4
	ffn_act_gpu_4 -> ffn_fc2_gpu_4
	ffn_fc2_gpu_4 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_5
	ffn_fc1_gpu_5 -> ffn_act_gpu_5
	ffn_act_gpu_5 -> ffn_fc2_gpu_5
	ffn_fc2_gpu_5 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_6
	ffn_fc1_gpu_6 -> ffn_act_gpu_6
	ffn_act_gpu_6 -> ffn_fc2_gpu_6
	ffn_fc2_gpu_6 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_7
	ffn_fc1_gpu_7 -> ffn_act_gpu_7
	ffn_act_gpu_7 -> ffn_fc2_gpu_7
	ffn_fc2_gpu_7 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_8
	ffn_fc1_gpu_8 -> ffn_act_gpu_8
	ffn_act_gpu_8 -> ffn_fc2_gpu_8
	ffn_fc2_gpu_8 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_9
	ffn_fc1_gpu_9 -> ffn_act_gpu_9
	ffn_act_gpu_9 -> ffn_fc2_gpu_9
	ffn_fc2_gpu_9 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_10
	ffn_fc1_gpu_10 -> ffn_act_gpu_10
	ffn_act_gpu_10 -> ffn_fc2_gpu_10
	ffn_fc2_gpu_10 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_11
	ffn_fc1_gpu_11 -> ffn_act_gpu_11
	ffn_act_gpu_11 -> ffn_fc2_gpu_11
	ffn_fc2_gpu_11 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_12
	ffn_fc1_gpu_12 -> ffn_act_gpu_12
	ffn_act_gpu_12 -> ffn_fc2_gpu_12
	ffn_fc2_gpu_12 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_13
	ffn_fc1_gpu_13 -> ffn_act_gpu_13
	ffn_act_gpu_13 -> ffn_fc2_gpu_13
	ffn_fc2_gpu_13 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_14
	ffn_fc1_gpu_14 -> ffn_act_gpu_14
	ffn_act_gpu_14 -> ffn_fc2_gpu_14
	ffn_fc2_gpu_14 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_15
	ffn_fc1_gpu_15 -> ffn_act_gpu_15
	ffn_act_gpu_15 -> ffn_fc2_gpu_15
	ffn_fc2_gpu_15 -> ffn_all_reduce
	ffn_all_reduce -> residual_ffn
	rmsnorm1 -> residual_ffn
	residual_ffn -> rmsnorm2
	rmsnorm2 -> output
}
