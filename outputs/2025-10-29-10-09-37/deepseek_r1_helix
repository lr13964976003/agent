digraph DeepSeek_R1_Helix_Parallelism {
	rankdir=TB size="30,20"
	node [fillcolor=lightblue shape=rectangle style=filled]
	input [label="Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: CPU" fillcolor=lightgreen shape=ellipse]
	split_all_gpus [label="Split Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	qkv_proj_gpu_0 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: gpu_0" fillcolor=lightcoral]
	qkv_proj_gpu_1 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: gpu_1" fillcolor=lightcoral]
	qkv_proj_gpu_2 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: gpu_2" fillcolor=lightcoral]
	qkv_proj_gpu_3 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: gpu_3" fillcolor=lightcoral]
	qkv_proj_gpu_4 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: gpu_4" fillcolor=lightcoral]
	qkv_proj_gpu_5 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: gpu_5" fillcolor=lightcoral]
	qkv_proj_gpu_6 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: gpu_6" fillcolor=lightcoral]
	qkv_proj_gpu_7 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: gpu_7" fillcolor=lightcoral]
	kv_cache_gpu_0 [label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: gpu_0\nTokens: 0-124999" fillcolor=lightpink]
	kv_cache_gpu_1 [label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: gpu_1\nTokens: 125000-249999" fillcolor=lightpink]
	kv_cache_gpu_2 [label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: gpu_2\nTokens: 250000-374999" fillcolor=lightpink]
	kv_cache_gpu_3 [label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: gpu_3\nTokens: 375000-499999" fillcolor=lightpink]
	kv_cache_gpu_4 [label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: gpu_4\nTokens: 500000-624999" fillcolor=lightpink]
	kv_cache_gpu_5 [label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: gpu_5\nTokens: 625000-749999" fillcolor=lightpink]
	kv_cache_gpu_6 [label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: gpu_6\nTokens: 750000-874999" fillcolor=lightpink]
	kv_cache_gpu_7 [label="KV Cache\nInput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=1, d_k=128]\nDevice: gpu_7\nTokens: 875000-999999" fillcolor=lightpink]
	flash_attn_gpu_0 [label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nDevice: gpu_0" fillcolor=lightblue]
	flash_attn_gpu_1 [label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nDevice: gpu_1" fillcolor=lightblue]
	flash_attn_gpu_2 [label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nDevice: gpu_2" fillcolor=lightblue]
	flash_attn_gpu_3 [label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nDevice: gpu_3" fillcolor=lightblue]
	flash_attn_gpu_4 [label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nDevice: gpu_4" fillcolor=lightblue]
	flash_attn_gpu_5 [label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nDevice: gpu_5" fillcolor=lightblue]
	flash_attn_gpu_6 [label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nDevice: gpu_6" fillcolor=lightblue]
	flash_attn_gpu_7 [label="FlashAttention\nInput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=125000, heads=128, d_k=128]\nDevice: gpu_7" fillcolor=lightblue]
	all2all [label="All-to-All\nExchange Query Heads\nInput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nVolume: 1×128×128×0.5 bytes\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	attn_out_gpu_0 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_0\nRows: 0-2047" fillcolor=lightcoral]
	attn_out_gpu_1 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_1\nRows: 2048-4095" fillcolor=lightcoral]
	attn_out_gpu_2 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_2\nRows: 4096-6143" fillcolor=lightcoral]
	attn_out_gpu_3 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_3\nRows: 6144-8191" fillcolor=lightcoral]
	attn_out_gpu_4 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_4\nRows: 8192-10239" fillcolor=lightcoral]
	attn_out_gpu_5 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_5\nRows: 10240-12287" fillcolor=lightcoral]
	attn_out_gpu_6 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_6\nRows: 12288-14335" fillcolor=lightcoral]
	attn_out_gpu_7 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_7\nRows: 14336-16383" fillcolor=lightcoral]
	attn_all_reduce [label="All-Reduce\nAttention Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	residual_attn [label="Residual Add\nAttention\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgreen shape=ellipse]
	rmsnorm1 [label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgray]
	expert_gate_gpu_0 [label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_0\nExperts: 224-255" fillcolor=orange shape=diamond style=dashed]
	expert_gate_gpu_1 [label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_1\nExperts: 224-255" fillcolor=orange shape=diamond style=dashed]
	expert_gate_gpu_2 [label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_2\nExperts: 224-255" fillcolor=orange shape=diamond style=dashed]
	expert_gate_gpu_3 [label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_3\nExperts: 224-255" fillcolor=orange shape=diamond style=dashed]
	expert_gate_gpu_4 [label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_4\nExperts: 224-255" fillcolor=orange shape=diamond style=dashed]
	expert_gate_gpu_5 [label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_5\nExperts: 224-255" fillcolor=orange shape=diamond style=dashed]
	expert_gate_gpu_6 [label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_6\nExperts: 224-255" fillcolor=orange shape=diamond style=dashed]
	expert_gate_gpu_7 [label="Expert Gate\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_7\nExperts: 224-255" fillcolor=orange shape=diamond style=dashed]
	expert_fc1_gpu_0 [label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_0" fillcolor=lightblue]
	expert_fc1_gpu_1 [label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_1" fillcolor=lightblue]
	expert_fc1_gpu_2 [label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_2" fillcolor=lightblue]
	expert_fc1_gpu_3 [label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_3" fillcolor=lightblue]
	expert_fc1_gpu_4 [label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_4" fillcolor=lightblue]
	expert_fc1_gpu_5 [label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_5" fillcolor=lightblue]
	expert_fc1_gpu_6 [label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_6" fillcolor=lightblue]
	expert_fc1_gpu_7 [label="Expert FC1\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_7" fillcolor=lightblue]
	expert_act_gpu_0 [label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_0" fillcolor=lightblue]
	expert_act_gpu_1 [label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_1" fillcolor=lightblue]
	expert_act_gpu_2 [label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_2" fillcolor=lightblue]
	expert_act_gpu_3 [label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_3" fillcolor=lightblue]
	expert_act_gpu_4 [label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_4" fillcolor=lightblue]
	expert_act_gpu_5 [label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_5" fillcolor=lightblue]
	expert_act_gpu_6 [label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_6" fillcolor=lightblue]
	expert_act_gpu_7 [label="SiLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_7" fillcolor=lightblue]
	expert_fc2_gpu_0 [label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_0" fillcolor=lightblue]
	expert_fc2_gpu_1 [label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_1" fillcolor=lightblue]
	expert_fc2_gpu_2 [label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_2" fillcolor=lightblue]
	expert_fc2_gpu_3 [label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_3" fillcolor=lightblue]
	expert_fc2_gpu_4 [label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_4" fillcolor=lightblue]
	expert_fc2_gpu_5 [label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_5" fillcolor=lightblue]
	expert_fc2_gpu_6 [label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_6" fillcolor=lightblue]
	expert_fc2_gpu_7 [label="Expert FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: gpu_7" fillcolor=lightblue]
	ffn_all_reduce [label="All-Reduce\nFFN Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	residual_ffn [label="Residual Add\nFFN\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgreen shape=ellipse]
	rmsnorm2 [label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgray]
	output [label="Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, vocab_size=32000]\nDevice: CPU" fillcolor=lightgreen shape=ellipse]
	input -> split_all_gpus
	split_all_gpus -> qkv_proj_gpu_0
	qkv_proj_gpu_0 -> kv_cache_gpu_0
	kv_cache_gpu_0 -> flash_attn_gpu_0
	split_all_gpus -> qkv_proj_gpu_1
	qkv_proj_gpu_1 -> kv_cache_gpu_1
	kv_cache_gpu_1 -> flash_attn_gpu_1
	split_all_gpus -> qkv_proj_gpu_2
	qkv_proj_gpu_2 -> kv_cache_gpu_2
	kv_cache_gpu_2 -> flash_attn_gpu_2
	split_all_gpus -> qkv_proj_gpu_3
	qkv_proj_gpu_3 -> kv_cache_gpu_3
	kv_cache_gpu_3 -> flash_attn_gpu_3
	split_all_gpus -> qkv_proj_gpu_4
	qkv_proj_gpu_4 -> kv_cache_gpu_4
	kv_cache_gpu_4 -> flash_attn_gpu_4
	split_all_gpus -> qkv_proj_gpu_5
	qkv_proj_gpu_5 -> kv_cache_gpu_5
	kv_cache_gpu_5 -> flash_attn_gpu_5
	split_all_gpus -> qkv_proj_gpu_6
	qkv_proj_gpu_6 -> kv_cache_gpu_6
	kv_cache_gpu_6 -> flash_attn_gpu_6
	split_all_gpus -> qkv_proj_gpu_7
	qkv_proj_gpu_7 -> kv_cache_gpu_7
	kv_cache_gpu_7 -> flash_attn_gpu_7
	flash_attn_gpu_0 -> all2all
	flash_attn_gpu_1 -> all2all
	flash_attn_gpu_2 -> all2all
	flash_attn_gpu_3 -> all2all
	flash_attn_gpu_4 -> all2all
	flash_attn_gpu_5 -> all2all
	flash_attn_gpu_6 -> all2all
	flash_attn_gpu_7 -> all2all
	all2all -> attn_out_gpu_0
	attn_out_gpu_0 -> attn_all_reduce
	all2all -> attn_out_gpu_1
	attn_out_gpu_1 -> attn_all_reduce
	all2all -> attn_out_gpu_2
	attn_out_gpu_2 -> attn_all_reduce
	all2all -> attn_out_gpu_3
	attn_out_gpu_3 -> attn_all_reduce
	all2all -> attn_out_gpu_4
	attn_out_gpu_4 -> attn_all_reduce
	all2all -> attn_out_gpu_5
	attn_out_gpu_5 -> attn_all_reduce
	all2all -> attn_out_gpu_6
	attn_out_gpu_6 -> attn_all_reduce
	all2all -> attn_out_gpu_7
	attn_out_gpu_7 -> attn_all_reduce
	attn_all_reduce -> residual_attn
	split_all_gpus -> residual_attn
	residual_attn -> rmsnorm1
	rmsnorm1 -> expert_gate_gpu_0
	expert_gate_gpu_0 -> expert_fc1_gpu_0
	expert_fc1_gpu_0 -> expert_act_gpu_0
	expert_act_gpu_0 -> expert_fc2_gpu_0
	expert_fc2_gpu_0 -> ffn_all_reduce
	rmsnorm1 -> expert_gate_gpu_1
	expert_gate_gpu_1 -> expert_fc1_gpu_1
	expert_fc1_gpu_1 -> expert_act_gpu_1
	expert_act_gpu_1 -> expert_fc2_gpu_1
	expert_fc2_gpu_1 -> ffn_all_reduce
	rmsnorm1 -> expert_gate_gpu_2
	expert_gate_gpu_2 -> expert_fc1_gpu_2
	expert_fc1_gpu_2 -> expert_act_gpu_2
	expert_act_gpu_2 -> expert_fc2_gpu_2
	expert_fc2_gpu_2 -> ffn_all_reduce
	rmsnorm1 -> expert_gate_gpu_3
	expert_gate_gpu_3 -> expert_fc1_gpu_3
	expert_fc1_gpu_3 -> expert_act_gpu_3
	expert_act_gpu_3 -> expert_fc2_gpu_3
	expert_fc2_gpu_3 -> ffn_all_reduce
	rmsnorm1 -> expert_gate_gpu_4
	expert_gate_gpu_4 -> expert_fc1_gpu_4
	expert_fc1_gpu_4 -> expert_act_gpu_4
	expert_act_gpu_4 -> expert_fc2_gpu_4
	expert_fc2_gpu_4 -> ffn_all_reduce
	rmsnorm1 -> expert_gate_gpu_5
	expert_gate_gpu_5 -> expert_fc1_gpu_5
	expert_fc1_gpu_5 -> expert_act_gpu_5
	expert_act_gpu_5 -> expert_fc2_gpu_5
	expert_fc2_gpu_5 -> ffn_all_reduce
	rmsnorm1 -> expert_gate_gpu_6
	expert_gate_gpu_6 -> expert_fc1_gpu_6
	expert_fc1_gpu_6 -> expert_act_gpu_6
	expert_act_gpu_6 -> expert_fc2_gpu_6
	expert_fc2_gpu_6 -> ffn_all_reduce
	rmsnorm1 -> expert_gate_gpu_7
	expert_gate_gpu_7 -> expert_fc1_gpu_7
	expert_fc1_gpu_7 -> expert_act_gpu_7
	expert_act_gpu_7 -> expert_fc2_gpu_7
	expert_fc2_gpu_7 -> ffn_all_reduce
	ffn_all_reduce -> residual_ffn
	rmsnorm1 -> residual_ffn
	residual_ffn -> rmsnorm2
	rmsnorm2 -> output
}
