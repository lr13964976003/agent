digraph Llama_405B_Baseline_TP {
	rankdir=TB size="30,20"
	node [fillcolor=lightblue shape=rectangle style=filled]
	input [label="Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: CPU" fillcolor=lightgreen shape=ellipse]
	split_all_gpus [label="Split Input\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	qkv_proj_gpu_0 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_0\nColumns: 0-2047\nQueries: 0-15, KV: 0-0" fillcolor=lightcoral]
	qkv_proj_gpu_1 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_1\nColumns: 2048-4095\nQueries: 16-31, KV: 1-1" fillcolor=lightcoral]
	qkv_proj_gpu_2 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_2\nColumns: 4096-6143\nQueries: 32-47, KV: 2-2" fillcolor=lightcoral]
	qkv_proj_gpu_3 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_3\nColumns: 6144-8191\nQueries: 48-63, KV: 3-3" fillcolor=lightcoral]
	qkv_proj_gpu_4 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_4\nColumns: 8192-10239\nQueries: 64-79, KV: 4-4" fillcolor=lightcoral]
	qkv_proj_gpu_5 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_5\nColumns: 10240-12287\nQueries: 80-95, KV: 5-5" fillcolor=lightcoral]
	qkv_proj_gpu_6 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_6\nColumns: 12288-14335\nQueries: 96-111, KV: 6-6" fillcolor=lightcoral]
	qkv_proj_gpu_7 [label="QKV Projection\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_7\nColumns: 14336-16383\nQueries: 112-127, KV: 7-7" fillcolor=lightcoral]
	kv_cache_gpu_0 [label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: gpu_0\nDUPLICATED ACROSS GPUS" fillcolor=lightpink]
	kv_cache_gpu_1 [label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: gpu_1\nDUPLICATED ACROSS GPUS" fillcolor=lightpink]
	kv_cache_gpu_2 [label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: gpu_2\nDUPLICATED ACROSS GPUS" fillcolor=lightpink]
	kv_cache_gpu_3 [label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: gpu_3\nDUPLICATED ACROSS GPUS" fillcolor=lightpink]
	kv_cache_gpu_4 [label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: gpu_4\nDUPLICATED ACROSS GPUS" fillcolor=lightpink]
	kv_cache_gpu_5 [label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: gpu_5\nDUPLICATED ACROSS GPUS" fillcolor=lightpink]
	kv_cache_gpu_6 [label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: gpu_6\nDUPLICATED ACROSS GPUS" fillcolor=lightpink]
	kv_cache_gpu_7 [label="KV Cache\nInput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=8, d_k=128]\nDevice: gpu_7\nDUPLICATED ACROSS GPUS" fillcolor=lightpink]
	flash_attn_gpu_0 [label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_0" fillcolor=lightblue]
	flash_attn_gpu_1 [label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_1" fillcolor=lightblue]
	flash_attn_gpu_2 [label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_2" fillcolor=lightblue]
	flash_attn_gpu_3 [label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_3" fillcolor=lightblue]
	flash_attn_gpu_4 [label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_4" fillcolor=lightblue]
	flash_attn_gpu_5 [label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_5" fillcolor=lightblue]
	flash_attn_gpu_6 [label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_6" fillcolor=lightblue]
	flash_attn_gpu_7 [label="FlashAttention\nInput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=16, d_k=128]\nDevice: gpu_7" fillcolor=lightblue]
	attn_all_reduce [label="All-Reduce\nAttention Output\nInput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nOutput: [batch_size=1, seq_len=1000000, heads=128, d_k=128]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	attn_out_gpu_0 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_0\nRows: 0-2047" fillcolor=lightcoral]
	attn_out_gpu_1 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_1\nRows: 2048-4095" fillcolor=lightcoral]
	attn_out_gpu_2 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_2\nRows: 4096-6143" fillcolor=lightcoral]
	attn_out_gpu_3 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_3\nRows: 6144-8191" fillcolor=lightcoral]
	attn_out_gpu_4 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_4\nRows: 8192-10239" fillcolor=lightcoral]
	attn_out_gpu_5 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_5\nRows: 10240-12287" fillcolor=lightcoral]
	attn_out_gpu_6 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_6\nRows: 12288-14335" fillcolor=lightcoral]
	attn_out_gpu_7 [label="Attention Output\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_7\nRows: 14336-16383" fillcolor=lightcoral]
	attn_out_all_reduce [label="All-Reduce\nFinal Attention\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	residual_attn [label="Residual Add\nAttention\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgreen shape=ellipse]
	rmsnorm1 [label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgray]
	ffn_fc1_gpu_0 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_0\nColumns: 0-8191" fillcolor=lightblue]
	ffn_fc1_gpu_1 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_1\nColumns: 8192-16383" fillcolor=lightblue]
	ffn_fc1_gpu_2 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_2\nColumns: 16384-24575" fillcolor=lightblue]
	ffn_fc1_gpu_3 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_3\nColumns: 24576-32767" fillcolor=lightblue]
	ffn_fc1_gpu_4 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_4\nColumns: 32768-40959" fillcolor=lightblue]
	ffn_fc1_gpu_5 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_5\nColumns: 40960-49151" fillcolor=lightblue]
	ffn_fc1_gpu_6 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_6\nColumns: 49152-57343" fillcolor=lightblue]
	ffn_fc1_gpu_7 [label="FFN FC1\nInput: [batch_size=1, seq_len=1000000, hidden=2048]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_7\nColumns: 57344-65535" fillcolor=lightblue]
	ffn_act_gpu_0 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_0" fillcolor=lightblue]
	ffn_act_gpu_1 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_1" fillcolor=lightblue]
	ffn_act_gpu_2 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_2" fillcolor=lightblue]
	ffn_act_gpu_3 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_3" fillcolor=lightblue]
	ffn_act_gpu_4 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_4" fillcolor=lightblue]
	ffn_act_gpu_5 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_5" fillcolor=lightblue]
	ffn_act_gpu_6 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_6" fillcolor=lightblue]
	ffn_act_gpu_7 [label="SwiGLU Activation\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, ffn=8192]\nDevice: gpu_7" fillcolor=lightblue]
	ffn_fc2_gpu_0 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_0\nRows: 0-2047" fillcolor=lightblue]
	ffn_fc2_gpu_1 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_1\nRows: 2048-4095" fillcolor=lightblue]
	ffn_fc2_gpu_2 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_2\nRows: 4096-6143" fillcolor=lightblue]
	ffn_fc2_gpu_3 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_3\nRows: 6144-8191" fillcolor=lightblue]
	ffn_fc2_gpu_4 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_4\nRows: 8192-10239" fillcolor=lightblue]
	ffn_fc2_gpu_5 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_5\nRows: 10240-12287" fillcolor=lightblue]
	ffn_fc2_gpu_6 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_6\nRows: 12288-14335" fillcolor=lightblue]
	ffn_fc2_gpu_7 [label="FFN FC2\nInput: [batch_size=1, seq_len=1000000, ffn=8192]\nOutput: [batch_size=1, seq_len=1000000, hidden=2048]\nDevice: gpu_7\nRows: 14336-16383" fillcolor=lightblue]
	ffn_all_reduce [label="All-Reduce\nFFN Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=yellow shape=parallelogram]
	residual_ffn [label="Residual Add\nFFN\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgreen shape=ellipse]
	rmsnorm2 [label="RMSNorm\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, hidden=16384]\nDevice: all GPUs" fillcolor=lightgray]
	output [label="Output\nInput: [batch_size=1, seq_len=1000000, hidden=16384]\nOutput: [batch_size=1, seq_len=1000000, vocab_size=128000]\nDevice: CPU" fillcolor=lightgreen shape=ellipse]
	input -> split_all_gpus
	split_all_gpus -> qkv_proj_gpu_0
	qkv_proj_gpu_0 -> kv_cache_gpu_0
	kv_cache_gpu_0 -> flash_attn_gpu_0
	split_all_gpus -> qkv_proj_gpu_1
	qkv_proj_gpu_1 -> kv_cache_gpu_1
	kv_cache_gpu_1 -> flash_attn_gpu_1
	split_all_gpus -> qkv_proj_gpu_2
	qkv_proj_gpu_2 -> kv_cache_gpu_2
	kv_cache_gpu_2 -> flash_attn_gpu_2
	split_all_gpus -> qkv_proj_gpu_3
	qkv_proj_gpu_3 -> kv_cache_gpu_3
	kv_cache_gpu_3 -> flash_attn_gpu_3
	split_all_gpus -> qkv_proj_gpu_4
	qkv_proj_gpu_4 -> kv_cache_gpu_4
	kv_cache_gpu_4 -> flash_attn_gpu_4
	split_all_gpus -> qkv_proj_gpu_5
	qkv_proj_gpu_5 -> kv_cache_gpu_5
	kv_cache_gpu_5 -> flash_attn_gpu_5
	split_all_gpus -> qkv_proj_gpu_6
	qkv_proj_gpu_6 -> kv_cache_gpu_6
	kv_cache_gpu_6 -> flash_attn_gpu_6
	split_all_gpus -> qkv_proj_gpu_7
	qkv_proj_gpu_7 -> kv_cache_gpu_7
	kv_cache_gpu_7 -> flash_attn_gpu_7
	flash_attn_gpu_0 -> attn_all_reduce
	attn_all_reduce -> attn_out_gpu_0
	attn_out_gpu_0 -> attn_out_all_reduce
	flash_attn_gpu_1 -> attn_all_reduce
	attn_all_reduce -> attn_out_gpu_1
	attn_out_gpu_1 -> attn_out_all_reduce
	flash_attn_gpu_2 -> attn_all_reduce
	attn_all_reduce -> attn_out_gpu_2
	attn_out_gpu_2 -> attn_out_all_reduce
	flash_attn_gpu_3 -> attn_all_reduce
	attn_all_reduce -> attn_out_gpu_3
	attn_out_gpu_3 -> attn_out_all_reduce
	flash_attn_gpu_4 -> attn_all_reduce
	attn_all_reduce -> attn_out_gpu_4
	attn_out_gpu_4 -> attn_out_all_reduce
	flash_attn_gpu_5 -> attn_all_reduce
	attn_all_reduce -> attn_out_gpu_5
	attn_out_gpu_5 -> attn_out_all_reduce
	flash_attn_gpu_6 -> attn_all_reduce
	attn_all_reduce -> attn_out_gpu_6
	attn_out_gpu_6 -> attn_out_all_reduce
	flash_attn_gpu_7 -> attn_all_reduce
	attn_all_reduce -> attn_out_gpu_7
	attn_out_gpu_7 -> attn_out_all_reduce
	attn_out_all_reduce -> residual_attn
	split_all_gpus -> residual_attn
	residual_attn -> rmsnorm1
	rmsnorm1 -> ffn_fc1_gpu_0
	ffn_fc1_gpu_0 -> ffn_act_gpu_0
	ffn_act_gpu_0 -> ffn_fc2_gpu_0
	ffn_fc2_gpu_0 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_1
	ffn_fc1_gpu_1 -> ffn_act_gpu_1
	ffn_act_gpu_1 -> ffn_fc2_gpu_1
	ffn_fc2_gpu_1 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_2
	ffn_fc1_gpu_2 -> ffn_act_gpu_2
	ffn_act_gpu_2 -> ffn_fc2_gpu_2
	ffn_fc2_gpu_2 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_3
	ffn_fc1_gpu_3 -> ffn_act_gpu_3
	ffn_act_gpu_3 -> ffn_fc2_gpu_3
	ffn_fc2_gpu_3 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_4
	ffn_fc1_gpu_4 -> ffn_act_gpu_4
	ffn_act_gpu_4 -> ffn_fc2_gpu_4
	ffn_fc2_gpu_4 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_5
	ffn_fc1_gpu_5 -> ffn_act_gpu_5
	ffn_act_gpu_5 -> ffn_fc2_gpu_5
	ffn_fc2_gpu_5 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_6
	ffn_fc1_gpu_6 -> ffn_act_gpu_6
	ffn_act_gpu_6 -> ffn_fc2_gpu_6
	ffn_fc2_gpu_6 -> ffn_all_reduce
	rmsnorm1 -> ffn_fc1_gpu_7
	ffn_fc1_gpu_7 -> ffn_act_gpu_7
	ffn_act_gpu_7 -> ffn_fc2_gpu_7
	ffn_fc2_gpu_7 -> ffn_all_reduce
	ffn_all_reduce -> residual_ffn
	rmsnorm1 -> residual_ffn
	residual_ffn -> rmsnorm2
	rmsnorm2 -> output
}
