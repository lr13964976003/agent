{
  "generated_dags": {
    "helix_two_level_attention_partitioning": {
      "method": "Two-level attention partitioning as described in Helix paper",
      "total_devices": 16,
      "head_partitions": 4,
      "dimension_partitions": 4,
      "total_partitions": 16,
      "heads_per_partition": 8,
      "dimensions_per_head_per_partition": 32,
      "files": {
        "dot_file": "../outputs/2025-11-19-10-01-35/helix_two_level_attention_partitioning.dot",
        "svg_image": "../outputs/2025-11-19-10-01-35/helix_two_level_attention_partitioning.svg"
      },
      "model_config": {
        "batch_size": 128,
        "sequence_length": 10000,
        "hidden_size": 4096,
        "num_heads": 32,
        "head_dim": 128,
        "num_layers": 4,
        "intermediate_size": 32768,
        "vocab_size": 50257
      }
    },
    "baseline_tensor_pipeline": {
      "method": "Tensor parallelism + Pipeline parallelism baseline",
      "total_devices": 16,
      "tensor_parallel_degree": 8,
      "pipeline_parallel_degree": 2,
      "pipeline_stages": 2,
      "devices_per_stage": 8,
      "files": {
        "dot_file": "../outputs/2025-11-19-10-01-35/baseline_tensor_pipeline.dot",
        "svg_image": "../outputs/2025-11-19-10-01-35/baseline_tensor_pipeline.svg"
      },
      "model_config": {
        "batch_size": 128,
        "sequence_length": 10000,
        "hidden_size": 4096,
        "num_heads": 32,
        "head_dim": 128,
        "num_layers": 2,
        "intermediate_size": 32768,
        "vocab_size": 50257
      }
    }
  },
  "generation_details": {
    "helix_method": {
      "partitioning_strategy": "Two-level: head-level and intra-head dimension-level",
      "communication_pattern": "Hierarchical aggregation with two stages",
      "load_balancing": "Equal work distribution across 16 GPUs",
      "dimension_preservation": "All tensor dimensions explicitly tracked and preserved"
    },
    "baseline_method": {
      "partitioning_strategy": "Tensor parallelism within stages, pipeline across stages",
      "communication_pattern": "All-reduce within stages, pipeline communication between stages",
      "load_balancing": "Equal work distribution within tensor parallel groups",
      "dimension_preservation": "Global dimensions maintained through all-reduce operations"
    }
  },
  "verification": {
    "helix_dag": {
      "total_nodes": 1026,
      "total_edges": 1164,
      "has_cycles": false,
      "acyclic": true
    },
    "baseline_dag": {
      "total_nodes": 162,
      "total_edges": 174,
      "has_cycles": false,
      "acyclic": true
    }
  },
  "all_files": [
    "../outputs/2025-11-19-10-01-35/helix_two_level_attention_partitioning.dot",
    "../outputs/2025-11-19-10-01-35/helix_two_level_attention_partitioning.svg",
    "../outputs/2025-11-19-10-01-35/baseline_tensor_pipeline.dot",
    "../outputs/2025-11-19-10-01-35/baseline_tensor_pipeline.svg"
  ]
}