digraph Data_Parallel_Replicas {
	graph [concentrate=true rankdir=TB splines=ortho]
	label="Data Parallelism Across Model Replicas"
	dp_input [label="Training Data
Batched across replicas" fillcolor=lightgrey shape=ellipse style=filled]
	subgraph cluster_replica_0 {
		label="Model Replica 0
1856 GPUs"
		dp_replica_0_model [label="Complete MoE Model
1856 GPUs
Processing batch 0" fillcolor=lightblue shape=rectangle style=filled]
	}
	subgraph cluster_replica_1 {
		label="Model Replica 1
1856 GPUs"
		dp_replica_1_model [label="Complete MoE Model
1856 GPUs
Processing batch 1" fillcolor=lightblue shape=rectangle style=filled]
	}
	subgraph cluster_replica_2 {
		label="Model Replica 2
1856 GPUs"
		dp_replica_2_model [label="Complete MoE Model
1856 GPUs
Processing batch 2" fillcolor=lightblue shape=rectangle style=filled]
	}
	subgraph cluster_replica_3 {
		label="Model Replica 3
1856 GPUs"
		dp_replica_3_model [label="Complete MoE Model
1856 GPUs
Processing batch 3" fillcolor=lightblue shape=rectangle style=filled]
	}
	dp_sync [label="Gradient Synchronization
All-reduce across replicas" fillcolor=lightgreen shape=ellipse style="filled,dashed"]
	dp_output [label="Synchronized Model
Updated Weights" fillcolor=lightgrey shape=ellipse style=filled]
	dp_input -> dp_replica_0_model
	dp_input -> dp_replica_1_model
	dp_input -> dp_replica_2_model
	dp_input -> dp_replica_3_model
	dp_replica_0_model -> dp_sync
	dp_replica_1_model -> dp_sync
	dp_replica_2_model -> dp_sync
	dp_replica_3_model -> dp_sync
	dp_sync -> dp_output
}
