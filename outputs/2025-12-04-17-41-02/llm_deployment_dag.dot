// LLM Deployment DAG - EP64_TP2_PP1 Strategy
digraph {
	dpi=300 rankdir=TB size="30,40"
	node [fontname=Arial fontsize=10]
	edge [fontname=Arial fontsize=9]
	node [fillcolor=lightblue shape=rectangle style=filled]
	node [fillcolor=lightgreen shape=ellipse style=filled]
	node [fillcolor=yellow shape=parallelogram style=filled]
	subgraph cluster_input {
		bgcolor=lightgray label="Input Layer" style=rounded
		input [label="Input Tokens
GPU: All
Input: [batch_size=128, seq_len=1024, hidden=1024]
Output: [batch_size=128, seq_len=1024, hidden=1024]" fillcolor=lightcoral shape=rectangle]
	}
	subgraph cluster_layer1 {
		bgcolor=lightblue label="Layer 1 - Attention + MoE" style=rounded
		attn_norm_1 [label="Layer Norm (Attention)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		attn_q_1 [label="Q Projection
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		attn_k_1 [label="K Projection
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		attn_v_1 [label="V Projection
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		attn_score_1 [label="Attention Scores
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		attn_out_1 [label="Attention Output
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		moe_gate_1 [label="MoE Gate
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 64]" fillcolor=yellow shape=parallelogram]
		expert_0_1 [label="Expert 0
GPU: 0-1
Input: [128, 1024, 16]
Output: [128, 1024, 2048]" fillcolor=lightblue shape=rectangle]
		tp_split_0_1 [label="TP Split
GPU: 0-1
Input: [128, 1024, 16]
Output: [128, 1024, 8]" fillcolor=lightgreen shape=ellipse]
		expert_compute_0_0_1 [label="Expert Compute Part 0
GPU: 0
Input: [128, 1024, 8]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		expert_compute_1_0_1 [label="Expert Compute Part 1
GPU: 1
Input: [128, 1024, 8]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		tp_allreduce_0_1 [label="TP All-Reduce
GPU: 0-1
Input: [128, 1024, 1024]
Output: [128, 1024, 2048]" fillcolor=lightgreen shape=ellipse]
		expert_1_1 [label="Expert 1
GPU: 2-3
Input: [128, 1024, 16]
Output: [128, 1024, 2048]" fillcolor=lightblue shape=rectangle]
		tp_split_1_1 [label="TP Split
GPU: 2-3
Input: [128, 1024, 16]
Output: [128, 1024, 8]" fillcolor=lightgreen shape=ellipse]
		expert_compute_0_1_1 [label="Expert Compute Part 0
GPU: 2
Input: [128, 1024, 8]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		expert_compute_1_1_1 [label="Expert Compute Part 1
GPU: 3
Input: [128, 1024, 8]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		tp_allreduce_1_1 [label="TP All-Reduce
GPU: 2-3
Input: [128, 1024, 1024]
Output: [128, 1024, 2048]" fillcolor=lightgreen shape=ellipse]
		expert_2_1 [label="Expert 2
GPU: 4-5
Input: [128, 1024, 16]
Output: [128, 1024, 2048]" fillcolor=lightblue shape=rectangle]
		tp_split_2_1 [label="TP Split
GPU: 4-5
Input: [128, 1024, 16]
Output: [128, 1024, 8]" fillcolor=lightgreen shape=ellipse]
		expert_compute_0_2_1 [label="Expert Compute Part 0
GPU: 4
Input: [128, 1024, 8]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		expert_compute_1_2_1 [label="Expert Compute Part 1
GPU: 5
Input: [128, 1024, 8]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		tp_allreduce_2_1 [label="TP All-Reduce
GPU: 4-5
Input: [128, 1024, 1024]
Output: [128, 1024, 2048]" fillcolor=lightgreen shape=ellipse]
		expert_3_1 [label="Expert 3
GPU: 6-7
Input: [128, 1024, 16]
Output: [128, 1024, 2048]" fillcolor=lightblue shape=rectangle]
		tp_split_3_1 [label="TP Split
GPU: 6-7
Input: [128, 1024, 16]
Output: [128, 1024, 8]" fillcolor=lightgreen shape=ellipse]
		expert_compute_0_3_1 [label="Expert Compute Part 0
GPU: 6
Input: [128, 1024, 8]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		expert_compute_1_3_1 [label="Expert Compute Part 1
GPU: 7
Input: [128, 1024, 8]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		tp_allreduce_3_1 [label="TP All-Reduce
GPU: 6-7
Input: [128, 1024, 1024]
Output: [128, 1024, 2048]" fillcolor=lightgreen shape=ellipse]
		expert_agg_1 [label="Expert Aggregation
GPU: All 128 GPUs
Input: [128, 1024, 2048] x 64
Output: [128, 1024, 1024]" fillcolor=yellow shape=parallelogram]
		layer_norm_1 [label="Layer Norm
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
	}
	subgraph cluster_layers_2_16 {
		bgcolor=lightblue label="Layers 2-16 (Simplified)" style=rounded
		layer_2 [label="Layer 2 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_3 [label="Layer 3 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_4 [label="Layer 4 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_5 [label="Layer 5 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_6 [label="Layer 6 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_7 [label="Layer 7 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_8 [label="Layer 8 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_9 [label="Layer 9 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_10 [label="Layer 10 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_11 [label="Layer 11 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_12 [label="Layer 12 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_13 [label="Layer 13 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_14 [label="Layer 14 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_15 [label="Layer 15 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		layer_16 [label="Layer 16 (Attention + MoE)
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
	}
	subgraph cluster_output {
		bgcolor=lightgray label="Output Layer" style=rounded
		output_norm [label="Final Layer Norm
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, 1024]" fillcolor=lightblue shape=rectangle]
		output_proj [label="Output Projection
GPU: All 128 GPUs
Input: [128, 1024, 1024]
Output: [128, 1024, vocab_size]" fillcolor=lightblue shape=rectangle]
		output [label="Output Tokens
GPU: All 128 GPUs
Input: [128, 1024, vocab_size]
Output: [128, 1024]" fillcolor=lightcoral shape=rectangle]
	}
	input -> attn_norm_1 [label="Token embeddings"]
	attn_norm_1 -> attn_q_1
	attn_norm_1 -> attn_k_1
	attn_norm_1 -> attn_v_1
	attn_q_1 -> attn_score_1 [label="Q matrix"]
	attn_k_1 -> attn_score_1 [label="K matrix"]
	attn_v_1 -> attn_out_1 [label="V matrix"]
	attn_score_1 -> attn_out_1 [label="Attention weights"]
	attn_out_1 -> moe_gate_1 [label="Attention output"]
	moe_gate_1 -> tp_split_0_1 [label="Gate selection 0" style=dashed]
	tp_split_0_1 -> expert_compute_0_0_1
	tp_split_0_1 -> expert_compute_1_0_1
	expert_compute_0_0_1 -> tp_allreduce_0_1
	expert_compute_1_0_1 -> tp_allreduce_0_1
	tp_allreduce_0_1 -> expert_0_1
	expert_0_1 -> expert_agg_1 [label="Expert 0 output"]
	moe_gate_1 -> tp_split_1_1 [label="Gate selection 1" style=dashed]
	tp_split_1_1 -> expert_compute_0_1_1
	tp_split_1_1 -> expert_compute_1_1_1
	expert_compute_0_1_1 -> tp_allreduce_1_1
	expert_compute_1_1_1 -> tp_allreduce_1_1
	tp_allreduce_1_1 -> expert_1_1
	expert_1_1 -> expert_agg_1 [label="Expert 1 output"]
	moe_gate_1 -> tp_split_2_1 [label="Gate selection 2" style=dashed]
	tp_split_2_1 -> expert_compute_0_2_1
	tp_split_2_1 -> expert_compute_1_2_1
	expert_compute_0_2_1 -> tp_allreduce_2_1
	expert_compute_1_2_1 -> tp_allreduce_2_1
	tp_allreduce_2_1 -> expert_2_1
	expert_2_1 -> expert_agg_1 [label="Expert 2 output"]
	moe_gate_1 -> tp_split_3_1 [label="Gate selection 3" style=dashed]
	tp_split_3_1 -> expert_compute_0_3_1
	tp_split_3_1 -> expert_compute_1_3_1
	expert_compute_0_3_1 -> tp_allreduce_3_1
	expert_compute_1_3_1 -> tp_allreduce_3_1
	tp_allreduce_3_1 -> expert_3_1
	expert_3_1 -> expert_agg_1 [label="Expert 3 output"]
	expert_agg_1 -> layer_norm_1
	layer_norm_1 -> layer_2
	layer_2 -> layer_3
	layer_3 -> layer_4
	layer_4 -> layer_5
	layer_5 -> layer_6
	layer_6 -> layer_7
	layer_7 -> layer_8
	layer_8 -> layer_9
	layer_9 -> layer_10
	layer_10 -> layer_11
	layer_11 -> layer_12
	layer_12 -> layer_13
	layer_13 -> layer_14
	layer_14 -> layer_15
	layer_15 -> layer_16
	layer_16 -> output_norm
	output_norm -> output_proj
	output_proj -> output
}
