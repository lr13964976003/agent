{
  "deployment_configurations": {
    "baseline": {
      "name": "Tensor Parallel + Pipeline Parallel Baseline",
      "model": "dense_transformer",
      "total_devices": 16,
      "parallel_strategy": {
        "type": "hybrid",
        "tensor_parallelism": {
          "degree": 8,
          "method": "column_row_split"
        },
        "pipeline_parallelism": {
          "degree": 2,
          "method": "layer_wise_split"
        }
      },
      "model_parameters": {
        "layers": 16,
        "hidden_size": 4096,
        "attention_heads": 32,
        "head_dimension": 128,
        "mlp_hidden_size": 16384,
        "sequence_length": 100000,
        "batch_size": 128,
        "precision": "bf16"
      },
      "device_mapping": {
        "pipeline_stage_0": {
          "devices": [0, 1, 2, 3, 4, 5, 6, 7],
          "layers": [0, 1, 2, 3, 4, 5, 6, 7],
          "tensor_split": {
            "qkv_projection": {"type": "column", "partitions": [[0,1,2,3], [4,5,6,7]]},
            "attention_output": {"type": "row", "partitions": [[0,1,2,3], [4,5,6,7]]},
            "mlp_gate": {"type": "column", "partitions": [[0,1,2,3], [4,5,6,7]]},
            "mlp_up": {"type": "column", "partitions": [[0,1,2,3], [4,5,6,7]]},
            "mlp_down": {"type": "row", "partitions": [[0,1,2,3], [4,5,6,7]]}
          }
        },
        "pipeline_stage_1": {
          "devices": [8, 9, 10, 11, 12, 13, 14, 15],
          "layers": [8, 9, 10, 11, 12, 13, 14, 15],
          "tensor_split": {
            "qkv_projection": {"type": "column", "partitions": [[8,9,10,11], [12,13,14,15]]},
            "attention_output": {"type": "row", "partitions": [[8,9,10,11], [12,13,14,15]]},
            "mlp_gate": {"type": "column", "partitions": [[8,9,10,11], [12,13,14,15]]},
            "mlp_up": {"type": "column", "partitions": [[8,9,10,11], [12,13,14,15]]},
            "mlp_down": {"type": "row", "partitions": [[8,9,10,11], [12,13,14,15]]}
          }
        }
      },
      "communication": {
        "tensor_parallel": {
          "all_reduce": "nccl_allreduce",
          "bandwidth": "nvlink_600GBps"
        },
        "pipeline_parallel": {
          "send_recv": "nccl_sendrecv",
          "micro_batches": 4
        }
      },
      "memory_usage": {
        "activation_memory_per_device": "L * d_model / 1 * bf16_bytes",
        "peak_memory": "sequence_length * hidden_size * 4 * batch_size * bf16_bytes"
      }
    },
    "ring_attention_sequence_parallelism": {
      "name": "Ring Attention + Sequence Parallelism",
      "model": "dense_transformer",
      "total_devices": 16,
      "parallel_strategy": {
        "type": "ring_sequence_parallelism",
        "sequence_parallelism": {
          "degree": 16,
          "split_dimension": "sequence_length",
          "tokens_per_device": 6250
        },
        "ring_attention": {
          "topology": "circular_ring",
          "stages": 16,
          "communication_pattern": "point_to_point",
          "overlap": "compute_communication"
        }
      },
      "model_parameters": {
        "layers": 16,
        "hidden_size": 4096,
        "attention_heads": 32,
        "head_dimension": 128,
        "mlp_hidden_size": 16384,
        "sequence_length": 100000,
        "sequence_per_device": 6250,
        "batch_size": 128,
        "precision": "bf16"
      },
      "device_mapping": {
        "ring_topology": {
          "device_order": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
          "next_device": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0],
          "previous_device": [15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
        },
        "per_device_work": {
          "device_0": {
            "sequence_slice": "tokens_0_to_6249",
            "layers": "all_16_layers",
            "modules": {
              "qkv_projection": {"input": "local_6250_tokens", "output": "qkv_6250_tokens"},
              "attention_computation": {
                "local_q": "6250_tokens",
                "kv_blocks": "receive_from_all_devices",
                "stages": 16,
                "current_kv_source": "dynamic_per_stage"
              },
              "mlp": {"input": "6250_tokens", "output": "6250_tokens"},
              "layer_norm": {"input": "6250_tokens", "output": "6250_tokens"}
            }
          },
          "device_1": {
            "sequence_slice": "tokens_6250_to_12499",
            "layers": "all_16_layers",
            "modules": {
              "qkv_projection": {"input": "local_6250_tokens", "output": "qkv_6250_tokens"},
              "attention_computation": {
                "local_q": "6250_tokens",
                "kv_blocks": "receive_from_all_devices",
                "stages": 16,
                "current_kv_source": "dynamic_per_stage"
              },
              "mlp": {"input": "6250_tokens", "output": "6250_tokens"},
              "layer_norm": {"input": "6250_tokens", "output": "6250_tokens"}
            }
          }
        }
      },
      "communication": {
        "ring_stages": {
          "stage_0": {
            "device_0": {"send_kv_to": 1, "receive_kv_from": 15, "compute_with": "kv_from_0"},
            "device_1": {"send_kv_to": 2, "receive_kv_from": 0, "compute_with": "kv_from_1"},
            "device_2": {"send_kv_to": 3, "receive_kv_from": 1, "compute_with": "kv_from_2"}
          },
          "stage_t": {
            "general_pattern": {
              "device_p": {
                "source_index": "(p - t) mod 16",
                "send_to": "(p + 1) mod 16",
                "receive_from": "(p - 1) mod 16"
              }
            }
          }
        },
        "primitives": {
          "send": "nccl_send",
          "receive": "nccl_recv",
          "async": true,
          "overlap": "compute_attention"
        },
        "buffer_sizes": {
          "kv_block_size": "6250_tokens * head_dim * num_heads * bf16_bytes = 6250 * 128 * 32 * 2 = 50MB",
          "ring_buffer_count": 2,
          "total_memory_per_device": "6250_sequence * 4096_hidden * 128_batch * bf16_bytes / 16_devices"
        }
      },
      "memory_usage": {
        "activation_memory_per_device": "L * d_model * bf16_bytes / P = 100000 * 4096 * 2 / 16 = 51.2MB",
        "reduction_factor": 16,
        "peak_memory": "sequence_length_per_device * hidden_size * batch_size * bf16_bytes + kv_ring_buffer"
      },
      "performance_metrics": {
        "tps": "1.45M_tokens_per_second",
        "tpot": "0.70ms_per_token",
        "improvement": {
          "throughput": "+20.8%_vs_baseline",
          "latency": "-17.6%_vs_baseline"
        }
      }
    }
  },
  "deployment_steps": {
    "step_1": "Initialize 16 H100 GPUs with NVLink interconnect",
    "step_2": "Configure NCCL for point-to-point communication",
    "step_3": "Partition sequence: 100000 tokens / 16 devices = 6250 tokens per device",
    "step_4": "Setup ring topology: device_0→1→2→...→15→0",
    "step_5": "Initialize model weights on all devices (full copy per device)",
    "step_6": "Allocate ring buffers for KV blocks (50MB per device)",
    "step_7": "Launch 16-stage ring attention computation",
    "step_8": "Verify communication overlap with computation"
  },
  "critical_parameters": {
    "sequence_length": 100000,
    "batch_size": 128,
    "hidden_size": 4096,
    "attention_heads": 32,
    "head_dimension": 128,
    "num_devices": 16,
    "precision": "bf16",
    "tokens_per_device": 6250,
    "ring_stages": 16,
    "communication_primitive": "nccl_sendrecv",
    "overlap": true
  }
}