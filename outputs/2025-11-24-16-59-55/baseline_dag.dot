digraph baseline_tp_pp {
	graph [bb="0,0,1488.4,10915",
		nodesep=0.8,
		rankdir=TB,
		ranksep=1.5
	];
	node [label="\N",
		shape=rectangle,
		style=filled
	];
	subgraph cluster_pipeline_0 {
		graph [bb="412.44,128,750.44,10770",
			color=blue,
			label="Pipeline Stage 0 (Devices 0-7)",
			lheight=0.21,
			lp="581.44,10758",
			lwidth=2.99,
			style=dashed
		];
		subgraph cluster_mha_0 {
			graph [bb="459.44,10141,703.44,10739",
				color=green,
				label="Layer 0 - MHA Block",
				lheight=0.21,
				lp="581.44,10728",
				lwidth=2.06,
				style="rounded,dashed"
			];
			layer_0_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 0\nQKV Projection\n(Column Parallel)",
				pos="581.44,10682",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
0,1,2,3] vs [4,5,6,7]",
				xlp="261.44,10678"];
			layer_0_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 0\nMulti-Head Attention\n(All Devices)",
				pos="581.44,10520",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
0-7",
				xlp="253.44,10570"];
			layer_0_qkv -> layer_0_attn	[pos="e,581.44,10547 581.44,10655 581.44,10629 581.44,10587 581.44,10558"];
			layer_0_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 0\nAttention Output\n(Row Parallel)",
				pos="581.44,10360",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,2,3] \
vs [4,5,6,7]",
				xlp="298.44,10408"];
			layer_0_attn -> layer_0_attn_out	[pos="e,581.44,10386 581.44,10494 581.44,10468 581.44,10426 581.44,10397"];
			layer_0_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 0\nResidual Add",
				pos="581.44,10187",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="253.4,10248"];
			layer_0_attn_out -> layer_0_residual	[pos="e,581.44,10225 581.44,10333 581.44,10307 581.44,10267 581.44,10235"];
		}
		subgraph cluster_mlp_0 {
			graph [bb="420.44,9474,742.44,10072",
				color=red,
				label="Layer 0 - MLP Block",
				lheight=0.21,
				lp="581.44,10060",
				lwidth=2.01,
				style="rounded,dashed"
			];
			layer_0_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 0\nMLP Gate\n(Column Parallel)",
				pos="581.44,10014",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,10010"];
			layer_0_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 0\nMLP Up\n(Column Parallel)",
				pos="581.44,9853.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,9902.5"];
			layer_0_gate -> layer_0_up	[pos="e,581.44,9880.2 581.44,9988 581.44,9961.6 581.44,9920.3 581.44,9890.6"];
			layer_0_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 0\nMLP Down\n(Row Parallel)",
				pos="581.44,9692.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="303.94,9741.5"];
			layer_0_up -> layer_0_down	[pos="e,581.44,9719.2 581.44,9827 581.44,9800.6 581.44,9759.3 581.44,9729.6"];
			layer_0_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 0\nMLP Residual Add",
				pos="581.44,9520",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="214,9580.5"];
			layer_0_down -> layer_0_mlp_residual	[pos="e,581.44,9558.1 581.44,9666 581.44,9640.3 581.44,9600 581.44,9568.4"];
		}
		subgraph cluster_mha_1 {
			graph [bb="459.44,8807,703.44,9405",
				color=green,
				label="Layer 1 - MHA Block",
				lheight=0.21,
				lp="581.44,9393.5",
				lwidth=2.06,
				style="rounded,dashed"
			];
			layer_1_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 1\nQKV Projection\n(Column Parallel)",
				pos="581.44,9347.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
0,1,2,3] vs [4,5,6,7]",
				xlp="261.44,9343.5"];
			layer_1_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 1\nMulti-Head Attention\n(All Devices)",
				pos="581.44,9186.5",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
0-7",
				xlp="253.44,9235.5"];
			layer_1_qkv -> layer_1_attn	[pos="e,581.44,9213.2 581.44,9321 581.44,9294.6 581.44,9253.3 581.44,9223.6"];
			layer_1_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 1\nAttention Output\n(Row Parallel)",
				pos="581.44,9025.5",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,2,3] \
vs [4,5,6,7]",
				xlp="298.44,9074.5"];
			layer_1_attn -> layer_1_attn_out	[pos="e,581.44,9052.2 581.44,9160 581.44,9133.6 581.44,9092.3 581.44,9062.6"];
			layer_1_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 1\nResidual Add",
				pos="581.44,8853",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="253.4,8913.5"];
			layer_1_attn_out -> layer_1_residual	[pos="e,581.44,8891.1 581.44,8999 581.44,8973.3 581.44,8933 581.44,8901.4"];
		}
		subgraph cluster_mlp_1 {
			graph [bb="420.44,8140,742.44,8738",
				color=red,
				label="Layer 1 - MLP Block",
				lheight=0.21,
				lp="581.44,8726.5",
				lwidth=2.01,
				style="rounded,dashed"
			];
			layer_1_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 1\nMLP Gate\n(Column Parallel)",
				pos="581.44,8680.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,8676.5"];
			layer_1_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 1\nMLP Up\n(Column Parallel)",
				pos="581.44,8519.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,8568.5"];
			layer_1_gate -> layer_1_up	[pos="e,581.44,8546.2 581.44,8654 581.44,8627.6 581.44,8586.3 581.44,8556.6"];
			layer_1_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 1\nMLP Down\n(Row Parallel)",
				pos="581.44,8358.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="303.94,8407.5"];
			layer_1_up -> layer_1_down	[pos="e,581.44,8385.2 581.44,8493 581.44,8466.6 581.44,8425.3 581.44,8395.6"];
			layer_1_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 1\nMLP Residual Add",
				pos="581.44,8186",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="214,8246.5"];
			layer_1_down -> layer_1_mlp_residual	[pos="e,581.44,8224.1 581.44,8332 581.44,8306.3 581.44,8266 581.44,8234.4"];
		}
		subgraph cluster_mha_2 {
			graph [bb="459.44,7473,703.44,8071",
				color=green,
				label="Layer 2 - MHA Block",
				lheight=0.21,
				lp="581.44,8059.5",
				lwidth=2.06,
				style="rounded,dashed"
			];
			layer_2_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 2\nQKV Projection\n(Column Parallel)",
				pos="581.44,8013.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
0,1,2,3] vs [4,5,6,7]",
				xlp="261.44,8009.5"];
			layer_2_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 2\nMulti-Head Attention\n(All Devices)",
				pos="581.44,7852.5",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
0-7",
				xlp="253.44,7901.5"];
			layer_2_qkv -> layer_2_attn	[pos="e,581.44,7879.2 581.44,7987 581.44,7960.6 581.44,7919.3 581.44,7889.6"];
			layer_2_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 2\nAttention Output\n(Row Parallel)",
				pos="581.44,7691.5",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,2,3] \
vs [4,5,6,7]",
				xlp="298.44,7740.5"];
			layer_2_attn -> layer_2_attn_out	[pos="e,581.44,7718.2 581.44,7826 581.44,7799.6 581.44,7758.3 581.44,7728.6"];
			layer_2_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 2\nResidual Add",
				pos="581.44,7519",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="253.4,7579.5"];
			layer_2_attn_out -> layer_2_residual	[pos="e,581.44,7557.1 581.44,7665 581.44,7639.3 581.44,7599 581.44,7567.4"];
		}
		subgraph cluster_mlp_2 {
			graph [bb="420.44,6806,742.44,7404",
				color=red,
				label="Layer 2 - MLP Block",
				lheight=0.21,
				lp="581.44,7392.5",
				lwidth=2.01,
				style="rounded,dashed"
			];
			layer_2_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 2\nMLP Gate\n(Column Parallel)",
				pos="581.44,7346.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,7342.5"];
			layer_2_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 2\nMLP Up\n(Column Parallel)",
				pos="581.44,7185.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,7234.5"];
			layer_2_gate -> layer_2_up	[pos="e,581.44,7212.2 581.44,7320 581.44,7293.6 581.44,7252.3 581.44,7222.6"];
			layer_2_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 2\nMLP Down\n(Row Parallel)",
				pos="581.44,7024.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="303.94,7073.5"];
			layer_2_up -> layer_2_down	[pos="e,581.44,7051.2 581.44,7159 581.44,7132.6 581.44,7091.3 581.44,7061.6"];
			layer_2_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 2\nMLP Residual Add",
				pos="581.44,6852",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="214,6912.5"];
			layer_2_down -> layer_2_mlp_residual	[pos="e,581.44,6890.1 581.44,6998 581.44,6972.3 581.44,6932 581.44,6900.4"];
		}
		subgraph cluster_mha_3 {
			graph [bb="459.44,6139,703.44,6737",
				color=green,
				label="Layer 3 - MHA Block",
				lheight=0.21,
				lp="581.44,6725.5",
				lwidth=2.06,
				style="rounded,dashed"
			];
			layer_3_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 3\nQKV Projection\n(Column Parallel)",
				pos="581.44,6679.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
0,1,2,3] vs [4,5,6,7]",
				xlp="261.44,6675.5"];
			layer_3_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 3\nMulti-Head Attention\n(All Devices)",
				pos="581.44,6518.5",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
0-7",
				xlp="253.44,6567.5"];
			layer_3_qkv -> layer_3_attn	[pos="e,581.44,6545.2 581.44,6653 581.44,6626.6 581.44,6585.3 581.44,6555.6"];
			layer_3_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 3\nAttention Output\n(Row Parallel)",
				pos="581.44,6357.5",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,2,3] \
vs [4,5,6,7]",
				xlp="298.44,6406.5"];
			layer_3_attn -> layer_3_attn_out	[pos="e,581.44,6384.2 581.44,6492 581.44,6465.6 581.44,6424.3 581.44,6394.6"];
			layer_3_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 3\nResidual Add",
				pos="581.44,6185",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="253.4,6245.5"];
			layer_3_attn_out -> layer_3_residual	[pos="e,581.44,6223.1 581.44,6331 581.44,6305.3 581.44,6265 581.44,6233.4"];
		}
		subgraph cluster_mlp_3 {
			graph [bb="420.44,5472,742.44,6070",
				color=red,
				label="Layer 3 - MLP Block",
				lheight=0.21,
				lp="581.44,6058.5",
				lwidth=2.01,
				style="rounded,dashed"
			];
			layer_3_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 3\nMLP Gate\n(Column Parallel)",
				pos="581.44,6012.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,6008.5"];
			layer_3_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 3\nMLP Up\n(Column Parallel)",
				pos="581.44,5851.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,5900.5"];
			layer_3_gate -> layer_3_up	[pos="e,581.44,5878.2 581.44,5986 581.44,5959.6 581.44,5918.3 581.44,5888.6"];
			layer_3_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 3\nMLP Down\n(Row Parallel)",
				pos="581.44,5690.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="303.94,5739.5"];
			layer_3_up -> layer_3_down	[pos="e,581.44,5717.2 581.44,5825 581.44,5798.6 581.44,5757.3 581.44,5727.6"];
			layer_3_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 3\nMLP Residual Add",
				pos="581.44,5518",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="214,5578.5"];
			layer_3_down -> layer_3_mlp_residual	[pos="e,581.44,5556.1 581.44,5664 581.44,5638.3 581.44,5598 581.44,5566.4"];
		}
		subgraph cluster_mha_4 {
			graph [bb="459.44,4805,703.44,5403",
				color=green,
				label="Layer 4 - MHA Block",
				lheight=0.21,
				lp="581.44,5391.5",
				lwidth=2.06,
				style="rounded,dashed"
			];
			layer_4_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 4\nQKV Projection\n(Column Parallel)",
				pos="581.44,5345.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
0,1,2,3] vs [4,5,6,7]",
				xlp="261.44,5341.5"];
			layer_4_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 4\nMulti-Head Attention\n(All Devices)",
				pos="581.44,5184.5",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
0-7",
				xlp="253.44,5233.5"];
			layer_4_qkv -> layer_4_attn	[pos="e,581.44,5211.2 581.44,5319 581.44,5292.6 581.44,5251.3 581.44,5221.6"];
			layer_4_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 4\nAttention Output\n(Row Parallel)",
				pos="581.44,5023.5",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,2,3] \
vs [4,5,6,7]",
				xlp="298.44,5072.5"];
			layer_4_attn -> layer_4_attn_out	[pos="e,581.44,5050.2 581.44,5158 581.44,5131.6 581.44,5090.3 581.44,5060.6"];
			layer_4_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 4\nResidual Add",
				pos="581.44,4851",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="253.4,4911.5"];
			layer_4_attn_out -> layer_4_residual	[pos="e,581.44,4889.1 581.44,4997 581.44,4971.3 581.44,4931 581.44,4899.4"];
		}
		subgraph cluster_mlp_4 {
			graph [bb="420.44,4138,742.44,4736",
				color=red,
				label="Layer 4 - MLP Block",
				lheight=0.21,
				lp="581.44,4724.5",
				lwidth=2.01,
				style="rounded,dashed"
			];
			layer_4_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 4\nMLP Gate\n(Column Parallel)",
				pos="581.44,4678.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,4674.5"];
			layer_4_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 4\nMLP Up\n(Column Parallel)",
				pos="581.44,4517.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,4566.5"];
			layer_4_gate -> layer_4_up	[pos="e,581.44,4544.2 581.44,4652 581.44,4625.6 581.44,4584.3 581.44,4554.6"];
			layer_4_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 4\nMLP Down\n(Row Parallel)",
				pos="581.44,4356.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="303.94,4405.5"];
			layer_4_up -> layer_4_down	[pos="e,581.44,4383.2 581.44,4491 581.44,4464.6 581.44,4423.3 581.44,4393.6"];
			layer_4_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 4\nMLP Residual Add",
				pos="581.44,4184",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="214,4244.5"];
			layer_4_down -> layer_4_mlp_residual	[pos="e,581.44,4222.1 581.44,4330 581.44,4304.3 581.44,4264 581.44,4232.4"];
		}
		subgraph cluster_mha_5 {
			graph [bb="459.44,3471,703.44,4069",
				color=green,
				label="Layer 5 - MHA Block",
				lheight=0.21,
				lp="581.44,4057.5",
				lwidth=2.06,
				style="rounded,dashed"
			];
			layer_5_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 5\nQKV Projection\n(Column Parallel)",
				pos="581.44,4011.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
0,1,2,3] vs [4,5,6,7]",
				xlp="261.44,4007.5"];
			layer_5_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 5\nMulti-Head Attention\n(All Devices)",
				pos="581.44,3850.5",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
0-7",
				xlp="253.44,3899.5"];
			layer_5_qkv -> layer_5_attn	[pos="e,581.44,3877.2 581.44,3985 581.44,3958.6 581.44,3917.3 581.44,3887.6"];
			layer_5_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 5\nAttention Output\n(Row Parallel)",
				pos="581.44,3689.5",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,2,3] \
vs [4,5,6,7]",
				xlp="298.44,3738.5"];
			layer_5_attn -> layer_5_attn_out	[pos="e,581.44,3716.2 581.44,3824 581.44,3797.6 581.44,3756.3 581.44,3726.6"];
			layer_5_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 5\nResidual Add",
				pos="581.44,3517",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="253.4,3577.5"];
			layer_5_attn_out -> layer_5_residual	[pos="e,581.44,3555.1 581.44,3663 581.44,3637.3 581.44,3597 581.44,3565.4"];
		}
		subgraph cluster_mlp_5 {
			graph [bb="420.44,2804,742.44,3402",
				color=red,
				label="Layer 5 - MLP Block",
				lheight=0.21,
				lp="581.44,3390.5",
				lwidth=2.01,
				style="rounded,dashed"
			];
			layer_5_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 5\nMLP Gate\n(Column Parallel)",
				pos="581.44,3344.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,3340.5"];
			layer_5_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 5\nMLP Up\n(Column Parallel)",
				pos="581.44,3183.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,3232.5"];
			layer_5_gate -> layer_5_up	[pos="e,581.44,3210.2 581.44,3318 581.44,3291.6 581.44,3250.3 581.44,3220.6"];
			layer_5_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 5\nMLP Down\n(Row Parallel)",
				pos="581.44,3022.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="303.94,3071.5"];
			layer_5_up -> layer_5_down	[pos="e,581.44,3049.2 581.44,3157 581.44,3130.6 581.44,3089.3 581.44,3059.6"];
			layer_5_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 5\nMLP Residual Add",
				pos="581.44,2850",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="214,2910.5"];
			layer_5_down -> layer_5_mlp_residual	[pos="e,581.44,2888.1 581.44,2996 581.44,2970.3 581.44,2930 581.44,2898.4"];
		}
		subgraph cluster_mha_6 {
			graph [bb="459.44,2137,703.44,2735",
				color=green,
				label="Layer 6 - MHA Block",
				lheight=0.21,
				lp="581.44,2723.5",
				lwidth=2.06,
				style="rounded,dashed"
			];
			layer_6_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 6\nQKV Projection\n(Column Parallel)",
				pos="581.44,2677.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
0,1,2,3] vs [4,5,6,7]",
				xlp="261.44,2673.5"];
			layer_6_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 6\nMulti-Head Attention\n(All Devices)",
				pos="581.44,2516.5",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
0-7",
				xlp="253.44,2565.5"];
			layer_6_qkv -> layer_6_attn	[pos="e,581.44,2543.2 581.44,2651 581.44,2624.6 581.44,2583.3 581.44,2553.6"];
			layer_6_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 6\nAttention Output\n(Row Parallel)",
				pos="581.44,2355.5",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,2,3] \
vs [4,5,6,7]",
				xlp="298.44,2404.5"];
			layer_6_attn -> layer_6_attn_out	[pos="e,581.44,2382.2 581.44,2490 581.44,2463.6 581.44,2422.3 581.44,2392.6"];
			layer_6_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 6\nResidual Add",
				pos="581.44,2183",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="253.4,2243.5"];
			layer_6_attn_out -> layer_6_residual	[pos="e,581.44,2221.1 581.44,2329 581.44,2303.3 581.44,2263 581.44,2231.4"];
		}
		subgraph cluster_mlp_6 {
			graph [bb="420.44,1470,742.44,2068",
				color=red,
				label="Layer 6 - MLP Block",
				lheight=0.21,
				lp="581.44,2056.5",
				lwidth=2.01,
				style="rounded,dashed"
			];
			layer_6_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 6\nMLP Gate\n(Column Parallel)",
				pos="581.44,2010.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,2006.5"];
			layer_6_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 6\nMLP Up\n(Column Parallel)",
				pos="581.44,1849.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,1898.5"];
			layer_6_gate -> layer_6_up	[pos="e,581.44,1876.2 581.44,1984 581.44,1957.6 581.44,1916.3 581.44,1886.6"];
			layer_6_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 6\nMLP Down\n(Row Parallel)",
				pos="581.44,1688.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="303.94,1737.5"];
			layer_6_up -> layer_6_down	[pos="e,581.44,1715.2 581.44,1823 581.44,1796.6 581.44,1755.3 581.44,1725.6"];
			layer_6_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 6\nMLP Residual Add",
				pos="581.44,1516",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="214,1576.5"];
			layer_6_down -> layer_6_mlp_residual	[pos="e,581.44,1554.1 581.44,1662 581.44,1636.3 581.44,1596 581.44,1564.4"];
		}
		subgraph cluster_mha_7 {
			graph [bb="459.44,803,703.44,1401",
				color=green,
				label="Layer 7 - MHA Block",
				lheight=0.21,
				lp="581.44,1389.5",
				lwidth=2.06,
				style="rounded,dashed"
			];
			layer_7_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 7\nQKV Projection\n(Column Parallel)",
				pos="581.44,1343.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
0,1,2,3] vs [4,5,6,7]",
				xlp="261.44,1339.5"];
			layer_7_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 7\nMulti-Head Attention\n(All Devices)",
				pos="581.44,1182.5",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
0-7",
				xlp="253.44,1231.5"];
			layer_7_qkv -> layer_7_attn	[pos="e,581.44,1209.2 581.44,1317 581.44,1290.6 581.44,1249.3 581.44,1219.6"];
			layer_7_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 7\nAttention Output\n(Row Parallel)",
				pos="581.44,1021.5",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,2,3] \
vs [4,5,6,7]",
				xlp="298.44,1070.5"];
			layer_7_attn -> layer_7_attn_out	[pos="e,581.44,1048.2 581.44,1156 581.44,1129.6 581.44,1088.3 581.44,1058.6"];
			layer_7_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 7\nResidual Add",
				pos="581.44,849",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="253.4,909.5"];
			layer_7_attn_out -> layer_7_residual	[pos="e,581.44,887.08 581.44,994.97 581.44,969.27 581.44,929 581.44,897.42"];
		}
		subgraph cluster_mlp_7 {
			graph [bb="420.44,136,742.44,734",
				color=red,
				label="Layer 7 - MLP Block",
				lheight=0.21,
				lp="581.44,722.5",
				lwidth=2.01,
				style="rounded,dashed"
			];
			layer_7_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 7\nMLP Gate\n(Column Parallel)",
				pos="581.44,676.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,672.5"];
			layer_7_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 7\nMLP Up\n(Column Parallel)",
				pos="581.44,515.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="285.94,564.5"];
			layer_7_gate -> layer_7_up	[pos="e,581.44,542.25 581.44,649.95 581.44,623.61 581.44,582.29 581.44,552.61"];
			layer_7_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 7\nMLP Down\n(Row Parallel)",
				pos="581.44,354.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [0,1,\
2,3] vs [4,5,6,7]",
				xlp="303.94,403.5"];
			layer_7_up -> layer_7_down	[pos="e,581.44,381.25 581.44,488.95 581.44,462.61 581.44,421.29 581.44,391.61"];
			layer_7_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 7\nMLP Residual Add",
				pos="581.44,182",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="214,242.5"];
			layer_7_down -> layer_7_mlp_residual	[pos="e,581.44,220.08 581.44,327.97 581.44,302.27 581.44,262 581.44,230.42"];
		}
		layer_0_residual -> layer_0_gate	[pos="e,581.44,10041 581.44,10149 581.44,10120 581.44,10080 581.44,10052"];
		layer_0_mlp_residual -> layer_1_qkv	[pos="e,581.44,9374.2 581.44,9481.6 581.44,9452.7 581.44,9413.1 581.44,9384.5"];
		layer_1_residual -> layer_1_gate	[pos="e,581.44,8707.2 581.44,8814.6 581.44,8785.7 581.44,8746.1 581.44,8717.5"];
		layer_1_mlp_residual -> layer_2_qkv	[pos="e,581.44,8040.2 581.44,8147.6 581.44,8118.7 581.44,8079.1 581.44,8050.5"];
		layer_2_residual -> layer_2_gate	[pos="e,581.44,7373.2 581.44,7480.6 581.44,7451.7 581.44,7412.1 581.44,7383.5"];
		layer_2_mlp_residual -> layer_3_qkv	[pos="e,581.44,6706.2 581.44,6813.6 581.44,6784.7 581.44,6745.1 581.44,6716.5"];
		layer_3_residual -> layer_3_gate	[pos="e,581.44,6039.2 581.44,6146.6 581.44,6117.7 581.44,6078.1 581.44,6049.5"];
		layer_3_mlp_residual -> layer_4_qkv	[pos="e,581.44,5372.2 581.44,5479.6 581.44,5450.7 581.44,5411.1 581.44,5382.5"];
		layer_4_residual -> layer_4_gate	[pos="e,581.44,4705.2 581.44,4812.6 581.44,4783.7 581.44,4744.1 581.44,4715.5"];
		layer_4_mlp_residual -> layer_5_qkv	[pos="e,581.44,4038.2 581.44,4145.6 581.44,4116.7 581.44,4077.1 581.44,4048.5"];
		layer_5_residual -> layer_5_gate	[pos="e,581.44,3371.2 581.44,3478.6 581.44,3449.7 581.44,3410.1 581.44,3381.5"];
		layer_5_mlp_residual -> layer_6_qkv	[pos="e,581.44,2704.2 581.44,2811.6 581.44,2782.7 581.44,2743.1 581.44,2714.5"];
		layer_6_residual -> layer_6_gate	[pos="e,581.44,2037.2 581.44,2144.6 581.44,2115.7 581.44,2076.1 581.44,2047.5"];
		layer_6_mlp_residual -> layer_7_qkv	[pos="e,581.44,1370.2 581.44,1477.6 581.44,1448.7 581.44,1409.1 581.44,1380.5"];
		layer_7_residual -> layer_7_gate	[pos="e,581.44,703.21 581.44,810.58 581.44,781.73 581.44,742.05 581.44,713.55"];
	}
	subgraph cluster_pipeline_1 {
		graph [bb="777.44,1462,1480.4,10770",
			color=purple,
			label="Pipeline Stage 1 (Devices 8-15)",
			lheight=0.21,
			lp="1128.9,10758",
			lwidth=3.11,
			style=dashed
		];
		subgraph cluster_mha_8 {
			graph [bb="1189.4,10141,1433.4,10739",
				color=green,
				label="Layer 8 - MHA Block",
				lheight=0.21,
				lp="1311.4,10728",
				lwidth=2.06,
				style="rounded,dashed"
			];
			layer_8_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 8\nQKV Projection\n(Column Parallel)",
				pos="1311.4,10682",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
8,9,10,11] vs [12,13,14,15]",
				xlp="991.44,10632"];
			layer_8_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 8\nMulti-Head Attention\n(All Devices)",
				pos="1311.4,10520",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
8-15",
				xlp="983.44,10570"];
			layer_8_qkv -> layer_8_attn	[pos="e,1311.4,10547 1311.4,10655 1311.4,10629 1311.4,10587 1311.4,10558"];
			layer_8_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 8\nAttention Output\n(Row Parallel)",
				pos="1311.4,10360",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,10,\
11] vs [12,13,14,15]",
				xlp="1028.4,10408"];
			layer_8_attn -> layer_8_attn_out	[pos="e,1311.4,10386 1311.4,10494 1311.4,10468 1311.4,10426 1311.4,10397"];
			layer_8_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 8\nResidual Add",
				pos="1311.4,10187",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="983.4,10248"];
			layer_8_attn_out -> layer_8_residual	[pos="e,1311.4,10225 1311.4,10333 1311.4,10307 1311.4,10267 1311.4,10235"];
		}
		subgraph cluster_mlp_8 {
			graph [bb="1150.4,9474,1472.4,10072",
				color=red,
				label="Layer 8 - MLP Block",
				lheight=0.21,
				lp="1311.4,10060",
				lwidth=2.01,
				style="rounded,dashed"
			];
			layer_8_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 8\nMLP Gate\n(Column Parallel)",
				pos="1311.4,10014",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="1015.9,9965.5"];
			layer_8_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 8\nMLP Up\n(Column Parallel)",
				pos="1311.4,9853.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="1015.9,9902.5"];
			layer_8_gate -> layer_8_up	[pos="e,1311.4,9880.2 1311.4,9988 1311.4,9961.6 1311.4,9920.3 1311.4,9890.6"];
			layer_8_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 8\nMLP Down\n(Row Parallel)",
				pos="1311.4,9692.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="1033.9,9741.5"];
			layer_8_up -> layer_8_down	[pos="e,1311.4,9719.2 1311.4,9827 1311.4,9800.6 1311.4,9759.3 1311.4,9729.6"];
			layer_8_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 8\nMLP Residual Add",
				pos="1311.4,9520",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="944,9580.5"];
			layer_8_down -> layer_8_mlp_residual	[pos="e,1311.4,9558.1 1311.4,9666 1311.4,9640.3 1311.4,9600 1311.4,9568.4"];
		}
		subgraph cluster_mha_9 {
			graph [bb="824.44,10141,1068.4,10739",
				color=green,
				label="Layer 9 - MHA Block",
				lheight=0.21,
				lp="946.44,10728",
				lwidth=2.06,
				style="rounded,dashed"
			];
			layer_9_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 9\nQKV Projection\n(Column Parallel)",
				pos="946.44,10682",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
8,9,10,11] vs [12,13,14,15]",
				xlp="626.44,10730"];
			layer_9_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 9\nMulti-Head Attention\n(All Devices)",
				pos="946.44,10520",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
8-15",
				xlp="618.44,10472"];
			layer_9_qkv -> layer_9_attn	[pos="e,946.44,10547 946.44,10655 946.44,10629 946.44,10587 946.44,10558"];
			layer_9_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 9\nAttention Output\n(Row Parallel)",
				pos="946.44,10360",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,10,\
11] vs [12,13,14,15]",
				xlp="663.44,10310"];
			layer_9_attn -> layer_9_attn_out	[pos="e,946.44,10386 946.44,10494 946.44,10468 946.44,10426 946.44,10397"];
			layer_9_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 9\nResidual Add",
				pos="946.44,10187",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="618.4,10126"];
			layer_9_attn_out -> layer_9_residual	[pos="e,946.44,10225 946.44,10333 946.44,10307 946.44,10267 946.44,10235"];
		}
		subgraph cluster_mlp_9 {
			graph [bb="785.44,9474,1107.4,10072",
				color=red,
				label="Layer 9 - MLP Block",
				lheight=0.21,
				lp="946.44,10060",
				lwidth=2.01,
				style="rounded,dashed"
			];
			layer_9_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 9\nMLP Gate\n(Column Parallel)",
				pos="946.44,10014",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="1241.9,10064"];
			layer_9_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 9\nMLP Up\n(Column Parallel)",
				pos="946.44,9853.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="650.94,9804.5"];
			layer_9_gate -> layer_9_up	[pos="e,946.44,9880.2 946.44,9988 946.44,9961.6 946.44,9920.3 946.44,9890.6"];
			layer_9_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 9\nMLP Down\n(Row Parallel)",
				pos="946.44,9692.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="668.94,9643.5"];
			layer_9_up -> layer_9_down	[pos="e,946.44,9719.2 946.44,9827 946.44,9800.6 946.44,9759.3 946.44,9729.6"];
			layer_9_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 9\nMLP Residual Add",
				pos="946.44,9520",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="579,9459.5"];
			layer_9_down -> layer_9_mlp_residual	[pos="e,946.44,9558.1 946.44,9666 946.44,9640.3 946.44,9600 946.44,9568.4"];
		}
		subgraph cluster_mha_10 {
			graph [bb="824.44,8807,1068.4,9405",
				color=green,
				label="Layer 10 - MHA Block",
				lheight=0.21,
				lp="946.44,9393.5",
				lwidth=2.18,
				style="rounded,dashed"
			];
			layer_10_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 10\nQKV Projection\n(Column Parallel)",
				pos="946.44,9347.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
8,9,10,11] vs [12,13,14,15]",
				xlp="626.44,9298.5"];
			layer_10_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 10\nMulti-Head Attention\n(All Devices)",
				pos="946.44,9186.5",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
8-15",
				xlp="618.44,9137.5"];
			layer_10_qkv -> layer_10_attn	[pos="e,946.44,9213.2 946.44,9321 946.44,9294.6 946.44,9253.3 946.44,9223.6"];
			layer_10_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 10\nAttention Output\n(Row Parallel)",
				pos="946.44,9025.5",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,10,\
11] vs [12,13,14,15]",
				xlp="663.44,8976.5"];
			layer_10_attn -> layer_10_attn_out	[pos="e,946.44,9052.2 946.44,9160 946.44,9133.6 946.44,9092.3 946.44,9062.6"];
			layer_10_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 10\nResidual Add",
				pos="946.44,8853",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="618.4,8792.5"];
			layer_10_attn_out -> layer_10_residual	[pos="e,946.44,8891.1 946.44,8999 946.44,8973.3 946.44,8933 946.44,8901.4"];
		}
		subgraph cluster_mlp_10 {
			graph [bb="785.44,8140,1107.4,8738",
				color=red,
				label="Layer 10 - MLP Block",
				lheight=0.21,
				lp="946.44,8726.5",
				lwidth=2.14,
				style="rounded,dashed"
			];
			layer_10_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 10\nMLP Gate\n(Column Parallel)",
				pos="946.44,8680.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="650.94,8631.5"];
			layer_10_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 10\nMLP Up\n(Column Parallel)",
				pos="946.44,8519.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="650.94,8470.5"];
			layer_10_gate -> layer_10_up	[pos="e,946.44,8546.2 946.44,8654 946.44,8627.6 946.44,8586.3 946.44,8556.6"];
			layer_10_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 10\nMLP Down\n(Row Parallel)",
				pos="946.44,8358.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="668.94,8309.5"];
			layer_10_up -> layer_10_down	[pos="e,946.44,8385.2 946.44,8493 946.44,8466.6 946.44,8425.3 946.44,8395.6"];
			layer_10_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 10\nMLP Residual Add",
				pos="946.44,8186",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="579,8125.5"];
			layer_10_down -> layer_10_mlp_residual	[pos="e,946.44,8224.1 946.44,8332 946.44,8306.3 946.44,8266 946.44,8234.4"];
		}
		subgraph cluster_mha_11 {
			graph [bb="824.44,7473,1068.4,8071",
				color=green,
				label="Layer 11 - MHA Block",
				lheight=0.21,
				lp="946.44,8059.5",
				lwidth=2.18,
				style="rounded,dashed"
			];
			layer_11_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 11\nQKV Projection\n(Column Parallel)",
				pos="946.44,8013.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
8,9,10,11] vs [12,13,14,15]",
				xlp="626.44,7964.5"];
			layer_11_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 11\nMulti-Head Attention\n(All Devices)",
				pos="946.44,7852.5",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
8-15",
				xlp="618.44,7803.5"];
			layer_11_qkv -> layer_11_attn	[pos="e,946.44,7879.2 946.44,7987 946.44,7960.6 946.44,7919.3 946.44,7889.6"];
			layer_11_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 11\nAttention Output\n(Row Parallel)",
				pos="946.44,7691.5",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,10,\
11] vs [12,13,14,15]",
				xlp="663.44,7642.5"];
			layer_11_attn -> layer_11_attn_out	[pos="e,946.44,7718.2 946.44,7826 946.44,7799.6 946.44,7758.3 946.44,7728.6"];
			layer_11_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 11\nResidual Add",
				pos="946.44,7519",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="618.4,7458.5"];
			layer_11_attn_out -> layer_11_residual	[pos="e,946.44,7557.1 946.44,7665 946.44,7639.3 946.44,7599 946.44,7567.4"];
		}
		subgraph cluster_mlp_11 {
			graph [bb="785.44,6806,1107.4,7404",
				color=red,
				label="Layer 11 - MLP Block",
				lheight=0.21,
				lp="946.44,7392.5",
				lwidth=2.14,
				style="rounded,dashed"
			];
			layer_11_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 11\nMLP Gate\n(Column Parallel)",
				pos="946.44,7346.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="650.94,7297.5"];
			layer_11_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 11\nMLP Up\n(Column Parallel)",
				pos="946.44,7185.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="650.94,7136.5"];
			layer_11_gate -> layer_11_up	[pos="e,946.44,7212.2 946.44,7320 946.44,7293.6 946.44,7252.3 946.44,7222.6"];
			layer_11_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 11\nMLP Down\n(Row Parallel)",
				pos="946.44,7024.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="668.94,6975.5"];
			layer_11_up -> layer_11_down	[pos="e,946.44,7051.2 946.44,7159 946.44,7132.6 946.44,7091.3 946.44,7061.6"];
			layer_11_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 11\nMLP Residual Add",
				pos="946.44,6852",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="579,6791.5"];
			layer_11_down -> layer_11_mlp_residual	[pos="e,946.44,6890.1 946.44,6998 946.44,6972.3 946.44,6932 946.44,6900.4"];
		}
		subgraph cluster_mha_12 {
			graph [bb="824.44,6139,1068.4,6737",
				color=green,
				label="Layer 12 - MHA Block",
				lheight=0.21,
				lp="946.44,6725.5",
				lwidth=2.18,
				style="rounded,dashed"
			];
			layer_12_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 12\nQKV Projection\n(Column Parallel)",
				pos="946.44,6679.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
8,9,10,11] vs [12,13,14,15]",
				xlp="626.44,6630.5"];
			layer_12_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 12\nMulti-Head Attention\n(All Devices)",
				pos="946.44,6518.5",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
8-15",
				xlp="618.44,6469.5"];
			layer_12_qkv -> layer_12_attn	[pos="e,946.44,6545.2 946.44,6653 946.44,6626.6 946.44,6585.3 946.44,6555.6"];
			layer_12_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 12\nAttention Output\n(Row Parallel)",
				pos="946.44,6357.5",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,10,\
11] vs [12,13,14,15]",
				xlp="663.44,6308.5"];
			layer_12_attn -> layer_12_attn_out	[pos="e,946.44,6384.2 946.44,6492 946.44,6465.6 946.44,6424.3 946.44,6394.6"];
			layer_12_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 12\nResidual Add",
				pos="946.44,6185",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="618.4,6124.5"];
			layer_12_attn_out -> layer_12_residual	[pos="e,946.44,6223.1 946.44,6331 946.44,6305.3 946.44,6265 946.44,6233.4"];
		}
		subgraph cluster_mlp_12 {
			graph [bb="785.44,5472,1107.4,6070",
				color=red,
				label="Layer 12 - MLP Block",
				lheight=0.21,
				lp="946.44,6058.5",
				lwidth=2.14,
				style="rounded,dashed"
			];
			layer_12_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 12\nMLP Gate\n(Column Parallel)",
				pos="946.44,6012.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="650.94,5963.5"];
			layer_12_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 12\nMLP Up\n(Column Parallel)",
				pos="946.44,5851.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="650.94,5802.5"];
			layer_12_gate -> layer_12_up	[pos="e,946.44,5878.2 946.44,5986 946.44,5959.6 946.44,5918.3 946.44,5888.6"];
			layer_12_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 12\nMLP Down\n(Row Parallel)",
				pos="946.44,5690.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="668.94,5641.5"];
			layer_12_up -> layer_12_down	[pos="e,946.44,5717.2 946.44,5825 946.44,5798.6 946.44,5757.3 946.44,5727.6"];
			layer_12_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 12\nMLP Residual Add",
				pos="946.44,5518",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="579,5457.5"];
			layer_12_down -> layer_12_mlp_residual	[pos="e,946.44,5556.1 946.44,5664 946.44,5638.3 946.44,5598 946.44,5566.4"];
		}
		subgraph cluster_mha_13 {
			graph [bb="824.44,4805,1068.4,5403",
				color=green,
				label="Layer 13 - MHA Block",
				lheight=0.21,
				lp="946.44,5391.5",
				lwidth=2.18,
				style="rounded,dashed"
			];
			layer_13_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 13\nQKV Projection\n(Column Parallel)",
				pos="946.44,5345.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
8,9,10,11] vs [12,13,14,15]",
				xlp="626.44,5296.5"];
			layer_13_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 13\nMulti-Head Attention\n(All Devices)",
				pos="946.44,5184.5",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
8-15",
				xlp="618.44,5135.5"];
			layer_13_qkv -> layer_13_attn	[pos="e,946.44,5211.2 946.44,5319 946.44,5292.6 946.44,5251.3 946.44,5221.6"];
			layer_13_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 13\nAttention Output\n(Row Parallel)",
				pos="946.44,5023.5",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,10,\
11] vs [12,13,14,15]",
				xlp="663.44,4974.5"];
			layer_13_attn -> layer_13_attn_out	[pos="e,946.44,5050.2 946.44,5158 946.44,5131.6 946.44,5090.3 946.44,5060.6"];
			layer_13_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 13\nResidual Add",
				pos="946.44,4851",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="618.4,4790.5"];
			layer_13_attn_out -> layer_13_residual	[pos="e,946.44,4889.1 946.44,4997 946.44,4971.3 946.44,4931 946.44,4899.4"];
		}
		subgraph cluster_mlp_13 {
			graph [bb="785.44,4138,1107.4,4736",
				color=red,
				label="Layer 13 - MLP Block",
				lheight=0.21,
				lp="946.44,4724.5",
				lwidth=2.14,
				style="rounded,dashed"
			];
			layer_13_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 13\nMLP Gate\n(Column Parallel)",
				pos="946.44,4678.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="650.94,4629.5"];
			layer_13_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 13\nMLP Up\n(Column Parallel)",
				pos="946.44,4517.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="650.94,4468.5"];
			layer_13_gate -> layer_13_up	[pos="e,946.44,4544.2 946.44,4652 946.44,4625.6 946.44,4584.3 946.44,4554.6"];
			layer_13_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 13\nMLP Down\n(Row Parallel)",
				pos="946.44,4356.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="668.94,4307.5"];
			layer_13_up -> layer_13_down	[pos="e,946.44,4383.2 946.44,4491 946.44,4464.6 946.44,4423.3 946.44,4393.6"];
			layer_13_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 13\nMLP Residual Add",
				pos="946.44,4184",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="579,4123.5"];
			layer_13_down -> layer_13_mlp_residual	[pos="e,946.44,4222.1 946.44,4330 946.44,4304.3 946.44,4264 946.44,4232.4"];
		}
		subgraph cluster_mha_14 {
			graph [bb="824.44,3471,1068.4,4069",
				color=green,
				label="Layer 14 - MHA Block",
				lheight=0.21,
				lp="946.44,4057.5",
				lwidth=2.18,
				style="rounded,dashed"
			];
			layer_14_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 14\nQKV Projection\n(Column Parallel)",
				pos="946.44,4011.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
8,9,10,11] vs [12,13,14,15]",
				xlp="626.44,3962.5"];
			layer_14_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 14\nMulti-Head Attention\n(All Devices)",
				pos="946.44,3850.5",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
8-15",
				xlp="618.44,3801.5"];
			layer_14_qkv -> layer_14_attn	[pos="e,946.44,3877.2 946.44,3985 946.44,3958.6 946.44,3917.3 946.44,3887.6"];
			layer_14_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 14\nAttention Output\n(Row Parallel)",
				pos="946.44,3689.5",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,10,\
11] vs [12,13,14,15]",
				xlp="663.44,3640.5"];
			layer_14_attn -> layer_14_attn_out	[pos="e,946.44,3716.2 946.44,3824 946.44,3797.6 946.44,3756.3 946.44,3726.6"];
			layer_14_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 14\nResidual Add",
				pos="946.44,3517",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="618.4,3456.5"];
			layer_14_attn_out -> layer_14_residual	[pos="e,946.44,3555.1 946.44,3663 946.44,3637.3 946.44,3597 946.44,3565.4"];
		}
		subgraph cluster_mlp_14 {
			graph [bb="785.44,2804,1107.4,3402",
				color=red,
				label="Layer 14 - MLP Block",
				lheight=0.21,
				lp="946.44,3390.5",
				lwidth=2.14,
				style="rounded,dashed"
			];
			layer_14_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 14\nMLP Gate\n(Column Parallel)",
				pos="946.44,3344.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="650.94,3295.5"];
			layer_14_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 14\nMLP Up\n(Column Parallel)",
				pos="946.44,3183.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="650.94,3134.5"];
			layer_14_gate -> layer_14_up	[pos="e,946.44,3210.2 946.44,3318 946.44,3291.6 946.44,3250.3 946.44,3220.6"];
			layer_14_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 14\nMLP Down\n(Row Parallel)",
				pos="946.44,3022.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="668.94,2973.5"];
			layer_14_up -> layer_14_down	[pos="e,946.44,3049.2 946.44,3157 946.44,3130.6 946.44,3089.3 946.44,3059.6"];
			layer_14_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 14\nMLP Residual Add",
				pos="946.44,2850",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="579,2789.5"];
			layer_14_down -> layer_14_mlp_residual	[pos="e,946.44,2888.1 946.44,2996 946.44,2970.3 946.44,2930 946.44,2898.4"];
		}
		subgraph cluster_mha_15 {
			graph [bb="824.44,2137,1068.4,2735",
				color=green,
				label="Layer 15 - MHA Block",
				lheight=0.21,
				lp="946.44,2723.5",
				lwidth=2.18,
				style="rounded,dashed"
			];
			layer_15_qkv	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 15\nQKV Projection\n(Column Parallel)",
				pos="946.44,2677.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nDevices: [\
8,9,10,11] vs [12,13,14,15]",
				xlp="626.44,2628.5"];
			layer_15_attn	[fillcolor=yellow,
				height=0.73611,
				label="Layer 15\nMulti-Head Attention\n(All Devices)",
				pos="946.44,2516.5",
				width=2.3333,
				xlabel="Input: [batch_size=128, seq_len=100000, heads=32, qkv_dim=128]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nAll GPUs: \
8-15",
				xlp="618.44,2467.5"];
			layer_15_qkv -> layer_15_attn	[pos="e,946.44,2543.2 946.44,2651 946.44,2624.6 946.44,2583.3 946.44,2553.6"];
			layer_15_attn_out	[fillcolor=lightgreen,
				height=0.73611,
				label="Layer 15\nAttention Output\n(Row Parallel)",
				pos="946.44,2355.5",
				width=1.9167,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,10,\
11] vs [12,13,14,15]",
				xlp="663.44,2306.5"];
			layer_15_attn -> layer_15_attn_out	[pos="e,946.44,2382.2 946.44,2490 946.44,2463.6 946.44,2422.3 946.44,2392.6"];
			layer_15_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 15\nResidual Add",
				pos="946.44,2183",
				shape=parallelogram,
				width=3.1679,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="618.4,2122.5"];
			layer_15_attn_out -> layer_15_residual	[pos="e,946.44,2221.1 946.44,2329 946.44,2303.3 946.44,2263 946.44,2231.4"];
		}
		subgraph cluster_mlp_15 {
			graph [bb="785.44,1470,1107.4,2068",
				color=red,
				label="Layer 15 - MLP Block",
				lheight=0.21,
				lp="946.44,2056.5",
				lwidth=2.14,
				style="rounded,dashed"
			];
			layer_15_gate	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 15\nMLP Gate\n(Column Parallel)",
				pos="946.44,2010.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="650.94,1961.5"];
			layer_15_up	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 15\nMLP Up\n(Column Parallel)",
				pos="946.44,1849.5",
				width=1.9444,
				xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="650.94,1800.5"];
			layer_15_gate -> layer_15_up	[pos="e,946.44,1876.2 946.44,1984 946.44,1957.6 946.44,1916.3 946.44,1886.6"];
			layer_15_down	[fillcolor=lightcoral,
				height=0.73611,
				label="Layer 15\nMLP Down\n(Row Parallel)",
				pos="946.44,1688.5",
				width=1.6111,
				xlabel="Input: [batch_size=128, seq_len=100000, mlp_hidden=8192]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]\nDevices: [8,9,\
10,11] vs [12,13,14,15]",
				xlp="668.94,1639.5"];
			layer_15_up -> layer_15_down	[pos="e,946.44,1715.2 946.44,1823 946.44,1796.6 946.44,1755.3 946.44,1725.6"];
			layer_15_mlp_residual	[fillcolor=lightgray,
				height=1.0556,
				label="Layer 15\nMLP Residual Add",
				pos="946.44,1516",
				shape=parallelogram,
				width=4.2622,
				xlabel="Input1: [batch_size=128, seq_len=100000, d_model=4096]\nInput2: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=\
128, seq_len=100000, d_model=4096]",
				xlp="579,1455.5"];
			layer_15_down -> layer_15_mlp_residual	[pos="e,946.44,1554.1 946.44,1662 946.44,1636.3 946.44,1596 946.44,1564.4"];
		}
		layer_8_residual -> layer_8_gate	[pos="e,1311.4,10041 1311.4,10149 1311.4,10120 1311.4,10080 1311.4,10052"];
		layer_9_residual -> layer_9_gate	[pos="e,946.44,10041 946.44,10149 946.44,10120 946.44,10080 946.44,10052"];
		layer_9_mlp_residual -> layer_10_qkv	[pos="e,946.44,9374.2 946.44,9481.6 946.44,9452.7 946.44,9413.1 946.44,9384.5"];
		layer_10_residual -> layer_10_gate	[pos="e,946.44,8707.2 946.44,8814.6 946.44,8785.7 946.44,8746.1 946.44,8717.5"];
		layer_10_mlp_residual -> layer_11_qkv	[pos="e,946.44,8040.2 946.44,8147.6 946.44,8118.7 946.44,8079.1 946.44,8050.5"];
		layer_11_residual -> layer_11_gate	[pos="e,946.44,7373.2 946.44,7480.6 946.44,7451.7 946.44,7412.1 946.44,7383.5"];
		layer_11_mlp_residual -> layer_12_qkv	[pos="e,946.44,6706.2 946.44,6813.6 946.44,6784.7 946.44,6745.1 946.44,6716.5"];
		layer_12_residual -> layer_12_gate	[pos="e,946.44,6039.2 946.44,6146.6 946.44,6117.7 946.44,6078.1 946.44,6049.5"];
		layer_12_mlp_residual -> layer_13_qkv	[pos="e,946.44,5372.2 946.44,5479.6 946.44,5450.7 946.44,5411.1 946.44,5382.5"];
		layer_13_residual -> layer_13_gate	[pos="e,946.44,4705.2 946.44,4812.6 946.44,4783.7 946.44,4744.1 946.44,4715.5"];
		layer_13_mlp_residual -> layer_14_qkv	[pos="e,946.44,4038.2 946.44,4145.6 946.44,4116.7 946.44,4077.1 946.44,4048.5"];
		layer_14_residual -> layer_14_gate	[pos="e,946.44,3371.2 946.44,3478.6 946.44,3449.7 946.44,3410.1 946.44,3381.5"];
		layer_14_mlp_residual -> layer_15_qkv	[pos="e,946.44,2704.2 946.44,2811.6 946.44,2782.7 946.44,2743.1 946.44,2714.5"];
		layer_15_residual -> layer_15_gate	[pos="e,946.44,2037.2 946.44,2144.6 946.44,2115.7 946.44,2076.1 946.44,2047.5"];
	}
	input	[fillcolor=lightblue,
		height=0.5,
		label="Input Embedding",
		pos="581.44,10843",
		shape=ellipse,
		width=2.5456,
		xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, d_model=4096]",
		xlp="275.8,10876"];
	input -> layer_0_qkv	[pos="e,581.44,10708 581.44,10825 581.44,10800 581.44,10752 581.44,10718"];
	pipeline_commulation	[height=0.5,
		pos="581.44,18",
		width=2.4028];
	layer_7_mlp_residual -> pipeline_commulation	[pos="e,581.44,36.158 581.44,143.83 581.44,114.11 581.44,73.125 581.44,46.349"];
	output	[fillcolor=lightblue,
		height=0.5,
		label="Output Layer",
		pos="946.44,1343.5",
		shape=ellipse,
		width=2.022,
		xlabel="Input: [batch_size=128, seq_len=100000, d_model=4096]\nOutput: [batch_size=128, seq_len=100000, vocab_size]",
		xlp="1081.6,1376.5"];
	layer_15_mlp_residual -> output	[pos="e,946.44,1361.6 946.44,1477.6 946.44,1445.6 946.44,1400.3 946.44,1371.7"];
	pipeline_communication	[fillcolor=orange,
		height=0.74639,
		label="Pipeline Communication\n(Micro-batch Transfer)",
		pos="1311.4,10843",
		shape=ellipse,
		width=3.7516,
		xlabel="Transfer micro-batches\nFrom Stage 0 to Stage 1\nDevices: 0-7  8-15",
		xlp="1088.4,10892"];
	pipeline_communication -> layer_8_qkv	[pos="e,1311.4,10708 1311.4,10816 1311.4,10789 1311.4,10748 1311.4,10719"];
}
