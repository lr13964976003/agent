// LLM Hybrid Parallelism Deployment DAG
digraph {
	dpi=300 rankdir=TB size="50,50"
	node [fontname=Arial fontsize=10]
	edge [fontname=Arial fontsize=8]
	node [fillcolor=lightblue shape=ellipse style=filled]
	node [fillcolor=lightgreen shape=rectangle style=filled]
	node [fillcolor=lightyellow shape=parallelogram style=filled]
	input [label="Input\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" fontname="Arial Bold" shape=plaintext]
	subgraph cluster_pipeline_stage_0 {
		fillcolor=lightgray label="Pipeline Stage 0 (Layers 0-3)" style="rounded,filled"
		subgraph cluster_expert_group_0 {
			fillcolor=lightcyan label="Expert Group 0 (GPU 0)" style="rounded,filled"
			layer0_expert0_route [label="Expert Routing L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert0_gate [label="Expert Gate L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, num_experts=8]" shape=parallelogram]
			layer0_expert0_a2a [label="All-to-All Comm L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert0_attn_split [label="Attention Split L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=parallelogram]
			layer0_expert0_q_proj [label="Q Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert0_k_proj [label="K Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert0_v_proj [label="V Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert0_attn [label="Attention Computation L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert0_attn_out [label="Attention Output L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert0_attn_ar [label="Attention All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert0_attn_res [label="Attention Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert0_ln1 [label="Layer Norm 1 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert0_mlp1 [label="MLP First Linear L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert0_gelu [label="GELU Activation L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert0_mlp2 [label="MLP Second Linear L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert0_mlp_ar [label="MLP All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert0_mlp_res [label="MLP Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert0_ln2 [label="Layer Norm 2 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
		}
		subgraph cluster_expert_group_1 {
			fillcolor=lightcyan label="Expert Group 1 (GPU 1)" style="rounded,filled"
			layer0_expert1_route [label="Expert Routing L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert1_gate [label="Expert Gate L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, num_experts=8]" shape=parallelogram]
			layer0_expert1_a2a [label="All-to-All Comm L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert1_attn_split [label="Attention Split L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=parallelogram]
			layer0_expert1_q_proj [label="Q Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert1_k_proj [label="K Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert1_v_proj [label="V Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert1_attn [label="Attention Computation L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert1_attn_out [label="Attention Output L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert1_attn_ar [label="Attention All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert1_attn_res [label="Attention Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert1_ln1 [label="Layer Norm 1 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert1_mlp1 [label="MLP First Linear L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert1_gelu [label="GELU Activation L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert1_mlp2 [label="MLP Second Linear L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert1_mlp_ar [label="MLP All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert1_mlp_res [label="MLP Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert1_ln2 [label="Layer Norm 2 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
		}
		subgraph cluster_expert_group_2 {
			fillcolor=lightcyan label="Expert Group 2 (GPU 2)" style="rounded,filled"
			layer0_expert2_route [label="Expert Routing L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert2_gate [label="Expert Gate L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, num_experts=8]" shape=parallelogram]
			layer0_expert2_a2a [label="All-to-All Comm L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert2_attn_split [label="Attention Split L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=parallelogram]
			layer0_expert2_q_proj [label="Q Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert2_k_proj [label="K Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert2_v_proj [label="V Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert2_attn [label="Attention Computation L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert2_attn_out [label="Attention Output L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert2_attn_ar [label="Attention All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert2_attn_res [label="Attention Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert2_ln1 [label="Layer Norm 1 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert2_mlp1 [label="MLP First Linear L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert2_gelu [label="GELU Activation L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert2_mlp2 [label="MLP Second Linear L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert2_mlp_ar [label="MLP All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert2_mlp_res [label="MLP Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert2_ln2 [label="Layer Norm 2 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
		}
		subgraph cluster_expert_group_3 {
			fillcolor=lightcyan label="Expert Group 3 (GPU 3)" style="rounded,filled"
			layer0_expert3_route [label="Expert Routing L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert3_gate [label="Expert Gate L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, num_experts=8]" shape=parallelogram]
			layer0_expert3_a2a [label="All-to-All Comm L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert3_attn_split [label="Attention Split L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=parallelogram]
			layer0_expert3_q_proj [label="Q Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert3_k_proj [label="K Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert3_v_proj [label="V Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert3_attn [label="Attention Computation L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert3_attn_out [label="Attention Output L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert3_attn_ar [label="Attention All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert3_attn_res [label="Attention Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert3_ln1 [label="Layer Norm 1 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert3_mlp1 [label="MLP First Linear L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert3_gelu [label="GELU Activation L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert3_mlp2 [label="MLP Second Linear L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert3_mlp_ar [label="MLP All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert3_mlp_res [label="MLP Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert3_ln2 [label="Layer Norm 2 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
		}
		subgraph cluster_expert_group_4 {
			fillcolor=lightcyan label="Expert Group 4 (GPU 4)" style="rounded,filled"
			layer0_expert4_route [label="Expert Routing L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert4_gate [label="Expert Gate L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, num_experts=8]" shape=parallelogram]
			layer0_expert4_a2a [label="All-to-All Comm L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert4_attn_split [label="Attention Split L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=parallelogram]
			layer0_expert4_q_proj [label="Q Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert4_k_proj [label="K Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert4_v_proj [label="V Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert4_attn [label="Attention Computation L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert4_attn_out [label="Attention Output L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert4_attn_ar [label="Attention All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert4_attn_res [label="Attention Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert4_ln1 [label="Layer Norm 1 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert4_mlp1 [label="MLP First Linear L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert4_gelu [label="GELU Activation L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert4_mlp2 [label="MLP Second Linear L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert4_mlp_ar [label="MLP All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert4_mlp_res [label="MLP Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert4_ln2 [label="Layer Norm 2 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
		}
		subgraph cluster_expert_group_5 {
			fillcolor=lightcyan label="Expert Group 5 (GPU 5)" style="rounded,filled"
			layer0_expert5_route [label="Expert Routing L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert5_gate [label="Expert Gate L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, num_experts=8]" shape=parallelogram]
			layer0_expert5_a2a [label="All-to-All Comm L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert5_attn_split [label="Attention Split L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=parallelogram]
			layer0_expert5_q_proj [label="Q Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert5_k_proj [label="K Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert5_v_proj [label="V Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert5_attn [label="Attention Computation L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert5_attn_out [label="Attention Output L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert5_attn_ar [label="Attention All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert5_attn_res [label="Attention Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert5_ln1 [label="Layer Norm 1 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert5_mlp1 [label="MLP First Linear L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert5_gelu [label="GELU Activation L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert5_mlp2 [label="MLP Second Linear L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert5_mlp_ar [label="MLP All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert5_mlp_res [label="MLP Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert5_ln2 [label="Layer Norm 2 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
		}
		subgraph cluster_expert_group_6 {
			fillcolor=lightcyan label="Expert Group 6 (GPU 6)" style="rounded,filled"
			layer0_expert6_route [label="Expert Routing L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert6_gate [label="Expert Gate L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, num_experts=8]" shape=parallelogram]
			layer0_expert6_a2a [label="All-to-All Comm L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert6_attn_split [label="Attention Split L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=parallelogram]
			layer0_expert6_q_proj [label="Q Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert6_k_proj [label="K Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert6_v_proj [label="V Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert6_attn [label="Attention Computation L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert6_attn_out [label="Attention Output L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert6_attn_ar [label="Attention All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert6_attn_res [label="Attention Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert6_ln1 [label="Layer Norm 1 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert6_mlp1 [label="MLP First Linear L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert6_gelu [label="GELU Activation L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert6_mlp2 [label="MLP Second Linear L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert6_mlp_ar [label="MLP All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert6_mlp_res [label="MLP Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert6_ln2 [label="Layer Norm 2 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
		}
		subgraph cluster_expert_group_7 {
			fillcolor=lightcyan label="Expert Group 7 (GPU 7)" style="rounded,filled"
			layer0_expert7_route [label="Expert Routing L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert7_gate [label="Expert Gate L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, num_experts=8]" shape=parallelogram]
			layer0_expert7_a2a [label="All-to-All Comm L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert7_attn_split [label="Attention Split L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=parallelogram]
			layer0_expert7_q_proj [label="Q Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert7_k_proj [label="K Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert7_v_proj [label="V Projection L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert7_attn [label="Attention Computation L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, heads=8, d_k=64]" shape=rectangle]
			layer0_expert7_attn_out [label="Attention Output L0\nInput: [batch_size=128, seq_len=1024, heads=8, d_k=64]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert7_attn_ar [label="Attention All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert7_attn_res [label="Attention Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert7_ln1 [label="Layer Norm 1 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert7_mlp1 [label="MLP First Linear L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert7_gelu [label="GELU Activation L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, ffn_hidden=1024]" shape=rectangle]
			layer0_expert7_mlp2 [label="MLP Second Linear L0\nInput: [batch_size=128, seq_len=1024, ffn_hidden=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
			layer0_expert7_mlp_ar [label="MLP All-Reduce L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=ellipse]
			layer0_expert7_mlp_res [label="MLP Residual L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024], [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=parallelogram]
			layer0_expert7_ln2 [label="Layer Norm 2 L0\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" shape=rectangle]
		}
	}
	input -> layer0_expert0_route
	layer0_expert0_route -> layer0_expert0_gate
	layer0_expert0_gate -> layer0_expert0_a2a
	layer0_expert0_a2a -> layer0_expert0_attn_split
	layer0_expert0_attn_split -> layer0_expert0_q_proj
	layer0_expert0_attn_split -> layer0_expert0_k_proj
	layer0_expert0_attn_split -> layer0_expert0_v_proj
	layer0_expert0_q_proj -> layer0_expert0_attn
	layer0_expert0_k_proj -> layer0_expert0_attn
	layer0_expert0_v_proj -> layer0_expert0_attn
	layer0_expert0_attn -> layer0_expert0_attn_out
	layer0_expert0_attn_out -> layer0_expert0_attn_ar
	layer0_expert0_attn_ar -> layer0_expert0_attn_res
	layer0_expert0_attn_res -> layer0_expert0_ln1
	layer0_expert0_ln1 -> layer0_expert0_mlp1
	layer0_expert0_mlp1 -> layer0_expert0_gelu
	layer0_expert0_gelu -> layer0_expert0_mlp2
	layer0_expert0_mlp2 -> layer0_expert0_mlp_ar
	layer0_expert0_mlp_ar -> layer0_expert0_mlp_res
	layer0_expert0_mlp_res -> layer0_expert0_ln2
	pipeline_send_0_1 [label="Pipeline Send Stage0â†’1\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=orange shape=ellipse]
	pipeline_recv_1_0 [label="Pipeline Recv Stage0â†’1\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=orange shape=ellipse]
	layer0_expert0_ln2 -> pipeline_send_0_1
	pipeline_send_0_1 -> pipeline_recv_1_0
	output [label="Output\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" fontname="Arial Bold" shape=plaintext]
	pipeline_recv_1_0 -> output
}
