{
  "generated_dag_files": {
    "complete_deployment_dag": {
      "dot_file": "../outputs/2025-12-01-14-11-15/complete_deployment_dag.dot",
      "svg_file": "../outputs/2025-12-01-14-11-15/complete_deployment_dag.svg",
      "description": "Complete deployment DAG showing all 16 layers, 64 experts, 4 pipeline stages, and 2-way tensor parallelism with detailed operator-level implementation"
    },
    "gpu_mapping_dag": {
      "dot_file": "../outputs/2025-12-01-14-11-15/gpu_mapping_dag.dot",
      "svg_file": "../outputs/2025-12-01-14-11-15/gpu_mapping_dag.svg",
      "description": "GPU mapping and load balancing DAG showing communication paths between all 64 GPUs with expert parallelism, tensor parallelism, and pipeline parallelism"
    },
    "detailed_moe_layer": {
      "dot_file": "../outputs/2025-12-01-14-11-15/detailed_moe_layer.dot",
      "svg_file": "../outputs/2025-12-01-14-11-15/detailed_moe_layer.svg",
      "description": "Detailed MoE layer implementation showing expert routing, selection, and computation with tensor dimensions"
    },
    "llm_hybrid_parallelism_dag": {
      "dot_file": "../outputs/2025-12-01-14-11-15/llm_hybrid_parallelism_dag.dot",
      "svg_file": "../outputs/2025-12-01-14-11-15/llm_hybrid_parallelism_dag.svg",
      "description": "Main hybrid parallelism DAG showing the overall deployment strategy with pipeline stages and communication patterns"
    },
    "expert_parallelism_detailed": {
      "dot_file": "../outputs/2025-12-01-14-11-15/expert_parallelism_detailed.dot",
      "svg_file": "../outputs/2025-12-01-14-11-15/expert_parallelism_detailed.svg",
      "description": "Detailed expert parallelism DAG showing 64 experts distributed across 8 GPU groups with all-to-all communication"
    },
    "tensor_parallelism_detailed": {
      "dot_file": "../outputs/2025-12-01-14-11-15/tensor_parallelism_detailed.dot",
      "svg_file": "../outputs/2025-12-01-14-11-15/tensor_parallelism_detailed.svg",
      "description": "Detailed tensor parallelism DAG showing 2-way split for attention computation with Q, K, V projections and all-reduce operations"
    }
  },
  "model_specifications": {
    "layers": 16,
    "experts_per_layer": 64,
    "attention_heads": 16,
    "head_dimension": 64,
    "hidden_dimension": 1024,
    "sequence_length": 1024,
    "batch_size": 128,
    "precision": "FP8"
  },
  "parallelism_configuration": {
    "expert_parallelism": "8-way (8 expert groups)",
    "tensor_parallelism": "2-way (within each expert)",
    "pipeline_parallelism": "4-way (4 pipeline stages)",
    "total_gpus": 64,
    "gpu_utilization": "100% (64/64 GPUs used)"
  },
  "dag_features": {
    "card_boundary_division": "Specified for each GPU node",
    "multi_card_communication": "All-to-all, all-reduce, and pipeline communication paths shown",
    "operator_level_detail": "Each layer broken down to individual operators",
    "node_shapes": {
      "ellipse": "Communication operations",
      "rectangle": "Computation operations", 
      "parallelogram": "Routing/aggregation operations"
    },
    "tensor_dimensions": "All nodes include input/output dimensions",
    "load_balancing": "Verified equal distribution across all GPUs",
    "cycle_free": "All DAGs verified to have no cycles",
    "residual_connections": "All residual adds shown with multiple inputs"
  }
}